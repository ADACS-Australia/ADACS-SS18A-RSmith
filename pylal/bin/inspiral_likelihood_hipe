#!/usr/bin/python
"""
This script generates the condor DAG that represents the workflow for likelihood calculation.
Currently the calculation consist of two main step
1) running COIRE on all individual THINCA files containing coicident inspiral events
2) running inspiral_likelihood jobs for each individual COIRE file.
"""
__author__ = 'Ruslan Vaulin <vaulin@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import sys, os
from optparse import *
import ConfigParser
import re
import tempfile
sys.path.append('@PYTHONLIBDIR@')

##############################################################################
# import the modules we need to build the pipeline
from glue import pipeline
from glue import lal
from lalapps import inspiral
import lalapps.inspiralutils
from pylal import CoincInspiralUtils
from pylal import SnglInspiralUtils
from glue.ligolw import lsctables
import glue.segments
##############################################################################
class LikelihoodJob(inspiral.InspiralAnalysisJob):
  """
  A likelihood job.The static options are read from the section
  [plotthinca] in the ini file.  The stdout and stderr from the job
  are directed to the logs directory.  The path to the executable is
  determined from the ini file.
  """
  def __init__(self, cp, dax=False):
	"""
	cp = ConfigParser object from which options are read.
	"""
	exec_name = "likelihood"
	sections = ["likelihood"]
	inspiral.InspiralAnalysisJob.__init__(self, cp, sections, exec_name, dax)
	self.add_condor_cmd('getenv', 'True')
	
	
class LikelihoodNode(inspiral.InspiralAnalysisNode):
  """
  A LikelihoodNode runs an instance of the inspiral_likelihood code in a Condor DAG.
  """
  def __init__(self,job):
    """
    job = A CondorDAGJob that can run an instance of plotthinca.
    """
    inspiral.InspiralAnalysisNode.__init__(self,job)



# function to add likelihood nodes to the dag
def add_likelihood_nodes(coire_files, likelihood_job, dag, Noop_Node = None):
  """
  Add a likelihood node for each of the file in the coire_files list.
  
  coire_files = list of coire files cantaing triggers for which likelihood will be calculated
  likelihood_job = likelihood job to use for calculation
  dag = name of the dag
  Noop_Node = noop node, when given, may be used to set parent-child relation
  """
  likelihood_files = []
  for file in coire_files:
	likelihood = LikelihoodNode(likelihood_job)
	# set the options:
	# set the glob file for candidate events
	likelihood.add_var_opt("candidate-events-glob", file)
	
	# set output file
	# strip the path
	filename = file.split("/")[-1]
	output_file = re.sub("COIRE", "LIKELIHOOD", filename)
	likelihood.add_var_opt("output-file", output_file)
	# add noop node as a parent if it is given
	if Noop_Node:
	  likelihood.add_parent(Noop_Node)
	  
	dag.add_node(likelihood)
	likelihood_files.append(output_file)
  return likelihood_files


def combine_files(FileCache, max_size, output_directory):
  """ Combines smaller THINCA or COIRE files into larger ones using ligolw_add"""
    
  new_files_cache = lal.Cache()
  description = FileCache[0].description
  ifo_times = FileCache[0].observatory
  start_time = FileCache[0].segment[0]
  size = os.stat(FileCache[0].path()).st_size
  duration = abs(FileCache[0].segment)
  command_line = FileCache[0].path() + " "
  number_files = 0
  for entry in FileCache[1:]:
	number_files += 1
	if (size + os.stat(entry.path()).st_size) > max_size:
	  #print size, number_files
	  new_file_name = ifo_times + "-" + description + "-" + str(start_time) + "-" + str(duration) +".xml"
	  output = output_directory + "/" + new_file_name  
	  command_line = "ligolw_add " + command_line + "--output=" + output + " --verbose"
	  lalapps.inspiralutils.make_external_call(command_line, show_stdout=True)	
	  new_files_cache.append(lal.Cache.from_urls([output])[0])
	  
	  size = 0.0
	  start_time = entry.segment[0]
	  command_line = ""
	  duration = 0.0
	#print size, start_time, duration
	command_line += entry.path() + " "
	size += os.stat(entry.path()).st_size
	duration += abs(entry.segment)
	#print size, start_time, duration 
	
  new_file_name = ifo_times + "-" + description + "-" + str(start_time) + "-" + str(duration) +".xml"
  output = output_directory + "/" + new_file_name  
  command_line = "ligolw_add " + command_line + "--output=" + output + " --verbose"
  lalapps.inspiralutils.make_external_call(command_line, show_stdout=True)	
  new_files_cache.append(lal.Cache.from_urls([output])[0])
  return new_files_cache

def combine_injections_files(FoundInjCache, MissedInjCache, max_size, output_directory):
  """ Combines smaller THINCA or COIRE files with found and missed injections into larger ones using ligolw_add"""
  
  if not len(FoundInjCache) == len(MissedInjCache):
	print >> sys.stderr, "Number of found injections files does not match the number of missed injections files"
	sys.exit(1)
	
  new_files_cache = lal.Cache()
  found_description = FoundInjCache[0].description
  missed_description = MissedInjCache[0].description
  ifo_times = FoundInjCache[0].observatory
  start_time = FoundInjCache[0].segment[0]
  size = os.stat(FoundInjCache[0].path()).st_size
  duration = abs(FoundInjCache[0].segment)
  command_line = FoundInjCache[0].path() + " "
  command_line_missed = MissedInjCache[0].path() + " "
  number_files = 0
  for found_entry, missed_entry in zip(FoundInjCache[1:], MissedInjCache[1:]):
	number_files += 1
	if (size + os.stat(found_entry.path()).st_size) > max_size:
	  #print size, number_files
	  new_file_name = ifo_times + "-" +found_description + "-" + str(start_time) + "-" + str(duration) +".xml"
	  output = output_directory + "/" + new_file_name  
	  command_line = "ligolw_add " + command_line + "--output=" + output + " --verbose"
	  lalapps.inspiralutils.make_external_call(command_line, show_stdout=True)	
	  new_files_cache.append(lal.Cache.from_urls([output])[0])
	  
	  new_file_name_missed = ifo_times + "-" + missed_description + "-" + str(start_time) + "-" + str(duration) +".xml"
	  output = output_directory + "/" + new_file_name_missed  
	  command_line_missed = "ligolw_add " + command_line_missed + "--output=" + output + " --verbose"
	  lalapps.inspiralutils.make_external_call(command_line_missed, show_stdout=True)	
	  new_files_cache.append(lal.Cache.from_urls([output])[0])

	  
	  
	  size = 0.0
	  start_time = found_entry.segment[0]
	  command_line = ""
	  command_line_missed = ""
	  duration = 0.0
	#print size, start_time, duration
	command_line += found_entry.path() + " "
	command_line_missed += missed_entry.path() + " "
	size += os.stat(found_entry.path()).st_size
	duration += abs(found_entry.segment)
	#print size, start_time, duration 
	
  new_file_name = ifo_times + "-" +found_description + "-" + str(start_time) + "-" + str(duration) +".xml"
  output = output_directory + "/" + new_file_name  
  command_line = "ligolw_add " + command_line + "--output=" + output + " --verbose"
  lalapps.inspiralutils.make_external_call(command_line, show_stdout=True)	
  new_files_cache.append(lal.Cache.from_urls([output])[0])

  new_file_name_missed = ifo_times + "-" + missed_description + "-" + str(start_time) + "-" + str(duration) +".xml"
  output = output_directory + "/" + new_file_name_missed  
  command_line_missed = "ligolw_add " + command_line_missed + "--output=" + output + " --verbose"
  lalapps.inspiralutils.make_external_call(command_line_missed, show_stdout=True)	
  new_files_cache.append(lal.Cache.from_urls([output])[0])
  return new_files_cache

def split_sngl_inspiral_files(cache, max_coinc_number, output_directory):
  """ 
  Splits files containing single inspiral table into smaller files
  """  
  files = cache.pfnlist()
  statistic = CoincInspiralUtils.coincStatistic("effective_snr")
  Triggers = SnglInspiralUtils.ReadSnglInspiralFromFiles(files, mangle_event_id=False)
  CoincTriggers = CoincInspiralUtils.coincInspiralTable(Triggers, statistic)
  ifo_times = cache[0].observatory
  files_description =cache[0].description
  new_files_cache = lal.Cache()
  cum_coinc_number = max_coinc_number
  while cum_coinc_number <= len(CoincTriggers):
	#print cum_coinc_number
	tmp_table = lsctables.New(lsctables.SnglInspiralTable)
	for i in range(max_coinc_number):
	  coinc = CoincTriggers[cum_coinc_number - max_coinc_number + i]
	  ifos,ifolist = coinc.get_ifos()
	  for ifo in ifolist:
		tmp_table.append(getattr(coinc, ifo))
	  if i == 0:
		start_time = getattr(coinc, ifolist[0]).end_time
	  if i == (max_coinc_number - 1):
		end_time = getattr(coinc, ifolist[0]).end_time
		duration = end_time - start_time
	  
	new_file_name = ifo_times + "-" + files_description + "-" + str(start_time) + "-" + str(abs(duration)) +".xml"
	output = output_directory + "/" + new_file_name
	new_files_cache.append(lal.Cache.from_urls([output])[0])
	new_file = open(output, "w")
	# Fixing the sngl inspiral table
	#for row in tmp_table:
	  #row.chisq_dof = 0
	  #row.kappa = 0.0
	  #row.chi = 0.0
	  #row.bank_chisq = 0.0
	  #row.bank_chisq_dof = 0
	  #row.cont_chisq = 0.0
	  #row.cont_chisq_dof = 0
	tmp_table.write(new_file)
	new_file.close()
	cum_coinc_number += max_coinc_number
  else:
	tmp_table = lsctables.New(lsctables.SnglInspiralTable)
	for i in range(len(CoincTriggers) - cum_coinc_number + max_coinc_number):
	  coinc = CoincTriggers[cum_coinc_number - max_coinc_number + i]
	  ifos,ifolist = coinc.get_ifos()
	  for ifo in ifolist:
		tmp_table.append(getattr(coinc, ifo))
	  if i == 0:
		start_time = getattr(coinc, ifolist[0]).end_time
	  if i == range(len(CoincTriggers) - cum_coinc_number + max_coinc_number)[-1]:
		end_time = getattr(coinc, ifolist[0]).end_time
		duration = end_time - start_time
	  
	new_file_name = ifo_times + "-" + files_description + "-" + str(start_time) + "-" + str(abs(duration)) + ".xml"
	output = output_directory + "/" + new_file_name
	new_files_cache.append(lal.Cache.from_urls([output])[0])
	new_file = open(output, "w")
	  # Fixing the sngl inspiral table
	#for row in tmp_table:
	  #row.chisq_dof = 0
	  #row.kappa = 0.0
	  #row.chi = 0.0
	  #row.bank_chisq = 0.0
	  #row.bank_chisq_dof = 0
	  #row.cont_chisq = 0.0
	  #row.cont_chisq_dof = 0
	tmp_table.write(new_file)
	new_file.close()
  return new_files_cache
  
# Convert ifo list to string
def combo2str(combo):
  ifo_list = ""
  for ifo in combo:
    ifo_list += ifo
  return ifo_list

##############################################################################
#
#  MAIN PROGRAM
#
##############################################################################
usage = """usage: %prog [options] """
parser = OptionParser( usage=usage, version="%prog CVS $Id$ " )

parser.add_option("", "--config-file",action="store",type="string",\
    metavar=" FILE",help="use configuration file FILE")

parser.add_option("", "--log_path",action="store",type="string",\
    metavar="LOGPATH", help="path for condor log")
	
parser.add_option("", "--user-tag",action="store",type="string",\
    metavar="USERTAG",help="user tag")
	
parser.add_option("", "--write-script",action="store_true",default=False, help="write dag generation script")

parser.add_option("", "--exact-match", action="store_true", default=False,\
      help="add time slides to the plots" )
	  
parser.add_option("", "--gps-start-time",action="store",type="int",\
    metavar=" GPS_START", help="begin analysis at GPS_START")

parser.add_option("", "--gps-end-time",action="store",type="int",\
    metavar=" GPS_END", help="end analysis at GPS_END")
	
parser.add_option("", "--combine-foreground-background-files",action="store_true",default=False,\
    help="combine input files into optimal size files for background/foreground distributions.")
	
parser.add_option("", "--split-files",action="store_true",default=False,\
    help="split input files into optimal size files for likelihood calculation.")	
	
(opts,args) = parser.parse_args()
	


##############################################################################
# create the config parser object and read in the ini file
cp = ConfigParser.ConfigParser()
cp.read(opts.config_file)

###############################################################################

##############################################################################
# create a log file that the Condor jobs will write to
basename = re.sub(r'\.ini',r'', opts.config_file)
if opts.user_tag:
  (handle,logfile) = tempfile.mkstemp(".dag.log", basename + "." + opts.user_tag + ".", opts.log_path)
else:
  (handle,logfile) = tempfile.mkstemp(".dag.log", basename + ".", opts.log_path)
fh = open(logfile, "w" )
fh.close()

##############################################################################
# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
if opts.user_tag:
  dag.set_dag_file(basename + '.' + opts.user_tag )
else:
  dag.set_dag_file(basename)

# set better submit file names than the default
if opts.user_tag:
  subsuffix = '.' + opts.user_tag + '.sub'
else:
  subsuffix = '.sub'
##################################################################################
# create (if it does not exist) logs directory, 
#which is the default directory for .err and .out files of the jobs in the DAG
try: os.mkdir("logs")
except: pass
    
##################################################################################
# read in  THINCA or COIRE files
if opts.gps_start_time and opts.gps_end_time :
  time_interval = glue.segments.segment(opts.gps_start_time, opts.gps_end_time)
else:
  time_interval = None
cache_file=cp.get("input", "cache-file")
all_cache = lal.Cache.fromfile(open(cache_file))
zerolag_pattern = cp.get("input", "zerolag-pattern")
zerolag_files = all_cache.sieve(description = zerolag_pattern, segment=time_interval, exact_match=opts.exact_match).checkfilesexist()[0].pfnlist()

##############################################################################
# read in time slides (background) and injections (foreground) THINCA or COIRE files
time_slides_pattern = cp.get("input", "time-slides-pattern")
time_slides_files = all_cache.sieve(description = time_slides_pattern, segment=time_interval, exact_match=opts.exact_match).checkfilesexist()[0].pfnlist()
found_injections_pattern = cp.get("input", "found-injections-pattern")
found_injections_files =  all_cache.sieve(description = found_injections_pattern, segment=time_interval, exact_match=opts.exact_match).checkfilesexist()[0].pfnlist()

##################################################################################
# set up likelihood jobs
##################################################################################

numslides = cp.get("input", "num-slides")

# zero lag
likelihood_job = LikelihoodJob(cp)
likelihood_job.add_opt("num-slides", numslides) 
likelihood_job.set_sub_file(basename + ".likelihood" +subsuffix)

# time slides
likelihood_slide_job = LikelihoodJob(cp)
likelihood_slide_job.add_opt("num-slides", numslides) 
likelihood_slide_job.set_sub_file(basename + ".likelihood_slide" +subsuffix)

# injections
likelihood_inj_job = LikelihoodJob(cp)
likelihood_inj_job.add_opt("num-slides", numslides) 
likelihood_inj_job.set_sub_file(basename + ".likelihood_inj" +subsuffix)


# seting up options related to foreground and background files;  
# combine time slides and injections files into larger files if needed.
if opts.combine_foreground_background_files:
  foreground_background_dir=cp.get("combine-foreground-background-files", "foreground-background-dir")
  lalapps.inspiralutils.mkdir(foreground_background_dir)
  foreground_cache_file_name = foreground_background_dir + "/" + cp.get("combine-foreground-background-files", "foreground-cache-file-name")
  background_cache_file_name = foreground_background_dir + "/" + cp.get("combine-foreground-background-files", "background-cache-file-name")
  max_size = cp.getint("combine-foreground-background-files", "max-size")
  IFOS=cp.get("input", "IFOS").split(",")
  ifo_combos=CoincInspiralUtils.get_ifo_combos(IFOS)
  background_cache = lal.Cache()
  foreground_cache = lal.Cache()
  bf_background_pattern = time_slides_pattern
  bf_found_injections_pattern = found_injections_pattern
  bf_missed_injections_pattern = found_injections_pattern.replace("FOUND", "MISSED")
  bf_cache = all_cache

  for ifo_combo in ifo_combos:
    ifo_times = combo2str(ifo_combo)
    ifo_tag = ifo_times
    print "Combining files from " + str(ifo_times) +" times"
    tmp_time_slides_cache = bf_cache.sieve(description = bf_background_pattern, segment=time_interval, exact_match=opts.exact_match).sieve(ifos = ifo_times, exact_match=True).checkfilesexist()[0]
    tmp_found_injections_cache =  bf_cache.sieve(description = bf_found_injections_pattern, segment=time_interval, exact_match=opts.exact_match).sieve(ifos = ifo_times, exact_match=True).checkfilesexist()[0]
    tmp_missed_injections_cache =  bf_cache.sieve(description = bf_missed_injections_pattern, segment=time_interval, exact_match=opts.exact_match).sieve(ifos = ifo_times, exact_match=True).checkfilesexist()[0]
    print "time slides ..." 
    combined_slides_cache = combine_files(tmp_time_slides_cache, max_size, foreground_background_dir)
    print "done."
    print "injections ..."
    combined_injections_cache = combine_injections_files(tmp_found_injections_cache, tmp_missed_injections_cache, max_size, foreground_background_dir)
    print "done." 
    background_cache.extend(combined_slides_cache)
    foreground_cache.extend(combined_injections_cache)
  print "Finished combining files, writing the output cache files ..."	
  foreground_cache.tofile(open(foreground_cache_file_name, "w"))
  background_cache.tofile(open(background_cache_file_name, "w"))
  # if foregorund/backgorund parameters are set in the corresponding section of ini file
  # use them instead.
  if cp.get("background-foreground", "bf-background-pattern"):
    bf_background_pattern = cp.get("background-foreground", "bf-background-pattern")
  if cp.get("background-foreground", "bf-found-injections-pattern"):
    bf_found_injections_pattern = cp.get("background-foreground", "bf-found-injections-pattern")
  if cp.get("background-foreground", "bf-missed-injections-pattern"):
    bf_missed_injections_pattern = cp.get("background-foreground", "bf-missed-injections-pattern")
  if cp.get("background-foreground", "foreground-cache-file-name"):
    foreground_cache_file_name = cp.get("background-foreground", "foreground-cache-file-name")
  if cp.get("background-foreground", "background-cache-file-name"):
    background_cache_file_name = cp.get("background-foreground", "background-cache-file-name")
else:  
  if cp.get("background-foreground", "bf-background-pattern"):
    bf_background_pattern = cp.get("background-foreground", "bf-background-pattern")
  else:
    print >> sys.stderr, "bf-background-pattern is not set in " + opts.config_file + " ini file"
    sys.exit(1)
  if cp.get("background-foreground", "bf-found-injections-pattern"):
    bf_found_injections_pattern = cp.get("background-foreground", "bf-found-injections-pattern")
  else: 
    print >> sys.stderr, "bf-found-injections-pattern is not set in " + opts.config_file + " ini file"
    sys.exit(1)
  if cp.get("background-foreground", "bf-missed-injections-pattern"):
    bf_missed_injections_pattern = cp.get("background-foreground", "bf-missed-injections-pattern")
  else: 
    print >> sys.stderr, "bf-missedinjections-pattern is not set in " + opts.config_file + " ini file"
    sys.exit(1)
  if cp.get("background-foreground", "foreground-cache-file-name"):
    foreground_cache_file_name = cp.get("background-foreground", "foreground-cache-file-name")
  else:
    print >> sys.stderr, "foreground-cache-file-name is not set in " + opts.config_file + " ini file"
    sys.exit(1)
  if cp.get("background-foreground", "background-cache-file-name"):
    background_cache_file_name = cp.get("background-foreground", "background-cache-file-name")
  else:
    print >> sys.stderr, " background-cache-file-name is not set in " + opts.config_file + " ini files"
    sys.exit(1)
  
# setting options related to foreground-background distributions for zero lag likelihood jobs
likelihood_job.add_opt("foreground-cache-files", foreground_cache_file_name)
likelihood_job.add_opt("background-cache-files", background_cache_file_name)  
likelihood_job.add_opt("background-pattern", bf_background_pattern)
likelihood_job.add_opt("found-injections-pattern", bf_found_injections_pattern)
likelihood_job.add_opt("missed-injections-pattern", bf_missed_injections_pattern)
  
#  setting options  related to foreground-background distributions for time slides likelihood jobs
likelihood_slide_job.add_opt("foreground-cache-files", foreground_cache_file_name)
likelihood_slide_job.add_opt("background-cache-files", background_cache_file_name)  
likelihood_slide_job.add_opt("background-pattern", bf_background_pattern)
likelihood_slide_job.add_opt("found-injections-pattern", bf_found_injections_pattern)
likelihood_slide_job.add_opt("missed-injections-pattern", bf_missed_injections_pattern)

  
#  setting options  related to foreground-background distributions for injections likelihood jobs
likelihood_inj_job.add_opt("foreground-cache-files", foreground_cache_file_name)
likelihood_inj_job.add_opt("background-cache-files", background_cache_file_name)  
likelihood_inj_job.add_opt("background-pattern", bf_background_pattern)
likelihood_inj_job.add_opt("found-injections-pattern", bf_found_injections_pattern)
likelihood_inj_job.add_opt("missed-injections-pattern", bf_missed_injections_pattern)


#prepare files that contain events for which likelihood is to be calculated by splitting them
# into smaller files if neccessary.
if opts.split_files:
  split_files_dir = cp.get("split-files", "split-files-dir")
  lalapps.inspiralutils.mkdir(split_files_dir)
  max_coinc = cp.getint("split-files", "max-coinc")
  split_files_cache_name = split_files_dir + "/" + cp.get("split-files", "output-cache-name")
  IFOS=cp.get("input", "IFOS").split(",")
  ifo_combos=CoincInspiralUtils.get_ifo_combos(IFOS)
  split_zerolag_pattern = zerolag_pattern  
  split_time_slides_pattern = time_slides_pattern
  split_found_injections_pattern = found_injections_pattern
  split_cache = all_cache
  split_zerolag_cache = lal.Cache()
  split_timeslides_cache = lal.Cache()
  split_injections_cache = lal.Cache()
  for ifo_combo in ifo_combos:
    ifo_times = combo2str(ifo_combo)
    ifo_tag = ifo_times
    print "splitting up files from " + ifo_times + " times ..."
    tmp_zerolag_cache = split_cache.sieve(description = split_zerolag_pattern, segment=time_interval, exact_match=opts.exact_match).sieve(ifos = ifo_times, exact_match=True).checkfilesexist()[0]
    tmp_time_slides_cache = split_cache.sieve(description = split_time_slides_pattern, segment=time_interval, exact_match=opts.exact_match).sieve(ifos = ifo_times, exact_match=True).checkfilesexist()[0]
    tmp_found_injections_cache =  split_cache.sieve(description = split_found_injections_pattern, segment=time_interval, exact_match=opts.exact_match).sieve(ifos = ifo_times, exact_match=True).checkfilesexist()[0]
    print "zerolag ..."
    split_zerolag_cache.extend(split_sngl_inspiral_files(tmp_zerolag_cache, max_coinc, split_files_dir))
    print "done."
    print "time slides ..."
    split_timeslides_cache.extend(split_sngl_inspiral_files(tmp_time_slides_cache, max_coinc, split_files_dir))    
    print "done."
    print "injections ..."
    split_injections_cache.extend(split_sngl_inspiral_files(tmp_found_injections_cache, max_coinc, split_files_dir))
    print "done."
  print "Finished splitting files, writing the output cache file ..."
  split_cache_all = lal.Cache()	
  split_cache_all.extend(split_zerolag_cache)
  split_cache_all.extend(split_timeslides_cache)
  split_cache_all.extend(split_injections_cache)
  split_cache_all.tofile(open(split_files_cache_name, "w"))
  zerolag_files = split_zerolag_cache.pfnlist()
  time_slides_files = split_timeslides_cache.pfnlist()
  found_injections_files = split_injections_cache.pfnlist() 

# adding likelihood jobs to dag
# zero lag
zerolag_likelihood_files = add_likelihood_nodes(zerolag_files, likelihood_job, dag)

# time slides 
background_likelihood_files = add_likelihood_nodes(time_slides_files, likelihood_slide_job, dag)

# injections
found_injections_likelihood_files = add_likelihood_nodes(found_injections_files, likelihood_inj_job, dag)

  
# write likelihood files to the output cache file
current_dir = os.getcwd()
output_likelihood_cache = lal.Cache()

# adding zerolag likelihood files to cache
for file in zerolag_likelihood_files:
  file = current_dir + "/" + file
  output_likelihood_cache.append(lal.Cache.from_urls([file])[0])
  
# adding background (time slides) likelihood files to cache
for file in background_likelihood_files:
  file = current_dir + "/" + file
  output_likelihood_cache.append(lal.Cache.from_urls([file])[0])
  
# adding found injections likelihood files to cache
for file in found_injections_likelihood_files:
  file = current_dir + "/" + file
  output_likelihood_cache.append(lal.Cache.from_urls([file])[0]) 
  
output_cache_name = basename + ".cache"
output_cache_file = open(output_cache_name, "w")
output_likelihood_cache.tofile(output_cache_file)
output_cache_file.close()

##############################################################################
#Write out the DAG, help message and log file
dag.set_nodes_finalized()
dag.write_sub_files()
dag.write_dag()

if opts.write_script:
  dag.write_script()

##############################################################################  
# write a message telling the user that the DAG has been written

print "\nCreated a DAG file which can be submitted by executing"
print "\n   condor_submit_dag", dag.get_dag_file()
print """\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n"""



  	
	
   


