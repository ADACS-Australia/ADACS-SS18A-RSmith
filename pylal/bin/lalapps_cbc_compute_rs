#!/usr/bin/python

#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#

description = \
"""
Compiles rate statistics about an ihope run and creates a rate_statistics table
and a file_rank table. The rate_statistics gives the average trigger rate for the
specified file type per ifo. The file_rank table ranks the files by trigger rate
gives the average trigger rate and average snr for each file.
"""
import os, sys, time, datetime
import numpy
from optparse import OptionParser

from glue import lal
from glue import git_version
from glue.ligolw import ligolw
from glue.ligolw import table
from glue.ligolw import utils
from glue.ligolw import lsctables
from glue.ligolw import ilwd
from glue.ligolw.utils import process

from pylal.xlal.date import XLALGPSToUTC
from pylal.xlal.datatypes.ligotimegps import LIGOTimeGPS

__prog__ = "lalapps_cbc_compute_rs"
__author__ = "Collin Capano <cdcapano@physics.syr.edu>"

# =============================================================================
#
#                                   Set Options
#
# =============================================================================


def parse_command_line():
    """
    Parse the command line, return options and check for consistency among the
    options.
    """
    parser = OptionParser(
        version = git_version.verbose_msg,
        usage   = "%prog [--file-type] [options] ihope.cache1 ihope.cache2 ...",
        description = description )

    parser.add_option( "-t", "--file-type", action = "store", type = "string",
        default = None,                  
        help = 
            "Required. The type of files to compute rates for; e.g., 'INSPIRAL_FIRST_FULL_DATA'. " +
            "Note: This is an exact-match sieve. For example, THINCA_SECOND*FULL_DATA will only match " +
            "THICNA_SECOND*FULL_DATA files; THINCA_SECOND*FULL_DATA* will match THINCA_SECOND*FULL_DATA, " +
            "THINCA_SECOND*FULL_DATA_CAT_2, etc."
            )
    parser.add_option("-o", "--output", action = "store", type = "string",
        default = None,
        help =
            "File to save results to. If not specified, results will be printed to stdout."
            )
    parser.add_option("-d", "--daily-ihope-pages-location", action = "store",
        default = "https://ldas-jobs.ligo.caltech.edu/~cbc/ihope_daily",
        help =
            "Web address of the daily ihope pages. " +
            "Default is https://ldas-jobs.ligo.caltech.edu/~cbc/ihope_daily"
            )
    parser.add_option("-F", "--force", action = "store_true", default = False,
        help =
            "Compute statistics for the run even if some of the files specified in the cache " +
            "file(s) are missing. (Default action is to raise an error.)"
            )
    parser.add_option("-v", "--verbose", action = "store_true", default = False,
        help =
            "Be verbose."
            )

    (options, filenames) = parser.parse_args()

    if not options.file_type:
        raise ValueError, "--file-type is a required option"

    return options, filenames

# =============================================================================
#
#                       Function Definitions
#
# =============================================================================

#
#   Some helper functions for manipulating times
#
def get_dst_start_end(ifo, year):
    """
    Figures out what dates daylight savings time starts and ends at a given site on a given year.
    """
    # in the United Stats, prior to 2007, DST began on the first Sunday in April
    # and ended on the last Sunday in October (http://aa.usno.navy.mil/faq/docs/daylight_time.php)
    if ("H" in ifo  or "L" in ifo) and year < 2007:
        for ii in range(1,28):
            dst_start = datetime.datetime(year, 4, ii, 2, 0, 0)
            if dst_start.strftime('%A') == 'Sunday':
                break
        for ii in range(31,0,-1):
            dst_end = datetime.datetime(year, 10, ii, 2, 0, 0)
            if dst_end.strftime('%A') == 'Sunday':
                break
    # in the US, starting in 2007, DST begins on the second Sunday in March and ends on the first
    # Sunday in November
    elif ("H" in ifo  or "L" in ifo) and year >= 2007:
        nn = 1
        for ii in range(1,31):
            dst_start = datetime.datetime(year, 3, ii, 2, 0, 0)
            if dst_start.strftime('%A') == 'Sunday' and nn == 2:
                break
            elif dst_start.strftime('%A') == 'Sunday':
                nn += 1
        for ii in range(1,28):
            dst_end = datetime.datetime(year, 11, ii, 2, 0, 0)
            if dst_end.strftime('%A') == 'Sunday':
                break
    # in Europe, DST begins on the last Sunday of March and ends on the last Sunday of October
    # source: http://www.timeanddate.com/news/time/europe-dst-starts-march-29-2009.html
    elif ("V" in ifo or "G" in ifo):
        for ii in range(31,0,-1):
            dst_start = datetime.datetime(year, 3, ii, 2, 0, 0)
            if dst_start.strftime('%A') == 'Sunday':
                break
        for ii in range(31,0,-1):
            dst_end = datetime.datetime(year, 10, ii, 2, 0, 0)
            if dst_end.strftime('%A') == 'Sunday':
                break
    else:
        raise ValueError, "unrecognized ifo %s" % ifo
    
    return dst_start, dst_end
        

def get_sitelocaltime_from_gps(ifo, gpstime):
    # get the utc time in datetime.datetime format
    utctime = XLALGPSToUTC(LIGOTimeGPS(gpstime, 0))
    utctime = datetime.datetime(utctime[0],utctime[1],utctime[2],utctime[3],utctime[4],utctime[5],utctime[6])
    # figure out if daylight savings time was on or not
    dst_start, dst_end = get_dst_start_end(ifo, utctime.year)
    # figure out the appropriate time offset
    if "H" in ifo:
        toffset = datetime.timedelta(hours=-7)
    elif "L" in ifo:
        toffset = datetime.timedelta(hours=-5)
    elif ("V" in ifo or "G" in ifo):
        toffset = datetime.timedelta(hours=+2)
    # apply the dst time offset to see if daylight savings was on; if not, adjust the toffset
    if not (utctime + toffset >= dst_start and utctime + toffset < dst_end):
        toffset = toffset + datetime.timedelta(hours=-1)

    return utctime + toffset

#
#   Tables for storing statistics about the files
#
class RateStatisticsTable(table.Table):
    tableName = "rate_statistics:table"
    validcolumns = {
        "ifo":"lstring",
        "file_type":"lstring",
        "average_rate":"real_8",
        "max_rate":"real_8",
        "min_rate":"real_8",
        "median_rate":"real_8",
        "standard_deviation":"real_8"
        }   
        
class RateStatistics(object):
    __slots__ = RateStatisticsTable.validcolumns.keys()

    def get_pyvalue(self):
        if self.value is None:
            return None
        return ligolwtypes.ToPyType[self.type or "lstring"](self.value)

class FileRankTable(table.Table):
    tableName = "file_rank:table"
    validcolumns = {
        "ifo":"lstring",
        "rank":"int_4u",
        "file_type":"lstring",
        "file_location":"lstring",
        "file_name":"lstring",
        "out_start_time":"int_4u",
        "out_start_time_utc":"lstring",
        "segment_duration":"int_4u",
        "num_triggers":"int_4u",
        "trigger_rate":"real_8",
        "average_snr":"real_8",
        "max_snr":"real_8",
        "min_snr":"real_8",
        "median_snr":"real_8",
        "snr_standard_deviation":"real_8",
        "daily_ihope_page":"lstring",
        "elog_page":"lstring"
        }

class FileRank(object):
    __slots__ = FileRankTable.validcolumns.keys()

    def set_elog_page(self):
        # set site_address
        if "H" in self.ifo:
            site_address = "http://ilog.ligo-wa.caltech.edu/ilog/pub/ilog.cgi?group=detector"
        elif "L" in self.ifo:
            site_address = "http://ilog.ligo-la.caltech.edu/ilog/pub/ilog.cgi?group=detector"
        elif "V" in self.ifo:
            #FIXME: What's the site address and format for Virgo log book?
            site_address = "http://www.youtube.com/watch?v=oHg5SJYRHA0"
        # get local time at the site
        site_localtime = get_sitelocaltime_from_gps(self.ifo, self.out_start_time)
        # set the address
        if "H" in self.ifo or "L" in self.ifo:
            site_address = "%s&date_to_view=%s" % ( site_address, site_localtime.strftime("%m/%d/%Y") )
        self.elog_page = '<a href="%s">link</a>' % site_address

    def get_pyvalue(self):
        if self.value is None:
            return None
        return ligolwtypes.ToPyType[self.type or "lstring"](self.value)

# connect rows to the tables
RateStatisticsTable.RowType = RateStatistics
FileRankTable.RowType = FileRank

#
#   Classes for gathering statistics about the files
#
class FileList:
    def __init__(self):
        self.files = []
        self.avg_rate = {}
        self.max_rate = {}
        self.min_rate = {}
        self.med_rate = {}
        self.std_rate = {}

    def append_file(self, filestat_obj):
        self.files.append(filestat_obj)
    
    def get_distinct_ifos(self):
        return set([ifo for file in self.files for ifo in file.ifos])

    def compute_rate_stats(self):
        rates = {}
        for file in self.files:
            for ifo in file.ifos:
                if ifo not in rates:
                    rates[ifo] = []
                rates[ifo].append( file.rate[ifo] )
        for ifo, rates_list in rates.items():
            rates_list = numpy.array(rates_list)
            if rates_list != numpy.array([]):
                self.avg_rate[ifo] = numpy.mean(rates_list)
                self.max_rate[ifo] = numpy.max(rates_list)
                self.min_rate[ifo] = numpy.min(rates_list)
                self.med_rate[ifo] = numpy.median(rates_list)
                self.std_rate[ifo] = numpy.std(rates_list)
            else:
                self.avg_rate[ifo] = self.max_rate[ifo] = self.min_rate[ifo] = self.med_rate[ifo] = self.std_rate[ifo] = 0. 
        del rates

    def get_glitchiest_files(self, num_files = 10):
        rate_index = [ (file.rate, file) for file in self.files ]
        rate_index.sort(reverse=True)
        glitchiest = [ rate_index[ii][1] for ii in range(min(num_files, len(rate_index))) ] 
        quietest = [ rate_index[-(ii+1)][1] for ii in range(min(num_files, len(rate_index))) ]
        del rate_index

        return glitchiest, quietest


class FileStats:
    def __init__(self, filename):
        if not os.path.isfile(filename):
            raise ValueError, "missing file %s" % filename
        self.name = filename
        self.xmldoc = None
        self.search_summ_table = None
        self.sngl_insp_table = None
        self.out_list = None
        self.seg_start = None
        self.seg_end = None
        self.duration = None
        self.ifos = None
        self.num_trigs = {}
        self.rate = {}
        self.avg_snr = {} 
        self.max_snr = {}
        self.min_snr = {}
        self.median_snr = {}
        self.std_snr = {}
        self.daily_ihope_page = {}

    def load_file(self):
        self.xmldoc = utils.load_filename( self.name, gz = self.name.endswith(".gz") )
        self.search_summ_table = table.get_table(self.xmldoc, lsctables.SearchSummaryTable.tableName)
        try:
            self.sngl_insp_table = table.get_table(self.xmldoc, lsctables.SnglInspiralTable.tableName)
        except ValueError:
            self.sngl_insp_table = lsctables.New(lsctables.SnglInspiralTable) 

    def close_file(self):
        del self.sngl_insp_table
        self.sngl_insp_table = None
        del self.search_summ_table
        self.search_summ_table = None
        del self.xmldoc
        self.xmldoc = None

    def get_ifos(self):
        self.ifos = set([ ifo for row in self.search_summ_table for ifo in row.get_ifos() ])

    def get_stats(self):
        self.get_ifos()
        snrs = dict([ [ifo, []] for ifo in self.ifos ]) 
        for row in self.sngl_insp_table:
            snrs[row.ifo].append( row.snr )
        for ifo, snr_vals in snrs.items():
            self.num_trigs[ifo] = len(snr_vals)
            if self.num_trigs[ifo] != 0:
                snr_vals = numpy.array(snr_vals)
                self.avg_snr[ifo] = numpy.mean(snr_vals)
                self.max_snr[ifo] = numpy.max(snr_vals)
                self.min_snr[ifo] = numpy.min(snr_vals)
                self.median_snr[ifo] = numpy.median(snr_vals)
                self.std_snr[ifo] = numpy.std(snr_vals)
            else:
                self.avg_snr[ifo] = self.max_snr[ifo] = self.min_snr[ifo] = self.median_snr[ifo] = self.std_snr[ifo] = 0.
        del snrs

    def get_seg_times(self):
        self.out_list = self.search_summ_table.get_outlist()
        if len(self.out_list) > 1:
            raise ValueError, "more than one in start/end time for file %s" % self.name
        self.seg_start, self.seg_end = self.out_list[0][0].seconds, self.out_list[0][1].seconds

    def set_daily_ihope_page(self, pages_location = "https://ldas-jobs.ligo.caltech.edu/~cbc/ihope_daily"):
        utctime = XLALGPSToUTC(LIGOTimeGPS(self.seg_start, 0))
        self.daily_ihope_page = "%s/%s/%s/" %(pages_location, time.strftime("%Y%m", utctime), time.strftime("%Y%m%d", utctime))

    def compute_rate(self):
        self.get_stats()
        self.get_seg_times()
        self.duration = self.seg_end - self.seg_start
        if self.duration != 0:
            self.rate = dict([ [ifo, float(self.num_trigs[ifo])/self.duration] for ifo in self.ifos ])

# =============================================================================
#
#                                     Main
#
# =============================================================================
    
opts, cachefiles = parse_command_line() 

if opts.verbose:
    print >> sys.stderr, "Reading cache file(s)..."
selected_cache = lal.Cache().fromfilenames( cachefiles ).sieve( description = opts.file_type, exact_match = True )

if opts.verbose:
    print >> sys.stderr, "Gathering statistics..."
    num_files = float(len(selected_cache))

filelist = FileList()
for n, entry in enumerate(selected_cache):
    if opts.verbose:
        print >> sys.stderr, "\tfile %i / %i\r" % (n+1, num_files),
    if opts.force and not os.path.isfile(entry.path()):
        print >> sys.stderr, "\nWarning: File %s not found. Skipping..." % entry.path()
        continue
    inspfile = FileStats( entry.path() )
    inspfile.load_file()
    inspfile.compute_rate()
    #if opts.glitchiest_times or opts.quietest_times:
        #inspfile.get_stats()
    inspfile.set_daily_ihope_page(pages_location = opts.daily_ihope_pages_location)
    inspfile.close_file()
    filelist.append_file(inspfile)

filelist.compute_rate_stats()
if opts.verbose:
    print >> sys.stderr, ""

#
#   Save results to tables
#

ratedoc = ligolw.Document()
# setup the LIGOLW tag
ratedoc.appendChild(ligolw.LIGO_LW())
# add this program's metadata
proc_id = process.register_to_xmldoc(ratedoc, __prog__, opts.__dict__)
# add the tables
rate_table = lsctables.New(RateStatisticsTable)
rank_table = lsctables.New(FileRankTable)
# connect the tables to the document
ratedoc.childNodes[0].appendChild(rate_table)
ratedoc.childNodes[0].appendChild(rank_table)

for ifo in sorted(filelist.get_distinct_ifos()):
    rate_row = RateStatistics()
    rate_row.ifo = ifo
    rate_row.file_type = opts.file_type
    rate_row.average_rate = filelist.avg_rate[ifo]
    rate_row.max_rate = filelist.max_rate[ifo]
    rate_row.min_rate = filelist.min_rate[ifo]
    rate_row.median_rate = filelist.med_rate[ifo]
    rate_row.standard_deviation = filelist.std_rate[ifo]
    rate_table.append(rate_row)
    selected_files = [file for file in filelist.files if ifo in file.ifos]
    selected_files.sort( key=lambda file: file.rate[ifo], reverse=True )
    for n, file in enumerate(selected_files):
        rank_row = FileRank()
        rank_row.ifo = ifo
        rank_row.rank = n + 1
        rank_row.file_type = opts.file_type
        rank_row.file_location = os.path.dirname(file.name)
        rank_row.file_name = os.path.basename(file.name)
        rank_row.out_start_time = file.seg_start
        rank_row.out_start_time_utc = time.strftime("%a %d %b %Y %H:%M:%S", XLALGPSToUTC(LIGOTimeGPS(file.seg_start, 0)))
        rank_row.segment_duration = file.duration
        rank_row.num_triggers = file.num_trigs[ifo]
        rank_row.trigger_rate = file.rate[ifo]
        rank_row.average_snr = file.avg_snr[ifo]
        rank_row.max_snr = file.max_snr[ifo]
        rank_row.min_snr = file.min_snr[ifo]
        rank_row.median_snr = file.median_snr[ifo]
        rank_row.snr_standard_deviation = file.std_snr[ifo]
        rank_row.daily_ihope_page = ''.join(['<a href ="', file.daily_ihope_page, '">link</a>'])
        rank_row.set_elog_page()
        rank_table.append(rank_row)

# save the results
utils.write_filename( ratedoc, opts.output, xsl_file = "ligolw.xsl")

sys.exit(0)

