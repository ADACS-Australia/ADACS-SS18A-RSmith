#!/usr/bin/python

import scipy
from scipy import interpolate
import numpy
from math import *

try:
  import sqlite3
except ImportError:
  # pre 2.5.x
  from pysqlite2 import dbapi2 as sqlite3

import os
import sys
import copy
from optparse import OptionParser

from glue import segments
from glue.ligolw import ligolw
from glue.ligolw import lsctables
from glue.ligolw import dbtables
from glue.ligolw import utils
from glue.ligolw import table
from glue import segmentsUtils
from glue import lal

from pylal import db_thinca_rings
from pylal import llwapp
from pylal import rate
from pylal import SimInspiralUtils
from pylal.xlal.datatypes.ligotimegps import LIGOTimeGPS

from pylal import git_version
__author__ = "Chad Hanna <channa@ligo.caltech.edu>"
__version__ = "git id %s" % git_version.id
__date__ = git_version.date

lsctables.LIGOTimeGPS = LIGOTimeGPS


class upper_limit(object):
  """
  The upper_limit class organizes the calculation of the sensitive search volume
  for a search described by the input database.
  """
  def __init__(self, database, opts):
    ## Instance variables ######################################
    self.fname = database
    self.opts = opts
    self.twoDMassBins = None  #m1-m2 rectangles into which we subdivide the upper limit calculation
    self.gw = None
    self.wnfunc = None
    self.bootnum = int(opts.bootstrap_iterations)
    self.minmass = None
    self.maxmass = None
    self.mintotal = None
    self.maxtotal = None
    self.segments = {}  #An ifo-keys segments dict for single ifo
    self.zero_lag_segments = {}  #An instruments-keyed segment dict for coincident ifo time
    self.coinc_inspiral_table = None
    self.sngl_sngl_coinc_def_id = None
    ############################################################

    if opts.verbose: print >> sys.stdout, "Gathering stats from: %s...." % (self.fname,)

    # seed the RNG to make output predictably random
    if opts.bootstrap_seed is not None:
      scipy.random.seed(seed=opts.bootstrap_seed)

    # open a connection to the input database
    working_filename = dbtables.get_connection_filename(self.fname, tmp_path=opts.tmp_space, verbose = opts.verbose)
    connection = sqlite3.connect(working_filename)
    dbtables.DBTable_set_connection(connection)
    xmldoc = dbtables.get_xml(connection)

    # extract useful info from db
    self.get_instruments(connection)  #find out which instruments were on at all
    self.get_segments(connection)  #get single ifo segments with vetos applied
    self.get_zero_lag_segments()  #get coincident ifo segments
    self.get_livetime(connection)  #get the livetime for each set of instruments
    self.get_mass_ranges(connection)  #determine the mass ranges and binning for this calculation
    self.get_2d_mass_bins(self.minmass, self.maxmass, opts.mass_bins) # set the m1-m2 binning
    self.get_far_thresholds(connection)
    self.get_gps_times_duration(connection)

    # close db connection
    connection.commit()
    dbtables.discard_connection_filename(self.fname, working_filename, verbose = opts.verbose)
    dbtables.DBTable_set_connection(None)


  def get_instruments(self, connection):
    '''Retrieve the sets of instruments which were on during this search.'''
    # FIXME bad to hardcode search_coinc_type
    self.sngl_sngl_coinc_def_id, = connection.cursor().execute("SELECT coinc_def_id FROM coinc_definer WHERE search == 'inspiral' AND search_coinc_type == 0;").fetchone()
    instruments_query = """
    SELECT DISTINCT(instruments) FROM coinc_event WHERE coinc_def_id == ?;
    """

    if self.opts.verbose:
      print >>sys.stdout,"\nQuerying database for on instruments..."
      print >>sys.stdout,"\t" + instruments_query.replace('?',self.sngl_sngl_coinc_def_id)

    self.instruments = []
    for instruments in connection.cursor().execute(instruments_query, (self.sngl_sngl_coinc_def_id,) ):
      inst =  frozenset(lsctables.instrument_set_from_ifos(instruments[0]))
      self.instruments.append(inst)
      if self.opts.verbose: print >>sys.stdout,"\t%s" % instruments[0]

    return self.instruments


  def get_segments(self, connection):
    '''Retrieve single-IFO segments from the database and apply vetoes to these segments. '''
    self.segments = db_thinca_rings.get_thinca_zero_lag_segments(connection, program_name = self.opts.live_time_program)  #Get raw zero lag segments
    self.segments -= self.get_veto_segments(connection) #Apply veto segments
    return self.segments


  def get_veto_segments(self, connection):
    if self.opts.veto_segments_name is not None:
      try: #FIXME super hack
        veto_segments = db_thinca_rings.get_veto_segments(connection, self.opts.veto_segments_name)
      except AttributeError:
        # will get an AttributeError if using newer format veto segment file because
        # the new format does not include _ns; if so, remove the _ns columns from the
        # segment table and reset the definitions of lsctables.Segment.get and lsctables.Segment.set
        from glue.lal import LIGOTimeGPS

        del lsctables.SegmentTable.validcolumns['start_time_ns']
        del lsctables.SegmentTable.validcolumns['end_time_ns']

        def get_segment(self):
            return segments.segment(LIGOTimeGPS(self.start_time, 0), LIGOTimeGPS(self.end_time, 0))
        def set_segment(self, segment):
            self.start_time = segment[0].seconds
            self.end_time = segment[1].seconds

        lsctables.Segment.get = get_segment
        lsctables.Segment.set = set_segment
        veto_segments = db_thinca_rings.get_veto_segments(connection, self.opts.veto_segments_name)
    else:
      veto_segments = segments.segmentlistdict()

    return veto_segments


  def get_zero_lag_segments(self):
    '''Compute multi-ifo (coincident) segment list from single ifo segments.'''
    for i in self.instruments:
      self.zero_lag_segments[i] = self.segments.intersection(i) - self.segments.union(set(self.segments.keys()) - i)
    return self.zero_lag_segments


  def set_instruments_to_calculate(self):
    if not opts.instruments: return self.instruments
    if opts.instruments in self.instruments:
      return frozenset(lsctables.instrument_set_from_ifos(i[0]))
    else:
      print >> sys.stderr, "Instruments %s do not exist in DB, nothing will be calculated" % (str(frozenset(lsctables.instrument_set_from_ifos(i[0]))),)
      return []


  def get_gps_times_duration(self, connection):
    self.start_time = int( connection.cursor().execute('SELECT MIN(gps_start_time) FROM experiment').fetchone()[0] )
    self.end_time = int( connection.cursor().execute('SELECT MAX(gps_end_time) FROM experiment').fetchone()[0] )
    return True


  def get_mass_ranges(self, connection):
    #FIXME assumes certain inspinj command lines were run.  probably a good idea but I don't know
    minmass1 = float(connection.cursor().execute('SELECT MIN(CAST(value as REAL)) FROM process_params JOIN process on process_params.process_id = process.process_id WHERE process.program = "inspinj" AND param == "--min-mass1"').fetchone()[0])
    minmass2 = float(connection.cursor().execute('SELECT MIN(CAST(value as REAL)) FROM process_params JOIN process on process_params.process_id = process.process_id WHERE process.program = "inspinj" AND param == "--min-mass2"').fetchone()[0])
    maxmass1 = float(connection.cursor().execute('SELECT MAX(CAST(value as REAL)) FROM process_params JOIN process on process_params.process_id = process.process_id WHERE process.program = "inspinj" AND param == "--max-mass1"').fetchone()[0])
    maxmass2 = float(connection.cursor().execute('SELECT MAX(CAST(value as REAL)) FROM process_params JOIN process on process_params.process_id = process.process_id WHERE process.program = "inspinj" AND param == "--max-mass2"').fetchone()[0])
    self.mintotal = float(connection.cursor().execute('SELECT MIN(CAST(value as REAL)) FROM process_params JOIN process on process_params.process_id = process.process_id WHERE process.program = "inspinj" AND param == "--min-mtotal"').fetchone()[0])
    self.maxtotal = float(connection.cursor().execute('SELECT MAX(CAST(value as REAL)) FROM process_params JOIN process on process_params.process_id = process.process_id WHERE process.program = "inspinj" AND param == "--max-mtotal"').fetchone()[0])

    self.minmass = min(minmass1,minmass2)
    self.maxmass = max(maxmass1,maxmass2)

    return


  def get_livetime(self,connection):
    livetime_query = """
    SELECT instruments,duration
    FROM experiment_summary
    JOIN experiment ON experiment_summary.experiment_id == experiment.experiment_id
    WHERE experiment_summary.datatype == "exclude_play";
    """
    if self.opts.verbose:
      print >>sys.stdout,"\nQuerying database for livetimes..."
      print >>sys.stdout,"\t" + livetime_query

    self.livetime = dict( (inst,0) for inst in self.instruments )
    for instruments,livetime in connection.cursor().execute(livetime_query):
      inst =  frozenset(lsctables.instrument_set_from_ifos(instruments))
      self.livetime[inst] = livetime
      if self.opts.verbose:
        print >>sys.stdout,"\t%s were on for %g seconds (excludes playground)" % (instruments,livetime)

    return self.livetime


  def get_far_thresholds(self, connection):
    """Returns the false alarm rate to use for computing the search volume (in the typical case, this will
    be the FAR of the most rare zero-lag coinc)."""
    if self.opts.verbose: print >>sys.stdout, "\nGetting FAR thresholds for the loudest event..."

    self.far = dict( (inst,-1) for inst in self.instruments )

    if self.opts.use_expected_loudest_event:
      for inst in self.instruments:
        self.far[inst] = 1./self.livetime[inst]
    elif self.opts.far:
      for inst in self.instruments:
        self.far[inst] = self.opts.far
    else:
      far_threshold_query = """
      SELECT coinc_event.instruments,MIN(combined_far)
      FROM coinc_inspiral
      JOIN coinc_event ON (coinc_inspiral.coinc_event_id == coinc_event.coinc_event_id)
      JOIN experiment_map ON (coinc_event.coinc_event_id == experiment_map.coinc_event_id)
      JOIN experiment_summary ON ( experiment_summary.experiment_summ_id == experiment_map.experiment_summ_id)
      WHERE experiment_summary.datatype == "exclude_play"
      GROUP BY coinc_event.instruments;
      """
      if self.opts.verbose: print >>sys.stdout,"\t"+far_threshold_query

      for inst, far in connection.cursor().execute(far_threshold_query):
        inst = frozenset(lsctables.instrument_set_from_ifos(inst))
        self.far[inst] = far # handle FAR = 0 case specially downstream

    if self.opts.verbose:
      for inst in self.instruments:
        print >>sys.stdout,"\tloudest event in %s time has FAR = %g" % (','.join(sorted(list(inst))),self.far[inst])

    return self.far


  def get_volume_derivative(self,instruments):
    """
    Compute the derivative of the search volume at the FAR of the loudest event
    """
    FAR = self.far[instruments]
    gw = self.gw
    twoDMassBins = self.twoDMassBins

    # If the loudest event has 0-FAR, then there is no volume above that event.
    # We return zeros for the volume arrays.
    if FAR == 0:
      return rate.BinnedArray(self.twoDMassBins)

    #determine binning up front for infinite FAR
    found, missed = self.get_injections(instruments, FAR=float("inf"))
    dbin = rate.LogarithmicBins(min([l.distance for l in found]),max([l.distance for l in found]), int(self.opts.dist_bins))

    livetime = float(abs(self.zero_lag_segments[instruments]))

    FARh = FAR*100000
    FARl = FAR*0.001
    nbins = 5
    FARS = rate.LogarithmicBins(FARl, FARh, nbins)
    vA = []
    vA2 = []

    for far in FARS.centres():
      vAt, vA2t = self.twoD_SearchVolume(instruments, dbin_in=dbin, FAR = far, bootnum=1)
      # we need to compute derivitive of log according to ul paper
      vAt.array = scipy.log10(vAt.array + 0.001)
      vA.append(vAt)

    # the derivitive is calcuated with respect to FAR * t
    FARTS = rate.LogarithmicBins(FARl * livetime, FARh * livetime, nbins)
    return self._derivitave_fit(FARTS, FAR * livetime, vA, twoDMassBins)

  def _derivitave_fit(self, farts, FARt, vAs, twodbin):
    '''
       Relies on scipy spline fits for each mass bin
       to find the derivitave of the volume at a given
       FAR.  See how this works for a simple case where
       I am clearly giving it a parabola.  To high precision it calculates
       the proper derivitave.
       A = [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]
       B = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
       C = interpolate.splrep(B,A,s=0, k=4)
       interpolate.splev(5,C,der=1)
       10.000
    '''
    dA = rate.BinnedArray(twodbin)
    for m1 in range(dA.array.shape[0]):
      for m2 in range(dA.array.shape[1]):
        da = []
        for f in farts.centres():
          da.append(vAs[farts[f]].array[m1][m2])
        fit = interpolate.splrep(farts.centres(),da,k=4)
        val = interpolate.splev(FARt,fit,der=1)
        #print val
        # FIXME this prevents negative derivitives arising from bad fits
        if val < 0: val = 0
        dA.array[m1][m2] = val # minus the derivitave
    return dA

  def get_injections(self, instruments, FAR=None):
    if not FAR: FAR = self.far[instruments]
    verbose = self.opts.verbose
    found = []
    missed = []

    working_filename = dbtables.get_connection_filename(self.fname, tmp_path = opts.tmp_space, verbose = verbose)
    connection = sqlite3.connect(working_filename)

    # WORK OUT CORRECT SEGMENTS FOR THIS FILE WHERE WE SHOULD SEE INJECTIONS
    segments = self.segments
    zero_lag_segments  = self.zero_lag_segments[instruments]
    ###############
    if zero_lag_segments:
        #FIXME we shouldn't assume that we want to remove playground
        zero_lag_segments -= segmentsUtils.S2playground(zero_lag_segments.extent())
        ###############

    # DEFINE THE INJECTION WAS MADE FUNCTION
    def injection_was_made(geocent_end_time, geocent_end_time_ns, zero_lag_segments = zero_lag_segments):
        """
        return True if injection was made in the given segmentlist
        """
        return lsctables.LIGOTimeGPS(geocent_end_time, geocent_end_time_ns) in zero_lag_segments

    connection.create_function("injection_was_made", 2, injection_was_made)
    make_sim_inspiral = lsctables.table.get_table(dbtables.get_xml(connection), lsctables.SimInspiralTable.tableName).row_from_cols

    injection_finding_query = """
       SELECT sim_inspiral.*,
       -- true if injection matched a coinc below the false alarm rate threshold
       EXISTS (
	     SELECT *
	     FROM coinc_event_map AS mapa
	     JOIN coinc_event_map AS mapb ON ( mapa.coinc_event_id == mapb.coinc_event_id )
	     JOIN coinc_inspiral ON ( mapb.table_name == "coinc_event" AND mapb.event_id == coinc_inspiral.coinc_event_id )
	     WHERE mapa.table_name == "sim_inspiral"
	     AND mapa.event_id == sim_inspiral.simulation_id
	     AND coinc_inspiral.combined_far < ?
	     )
       FROM sim_inspiral
       WHERE
       -- only interested in injections that were injected
       injection_was_made(sim_inspiral.geocent_end_time, sim_inspiral.geocent_end_time_ns)
	 """

    for values in connection.cursor().execute(injection_finding_query,(FAR,)):
      sim = make_sim_inspiral(values)
      if values[-1]:
        found.append(sim)
      else:
        missed.append(sim)

    # close connection to database
    connection.commit()
    dbtables.discard_connection_filename(self.fname, working_filename, verbose = verbose)
    dbtables.DBTable_set_connection(None)

    return found, missed


  def trim_mass_space(self, eff, instruments, minthresh=0.0, minM=25.0, maxM=100.0):
    """
    restricts array to only have data within the mass space and sets everything
    outside the mass space to some canonical value, minthresh
    """
    twodbin = self.twoDMassBins
    x = eff.array.shape[0]
    y = eff.array.shape[1]
    c1 = twodbin.centres()[0]
    c2 = twodbin.centres()[1]
    numbins = 0
    for i in range(x):
      for j in range(y):
        if c1[i] > c2[j] or (c1[i] + c2[j]) > maxM or (c1[i]+c2[j]) < minM: eff.array[i][j] = minthresh
        else: numbins+=1


  def fix_masses(self, sims):
    """
    Function to duplicate the mass pairs to remove edge effects
    on the equal mass line, takes a list of sim rows
    """
    sims2 = []
    for l in sims:
      l2 = copy.deepcopy(l)
      l2.mass1 = l.mass2
      l2.mass2 = l.mass1
      sims2.append(l2)
    sims.extend(sims2)

  def get_2d_mass_bins(self, low, high, bins):
    """
    Given the component mass range low, high of the search it will
    return 2D bins with size bins in each direction
    """
    mass1Bin = rate.LinearBins(low,high,bins)
    mass2Bin = rate.LinearBins(low,high,bins)
    self.twoDMassBins = rate.NDBins( (mass1Bin,mass2Bin) )
    return self.twoDMassBins

  def _scramble_pop(self, m, f):
    """
    A function to draw a new injection sample in the "boot strap" method
    http://en.wikipedia.org/wiki/Bootstrapping_(statistics)
    and included refereneces.
    This was used in the stack-a-flare search to get MC errors etc.
    """
    inj = m+f
    ix = scipy.random.randint(0,len(inj), (len(inj),))
    #return new missed, found
    missed = [inj[i] for i in ix if i < len(m) ]
    found = [inj[i] for i in ix if i >=len(m) ]
    return missed, found

  def _scramble_dist(self, inj, relerr, syserr):
    """
    function to handle random calibration error.  Individually scrambles the distances
    of injection by an error assumed to be log normal + a systematic.
    """
    return numpy.array([sim.distance * (1.0-syserr) * float(scipy.exp( relerr * scipy.random.standard_normal(1))) for sim in inj])

  def live_time_array(self, instruments):
    """
    return an array of live times, note every bin will be the same :) it is just a
    convenience.
    """
    live_time = rate.BinnedArray(self.twoDMassBins)
    live_time.array += 1.0
    live_time.array *= self.livetime[instruments]
    return live_time


  def twoD_SearchVolume(self, instruments, dbin_in=None, FAR=None, bootnum=None, derr=0.197, dsys=0.074):
    """
    Compute the search volume in the mass/mass plane, bootstrap
    and measure the first and second moment (assumes the underlying
    distribution can be characterized by those two parameters)
    This is gonna be brutally slow.
    derr = (0.134**2+.103**2+.102**2)**.5 = 0.197 which is the 3 detector
    calibration uncertainty in quadrature.  This is conservative since some injections
     will be H1L1 and have a lower error of .17
    the dsys is the DC offset which is the max offset of .074.
    """

    # if no FAR specified, use the FAR of the loudest event
    if FAR is None: FAR = self.far[instruments]

    # get the injections found above this FAR (found); and the injections
    # found below this FAR or not found at all (missed)
    found, missed = self.get_injections(instruments, FAR)

    if self.opts.verbose:
      print >>sys.stdout, "\tcomputing volume at FAR %g: found = %d, missed = %d" % (FAR, len(found), len(missed))

    # wnfunc is for smoothing across bins
    wnfunc = self.gw
    if wnfunc: wnfunc /= wnfunc[(wnfunc.shape[0]-1) / 2, (wnfunc.shape[1]-1) / 2]

    livetime = self.livetime[instruments]
    if not bootnum: bootnum = self.bootnum

    # mass bins for which to compute the search volume
    twodbin = self.twoDMassBins

    # If the loudest event has 0-FAR, then there is no volume above that event.
    # We return zeros for the volume arrays.
    if FAR == 0:
      return rate.BinnedArray(self.twoDMassBins),rate.BinnedArray(self.twoDMassBins)

    # set up an empty ratio array for each distance bin
    z = int(self.opts.dist_bins)
    rArrays = [rate.BinnedRatios(twodbin) for k in range(z)]

    # the search volume arrays are obtained by summing some function of
    # the binned arrays in rArrays over distance
    volArray=rate.BinnedArray(twodbin)
    volArray2=rate.BinnedArray(twodbin)

    #
    # Bootstrap to account for errors.
    #
    # In bootstrapping, we draw a new population of found (missed) injections
    # from the observed population of found (missed) injections, with replacement,
    # such that the total number of injections does not change. We then use this
    # scrambled population to recompute the search volume. This technique gives
    # a sense of how sensitive the calculation is to the particular set of data
    # we happened to acquire.
    #
    for n in range(bootnum):
      #
      # In each iteration, we will compute volArray and volArray2 by summing
      # some function of rArrays[k] over the distance index k.
      #

      # Initialize this sum by setting the binned arrays in rArrays to zero
      for k in range(z):
        rArrays[k].numerator.array = numpy.zeros(rArrays[k].numerator.bins.shape)
        rArrays[k].denominator.array = numpy.zeros(rArrays[k].numerator.bins.shape)

      # first time through, use the observed found/missed populations
      if bootnum == 1:
        sm, sf = missed, found
        f_dist = numpy.array([l.distance for l in found])
        m_dist = numpy.array([l.distance for l in missed])
      # after the first time, resample the found/missed populations (bootstrapping)
      else:
        # resample found/missed
        sm, sf = self._scramble_pop(missed, found)

	# randomly adjust injection distance to account for uncertainty in distance
        # I make a separate array of distances to speed up this calculation
        f_dist = self._scramble_dist(sf, derr, dsys)
        m_dist = self._scramble_dist(sm, derr, dsys)

      # compute the distance bins
      if not dbin_in:
        dbin = rate.LogarithmicBins(min(f_dist),max(f_dist), z)
      else: dbin = dbin_in

      # get rid of all missed injections outside the distance bins
      # to prevent binning errors
      f_m1 = numpy.array([l.mass1 for l in  sf])
      f_m2 = numpy.array([l.mass2 for l in  sf])
      m_m1 = numpy.array([l.mass1 for l in  sm])
      m_m2 = numpy.array([l.mass2 for l in  sm])

      m_m1, m_m2, m_dist = self.cut_distance(m_m1, m_m2, m_dist, dbin)
      f_m1, f_m2, f_dist = self.cut_distance(f_m1, f_m2, f_dist, dbin)

      # for each found injection, find the m1-m2-d bin it corresponds to
      # and increment the numerator there by one
      for i, l in enumerate(f_dist):
        tbin = rArrays[dbin[f_dist[i]]]
        tbin.incnumerator( (f_m1[i], f_m2[i]) )

      # check whether there are a sensible number of injections
      # FIXME this check could be cleaner
      if n == 0:
        farray = sum(rArrays[i].numerator.array for i in range(len(rArrays)))
        for m1 in twodbin.centres()[0]:
          for m2 in twodbin.centres()[1]:
            if m1+m2<self.maxtotal and m1+m2>self.mintotal and m1<=m2 and farray[twodbin[(m1,m2)]] < 100:
               print >>sys.stderr, "WARNING: Only %d injections were found in %s time in the %.1f-%.1f mass bin above FAR=%g over all injected distances."  % (farray[twodbin[(m1,m2)]],''.join(sorted(list(instruments))),m1,m2,FAR)
               print >>sys.stderr, "In order to obtain an accurate upper limit, you should run with more injections."


      # for each found injection, find the m1-m2-d bin it corresponds to
      # and increment the denominator there by one
      for i, l in enumerate(m_dist):
        tbin = rArrays[dbin[m_dist[i]]]
        tbin.incdenominator( (m_m1[i], m_m2[i]) )

      # the ratio of the numerator to the (numerator+denominator) is equal
      # to the efficiency at that m1-m2-d bin

      tmpArray2=rate.BinnedArray(twodbin) #start with a zero array to compute the mean square
      for k in range(z):
        tbins = rArrays[k]
        tbins.denominator.array += tbins.numerator.array
        if wnfunc: rate.filter_array(tbins.denominator.array,wnfunc)
        if wnfunc: rate.filter_array(tbins.numerator.array,wnfunc)
        tbins.regularize()

        # integral wrt log(r), not r:
	# integral( 4pi r**2 eff(r) )dr = integral( 4pi r**3 eff(r) )dlog(r)
        integrand = 4.0 * pi * tbins.ratio() * dbin.centres()[k]**3 * dbin.delta
        volArray.array += integrand
        tmpArray2.array += integrand

	if self.opts.verbose:
	  print >>sys.stdout, "\tbootstrapping:\t%.1f%% and calculating smoothed volume:\t%.1f%%\r" % ((100.0 * n / bootnum), (100.0 * k / z)),

      tmpArray2.array *= tmpArray2.array
      volArray2.array += tmpArray2.array

    if self.opts.verbose:
      print >>sys.stdout, ""
    #Mean and variance
    volArray.array /= bootnum
    volArray2.array /= bootnum
    volArray2.array -= volArray.array**2 # Variance
    volArray.array *= livetime
    volArray2.array *= livetime*livetime # this gets two powers of live time
    return volArray, volArray2


  def cut_distance(self, m1, m2, d, dbin):
    """
    Exclude sims outside some distance range to avoid errors when binning
    """
    mnd = min(dbin.lower())
    mxd = max(dbin.upper())
    indexes = numpy.logical_and(d <= mxd, d >= mnd)
    return m1[indexes], m2[indexes], d[indexes]


def parse_command_line():
  parser = OptionParser(version = git_version.verbose_msg, usage = "%prog [options] database", description = "%prog computes mass/mass upperlimit")

  parser.add_option("--bootstrap-iterations", default = 1000, metavar = "integer", type = "int", help = "Number of iterations to compute mean and variance of volume MUST BE GREATER THAN 1 TO GET USABLE NUMBERS, a good number is 10000")
  parser.add_option("--bootstrap-seed",default = None, metavar = "integer",type = "int", help = "Provide a seed for the bootstrap resampling. This is useful for testing code changes. Otherwise output is completely (pseudo) random!")
  parser.add_option("--instruments", metavar = "name[,name,...]", help = "Set the list of instruments.  Required.  Example \"H1,H2,L1\"")
  parser.add_option("--live-time-program", default = "thinca", metavar = "name", help = "Set the name of the program whose rings will be extracted from the search_summary table.  Default = \"thinca\".")
  parser.add_option("--output-name-tag", default = "", metavar = "name", help = "Set the file output name tag, real name is 2Dsearchvolume-<tag>-<ifos>.xml")
  parser.add_option("--dist-bins", default = 50, metavar = "integer", type = "int", help = "Number of bins to use for integration over distance.")
  parser.add_option("--d-err", default = 0.197, metavar = "float", type = "float", help = "When bootstrapping, simulate a random calibration error on distance of the specified magnitude.")
  parser.add_option("--d-sys-err", default = 0.074, metavar = "float", type = "float", help = "systematic calibration error on distance (should use worst)")
  parser.add_option("--mass-bins", default = 11, metavar = "integer", type = "int", help = "Number of mass bins along 1 dimension (Note the total number of mass bins will generally be less than --mass-bins * --mass-bins once the actual parameter space is carved out)")
  parser.add_option("--far", help = "FAR to use for injection finding instead of loudest event.")
  parser.add_option("--use-expected-loudest-event", default=False, action = "store_true", help = "Instead of using the observed FAR for the loudest event, estimate the FAR based on livetime. This option overrides the --far option.")
  parser.add_option("--veto-segments-name", default = "vetoes", help = "Set the name of the veto segments to use from the XML document.")
  parser.add_option("-t", "--tmp-space", metavar = "path", help = "Path to a directory suitable for use as a work area while manipulating the database file.  The database file will be worked on in this directory, and then moved to the final location when complete.  This option is intended to improve performance when running in a networked environment, where there might be a local disk with higher bandwidth than is available to the filesystem on which the final output will reside.")
  parser.add_option("--output-cache", default = None, help = "Name of output cache file. If none is specified, then no cache file will be written.")
  parser.add_option("--verbose", action = "store_true", help = "Be verbose.")

  opts, filenames = parser.parse_args()

  if opts.instruments: opts.instruments = lsctables.instrument_set_from_ifos(opts.instruments)
  if len(filenames) != 1:
    print >>sys.stderr, "must specify exactly one database file"
    sys.exit(1)

  return opts, filenames[0]

# FIXME this should come from a constants package
# Astronomers use Julian year
secs_in_year = 31557600.0



############################ MAIN PROGRAM #####################################
###############################################################################
###############################################################################

opts, database = parse_command_line()

#initialize an upper limit class
UL = upper_limit(database, opts)

#create an empty cache which will store the output files/metadata
cache_list = []

#loop over the requested instruments
for instruments in UL.set_instruments_to_calculate():

  #compute volume derivitive
  if opts.verbose:
    print >>sys.stdout, "Calculating volume derivative at FAR %g for %s" % (UL.far[instruments],",".join(sorted(list(instruments))),)
  dvA = UL.get_volume_derivative(instruments)

  #compute volume first and second moments
  if opts.verbose:
    print >>sys.stdout, "Calculating search volume above FAR %g for %s" % (UL.far[instruments],",".join(sorted(list(instruments))),)
  vA, vA2 = UL.twoD_SearchVolume(instruments)

  # get an array of livetimes for convenience
  ltA = UL.live_time_array(instruments)

  # FIXME convert to years (use some lal or pylal thing in the future)
  vA.array /= secs_in_year
  vA2.array /= secs_in_year * secs_in_year #two powers for this squared quantity

  #Trim the array to have sane values outside the total mass area of interest
  try: minvol = scipy.unique(vA.array)[1]/10.0
  except: minvol = 0
  UL.trim_mass_space(dvA, instruments, minthresh=0.0, minM=UL.mintotal, maxM=UL.maxtotal)
  UL.trim_mass_space(vA, instruments, minthresh=minvol, minM=UL.mintotal, maxM=UL.maxtotal)
  UL.trim_mass_space(vA2, instruments, minthresh=0.0, minM=UL.mintotal, maxM=UL.maxtotal)

  #output an XML file with the result
  xmldoc = ligolw.Document()
  xmldoc.appendChild(ligolw.LIGO_LW())
  xmldoc.childNodes[-1].appendChild(rate.binned_array_to_xml(vA, "2DsearchvolumeFirstMoment"))
  xmldoc.childNodes[-1].appendChild(rate.binned_array_to_xml(vA2, "2DsearchvolumeSecondMoment"))
  xmldoc.childNodes[-1].appendChild(rate.binned_array_to_xml(dvA, "2DsearchvolumeDerivative"))

  # DONE with vA, so it is okay to mess it up...
  # Compute range
  vA.array = (vA.array * secs_in_year / UL.livetime[instruments] / (4.0/3.0 * pi)) **(1.0/3.0)
  UL.trim_mass_space(vA, instruments, minthresh=0.0, minM=UL.mintotal, maxM=UL.maxtotal)
  xmldoc.childNodes[-1].appendChild(rate.binned_array_to_xml(vA, "2DsearchvolumeDistance"))

  # make a live time
  UL.trim_mass_space(ltA, instruments, minthresh=0.0, minM=UL.mintotal, maxM=UL.maxtotal)
  xmldoc.childNodes[-1].appendChild(rate.binned_array_to_xml(ltA, "2DsearchvolumeLiveTime"))

  output_filename = "2Dsearchvolume-%s-%s.xml" % (opts.output_name_tag, "".join(sorted(list(instruments))) )

  utils.write_filename(xmldoc, output_filename)
  cache_list.append( lal.CacheEntry( "".join(sorted(list(instruments))),
				     "SEARCH_VOLUME",
				     segments.segment(UL.start_time, UL.end_time),
				     "file://localhost%s/%s" % (os.getcwd(),output_filename) ) )


# write cache file
if opts.output_cache is not None:
  fd = open( opts.output_cache, "w" )
  for l in cache_list:
    fd.write( str(l) + "\n")
  fd.close()
