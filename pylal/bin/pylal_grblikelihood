#!/usr/bin/env python
#
# Copyright (C) 2007  Patrick Brady, Nickolas Fotopoulos
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
#
"""
This code computes the probabilities that go into the distance upper limits
for the CBC external trigger search.

There is a lot of advanced indexing technique here.  Reference:
http://scipy.org/Cookbook/Indexing
"""

from __future__ import division

__author__ = "Nickolas Fotopoulos <nvf@gravity.phys.uwm.edu>"
__version__ = "$Revision$"[11:-2]
__date__ = "$Date$"
__Id__ = "$Id$"
__prog__ = "pylal_grblikelihood"
__title__ = "GRB interpretation"

import itertools
import optparse
import os.path as op
import cPickle as pickle
import shutil
import sys

import numpy
numpy.seterr(all="raise")  # throw an exception on any funny business

from pylal import CoincInspiralUtils
from pylal import grbsummary
from pylal import InspiralUtils
from pylal import rate

# which m2 bin to trace for diagnostics
trace_m2_bin = 0

################################################################################
# utility functions

class BackgroundEstimator(object):
    """
    Abstract class to define an interface for background estimation.
    After initializing with loudest (binned) background data, __call__
    becomes active and will return the ln(p(c|0)) (for each bin).
    """
    def __init__(self, loudest_in_bg_trials):
        self.loudest_in_bg_trials = loudest_in_bg_trials

    def __call__(self, loudest_in_trial):
        raise NotImplemented

class ConstantBackgroundEstimator(BackgroundEstimator):
    """
    BackgroundEstimator that assumes a constant value for loudest_in_trial
    greater than the loudest_bg.
    """
    def __init__(self, loudest_in_bg_trials, eps):
        """
        Initialize ConstantBackgroundEstimator to return a ln(pc0) such that
        pc0[pc0 < eps] = eps. loudest_in_bg_trials can be any shape as long
        as the first axis (axis=0) is the trials axis.  If eps is a vector,
        then we require eps.shape == loudest_in_bg_trials.shape[1:]
        """
        if not numpy.isscalar(eps) \
            and eps.shape != loudest_in_bg_trials.shape[1:]:
            raise ValueError, "eps.shape != loudest_in_bg_trials.shape[1:]"
        self.eps = eps
        BackgroundEstimator.__init__(self, loudest_in_bg_trials)

    def __call__(self, loudest_in_trial):
        """
        Return ln(p(c|0)).  Where p(c|0) < eps, we return eps.
        """
        num_louder = (self.loudest_in_bg_trials > loudest_in_trial[None, ...])\
            .sum(axis=0, dtype=float)
        pc0 = num_louder / self.loudest_in_bg_trials.shape[0]
        pc0[pc0 < self.eps] = self.eps
        return numpy.log(pc0)

class SimpleExpExtrapEstimator(BackgroundEstimator):
    """
    BackgroundEstimator that takes the Nth loudest point and extrapolates
    the p(c|0) distribution based on its value.
    """
    def __init__(self, loudest_in_bg_trials, fac):
        """
        Initialize SimpleExpExtrapolator with the extrapolation parameters.
        Use the (fac*N)th point from the end as a fiducial point, where
        N is the number of nonempty trials per mchirp bin.
        """
        sorted_loudest = loudest_in_bg_trials.copy()
        sorted_loudest.sort(axis=0)

        self.default_evaluator = \
            ConstantBackgroundEstimator(loudest_in_bg_trials, 1e-10)
        
        num_nonempty_offsource_trials = (offsource_loudest_by_trial_mc>0).sum(axis=0)

        self.fiducial_x = numpy.asarray([ sorted_loudest[-int(fac*N), i] \
                                          for i,N in enumerate(num_nonempty_offsource_trials)])
        self.fiducial_y = self.default_evaluator(self.fiducial_x)

    def __call__(self, loudest_in_trial):
        """
        Return ln(p(c|0)).  Where c is loudest than the Nth loudest, use the
        the simplest possible extrapolation based on the Nth loudest event.
        """
        log_pc0 = self.default_evaluator(loudest_in_trial)
        extrap = self.fiducial_y + \
            self.fiducial_x**1.5 - loudest_in_trial**1.5
        numpy.putmask(log_pc0, loudest_in_trial > self.fiducial_x, extrap)
        return log_pc0

class ForegroundEstimator(object):
    """
    Abstract class to define an interface for foreground estimation.
    After initializing with loudest (binned) injection data, __call__
    becomes active and will return the ln(p(c(mc)|h(m2)) (for each bin).
    """
    def __init__(self, inj_loudest_by_inj_mc_m2, num_sims_by_m2):
        raise NotImplemented

    def __call__(self, trial_loudest_by_mc):
        raise NotImplemented

class ConstantForegroundEstimator(ForegroundEstimator):
    """
    ConstantForegroundEstimator evaluates the probability of getting events
    louder than any of the given events within each m2 bin.
    """
    def __init__(self, inj_loudest_by_inj_mc, num_sims_by_m2, m2_bin_by_inj,
        weight_by_inj=None, eps=None):
        """
        Initialize ConstantForegroundEstimator to return a ln(p(c|h)) such that
        pch[pch < eps] = eps.  If eps is a vector, then we require
        eps.shape == loudest_in_bg_trials.shape[1:].  If eps
        is None, then clip at 1/2 of the smallest otherwise-possible value.
        """
        self.num_sims_by_m2 = num_sims_by_m2
        self.m2_bin_by_inj = m2_bin_by_inj
        self.dtype = inj_loudest_by_inj_mc.dtype
        self.shape = inj_loudest_by_inj_mc.shape
        self.num_inj = self.shape[0]
        self.num_m2 = num_sims_by_m2.shape[0]
        self.num_mc = self.shape[1]

        if weight_by_inj is not None:
            # normalize to N (the number of injections)
            self.weight_by_inj = numpy.array(weight_by_inj, copy=True)
            self.weight_by_inj *= self.num_inj / self.weight_by_inj.sum()
        else:
            self.weight_by_inj = numpy.ones(self.num_inj, dtype=int)

        if eps is not None:
            if numpy.isscalar(eps):
                self.eps = self.dtype.type(eps)
            elif eps.shape == self.shape[1:]:
                self.eps = eps
            else:
                raise ValueError, "eps must be None, a scalar, or else "\
                    "eps.shape must be equal to "\
                    "inj_loudest_by_inj_mc.shape[1:]."
        else:
            self.eps = 0.5 * (self.weight_by_inj.min() or 1.0) / \
                (num_sims_by_m2[None, :].repeat(self.shape[1], axis=0) + 1e-10)

        # repack to speed inner loop by allowing us to call dot()
        self.inj_loudest_by_inj_mc_m2 = \
            numpy.zeros(self.shape + (self.num_m2,), dtype=self.dtype)
        self.inj_loudest_by_inj_mc_m2[numpy.arange(self.num_inj, dtype=int),
                                      :, m2_bin_by_inj] = inj_loudest_by_inj_mc

    def __call__(self, trial_loudest_by_mc):
        """
        Return ln(p(c|h)).  Where p(c|0) < eps, we return eps.
        """
        louder_by_inj_mc_m2 = \
            self.inj_loudest_by_inj_mc_m2 > trial_loudest_by_mc[None, :, None]

        # Do a weighted count
        # NB: self.weight_by_inj is already normalized in __init__
        # NB: Use the BLAS call dot() to speed up this multiplication and sum:
        #     num_louder_by_mc_m2 = (louder_by_inj_mc_m2 \
        #        * self.weight_by_inj[:, None, None]).sum(axis=0, dtype=float)
        num_louder_by_mc_m2 = \
            numpy.dot(self.weight_by_inj,
                      louder_by_inj_mc_m2\
                      .reshape((self.num_inj, self.num_mc * self.num_m2))\
                      .astype(self.weight_by_inj.dtype))\
            .reshape((self.num_mc, self.num_m2))
        pch_by_mc_m2 = num_louder_by_mc_m2 \
            / (self.num_sims_by_m2[None, :] + 1e-10)
        numpy.putmask(pch_by_mc_m2, pch_by_mc_m2 < self.eps, self.eps)

        return numpy.log(pch_by_mc_m2)

def parse_args():
    parser = optparse.OptionParser(version="%prog CVS $Id$ ")

    # cache input
    parser.add_option("--relic-onsource", help="output of pylal_relic "\
        "containing the onsource loudest coincs.")
    parser.add_option("--relic-offsource", help="output of pylal_relic "\
        "containing the offsource loudest coincs.")
    parser.add_option("--relic-injections", help="output of pylal_relic "\
        "containing the loudest injection coincs.")

    # odds and ends
    parser.add_option("--reweight-D", action="store_true", default=False,
        help="enable a reweighting of injections from a uniform distribution "\
        "in D to D^3.")
    parser.add_option("--user-tag", help="a tag with which to label outputs")
    parser.add_option("--verbose", action="store_true", default=False,
        help="extra information to the console")

    options, arguments = parser.parse_args()

    # check that mandatory switches are present
    for opt in ("relic_onsource", "relic_offsource", "relic_injections"):
        if getattr(options, opt) is None:
            raise ValueError, "--%s is required" % opt.replace("_", "-")

    return options, arguments

################################################################################
# parse arguments
opts, args = parse_args()

##############################################################################
# read everything into memory
# All vetoes have been taken into account

if opts.verbose:
    print "Reading in bin definitions and loudest statistics..."
statistic, mc_bins, onsource_loudest_by_mc \
    = pickle.load(open(opts.relic_onsource))
statistic, mc_bins, offsource_loudest_by_trial_mc \
    = pickle.load(open(opts.relic_offsource))
statistic, mc_bins, m2_bins, D_bins, m2_D_by_inj, inj_loudest_by_inj_mc \
    = pickle.load(open(opts.relic_injections))
if statistic != "effective_snr":
    raise NotImplemented

# compute useful quantities
num_inj = len(inj_loudest_by_inj_mc)
num_sims_by_m2 = numpy.zeros(len(m2_bins), dtype=int)
for m2_D in m2_D_by_inj:
    num_sims_by_m2[m2_bins[m2_D[0]]] += 1
m2_bin_by_inj = [m2_bins[m2_D[0]] for m2_D in m2_D_by_inj]

# set all empty trials to have likelihoods of -infinity
empty_log_L_by_m2 = -numpy.inf * numpy.ones(len(m2_bins), dtype=float)

##############################################################################
# Initialize background and foreground estimation functions
# NB: start extrapolating 10% from the end of the tail for robustness.
num_offsource_trials = offsource_loudest_by_trial_mc.shape[0]
compute_log_pc0_by_mc = SimpleExpExtrapEstimator(offsource_loudest_by_trial_mc, 0.3)



# Example of how to switch to constant extrapolator
# compute_log_pc0_by_mc = ConstantBackgroundEstimator(\
#     offsource_loudest_by_trial_mc,
#     0.5 / offsource_loudest_by_trial_mc.shape[0])

# Compute D^3 weighting.
if opts.reweight_D:
    if opts.verbose:
        print "Computing D^3 reweighting..."
    weight_by_inj = [m2_D[1]**3 for m2_D in m2_D_by_inj]
else:
    weight_by_inj = None
compute_log_pch_by_mc_m2 = ConstantForegroundEstimator(\
    inj_loudest_by_inj_mc, num_sims_by_m2, m2_bin_by_inj,
    weight_by_inj=weight_by_inj)

################################################################################
# compute the likelihoods of the loudest on-source coincidences
if opts.verbose:
    print "Computing on-source likelihoods..."
log_pc0_by_mc = compute_log_pc0_by_mc(onsource_loudest_by_mc)
log_pch_by_mc_m2 = compute_log_pch_by_mc_m2(onsource_loudest_by_mc)

# NB: Here are examples of how to swap pure effective SNR or log IFAR.
# Just remember to do it for off-source and injection cases as well.

# log_L_by_mc_m2 = onsource_loudest_by_mc[:, None].repeat(len(m2_bins), axis=1)
# log_L_by_mc_m2 = -log_pc0_by_mc[:, None].repeat(len(m2_bins), axis=1)
log_L_by_mc_m2 = log_pch_by_mc_m2 - log_pc0_by_mc[:, None]

# if there are any candidates, take max over likelihoods from candidates
actual_candidate_mask = (onsource_loudest_by_mc > 0)
if numpy.any(actual_candidate_mask):
    log_L_by_m2 = log_L_by_mc_m2[actual_candidate_mask, :].max(axis=0)
else:
    log_L_by_m2 = empty_log_L_by_m2

# write the on-source likelihoods to disk
onsource_outname = __prog__ + "_onsource"
if opts.user_tag is not None:
    onsource_outname += "_" + opts.user_tag
onsource_outname += ".pickle"
pickle.dump((log_pc0_by_mc, log_pch_by_mc_m2, actual_candidate_mask,
    log_L_by_m2), open(onsource_outname, "wb"), -1)

################################################################################
# compute P(observation | 0), the false-alarm probability
# tabulate off-source likelihoods
if opts.verbose:
    print "Computing off-source likelihoods..."
offsource_pc0_by_trial_mc = numpy.zeros(\
    (num_offsource_trials, len(mc_bins)), dtype=numpy.float32)
offsource_pch_by_trial_mc_m2 = numpy.zeros(\
    (num_offsource_trials, len(mc_bins), len(m2_bins)), dtype=numpy.float32)
offsource_likelihood_by_trial_m2 = numpy.zeros(\
    (num_offsource_trials, len(m2_bins)), dtype=numpy.float32)
for i, offsource_loudest_by_mc in enumerate(offsource_loudest_by_trial_mc):
    offsource_pc0_by_trial_mc[i, :] = \
        compute_log_pc0_by_mc(offsource_loudest_by_mc)
    offsource_pch_by_trial_mc_m2[i, :] = \
        compute_log_pch_by_mc_m2(offsource_loudest_by_mc)
    off_log_L_by_mc_m2 = offsource_pch_by_trial_mc_m2[i, :] \
        - offsource_pc0_by_trial_mc[i, :, None]

    # if there are any candidates, take max over likelihoods from candidates
    offsource_candidate_mask = (offsource_loudest_by_mc > 0)
    if numpy.any(offsource_candidate_mask):
        offsource_likelihood_by_trial_m2[i, :] = \
            off_log_L_by_mc_m2[offsource_candidate_mask, :].max(axis=0)
    else:
        offsource_likelihood_by_trial_m2[i, :] = empty_log_L_by_m2

# write the off-source likelihoods to disk
offsource_outname = onsource_outname.replace("_onsource", "_offsource", 1)
pickle.dump((offsource_pc0_by_trial_mc, offsource_pch_by_trial_mc_m2,
    offsource_likelihood_by_trial_m2), open(offsource_outname, "wb"), -1)

################################################################################
# compute P(Lambda > Lambda_0(m2) | h(m2, D)) for the upper limit
# FIXME: This can be done in O(N_{inj}) time because L is monotonic with SNR
# within an mc bin, so a threshold in L can be converted to a threshold in SNR
# for each mc bin.
if opts.verbose:
    print "Computing injection likelihoods..."

m2_by_inj = numpy.zeros(num_inj, dtype=float)
D_by_inj = numpy.zeros(num_inj, dtype=float)
inj_likelihood_by_trial_m2 = empty_log_L_by_m2[None, :].repeat(num_inj, axis=0)
log_pc0_by_inj_mc = []
log_pch_by_inj_mc = []
rho_by_inj_mc = []
for i, (inj_trial_loudest_by_mc, m2_D) in enumerate(zip(inj_loudest_by_inj_mc,
    m2_D_by_inj)):
    # compute likelihood of this (inj + noise) instance arising from m2'
    inj_trial_log_pc0_by_mc = compute_log_pc0_by_mc(inj_trial_loudest_by_mc)

    # this inner loop makes the algorithm O(n^2)
    inj_trial_log_pch_by_mc_m2 = \
        compute_log_pch_by_mc_m2(inj_trial_loudest_by_mc)

    inj_trial_log_L_by_mc_m2 = inj_trial_log_pch_by_mc_m2 \
        - inj_trial_log_pc0_by_mc[:, None]

    # if there are any candidates, take max over likelihoods from candidates
    inj_trial_candidate_mask = (inj_trial_loudest_by_mc > 0)
    if inj_trial_candidate_mask.any():
        inj_likelihood_by_trial_m2[i, :] = \
            inj_trial_log_L_by_mc_m2[inj_trial_candidate_mask, :].max(axis=0)
    else:
        pass  # already set to empty

    # record diagnostics
    if m2_bins[m2_D[0]] == trace_m2_bin:
        log_pc0_by_inj_mc.append(inj_trial_log_pc0_by_mc)
        log_pch_by_inj_mc.append(inj_trial_log_pch_by_mc_m2[:, trace_m2_bin]\
            .squeeze())
        rho_by_inj_mc.append(inj_trial_loudest_by_mc)
log_pc0_by_inj_mc = numpy.array(log_pc0_by_inj_mc)
log_pch_by_inj_mc = numpy.array(log_pch_by_inj_mc)
rho_by_inj_mc = numpy.array(rho_by_inj_mc)

# write the injection likelihoods to disk
inj_outname = onsource_outname.replace("_onsource", "_injections", 1)
pickle.dump((inj_likelihood_by_trial_m2, log_pc0_by_inj_mc, log_pch_by_inj_mc,
             rho_by_inj_mc, trace_m2_bin),
            open(inj_outname, "wb"), -1)

