#!/usr/bin/env python
#
# Copyright (C) 2008  Nickolas Fotopoulos
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
#
"""
pylal_expose: EXternal-trigger POpulation SEarch

This code computes the probability that a set of loudest statistics from
searches for external triggers is drawn from the same distribution as a
set of loudest statistics from background trials.
"""

from __future__ import division

__author__ = "Nickolas Fotopoulos <nvf@gravity.phys.uwm.edu>, Alexander Dietz alexander.dietz@lapp.in2p3.fr"
__prog__ = "pylal_expose"
__title__ = "External Trigger Population Search"

import sys
import copy
import glob
import optparse
import random
import cPickle as pickle
itertools = __import__("itertools")
                       
import numpy 
from scipy import stats
import pylab as plt
plt.rc('text', usetex=True)


from glue import iterutils
from pylal import InspiralUtils
from pylal import grbsummary
from pylal import plotutils
from pylal import rate
from pylal import git_version
import PopStatement

#
# Functions
#

def extract_grb_name(file):
    """
    Extracting the GRB name from a pickle file.
    """
    parts = file.split('_')
    for part in parts:
        if 'GRB' in part:
            return part[3:]

    return None
            
## def extract_grb_names(filelist):
##     """
##     Given a list of pickle filenames, this function
##     extracts the GRB names from the files.
##     """
##     grb_list = set()
##     for file in filelist:
##         grb_list.add(extract_grb_name(file))

##     return grb_list
    
def find_file(filelist, grb_name):
    for file in filelist:
        id = file.split('_')[3][3:]
        if grb_name == id:
            return file

    raise ValueError, "GRB %s not found in filelist" % grb_name

def load_grb_table(filename_grb, files_onsource, files_offsource,\
                    redshift_rejection = True):
    """
    Loads the GRB table from the given filename
    but selects only the GRB's that are listed in the
    grb_list and which are available; rejecting any GRB with a
    redshift measurement if the flag is set to True.
    """

    ## FIXME
    # this is a list of short GRB's being analyzed with redshift measurement,
    # which is not part of the usual GRB.xml file. This should be updated instead,
    # but we don't have a xml editor.
    additional_GRB_with_redshift = ['061217','070429B']
    
    if len(files_onsource)!=len(files_offsource):
        raise ValueError, "The number of onsource files and offsource files"\
              " is different. They should be the same!"

    list_onsource = set(extract_grb_name(f) for f in files_onsource)
    #list_onsource = extract_grb_names(files_onsource)
    #list_offsource = extract_grb_names(files_offsource)
    list_offsource = set(extract_grb_name(f) for f in files_offsource)
    if list_onsource!=list_offsource:
        raise ValueError, "The onsource files do not match the offsource files!"
    list_grbs = list_onsource
    
    # read all the GRBs
    ext_trigs = grbsummary.load_external_triggers(filename_grb)

    temp_trigs = []
    new_files_onsource = []
    new_files_offsource = []

    # main loop over all GRB's
    for grb in ext_trigs:
        
        if grb.event_number_grb in list_grbs:
            if redshift_rejection and (grb.event_z>0 or \
                                       grb.event_number_grb in additional_GRB_with_redshift):
                print >>sys.stderr, "GRB %s has a redshift measurement: "\
                      "rejected." % grb.event_number_grb
            else:
                temp_trigs.append(grb)
                new_files_onsource.append(find_file(files_onsource, grb.event_number_grb))
                new_files_offsource.append(find_file(files_offsource, grb.event_number_grb))                
                print >>sys.stderr, "GRB %s has been added to the analysis list " %\
                      grb.event_number_grb
                
    return list_grbs, temp_trigs, new_files_onsource,  new_files_offsource
    
def create_hist(data):
    nbins = 30
    val_min = min(data)-1.0
    val_max = max(data)+1.0
    bins = rate.LinearBins(val_min, val_max, nbins)
    px = bins.lower()

    # create the histogram and fill it
    hist = numpy.zeros(nbins)
    for value in data:
        hist[bins[value]] += 1
    return px, hist
    
def parse_args():
    """
    Parsing the command line arguments. 
    """
    parser = optparse.OptionParser(version=git_version.verbose_msg)

    # inputs
    parser.add_option("--onsource-glob", help="glob matching pickle files " \
        "containing the loudest on-source coincidences")
    parser.add_option("--offsource-glob", help="glob matching pickle files " \
        "containing the loudest off-source coincidences")
    parser.add_option("--grb-xml", default = "listGRB.xml", \
                      help="xml file containing the GRB data.")
    parser.add_option("--grb-pickle", default = "list_grb.pickle", \
                      help="pickle file containing the GRB data.")

    parser.add_option("--analyze", type="string",help="Specifies what to "\
                      "analyze. Possible values: {box,test}.")
    parser.add_option("--reject-redshift-grb", action="store_true",
                      help="Enables any GRB with a redshift value to be rejected. "\
                      "Not recommended.", default=False)
    
    
    parser.add_option("--m2-min", type="float", help="minimum m2 value")
    parser.add_option("--m2-max", type="float", help="maximum m2 value")
    parser.add_option("--m2-nbins", type="int", help="number of m2 bins")

    # InspiralUtils compatibility
    parser.add_option("--gps-start-time", type="int",
        help="GPS start time of data analyzed")
    parser.add_option("--gps-end-time", type="int",
        help="GPS end time of data analyzed")
    parser.add_option("--ifo-tag", help="IFO coincidence time analyzed")
    parser.add_option("--user-tag", help="a tag to label your plots")
    parser.add_option("--output-path", help="root of the HTML output")
    parser.add_option("--enable-output", action="store_true",
        default=False, help="enable plots and HTML output")
    parser.add_option("--html-for-cbcweb", action="store_true",
        default=False, help="enable HTML output with the appropriate headers "
        "for the CBC website")
    parser.add_option("--show-plot", action="store_true", default=False,
        help="display the plots to screen if an X11 display is available")
       
    # odds and ends
    parser.add_option("--verbose", action="store_true", default=False,
        help="print extra information to stdout")

    options, arguments = parser.parse_args()


    if options.ifo_tag is not None:
        options.ifo_times = options.ifo_tag

    for opt in ("onsource_glob", "offsource_glob"):
        if getattr(options, opt) is None:
            raise ValueError,  "--%s is required" % opt.replace("_", "-")

    check_list = ['box','test','random','single','max']
    if options.analyze not in check_list:
        raise ValueError,  "Option '--analyze' must take one of the "\
              "following options: %s"% check_list


    # Default hack: Not nice, but FIXME, i.e. load the grbUL pickle file!!
    if options.m2_min is None:
        options.m2_min = 1
        options.m2_max = 40
        options.m2_nbins = 13

    return options, arguments


#########################################################
####################  MAIN  ############################
#########################################################

#
# initialization
#
opts, args = parse_args()
page = InspiralUtils.InspiralPage(opts)


#
# Read input and select GRB data
#

# get the list of files
files_onsource = glob.glob(opts.onsource_glob)
files_offsource = glob.glob(opts.offsource_glob)
if len(files_onsource)!=len(files_offsource):
    print >> sys.stderr, "Length of the two globs are not equal. "\
          "Probable one or more files are missing?"
    sys.exit(1)


# select the GRB's to be used
list_grbs, ext_trigs, files_onsource,  files_offsource =\
           load_grb_table(opts.grb_xml, files_onsource, files_offsource,\
                          redshift_rejection = opts.reject_redshift_grb)
number_grb = len(ext_trigs)
page.write("Number of GRB's used: %d" % number_grb)


# create the PopStatement instances which are used to handle the
# population statement calculations
grb_data = pickle.load(file(opts.grb_pickle))
pop_stat = PopStatement.PopStatement(grb_data,'condensed')


# create the PopStatement instances for the m2-bin analysis
pop_m2 = {}
#dm = (opts.m2_max - opts.m2_min) / opts.m2_nbins
#m2_edgesx = opts.m2_min + numpy.arange(opts.m2_nbins + 1, dtype=float) * dm
m2_bins = rate.LinearBins(opts.m2_min, opts.m2_max, opts.m2_nbins )
for m2_low, m2_high in zip(m2_bins.lower(), m2_bins.upper()):
    mass_bin = "%.1f-%.1f" % (m2_low, m2_high)
    pop_m2[mass_bin] = PopStatement.PopStatement(grb_data, mass_bin)


for index_grb, (file_on, file_off) in enumerate(zip(files_onsource, files_offsource)):
    
    # make the double check of the detection statistic
    words = file_on.split('_')
    grb_name = words[3][3:]

    # read the data from the files
    dummy, dummy, dummy, onsource_L_by_m2, log_sum_L =\
           pickle.load(open(file_on))
    
    dummy, dummy, offsource_L_by_trial_m2, off_log_sum_L_by_trial =\
           pickle.load(open(file_off))
    n_trials = offsource_L_by_trial_m2.shape[0]

    # add data to the m2-pop statement instances
    for index_m2, (m2_low, m2_high) in \
        enumerate(zip(m2_bins.lower(), m2_bins.upper())):             
        mass_bin = "%.1f-%.1f" % (m2_low, m2_high)
        pop_m2[mass_bin].add_background_by_trial(grb_name, offsource_L_by_trial_m2[:,index_m2])
        pop_m2[mass_bin].add_foreground(grb_name, onsource_L_by_m2[index_m2])
        
        
    # double check the detection statistic
    count_louder = (off_log_sum_L_by_trial > log_sum_L).sum(axis=0)
    stat = count_louder/n_trials
    page.write("GRB%s p(c|0)= %f " %(grb_name, stat))
        

    # populate the popStatement instance with real background/onsource
    pop_stat.add_background_by_trial(grb_name, off_log_sum_L_by_trial)
    pop_stat.add_foreground(grb_name, log_sum_L)


# finalize the sampling
for pop_inst in pop_m2.values():
    pop_inst.finalize()
pop_stat.finalize()

# create overview plots (for the condense case)
plot = pop_stat.check_off_distribution_lik()
page.add_plot(plot.fig, "check_off_distribution-lik")

plot = pop_stat.check_off_distribution_far()
page.add_plot(plot.fig, "check_off_distribution-far")

# decide what to do: a general test or choose an onsource
if opts.analyze=='test':
    vec_lik = {}
    vec_ifar = {}
    
    for type in ['random','single','max']:
        print "Samples drawn from ", type
        
        dummy_lik = []
        dummy_ifar = []
        for i in range(5000):
            pop_stat.select_onsource(type)
            z_lik, z_ifar = pop_stat.compute_wmu()        
            dummy_lik.append(z_lik)
            dummy_ifar.append(z_ifar)

        vec_lik[type]=dummy_lik
        vec_ifar[type]=dummy_ifar    

    a1, a2 = create_hist(vec_lik['random'])
    b1, b2 = create_hist(vec_lik['single'])
    c1, c2 = create_hist(vec_lik['max'])     
    
    plt.clf()
    plt.plot(a1, a2, 'r-')
    plt.plot(b1, b2, 'b-')
    plt.plot(c1, c2, 'g-')
    plt.grid(True)
    plt.xlabel('sigma')
    plt.ylabel('number')
    plt.legend(('random','single','max'))
    page.add_plot(plt.gcf(), "test-lik")
        
    a1, a2 = create_hist(vec_ifar['random'])
    b1, b2 = create_hist(vec_ifar['single'])
    c1, c2 = create_hist(vec_ifar['max'])     
    
    plt.clf()
    plt.plot(a1, a2, 'r-')
    plt.plot(b1, b2, 'b-')
    plt.plot(c1, c2, 'g-')
    plt.grid(True)
    plt.xlabel('sigma')
    plt.ylabel('number')
    plt.legend(('random','single','max'))
    page.add_plot(plt.gcf(), "test-ifar")    
    
else:


    # prepare arrays to hold the data
    z_lik_by_m2 = numpy.zeros(opts.m2_nbins, float)
    z_ifar_by_m2 = numpy.zeros(opts.m2_nbins, float)
    p_by_m2 = numpy.zeros(opts.m2_nbins, float)

    # analyse each of the m2 pop instances
    for index_m2, (m2_low, m2_high) in \
            enumerate(zip(m2_bins.lower(), m2_bins.upper())):
                
        mass_bin = "%.1f-%.1f" % (m2_low, m2_high)
        pop_m2[mass_bin].select_onsource(opts.analyze)
        z_lik_by_m2[index_m2], z_ifar_by_m2[index_m2] = pop_m2[mass_bin].compute_wmu()
        p_by_m2[index_m2] = pop_m2[mass_bin].p_one_sided

        # create the plots
        plot = pop_m2[mass_bin].create_plot_hist()
        page.add_plot(plot.fig, "hist-"+mass_bin)
        plot = pop_m2[mass_bin].create_plot_qq()
        page.add_plot(plot.fig, "qq-"+mass_bin)

    # select either a fake trial or the box
    pop_stat.select_onsource(opts.analyze)
    z_lik, z_ifar = pop_stat.compute_wmu()

    plot = pop_stat.create_plot_hist()
    page.add_plot(plot.fig, "histCondense")
    plot = pop_stat.create_plot_qq()
    page.add_plot(plot.fig, "qqCondense")

    # write toe basic result to the output and to the web page
    text = "The result of the analysis for the condense statement is: "+\
               "%.2f (LIK)  %.2f (IFAR) sigma (i.e. %.3e Perc. )" %\
               (z_lik, z_ifar, 100.0*pop_stat.p_one_sided)
    page.write(text)

    
    # create the final plot
    plt.clf()
    plt.plot(m2_bins.centres(), z_lik_by_m2, 'r-', linewidth = 3)
    plt.plot(m2_bins.centres(), z_ifar_by_m2, 'b-', linewidth = 3)    
    plt.grid(True)
    plt.xlabel('m2')
    plt.ylabel('sigma')
    plt.legend(('likelihood','IFAR'))
    page.add_plot(plt.gcf(), "condense_sigma-"+opts.analyze)

    plt.clf()
    plt.plot(m2_bins.centres(), p_by_m2, 'r-', linewidth = 3)
    plt.grid(True)
    plt.xlabel('m2')
    plt.ylabel('p (one sided)')
    page.add_plot(plt.gcf(), "condense_prob-"+opts.analyze)

    

# create the html page    
page.write_page()
