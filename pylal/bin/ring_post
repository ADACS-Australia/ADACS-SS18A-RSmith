#!/usr/bin/python

import subprocess
import sys
import glob
import os
from glue import lal

from optparse import OptionParser

from glue.ligolw import ligolw
from glue.ligolw import table
from glue.ligolw import lsctables
from glue.ligolw import utils
from pylal import ligolw_tisi
from pylal import llwapp
from pylal import ligolw_cafe
from glue import pipeline
import ConfigParser
import tempfile
import string

def path_to_cache(pat, outname="ringfirst.cache"):
	#FIXME assumes a fixed file name for output cache file
	output = open(outname,"w")
	for l in glob.glob(pat):
		path, file = os.path.split(l)
		url = "file://localhost%s" % os.path.abspath(os.path.join(path, file))
		try:
			cache_entry = lal.CacheEntry.from_T050017(url)
		except ValueError, e:
			raise e
		print >>output, str(cache_entry)
	return outname

class Tisi(object):
#	#FIXME pick different time slide file name
#	def __init__(self, options, tisi_parse, filename="tisi.xml"):
#		self.tisi_parse = tisi_parse
#		self.time_slides = {}
#		self.dict_map()
#		self.convert(options)
#		self.filenames = [filename]
#		self.make_doc(options)
#		self.filename = filename

	#FIXME pick different time slide file name
	def __init__(self, options, tisi_parse, filename="tisi.xml"):
		self.tisi_parse = tisi_parse
		self.time_slides = {}
		self.dict_map()
		self.convert(options)
		self.filenames = [filename]
		self.make_doc(options)
		self.filename = filename

	def new_doc(self):
		doc = ligolw.Document()
		doc.appendChild(ligolw.LIGO_LW())
		proctable = lsctables.New(lsctables.ProcessTable)
		doc.childNodes[0].appendChild(proctable)
		procparamtable = lsctables.New(lsctables.ProcessParamsTable)
		doc.childNodes[0].appendChild(procparamtable)
		timeslidetable = lsctables.New(lsctables.TimeSlideTable)
		doc.childNodes[0].appendChild(timeslidetable)
		return doc

	def dict_map(self):
		for time_slide in ligolw_tisi.SlidesIter(self.tisi_parse):
		        self.time_slides[lsctables.TimeSlideTable.get_next_id()] = time_slide

	def convert(self,options):
		map(self.time_slides.pop, ligolw_tisi.time_slides_vacuum(self.time_slides, verbose = options.verbose).keys())
		self.time_slides = self.time_slides.items()
		self.time_slides.sort(reverse = True)


	def make_doc(self, options):
		while self.time_slides:
			doc = self.new_doc()
			timeslidetable = table.get_table(doc, lsctables.TimeSlideTable.tableName)
			process = ligolw_tisi.append_process(doc, **options.__dict__)
			N = int(round(float(len(self.time_slides)) / len(self.filenames)))
			while N:
				id, offsetdict = self.time_slides.pop()
				for row in ligolw_tisi.RowsFromOffsetDict(offsetdict, id, process):
					timeslidetable.append(row)
				N -= 1
			llwapp.set_process_end_time(process)
			self.filename = self.filenames.pop(0)
			utils.write_filename(doc, self.filename, options.verbose, gz = (self.filename or "stdout").endswith(".gz"))

#FIXME assumes cafe_ is base name
def cafe(cachenames, options, time_slide_file, base="cafe_"):
	cache = []
	for filename in cachenames:
		cache.extend(ligolw_cafe.load_cache(filename, options.verbose))
	seglists, outputcaches = ligolw_cafe.ligolw_cafe(cache, ligolw_tisi.load_time_slides(time_slide_file, verbose = options.verbose, gz = time_slide_file.endswith(".gz")).values(), options.verbose, extentlimit=3600)
	instruments = set(seglists.keys())
	return outputcaches, ligolw_cafe.write_caches(base, outputcaches, instruments, options.verbose)

class ring_post_DAG(pipeline.CondorDAG):
	def __init__(self, config_file, log_path):
		self.config_file = str(config_file)
		self.basename = self.config_file.replace('.ini','')
		tempfile.tempdir = log_path
		tempfile.template = self.basename + '.dag.log.'
		logfile = tempfile.mktemp()
		fh = open( logfile, "w" )
		fh.close()
		pipeline.CondorDAG.__init__(self,logfile)
		self.set_dag_file(self.basename)
		self.jobsDict = {}
		self.id = 0
		self.output_cache = []
		self.sqlite_cache_name=os.path.dirname(os.path.abspath(self.get_dag_file()))+"/"+self.basename+".cache"
	def add_node(self, node):
		self.id+=1
		pipeline.CondorDAG.add_node(self, node)
	def write_cache(self):
		f = open(self.sqlite_cache_name,"w")
		for c in self.output_cache:
			f.write(str(c)+"\n")
		f.close()

class ligolw_add_job(pipeline.CondorDAGJob):
	"""
	A ligolw_add job
	"""
	def __init__(self, cp, tag_base='LIGOLW_ADD'):
		"""
		"""
		self.__prog__ = 'ligolw_add'
		self.__executable = string.strip(cp.get('condor','ligolw_add'))
		self.__universe = "vanilla"
		pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
		self.add_condor_cmd('getenv','True')
		self.tag_base = tag_base
		self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")
		self.set_sub_file(tag_base+'.sub')
		self.set_stdout_file('logs/'+tag_base+'-$(macroid)-$(process).out')
		self.set_stderr_file('logs/'+tag_base+'-$(macroid)-$(process).err')


class ligolw_sqlite_job(pipeline.CondorDAGJob):
	"""
	A ligolw_sqlite job
	"""
	def __init__(self, cp, tag_base='LIGOLW_SQLITE'):
		"""
		"""
		self.__prog__ = 'ligolw_sqlite'
		self.__executable = string.strip(cp.get('condor','ligolw_sqlite'))
		self.__universe = "vanilla"
		pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
		self.add_condor_cmd('getenv','True')
		self.tag_base = tag_base
		self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")
		self.set_sub_file(tag_base+'.sub')
		self.set_stdout_file('logs/'+tag_base+'-$(macroid)-$(process).out')
		self.set_stderr_file('logs/'+tag_base+'-$(macroid)-$(process).err')
		self.add_opt("tmp-space",cp.get('input','tmp-space'))

class ligolw_sqlitex_job(pipeline.CondorDAGJob):
        """
        A ligolw_sqlite job
        """
        def __init__(self, cp, tag_base='LIGOLW_SQLITEX'):
                """
                """
                self.__prog__ = 'ligolw_sqlite'
                self.__executable = string.strip(cp.get('condor','ligolw_sqlite'))
                self.__universe = "vanilla"
                pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
                self.add_condor_cmd('getenv','True')
                self.tag_base = tag_base
                self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")
                self.set_sub_file(tag_base+'.sub')
                self.set_stdout_file('logs/'+tag_base+'-$(macroid)-$(process).out')
                self.set_stderr_file('logs/'+tag_base+'-$(macroid)-$(process).err')
                self.add_opt("tmp-space",cp.get('input','tmp-space'))

class lalapps_run_sqlite_job(pipeline.CondorDAGJob):
	"""
	A lalapps_run_sqlite job
	"""
	def __init__(self, cp, tag_base='LALAPPS_RUN_SQLITE'):
		"""
		"""
		self.__prog__ = 'lalapps_run_sqlite'
		self.__executable = string.strip(cp.get('condor','lalapps_run_sqlite'))
		self.__universe = "vanilla"
		pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
		self.add_condor_cmd('getenv','True')
		self.tag_base = tag_base
		self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")
		self.set_sub_file(tag_base+'.sub')
		self.set_stdout_file('logs/'+tag_base+'-$(macroid)-$(process).out')
		self.set_stderr_file('logs/'+tag_base+'-$(macroid)-$(process).err')
		self.add_opt("tmp-space",cp.get('input','tmp-space'))

class ligolw_add_node(pipeline.CondorDAGNode):
	"""
	"""
	def __init__(self, job, dag, cache, tisi_file, output, p_node=[]):
		pipeline.CondorDAGNode.__init__(self,job)
		#FIXME add tmp file space
		self.add_macro("macroid", dag.id)
		self.add_file_arg(tisi_file)
		self.add_var_opt("input-cache", cache)
		self.add_var_opt("output", output)
		self.add_output_file(output)
		for p in p_node:
			self.add_parent(p)
		dag.add_node(self)


class ligolw_rinca_job(pipeline.CondorDAGJob):
	"""
	A ligolw_rinca job
	"""
	def __init__(self, cp, tag_base='LIGOLW_RINCA'):
		"""
		"""
		self.__prog__ = 'ligolw_rinca'
		self.__executable = string.strip(cp.get('condor','ligolw_rinca'))
		self.__universe = "vanilla"
		pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
		self.add_condor_cmd('getenv','True')
		self.tag_base = tag_base
		self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")
		self.set_sub_file(tag_base+'.sub')
		self.set_stdout_file('logs/'+tag_base+'-$(macroid)-$(process).out')
		self.set_stderr_file('logs/'+tag_base+'-$(macroid)-$(process).err')

class lalapps_cbc_injfind_job(pipeline.CondorDAGJob):
        """
        A lalapps_cbc_injfind job
        """
        def __init__(self, cp, tag_base='LALAPPS_CBC_INJFIND'):
                """
                """
                self.__prog__ = 'lalapps_cbc_injfind'
                self.__executable = string.strip(cp.get('condor','lalapps_cbc_injfind'))
                self.__universe = "vanilla"
                pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
                self.add_condor_cmd('getenv','True')
                self.tag_base = tag_base
                self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")
                self.set_sub_file(tag_base+'.sub')
                self.set_stdout_file('logs/'+tag_base+'-$(macroid)-$(process).out')
                self.set_stderr_file('logs/'+tag_base+'-$(macroid)-$(process).err')

class lalapps_ringcorse_job(pipeline.CondorDAGJob):
        """
        A lalapps_ringcorse job
        """
        def __init__(self, cp, tag_base='LALAPPS_RINGCORSE'):
                """
                """
                self.__prog__ = 'lalapps_ringcorse'
                self.__executable = string.strip(cp.get('condor','lalapps_ringcorse'))
                self.__universe = "vanilla"
                pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
                self.add_condor_cmd('getenv','True')
                self.tag_base = tag_base
                self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")
                self.set_sub_file(tag_base+'.sub')
                self.set_stdout_file('logs/'+tag_base+'-$(macroid)-$(process).out')
                self.set_stderr_file('logs/'+tag_base+'-$(macroid)-$(process).err')

class ligolw_rinca_node(pipeline.CondorDAGNode):
        """
        """
        def __init__(self, job, dag, xml, ds_sq=0.5, extent=None, p_node=[]):
                pipeline.CondorDAGNode.__init__(self,job)
                #FIXME add tmp file space
                self.add_macro("macroid", dag.id)
                self.add_file_arg(xml)
                self.add_output_file(xml)
                self.add_var_opt("ds-sq-threshold", ds_sq)
                self.add_var_opt("save-small-coincs", "")
                if extent: self.add_var_opt("coinc-end-time-segment",":".join([str(t) for t in extent]))
                for p in p_node:
                        self.add_parent(p)
                dag.add_node(self)

class lalapps_cbc_injfind_node(pipeline.CondorDAGNode):
        """
        """
        def __init__(self, job, dag, xml_list, match_alg='ringdown',p_node=[]):
                pipeline.CondorDAGNode.__init__(self,job)
                #FIXME add tmp file space
                self.add_macro("macroid", dag.id)
                for x in xml_list: self.add_file_arg(x)
		#[sef.add_file_arg(x) for x in xml_list]
                self.add_var_opt("match-algorithm", match_alg)
                for p in p_node:
                        self.add_parent(p)
                dag.add_node(self)

class ligolw_sqlite_node(pipeline.CondorDAGNode):
	"""
	"""
	def __init__(self, job, dag, database, xml_list, p_node=[], replace=True, extract=False):
		pipeline.CondorDAGNode.__init__(self,job)
		#FIXME add tmp file s
		self.add_macro("macroid", dag.id)
		self.add_var_opt("database", database)
		if replace: self.add_var_opt("replace","")
		for xml in xml_list:
			self.add_file_arg(xml)
		dag.output_cache.append(lal.CacheEntry("- - - - file://localhost%s/%s" % (os.getcwd(),database)))
		for p in p_node:
			self.add_parent(p)
		dag.add_node(self)

class ligolw_sqlitex_node(pipeline.CondorDAGNode):
        """
        """
        def __init__(self, job, dag, database, extract, p_node=[]):
                pipeline.CondorDAGNode.__init__(self,job)
                #FIXME add tmp file s
                self.add_macro("macroid", dag.id)
                self.add_var_opt("database", database)
		self.add_var_opt("extract", extract)
                for p in p_node:
                        self.add_parent(p)
                dag.add_node(self)

class lalapps_run_sqlite_node(pipeline.CondorDAGNode):
	"""
	"""
	def __init__(self, job, dag, database, p_node=[]):
		pipeline.CondorDAGNode.__init__(self,job)
		#FIXME add tmp file s
		self.add_macro("macroid", dag.id)
		self.add_file_arg(database)
		self.add_file_opt("sql-file",cp.get('input','cluster_sql'))
		for p in p_node:
			self.add_parent(p)
		dag.add_node(self)

class lalapps_ringcorse_node(pipeline.CondorDAGNode):
        """
        """
        def __init__(self, job, dag, categories="frequency-ifos-oninstruments", rank_by="snr", frequency_bins="0,30,inf", live_time_program="lalapps_ring",p_node=[]):
                pipeline.CondorDAGNode.__init__(self,job)
                #FIXME add tmp file s
                self.add_macro("macroid", dag.id)
		# for x in database: self.add_file_arg(x)
		# self.add_file_arg(database)
		self.add_var_opt("input-cache",dag.sqlite_cache_name)
		self.add_var_opt("categories", categories)
                self.add_var_opt("rank-by", rank_by)
		self.add_var_opt("frequency-bins", frequency_bins)
		self.add_var_opt("live-time-program", live_time_program)
                for p in p_node:
                        self.add_parent(p)
                dag.add_node(self)

###
# NEW FUNCTIONS
###

def parse_cache(cachefile):
	c = {}
	[c.setdefault(lal.CacheEntry(l).description, []).append(l) for l in open(cachefile)]
	f = {}
	for k,v in c.items():
		if "RING_FIRST" in k:
			key = k.replace("RING_FIRST_","")
			f[key] = [key+".cache"]
			fp=open(f[key][0],"w")
			[fp.write(l) for l in v]
			fp.close()
	for k,v in f.items():
		for k1, v1 in c.items():
			# FIXME please make this sensible
			if "INJECTIONS" in k1 and not "COIRE" in k1 and k in k1:
				f[k].append(k1+".cache")
				fp=open(f[k][1],"w")
				[fp.write(l) for l in v1]
				fp.close()

	return f

###############################################################################
## MAIN #######################################################################
###############################################################################

def parse_command_line():
        parser = OptionParser(
		version = "%prog CVS $Id$",
		usage = "%prog [options] [filename ...]",
		description = "%prog FIXME"
		)
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")
	parser.add_option("-i", "--instrument", metavar = "name=first:last:step[,first:last:step[,...]]", action = "append", default = [], help = "Provide a description of the set of offsets to use for a particular instrument.  The set of offsets is (first + n * step) where n is an integer such that first <= offset <= last.  More than one set of offsets can be given for the same instrument, in which case the union is used.  As a short-hand, the sets can be combined into a single command line argument by separating the first:last:step triples with commas.")
	parser.add_option("-g", "--cache", help="cache of RING_FIRST files")
	parser.add_option("--comment", metavar = "text", help = "Set comment string in process table (default = None).")
	parser.add_option("--log-path", help = "set dagman log path")
	options, filenames = parser.parse_args()
	return options, filenames

#
# Parse command line 
#

opts, files = parse_command_line()

#
# SET UP THE DAG
#

try: os.mkdir("logs")
except: pass

cp = ConfigParser.ConfigParser()
#FIXME don't assume file name
ininame = "ring_post.ini"
cp.read(ininame)
ds_sq_threshold = float(cp.get("rinca","ds-sq-threshold"))
categories = str(cp.get("ringcorse","categories"))
rank_by = str(cp.get("ringcorse","rank-by"))
frequency_bins = str(cp.get("ringcorse","frequency-bins"))
live_time_program = str(cp.get("ringcorse","live-time-program"))
dag = ring_post_DAG(ininame, opts.log_path)

#ligolw_add
add_job = ligolw_add_job(cp)
add_node = {}

#rinca
rinca_job = ligolw_rinca_job(cp)
rinca_node = {}

#lalapps cbc injfind
injfind_job = lalapps_cbc_injfind_job(cp)
injfind_node = {}

#ligolw sqlite
sqlite_job = ligolw_sqlite_job(cp)
sqlite_node = {}

#ligolw sqlite
sqlitex_job = ligolw_sqlitex_job(cp)
sqlitex_node = {}

#lalapps run sqlite
runsql_job = lalapps_run_sqlite_job(cp)
runsql_node = {}

#lalapps ringcorse
ringcorse_job = lalapps_ringcorse_job(cp)
ringcorse_node = {}

#
# Parse cache file
#

cachenamestypes = parse_cache(opts.cache)
dbs = []

for (desc, cachefile) in cachenamestypes.items():
	if opts.verbose: print >> sys.stderr, "processing %s" % (desc,)

	#
	# injections get different time slide table,i.e. no slide
	#

	tisi_parse = ligolw_tisi.parse_slides(opts.instrument)
	if "INJ" not in desc: tisi = Tisi(opts,tisi_parse,filename = desc+"_tisi.xml")
	else: 
		for k in tisi_parse.keys(): tisi_parse[k] = [0.0]
		tisi = Tisi(opts,tisi_parse,desc+"_tisi.xml")

	cafe_caches, cafe_cache_files = cafe([cachefile[0]], opts, tisi.filename, base=desc+"_cafe")
	

	#Assemble dag
	for c, f in zip(cafe_caches, cafe_cache_files):
		# add the injection file if necessary, a hack to use --input-cache twice
		if "INJ" in desc: add_node[f] = ligolw_add_node(add_job, dag, f + " --input-cache " + cachefile[1], tisi.filename, f.replace(".cache",".xml.gz"))
		else: add_node[f] = ligolw_add_node(add_job, dag, f, tisi.filename, f.replace(".cache",".xml.gz"))
		rinca_node[f] = ligolw_rinca_node(rinca_job, dag, add_node[f].get_output_files()[0], ds_sq_threshold, extent=c.extent, p_node=[add_node[f]])
		db = f.replace(".cache",".sqlite")
		dbs.append(db)
		sqlite_node[f] = ligolw_sqlite_node(sqlite_job, dag, db, rinca_node[f].get_output_files(), p_node=[rinca_node[f]])
		runsql_node[f] = lalapps_run_sqlite_node(runsql_job, dag, db, p_node=[sqlite_node[f]])
		if "INJ" in desc:
			xml = db.replace('.sqlite','.xml.gz')
			sqlitex_node[f] = ligolw_sqlitex_node(sqlitex_job, dag, db, xml, p_node=[runsql_node[f]])
			injfind_node[f] = lalapps_cbc_injfind_node(injfind_job, dag, [xml], p_node=[sqlitex_node[f]])
			sqlite_node[f] = ligolw_sqlite_node(sqlite_job, dag, db, [xml], p_node=[injfind_node[f]])

ringcorse_node = lalapps_ringcorse_node(ringcorse_job, dag, categories, rank_by, frequency_bins, live_time_program, p_node=runsql_node.values())
dag.write_sub_files()
dag.write_dag()
dag.write_script()
dag.write_cache()
