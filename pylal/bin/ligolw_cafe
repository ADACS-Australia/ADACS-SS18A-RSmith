#!/usr/bin/python
#
# $Id$
#
# Copyright (C) 2006  Kipp C. Cannon
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#

"""
LIGO Light-Weight XML Coincidence Analysis Front End.
"""

import copy
from math import log10
from optparse import OptionParser
import sys

from glue import segments
from glue.lal import CacheEntry
from glue.ligolw import lsctables
from pylal import llwapp
from pylal.date import LIGOTimeGPS

__author__ = "Kipp Cannon <kipp@gravity.phys.uwm.edu>"
__date__ = "$Date$"[7:-2]
__version__ = "$Revision$"[11:-2]


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#

def parse_command_line():
	parser = OptionParser(version = "%prog CVS $Id$")
	parser.add_option("-s", "--single-instrument", action = "store_true", help = "select single instrument mode")
	parser.add_option("-t", "--time-slides", metavar = "filename", help = "read the time slide table from the given file")
	parser.add_option("-v", "--verbose", action = "store_true", help = "be verbose")
	parser.add_option("-b", "--base", metavar = "base", default = "cafe_", help = "set base for output caches")
	options, cachenames = parser.parse_args()

	if not options.time_slides:
		print >>sys.stderr, "warning: no time slide input file set, assuming 0 offsets"

	return options, cachenames

options, cachenames = parse_command_line()


#
# =============================================================================
#
#                                    Input
#
# =============================================================================
#

def load_cache(filename):
	if options.verbose:
		print >>sys.stderr, "loading %s" % filename
	cache = map(lambda l: CacheEntry(l, coltype = LIGOTimeGPS), file(filename))
	obs = cache[0].observatory
	for c in cache:
		if c.observatory != obs:
			raise ValueError, "mismatched observatories in %s" % filename
	return {obs: cache}


def cache_to_seglist(cache):
	return {cache[0].observatory: segments.segmentlist([c.segment for c in cache]).coalesce()}


def null_tisi(instruments):
	tisitable = lsctables.New(lsctables.TimeSlideTable, ["instrument", "offset"])
	for instrument in instruments:
		tisitable.append(lsctables.TimeSlide())
		tisitable[-1].instrument = instrument
		tisitable[-1].offset = 0.0
	return tisitable


try:
	caches = {}
	map(lambda filename: caches.update(load_cache(filename)), cachenames)
	seglists = {}
	map(lambda c: seglists.update(cache_to_seglist(c)), caches.itervalues())
	if options.time_slides:
		tisitable = llwapp.get_table(llwapp.load_filename(options.time_slides, options.verbose), lsctables.TimeSlideTable.tableName)
	else:
		tisitable = null_tisi(caches.keys())
except Exception, e:
	print >>sys.stderr, "error: %s" % str(e)
	sys.exit(1)


#
# =============================================================================
#
#                                 Preparation
#
# =============================================================================
#

#
# Extract the minimum and maximum offset for each instrument
#

def extreme_offsets(table):
	min = {}
	max = {}
	for row in table:
		if not min.has_key(row.instrument):
			min[row.instrument] = row.offset
		elif min[row.instrument] > row.offset:
			min[row.instrument] = row.offset
		if not max.has_key(row.instrument):
			max[row.instrument] = row.offset
		elif max[row.instrument] < row.offset:
			max[row.instrument] = row.offset
	return min, max

def check_instruments(offsetinstruments, cacheinstruments):
	for i in cacheinstruments:
		if i not in offsetinstruments:
			raise ValueError, "time slide table missing instrument %s" % i

minoffsets, maxoffsets = extreme_offsets(tisitable)
if options.verbose:
	print >>sys.stderr, "minimum offsets: %s" % str(minoffsets)
	print >>sys.stderr, "maximum offsets: %s" % str(maxoffsets)

try:
	check_instruments(minoffsets.keys(), caches.keys())
except Exception, e:
	print >>sys.stderr, "error: %s" % str(e)
	sys.exit(1)


#
# Dilate each segment list according to the extreme offsets for each
# instrument.
#

def dilate_seglist(l, min, max):
	return segments.segmentlist([segments.segment(s[0] + min, s[1] + max) for s in l]).coalesce()

def dilate_seglists(seglists, mins, maxs):
	for obs, segs in seglists.iteritems():
		seglists[obs] = dilate_seglist(segs, mins[obs], maxs[obs])

if options.verbose:
	print >>sys.stderr, "dilating segment lists..."
dilate_seglists(seglists, minoffsets, maxoffsets)


#
# Dilate each file's segment according to the extreme offsets for the
# corresponding instrument.
#

def dilate_cacheentry(cacheentry, min, max):
	cacheentry.segment = segments.segment(cacheentry.segment[0] + min, cacheentry.segment[1] + max)

def dilate_caches(caches, mins, maxs):
	caches = copy.deepcopy(caches)
	for obs, cache in caches.iteritems():
		min = mins[obs]
		max = maxs[obs]
		for c in cache:
			dilate_cacheentry(c, min, max)
	return caches

if options.verbose:
	print >>sys.stderr, "dilating file boundaries..."
dilatedcaches = dilate_caches(caches, minoffsets, maxoffsets)


#
# =============================================================================
#
#                                     Plan
#
# =============================================================================
#

#
# Compute the n-tuple intersection of the dilated lists.  Times not spanned
# by this list are those times for which no time slide can possibly produce
# coincident triggers, and thus this segment list provides the first
# estimate of the boundaries for subsequent coincidence jobs.
#

def coinc_segs(seglists):
	segs = ~segments.segmentlist()
	for l in seglists.itervalues():
		segs &= l
	return segs

if options.verbose:
	print >>sys.stderr, "computing segment list (pass 1)..."
coincsegs = coinc_segs(seglists)


#
# Merge coincident segments that intersect common trigger files.  If a
# single file can contribute triggers to more than one of the coincidence
# segments then those segments have to be analyzed together otherwise that
# file's triggers run the risk of being double-counted by appearing in more
# than one job.  The segment list produced by this operation defines the
# boundaries of coincidence jobs.
#

def merge_segs_by_file(segs, caches):
	for cache in caches.itervalues():
		for c in cache:
			spanned = segments.segmentlist([c.segment]) & segs
			if len(spanned) > 1:
				segs |= segments.segmentlist([spanned.extent()])
	return segs

if options.verbose:
	print >>sys.stderr, "computing segment list (pass 2)..."
merge_segs_by_file(coincsegs, dilatedcaches)


#
# =============================================================================
#
#                                    Output
#
# =============================================================================
#

def write_caches(base, coincsegs, dilatedcaches, caches):
	if len(coincsegs):
		pattern = "%%s%%0%dd.cache" % int(log10(len(coincsegs)) + 1)
	for n, seg in enumerate(coincsegs):
		filename = pattern % (base, n)
		if options.verbose:
			print >>sys.stderr, "writing %s" % filename
		f = file(filename, "w")
		for obs, dilatedcache in dilatedcaches.iteritems():
			for i in range(len(dilatedcache)):
				if dilatedcache[i].segment.intersects(seg):
					print >>f, str(caches[obs][i])


def write_single_instrument_caches(coincsegs, dilatedcaches, caches):
	for instrument in caches.keys():
		write_caches("%s_" % instrument, coincsegs, {instrument: dilatedcaches[instrument]}, {instrument: caches[instrument]})


if options.single_instrument:
	write_single_instrument_caches(coincsegs, dilatedcaches, caches)
else:
	write_caches(options.base, coincsegs, dilatedcaches, caches)
