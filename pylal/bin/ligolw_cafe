#!/usr/bin/python
#
# $Id$
#
# Copyright (C) 2006  Kipp C. Cannon
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#

"""
LIGO Light-Weight XML Coincidence Analysis Front End.
"""

from math import log10
from optparse import OptionParser
import sys

from glue import segments
from glue.lal import CacheEntry
from glue.ligolw import lsctables
from pylal import llwapp
from pylal import packing
from pylal.date import LIGOTimeGPS

__author__ = "Kipp Cannon <kipp@gravity.phys.uwm.edu>"
__date__ = "$Date$"[7:-2]
__version__ = "$Revision$"[11:-2]


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#

def parse_command_line():
	parser = OptionParser(version = "%prog CVS $Id$")
	parser.add_option("-s", "--single-instrument", action = "store_true", help = "select single instrument mode")
	parser.add_option("-t", "--time-slides", metavar = "filename", help = "read the time slide table from the given file")
	parser.add_option("-v", "--verbose", action = "store_true", help = "be verbose")
	parser.add_option("-b", "--base", metavar = "base", default = "cafe_", help = "set base for output caches")
	options, cachenames = parser.parse_args()

	if not options.time_slides:
		print >>sys.stderr, "warning: no time slide input file set, assuming 0 offsets"

	return options, cachenames

options, cachenames = parse_command_line()


#
# =============================================================================
#
#                                    Input
#
# =============================================================================
#

def load_cache(filename):
	if options.verbose:
		print >>sys.stderr, "loading %s" % filename
	cache = map(lambda l: CacheEntry(l, coltype = LIGOTimeGPS), file(filename))
	instrument = cache[0].observatory
	for c in cache[1:]:
		if c.observatory != instrument:
			raise ValueError, "mismatched observatories in %s" % filename
	return cache


def cache_to_seglistdict(cache):
	s = segments.segmentlistdict()
	for c in cache:
		if c.observatory in s:
			s[c.observatory].append(c.segment)
		else:
			s[c.observatory] = segments.segmentlist([c.segment])
	s.coalesce()
	return s


def get_time_slides(filename):
	doc = llwapp.load_filename(filename, options.verbose)
	tisitable = llwapp.get_table(doc, lsctables.TimeSlideTable.tableName)
	for row in tisitable:
		row.offset = LIGOTimeGPS(row.offset)
	doc.unlink()
	return map(tisitable.get_offset_dict, tisitable.dict.keys())


def null_time_slides(instruments):
	timeslide = {}
	for instrument in instruments:
		timeslide[instrument] = LIGOTimeGPS(0)
	return [timeslide]


try:
	cache = []
	map(lambda filename: cache.extend(load_cache(filename)), cachenames)
	seglists = cache_to_seglistdict(cache)
	if options.time_slides:
		timeslides = get_time_slides(filename)
	else:
		timeslides = null_time_slides(seglists.keys())
except Exception, e:
	print >>sys.stderr, "error: %s" % str(e)
	sys.exit(1)


#
# =============================================================================
#
#                             Output Cache Packing
#
# =============================================================================
#

def cacheentry_to_seglistdict(cacheentry):
	return segments.segmentlistdict({cacheentry.observatory: segments.segmentlist([cacheentry.segment])})


class OutputCache(packing.Bin):
	"""
	A representation of an output cache file.  The objects attribute
	contains the list of cache entries, and the size attribute holds a
	segmentlistdict summarizing the times spanned by the files in the
	cache.
	"""
	def __init__(self):
		packing.Bin.__init__(self)
		self.size = segments.segmentlistdict()


class CafePacker(packing.Packer):
	def set_time_slides(self, offsetdictlist):
		self.timeslides = offsetdictlist

	def pack(self, size, object):
		# find all bins in which this object belongs
		matching_bins = []
		for n, bin in enumerate(self.bins):
			for offsetdict in self.timeslides:
				size.offsets.update(offsetdict)
				bin.size.offsets.update(offsetdict)
				if size.is_coincident(bin.size):
					matching_bins.append((n, bin))
					break

		# reset all offsets
		size.offsets.clear()
		for bin in self.bins:
			bin.size.offsets.clear()

		# add object by either adding a new bin or putting it into
		# the first bin that was found
		if not matching_bins:
			self.bins.append(OutputCache())
			self.bins[-1].add_object(object, size)
			return
		matching_bins[0][1].add_object(object, size)

		# if object belongs in more than one bin, merge bins.
		# reverse the list of matching bins so that we delete the
		# highest-numbered bin first (otherwise the bins would
		# shift as we go and we would delete the wrong ones).
		matching_bins.reverse()
		for n, bin in matching_bins[:-1]:
			matching_bins[-1][1].objects.extend(bin.objects)
			matching_bins[-1][1].size += bin.size
			del self.bins[n]


#
# =============================================================================
#
#                               Pack Input Files
#
# =============================================================================
#

#
# For each instrument compute the times for which it will contribute
# triggers to a coincidence analysis.  Times not spanned by these lists are
# those times for which no time slide can possibly produce coincident
# triggers, and thus these lists provide the first estimate of the
# boundaries for subsequent coincidence jobs.
#

if options.verbose:
	print >>sys.stderr, "computing segment list..."
seglists = llwapp.get_coincident_segmentlistdict(seglists, timeslides)


#
# Find cache entries that intersect the surviving segments and pack into
# output caches.
#

outputcaches = []
packer = CafePacker(outputcaches)
packer.set_time_slides(timeslides)

if options.verbose:
	print >>sys.stderr, "packing files..."
for n, cacheentry in enumerate(cache):
	if options.verbose and not n % (len(cache)/1000):
		print >>sys.stderr, "	%.1f%% \r" % (100.0 * n / len(cache)),
	if seglists.intersects_segment(cacheentry.segment):
		packer.pack(cacheentry_to_seglistdict(cacheentry), cacheentry)
if options.verbose:
	print >>sys.stderr, "	100.0%%"


#
# =============================================================================
#
#                                    Output
#
# =============================================================================
#

def write_caches(base, bins, instruments):
	filenames = []
	if len(bins):
		pattern = "%%s%%0%dd.cache" % int(log10(len(bins)) + 1)
	for n, bin in enumerate(bins):
		filename = pattern % (base, n)
		filenames.append(filename)
		if options.verbose:
			print >>sys.stderr, "writing %s" % filename
		f = file(filename, "w")
		for cacheentry in bin.objects:
			if cacheentry.observatory in instruments:
				print >>f, str(cacheentry)
	return filenames


def write_single_instrument_caches(base, bins, instruments):
	for instrument in instruments:
		write_caches("%s%s_" % (base, instrument), bins, [instrument])


if options.single_instrument:
	write_single_instrument_caches(options.base, outputcaches, seglists.keys())
else:
	write_caches(options.base, outputcaches, seglists.keys())
