#!/usr/bin/python
__author__ = "Ruslan Vaulin <vaulin@gravity.phys.uwm.edu>, Rahul Biswas <rahul@gravity.phys.uwm.edu>"
__version__ = "$Revision$"[11:-2]
__date__ = "$Date$"[7:-2]
__prog__="findCutEfficiency"
__Id__ = "$Id$"


#loading standard modules
from optparse import *
import glob
import sys
#loading modules used for input/output of data 
from glue import lal
from glue.ligolw import lsctables
from pylal import CoincInspiralUtils
from pylal import SnglInspiralUtils
from pylal import SimInspiralUtils
from pylal import InspiralUtils
from glue.ligolw import ligolw
#loading modules used in calculations
import glue.iterutils
from pylal import inspiral_likelihood
import numpy
numpy.seterr("raise")
import cmath
from pylal import rate


############################################################################################
# Definitions of functions
############################################################################################

################################################################################
# Main program
################################################################################
usage= """
usage: %prog [options]

This code is a diagnostic tool for achieving optimal tuning of the threshold. Currently it does it for e-thinca parameter only.
"""
###############################################################################
# Options to read in Input
###############################################################################
def parse_command_line():

  """
  Parser function dedicated
  """

  parser = OptionParser( usage=usage, version="%prog CVS $Id$ " )

  parser.add_option("","--slides-glob",action="store",type="string",\
      default=None, metavar=" GLOB",help="GLOB time slides thinca files to read" )
	
  parser.add_option("","--found-injection-glob",action="store",type="string",\
      default=None, metavar=" GLOB",help="GLOB thinca files with found injections to read" )
	
  parser.add_option("","--missed-injection-glob",action="store",type="string",\
      default=None, metavar=" GLOB",help="GLOB thinca files with missed injections to read" )

  parser.add_option("","--input-cache-file",action="store",type="string",\
      default=None, metavar="ZEROLAGCACHEFILE",help="name of the cache file including the path" )

  parser.add_option("","--slides-pattern",\
      default="", metavar="SLIDESPATTERN", help="the time slides files pattern the cache file, specified by --input-cache-file option, will be seived with.")
	
  parser.add_option("","--found-injection-pattern",\
      default="", metavar="INJSPATTERN", help="the found injections files pattern the cache file, specified by --input-cache-file option, will be seived with.")

  parser.add_option("","--missed-injection-pattern",\
      default="", metavar="INJSPATTERN", help="the missed injections files pattern the cache file, specified by --input-cache-file option, will be seived with.")


################################################################################
# Options for plots and histograms
#################################################################################### Options to select ifo types.

  parser.add_option("", "--h1-triggers",action="store_true", default=False,\
      help="input files contain triggers from H1")

  parser.add_option("", "--h2-triggers",action="store_true", default=False,\
      help="input files contain triggers from H2")

  parser.add_option("", "--l1-triggers",action="store_true", default=False,\
      help="input files contain triggers from L1")

  parser.add_option("", "--g1-triggers",action="store_true", default=False,\
      help="input files contain triggers from G1")

  parser.add_option("", "--v1-triggers",action="store_true", default=False,\
      help="input files contain triggers from V1")

  parser.add_option("","--statistic",action="store",default='snr',\
      type="string",\
      help="choice of statistic used in building coinc table, valid arguments are: snr (DEFAULT), snr_over_chi, s3_snr_chi_stat, effective_snr, bitten_l, bitten_lsq, ifar, lvS5stat")
	
  parser.add_option("","--h1-slide-time",action="store",type="int",default=0,\
      metavar="SEC",help="time slid for H1 per slide number" )

  parser.add_option("","--h2-slide-time",action="store",type="int",default=10,\
      metavar="SEC",help="time slid for H2 per slide number" )

  parser.add_option("","--l1-slide-time",action="store",type="int",default=5,\
      metavar="SEC",help="time slid for L1 per slide number" )
	
  parser.add_option("","--g1-slide-time",action="store",type="int",default=0,\
      metavar="SEC",help="time slid for G1 per slide number" )

  parser.add_option("","--v1-slide-time",action="store",type="int",default=5,\
      metavar="SEC",help="time slid for V1 per slide number" )
	
  parser.add_option("", "--use-likelihood",action="store_true", default=False,\
      help="enables likelihood to be used as detection statistic")
	  
  parser.add_option("", "--enable-clustering",action="store_true", default=False,\
      help="enable clustering procedure in case input data is unclustered")

  parser.add_option("", "--skip-timeslides",action="store_true", default=False,\
      help="skip calculation of threshold statistic using time slides. The threshold statistic should be given explicitly.")
		
  parser.add_option("","--threshold-statistic", action="store",type="float",\
      default = None, help="threshold statistic. This option should be used together with --skip-timeslides" )
		
  parser.add_option("","--num-slides", action="store",type="int",\
      default = 0, metavar="numslides", help="number of time slides performed, must match the corresponding parameter from the .ini file of the search" )
	
  parser.add_option("", "--get-ROC-curve",action="store_true", default=False,\
      help="Get full ROC curve")
  
  parser.add_option("","--ignore-IFO-times",action="store",type="string",\
      default='', metavar=" USERTAG",\
      help="comma separated IFO times that should not be included in efficiency calculation e.g. H1H2,H2L1. This option will work only with s5 LV search files." )
  
  parser.add_option("","--show",action="store_true",default=False,\
      help="display the figures on the terminal" )
	
  parser.add_option("","--verbose", action="store_true",\
      default=False, help="print information" )

  parser.add_option("-u","--user-tag",action="store",type="string",\
      default=None, metavar=" USERTAG",\
      help="The user tag used in the name of the figures" )

  
  parser.add_option("-P","--output-path",action="store",\
      type="string",default=None,  metavar="PATH",\
      help="path where the figures would be stored")


  parser.add_option("-O","--enable-output",action="store_true",\
      default="false",  metavar="OUTPUT",\
      help="enable the generation of the html and cache documents")


  parser.add_option("", "--figure-resolution",action="store",type="int",\
      default=50, metavar="FIGURERESOLUTION", \
      help="resolution of the thumbnails (50 by default)" )

  parser.add_option("", "--html-for-cbcweb",action="store",\
      default=False, metavar = "CVS DIRECTORY", help="publish the html "\
      "output in a format that can be directly published on the cbc webpage "\
      "or in CVS. This only works IF --enable-output is also specified. The "\
      "argument should be the cvs directory where the html file will be placed "\
      "Example: --html-for-cbcweb protected/projects/s5/yourprojectdir")


  (opts,args) = parser.parse_args()

  return opts, sys.argv[1:]
#####################################################################
opts, args = parse_command_line()


# Sanity checks
######################################################################

if opts.skip_timeslides and not opts.threshold_statistic:
  print >> sys.stderr, "--threshold-statistic must be set if --skip-timeslides option is used"
  sys.exit(1)
  
if not opts.skip_timeslides and not opts.num_slides:
  print >> sys.stderr, "number of time slides should be provided if running with time slides triggers,"
  print >> sys.stderr, "use --num-slides option"
  sys.exit(1)
   
  

if not (opts.input_cache_file or (opts.found_injection_glob and opts.missed_injection_glob)):
  print >>sys.stderr, "Some of the options specifying the input files containing single inspiral tables are missing." 
  print >> sys.stderr, " Either (--input-cache-file option) or (--slides-glob, --found-injection-glob and --missed-injection-glob options) must be given."
  sys.exit(1)
  
# Initializing the html output
InspiralUtils.message(opts, "Initialisation...")
opts = InspiralUtils.initialise(opts, __prog__, __version__)
fnameList = []
tagList = []
fig_num = 0
comments = ""

# constructing the list of the IFO's
ifo_list = [ifo for ifo in ("G1", "H1", "H2", "L1", "V1") \
            if getattr(opts, "%s_triggers" % ifo.lower())]

#constructing shift vector for time slides.
if not opts.skip_timeslides:
  slides_shift_vector = {}
  for ifo in ifo_list:
    slides_shift_vector[ifo] = getattr(opts, "%s_slide_time" % ifo.lower())

			

#loading matplotlib module that can be used on a cluster
if not opts.show:
  import matplotlib
  matplotlib.use('Agg')
from pylab import*
import copy

#Calculating statistic for coincidences
statistic = CoincInspiralUtils.coincStatistic(opts.statistic) 

  
# contsructing lists of data files containing time slides and injections triggers respectively
########################################################################################################	
if opts.input_cache_file:
  InspiralUtils.message(opts, "Reading input-cache-file ...")
  slidesfiles = []
  found_injfiles = []
  missed_injfiles = []
  SnglInspiralCache = lal.Cache.fromfile(open(opts.input_cache_file))
  if not opts.skip_timeslides:
    slidesfiles = SnglInspiralCache.sieve(description = opts.slides_pattern, exact_match=True).checkfilesexist()[0].pfnlist()
  found_injfiles = SnglInspiralCache.sieve(description = opts.found_injection_pattern, exact_match=True).checkfilesexist()[0].pfnlist()
  missed_injfiles = SnglInspiralCache.sieve(description = opts.missed_injection_pattern, exact_match=True).checkfilesexist()[0].pfnlist()
else:
  slidesfiles = []
  found_injfiles = []
  missed_injfiles = []
  if not opts.skip_timeslides:
    slidesfiles = glob.glob(opts.slides_glob)
  found_injfiles = glob.glob(opts.found_injection_glob)
  missed_injfiles = glob.glob(opts.missed_injection_glob)
  
# check if file lists are not empty
if not opts.skip_timeslides:
  if not len(slidesfiles) > 0:
    print >>sys.stderr, "List of time slides files is empty: your sieve pattern may be wrong or files do not exist in the location given by the cache file"
    sys.exit(1)
if  not len(found_injfiles) > 0:
  print >>sys.stderr, "List of found injections files is empty: your sieve pattern may be wrong or files do not exist in the location given by the cache file"
  sys.exit(1) 
if  not len(missed_injfiles) > 0:
  print >>sys.stderr, "List of missed injections files is empty: your sieve pattern may be wrong or files do not exist in the location given by the cache file"
  sys.exit(1) 



# get rid of certain IFO times if necessary
if opts.ignore_IFO_times:

  ifo_times_to_ignore = opts.ignore_IFO_times.split(",")
  
  # time slides
  new_slidesfiles = []
  for file in slidesfiles:
	if not (file.split("/")[-1].split("_")[0] in ifo_times_to_ignore):
	  new_slidesfiles.append(file)
  
  slidesfiles = []
  slidesfiles = new_slidesfiles
  
  # found injections
  new_found_injfiles = []
  for file in found_injfiles:
	if not (file.split("/")[-1].split("_")[0] in ifo_times_to_ignore):
	  new_found_injfiles.append(file)
  
  found_injfiles = []
  found_injfiles = new_found_injfiles
  
  # missed injections
  new_missed_injfiles = []
  for file in missed_injfiles:
	if not (file.split("/")[-1].split("-")[0] in ifo_times_to_ignore):
	  new_missed_injfiles.append(file)
  
  missed_injfiles = []
  missed_injfiles = new_missed_injfiles


################################################################################################################
# DETECTOR'S CHARACTERISTICS: Here we assess performance of the detector by calculating various statistical
# measures such as Detection probability for a given False Alarm Probability, efficiency etc. 
# The relevant plots are generated at the end.
################################################################################################################

if not opts.skip_timeslides:
  # Calculate Likelihood threshold based on fixed False Alarm Probability.
  # The acceptable value of false alarm probability ios taken to be 0.1
  InspiralUtils.message(opts, "calculating threshold statistic ...")

  # define array that stores maximum statistic for each of the time slide
  max_stat_array = numpy.zeros(2*opts.num_slides, dtype = float)

  InspiralUtils.message(opts," reading in time slides ...")
  for file in slidesfiles:
    # read in time slides triggers 
    slidesTriggers = None
    InspiralUtils.message(opts," reading in " + file)
    slidesTriggers = SnglInspiralUtils.ReadSnglInspiralFromFiles([file], non_lsc_tables_ok=True)
    InspiralUtils.message(opts,"reconstructing coins ...")
    # construct the time slides coincs
    slidesCoincTriggers = CoincInspiralUtils.coincInspiralTable(slidesTriggers, statistic)

    # read InspiralLikelihoodTable if necessary and add likelihood values to coincs  
    if opts.use_likelihood:
      slidesLikelihoodTriggers = inspiral_likelihood.ReadInspiralLikelihoodFromFiles([file])
      # add likelihood values to coincs
      inspiral_likelihood.add_likelihood(slidesCoincTriggers, slidesLikelihoodTriggers)       


    for slide in range(1, opts.num_slides + 1):
      #  triggers in each time slide are sorted in descending order in statistic which is passed to an array

      # for slide forward
      # get coincs from the current slide
      forward_slide_coincs = slidesCoincTriggers.getslide(slide)

      # store this slide's maximum statistic
      if len(forward_slide_coincs) > 0:
        if opts.use_likelihood:
          max_stat_array[slide - 1] = max(max_stat_array[slide - 1], numpy.max(forward_slide_coincs.getlikelihood()))
        else:
           max_stat_array[slide - 1] = max(max_stat_array[slide - 1], numpy.max(forward_slide_coincs.getstat()))
 	
      # for slide backward
      # get coincs from the current slide
      backward_slide_coincs = slidesCoincTriggers.getslide(-slide)

      # store this slide's  maximum statistic
      if len( backward_slide_coincs) > 0:    
        if opts.use_likelihood:
          max_stat_array[slide - 1 + opts.num_slides] = max(max_stat_array[slide - 1 + opts.num_slides], numpy.max(backward_slide_coincs.getlikelihood()))
        else:
          max_stat_array[slide - 1 + opts.num_slides] = max(max_stat_array[slide - 1 + opts.num_slides], numpy.max(backward_slide_coincs.getstat()))


      # end of the loop over slides
	
  # calculate the threshold statistic
  # WARNING: it is hardcoded to be the 11th highest max stat in slide
  # if number of time slides is 100 it corresponds to 10% false alarm probability. 
  max_stat_array.sort()
  stat_threshold = max_stat_array[-11]
  InspiralUtils.message(opts, "threshold statistic is " + str(stat_threshold))
  
  mean_effective_snr = max_stat_array.mean()
  InspiralUtils.message(opts, "mean loudest effective snr: " + str(mean_effective_snr))
  
  sigma = max_stat_array.std()
  InspiralUtils.message(opts, "standard deviation of loudest effective snr: " + str(sigma))
  
  definite_detection_threshold = mean_effective_snr + 2.0 * sigma
  InspiralUtils.message(opts, "two-sigma detection threshold: " + str(definite_detection_threshold))
else:
  InspiralUtils.message(opts, "Skiping time slides...")
  InspiralUtils.message(opts, "using threshold statistic " + str(opts.threshold_statistic))
  stat_threshold = opts.threshold_statistic


InspiralUtils.message(opts,"Reading found injections ...")
# read in injection's sngl inspiral table
injectionTriggers = None
injectionTriggers = SnglInspiralUtils.ReadSnglInspiralFromFiles(found_injfiles, mangle_event_id=False, non_lsc_tables_ok=True)

InspiralUtils.message(opts," reconstructing coincs for found injections")

# construct coincidence for injections  
injectionCoincTriggers = CoincInspiralUtils.coincInspiralTable(injectionTriggers, statistic)

# read in sim inspirals if the injection triggers were already clustered with COIRE
if not opts.enable_clustering:
  InspiralUtils.message(opts, "adding sim inspirals to found injections ...")
  found_simTriggers = None
  found_simTriggers = SimInspiralUtils.ReadSimInspiralFromFiles(found_injfiles)
  #add sim inspirals
  injectionCoincTriggers.add_sim_inspirals(found_simTriggers)

# read InspiralLikelihoodTable if necessary  
if opts.use_likelihood:
  found_injLikelihoodTriggers = inspiral_likelihood.ReadInspiralLikelihoodFromFiles(found_injfiles)
  # add likelihood values to coincs
  inspiral_likelihood.add_likelihood(injectionCoincTriggers, found_injLikelihoodTriggers)       
    
	  
# read in sim inspirals with missed injections
if not opts.enable_clustering:
  InspiralUtils.message(opts,"reading in missed injections ...")
  missed_simTriggers = None
  missed_simTriggers = SimInspiralUtils.ReadSimInspiralFromFiles(missed_injfiles)
  missed_simTriggers_original = lsctables.New(lsctables.SimInspiralTable)
  missed_simTriggers_original = copy.copy(missed_simTriggers) 
  

# construct the list of found and missed injections

InspiralUtils.message(opts, "partitioning found injections ...")
# first partition found injections using the threshold statistic
if opts.use_likelihood:
  lesser_coincs, equal_coincs, greater_coincs = injectionCoincTriggers.partition_by_stat(stat_threshold, use_likelihood=True) 
else:
  lesser_coincs, equal_coincs, greater_coincs = injectionCoincTriggers.partition_by_stat(stat_threshold)   		

# construct list of all missed injections
if len(lesser_coincs) > 0:
  missed_simTriggers.extend(lesser_coincs.return_sim_inspirals())

# construct list of all found injections
if len(equal_coincs) > 0:  
  greater_coincs.extend(equal_coincs)
found_simTriggers = greater_coincs.return_sim_inspirals()

#Efficiency curve
InspiralUtils.message(opts, "calculating efficiency ...")
# determine the range

right_limit = max(max(found_simTriggers.get_column("distance")), max(missed_simTriggers.get_column("distance")))
left_limit =  min(min(found_simTriggers.get_column("distance")), min(missed_simTriggers.get_column("distance")))

# histogram found injections, unweighted
number_found, found_bins = inspiral_likelihood.weighted_histogram(found_simTriggers.get_column("distance"), weight=None, norm=False, nbins=20, set_max=right_limit, set_min=left_limit)

# histogram missed injections
number_missed, missed_bins = inspiral_likelihood.weighted_histogram(missed_simTriggers.get_column("distance"), weight=None, norm=False, nbins=20, set_max=right_limit, set_min=left_limit)

# calculate efficiency
efficiency = number_found/(number_found + number_missed)

# generate the plot
plot(missed_bins, efficiency)
xlabel("Distance, Mpc")
title("Efficiency curve")
if opts.enable_output:
  name = "_efficiency"
  fname = InspiralUtils.set_figure_name(opts, name)
  fname_thumb = InspiralUtils.savefig_pylal(filename=fname, doThumb=True, dpi_thumb=opts.figure_resolution)
  fnameList.append(fname)
  tagList.append("Efficiency curve")
if not opts.show:
  close()

# write efficiency curve in file
efficiency_file = open("efficiency.txt", "w")

for i in range(len(efficiency)):
  efficiency_file.write(str(missed_bins[i]) + " " + str(efficiency[i]) + "\n")
efficiency_file.close()

InspiralUtils.message(opts, "calculating detection pdf ...")
# calculate detection probability density function
total_volume = (4.0*numpy.pi/3.0) * (right_limit**3)
detection_pdf = 4.0*numpy.pi*(efficiency * missed_bins**2)/total_volume

# calculate detection probability
Qd = numpy.sum(detection_pdf)*(missed_bins[1] - missed_bins[0])
InspiralUtils.message(opts, "detection prbability of the detector is " + str(Qd))
# write detection pdf in file
detection_pdf_file = open("detection_pdf.txt", "w")

for i in range(len(detection_pdf)):
  detection_pdf_file.write(str(missed_bins[i]) + " " + str(detection_pdf[i]) + "\n")
detection_pdf_file.close()

# generate plot of detection probability density function
plot(missed_bins, detection_pdf)
xlabel("Distance, Mpc")
title("Detection probability density function")

if opts.enable_output:
  name = "_detection_probabilty"
  fname = InspiralUtils.set_figure_name(opts, name)
  fname_thumb = InspiralUtils.savefig_pylal(filename=fname, doThumb=True, dpi_thumb=opts.figure_resolution)
  fnameList.append(fname)
  tagList.append("Detection probability density function")
if not opts.show:
  close()

# calculate cumulative detection probability function

cum_det_function = numpy.zeros(len(detection_pdf))
cum_sum = 0
for i in range(len(cum_det_function)):
  cum_det_function[i] = cum_sum + detection_pdf[i]*(missed_bins[1] - missed_bins[0])
  cum_sum += detection_pdf[i]*(missed_bins[1] - missed_bins[0])
  
# write cumulative detection probability in file
cum_det_function_file = open("cumulative_det_prob.txt", "w")

for i in range(len(cum_det_function)):
  cum_det_function_file.write(str(missed_bins[i]) + " " + str(cum_det_function[i]) + "\n")
cum_det_function_file.close()

# generate plot of detection probability density function
plot(missed_bins, cum_det_function)
xlabel("Distance, Mpc")
title("Cumulative detection probability function")

if opts.enable_output:
  name = "_cumulative_detection_probabilty"
  fname = InspiralUtils.set_figure_name(opts, name)
  fname_thumb = InspiralUtils.savefig_pylal(filename=fname, doThumb=True, dpi_thumb=opts.figure_resolution)
  fnameList.append(fname)
  tagList.append("Cumulative detection probability function")
if not opts.show:
  close()

# write sim inspiral table of found injections into a file
found_injection_file = open(opts.user_tag + "_FOUND.xml", "w")
# create xml doc for found injections
found_doc = ligolw.Document()
#create LIGO_LW element for found injections
found_ligo_lw = ligolw.LIGO_LW()
#append it to xml doc
found_doc.appendChild(found_ligo_lw)
#append sim table with found injections to the xml doc
found_doc.childNodes[0].appendChild(found_simTriggers)
#writing the missed injections xml doc into a file
found_doc.write(found_injection_file)
found_injection_file.close()

#write missed injections into a file
missed_injection_file = open(opts.user_tag + "_MISSED.xml", "w")
# create xml doc for missed injections
missed_doc = ligolw.Document()
#create LIGO_LW element for missed injections
missed_ligo_lw = ligolw.LIGO_LW()
#append it to xml doc
missed_doc.appendChild(missed_ligo_lw)
#append sim table with missed injections to the xml doc
missed_doc.childNodes[0].appendChild(missed_simTriggers)
#writing the missed injections xml doc into a file
missed_doc.write(missed_injection_file)
missed_injection_file.close()
 
 
#Calculating full ROC curve 
if opts.get_ROC_curve:
  max_stat_array.sort()
  InspiralUtils.message(opts, "calculating ROC curve ...")
  Det_probabilities = numpy.ones(2*opts.num_slides)
  number_found_injections = numpy.zeros(2*opts.num_slides)
  number_missed_injections = numpy.zeros(2*opts.num_slides)
  for i in range(len(Det_probabilities)):
	# first partition found injections using the threshold statistic
	stat_threshold = max_stat_array[-(i+1)]
	if opts.use_likelihood:
	  lesser_coincs, equal_coincs, greater_coincs = injectionCoincTriggers.partition_by_stat(stat_threshold, use_likelihood=True) 
	else:
	  lesser_coincs, equal_coincs, greater_coincs = injectionCoincTriggers.partition_by_stat(stat_threshold)   	
	  	
	
	# construct list of all missed injections
	missed_simTriggers_at_threshold = lsctables.New(lsctables.SimInspiralTable)
	missed_simTriggers_at_threshold = copy.copy(missed_simTriggers_original) 
	
	if len(lesser_coincs) > 0:
	  missed_simTriggers_at_threshold.extend(lesser_coincs.return_sim_inspirals())

	# construct list of all found injections
	found_simTriggers_at_threshold = lsctables.New(lsctables.SimInspiralTable)
	if len(equal_coincs) > 0:  
	  greater_coincs.extend(equal_coincs)
	found_simTriggers_at_threshold = greater_coincs.return_sim_inspirals()
	
	# save number of dound and missed injections
	number_found_injections[i] = len(found_simTriggers_at_threshold)
	number_missed_injections[i] = len(missed_simTriggers_at_threshold)
	
	
	#Efficiency curve
	# determine the range
	right_limit = max(max(found_simTriggers_at_threshold.get_column("distance")), max(missed_simTriggers_at_threshold.get_column("distance")))
	left_limit =  min(min(found_simTriggers_at_threshold.get_column("distance")), min(missed_simTriggers_at_threshold.get_column("distance")))

	# histogram found injections, unweighted
	number_found, found_bins = inspiral_likelihood.weighted_histogram(found_simTriggers_at_threshold.get_column("distance"), weight=None, norm=False, nbins=20, set_max=right_limit, set_min=left_limit)

	# histogram missed injections
	number_missed, missed_bins = inspiral_likelihood.weighted_histogram(missed_simTriggers_at_threshold.get_column("distance"), weight=None, norm=False, nbins=20, set_max=right_limit, set_min=left_limit)

	# calculate efficiency
	efficiency = number_found/(number_found + number_missed)

	# calculate detection probability density function
	total_volume = (4.0*numpy.pi/3.0) * (right_limit**3)
	detection_pdf = 4.0*numpy.pi*(efficiency * missed_bins**2)/total_volume

	# calculate detection probability
	Det_probabilities[i] = numpy.sum(detection_pdf)*(missed_bins[1] - missed_bins[0])
	
  # write cumulative detection probability in file
  ROC_file = open("ROC_curve.txt", "w")
  
  ROC_file.write("FAP, DEP, stat*, Found, Missed \n")
  for i in range(len(Det_probabilities)):
	ROC_file.write(str(float(i)/100.0) + " " + str(Det_probabilities[i]) + " " + str(max_stat_array[-(i+1)]) + " " + str(number_found_injections[i])  + " " + str(number_missed_injections[i]) +  "\n")
  ROC_file.close()

		
##############################################################################################################

if opts.enable_output:
  html_filename = InspiralUtils.write_html_output(opts, args, fnameList, tagList, comment=comments)
  InspiralUtils.write_cache_output(opts, html_filename, fnameList)

  if opts.html_for_cbcweb:
    html_filename_publish = InspiralUtils.write_html_output(opts, args, fnameList, tagList, cbcweb=True)

##############################################################################################################
			
			  
				
				  
					
					  
						
						  
							
							  
								
								  
									
									    
