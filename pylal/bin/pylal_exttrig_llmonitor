#!/usr/bin/python

import os
import sys
import pickle
import time
import subprocess
import optparse
import ConfigParser

import matplotlib
matplotlib.use('Agg')

from pylal import git_version
from pylal import pylal_exttrig_llutils as peu

###################################################
usage = """usage: pylal_exttrig_monitor config_file

to be executed as a cron job with e.g. the following entry:

*/5  *  *    *    *   /bin/sh /home/dietz/Work/E14/Code/pylal_exttrig_llmonitor.sh >> ~/.llmonitor.log 2>&1

The only argument is the config_file, containing some
important decleration of paths etc. Example for UWM:


paths]
gcn_file = /home/dietz/Work/E14/GCN-testing/gcn.file
pylal_exttrig_llstart = /home/dietz/Work/E14/Code/pylal_exttrig_llstart
main = /home/dietz/Work/E14/Analysis
publishing_path = /home/dietz/public_html/E14/
publishing_url = https://ldas-jobs.phys.uwm.edu/~dietz/E14/
cvs = /home/dietz/Work/E14/Pseudo_CVS
glue = /opt/lscsoft/glue
lalapps = /home/dietz/opt/Install/s5_2yr_lv_lowcbc_20090410a

[data]
h1_segments = E14_H1_science.txt
l1_segments = E14_L1_science.txt
v1_segments = E14_V1_science.txt
ini_file = S6GRB_trigger_hipe.ini

[notifications]
email = alexander.dietz@lapp.in2p3.fr


[run]
log_path = /people/dietz
ini_file = S6GRB_trigger_hipe.ini

"""


# -----------------------------------------------------
def check_proxy():
  """
  Checks the status of the grid-proxy and sends an email 
  if it runs out
  """

  my_email = 'Alexander.Dietz@lapp.in2p3.fr'
  cmd = 'grid-proxy-info > grid.info'
  peu.system_call('monitor',cmd, False)

  flag = False  
  for line in open('grid.info'):
    if 'timeleft' in line:
      flag = True
      time_in_hours = int(line.split()[2].split(':')[0])
      if time_in_hours<40:
        peu.send_mail('Grid Cert aging, %d hleft'%time_in_hours,'Have to renew grid certificate',[my_email])
      if time_in_hours==0:
        peu.send_mail('Grid Cert expired. Suspended further actions.',\
                      'Need renew grid certificate NOW.',[my_email])
        raise SystemError, "Grid certificate is invalid."

  if not flag:
    peu.send_mail('Grid Cert not available','Have to init grid certificate',[my_email])

# -----------------------------------------------------
def check_status(grb, dag_key):
  """
  check the status of the given output file of a DAG
  @params grb: grb dictionary with all information in it
  @params dag_key: the key of the DAG to be checked
  """

  # get the dagman.out file
  dag = grb['dags'][dag_key]
  filename = grb_dict['path']+'/'+dag['filename']+'.dagman.out'

  # try to open the dagman.out file
  status = 0
  try:
    line = file(filename).readlines()[-1]
    if "EXITING WITH STATUS 1" in line:
      status = -1
    if "EXITING WITH STATUS 0" in line:
      status = 1
#    for line in file(filename).readlines()[-15:]:
#      if 'Aborting DAG...' in line:
#        status = -1
#      if 'All jobs Completed!' in line:
#        status = 1
  except IOError:
    status = -2

  # update the DAG status in case of an error
  dag_status = dag['status']

  if dag_status>0:
    if status==-1:
      peu.notify(grb_dict, 'DAG exited on error')
      dag_status = -dag_status
    elif status==-2:
      peu.notify(grb_dict, 'DAG file vanished!?')
      dag_status = -6

  # set the new status, if changes
  dag['status']=dag_status    

  return status


# -----------------------------------------------------
def start_new_analysis(monitor_list, grb_name, grb_ra, grb_dec, grb_time):
  """
  Start a new analysis of a GRB with a separate call.
  All data are given in strings.
  """

  # First check if enough time has passed
  # since the trigger-time
  gps_now = time.time() - peu.offset_gps_to_linux
  time_diff = gps_now-int(grb_time)
  if time_diff < int(cp.get('data','min_diff')):
    # do not analyze this GRB now.
    # postpone to later time
    peu.info("GRB%s: Analysis postponsed because time difference too small")
    return 

  analysis_path = cp.get('paths','main')+'/GRB'+grb_name
  cvs_path = cp.get('paths','cvs')+'/'
  seg_file = cp.get('paths','main')+'/plot_segments_grb%s.png' % grb_name
  log_file = cp.get('paths','main')+'/llmonitor.log'
  email_adresses = cp.get('notifications','email').replace(',',' ').split()

  # initialize a new DAG
  peu.info(grb_name, "New GRB found in the parse list: GRB %s at GPS %d"%(grb_name, grb_time))
  exttrig_dag = peu.ExttrigDag(grb_name = grb_name, grb_ra = grb_ra, \
                            grb_de=grb_dec, grb_time = grb_time)

  glue_dir = os.getenv('GLUE_LOCATION')
  lalapps_dir = os.getenv('LALAPPS_LOCATION')
  pylal_dir = os.getenv('PYLAL_LOCATION')
  exttrig_dag.set_paths(input_dir=cp.get('paths','cvs'), \
                        glue_dir= glue_dir, \
                        pylal_dir = pylal_dir, \
                        lalapps_dir= lalapps_dir,\
                        main_dir=cp.get('paths','main'),\
                        condor_log_path = cp.get('paths','condor_log_path'),\
                        log_file=log_file)
  exttrig_dag.set_ini_file(cp.get('data','ini_file'), cp.get('data','inj_file'))
  #exttrig_dag.set_monitor_file(monitor_file)
  exttrig_dag.set_addresses(email_adresses)

  # check if there is enough data for this GRB
  exttrig_dag.update_segment_lists(int(cp.get('data','min_diff')))
  off_source, ifo_list = exttrig_dag.get_segment_info(seg_file)
  ifo_times = "".join(ifo_list)
  exttrig_dag.calculate_optimality()

  # decide
  maindir = cp.get('paths','main') 
  if len(ifo_list)<2:
    peu.info(grb_name, "GRB%s: insufficient multi-IFO data to construct an "\
         "off-source segment. This GRB will be marked with NoData."%grb_name)

    # multiple notifications
    subject = "Not enough data for GRB%s " % grb_name
    email_msg = "A new GRB (%s) was found, but the analysis has not been started because "\
                "not enough data is available\n" % grb_name
    email_msg += 'The GRB was detected at GPS %s at position (%s/%s)\n' %\
                 (grb_time, grb_ra, grb_dec)    
    peu.send_mail(subject, email_msg)  

    # cleaning up
    cmd = 'mv %s/*grb%s* Files_Unused_GRB/' % (maindir, grb_name)
    peu.system_call(grb_name, cmd)

    # add this GRB to the database, marked wit NoData
    grb_dict = exttrig_dag.update_database(0)
  
  else:
    # run the creation functions
    exttrig_dag.set_seg_info(off_source, ifo_times)
    exttrig_dag.create_exttrig_xml_file()
    exttrig_dag.prepare_analysis_directory()
    grb_dict = exttrig_dag.update_database(1)

    # start the onoff analysis (condor_submit_dag)
    peu.start_dag(grb_dict, 'onoff', exttrig_dag.dag_file)

    # check if the DAG is running
    exttrig_dag.check_analysis_directory(grb_dict['dags']['onoff'])

    # cleaning up
    # 22 Oct 09; commented out cleaning because of easier finding of 
    # re-usable segment files
    #cmd = 'mv %s/*grb%s* %s'%(maindir, grb_name, analysis_path)
    #peu.system_call(grb_name, cmd)

    # multiple notifications
    subject = "New analysis started for  GRB%s " %grb_name
    email_msg = 'The analysis of GRB%s has started\n' % grb_name
    email_msg += 'The GRB was detected at GPS %s at position (%s/%s)\n' %\
                 (grb_time, grb_ra, grb_dec)
    email_msg += 'Data enough found for detectors %s\n' % ifo_list
    email_msg += 'The DAG is located at : %s\n'% analysis_path
    peu.send_mail(subject, email_msg)

  # store the new GRB in the monitor list
  monitor_list.append(grb_dict)

# -----------------------------------------------------
def check_start_inj_dag(monitor_list):

  # loop over the GRBs
  for grb in monitor_list:
    
    # check only GRBs with data and which have not started now
    if grb['has-data'] and grb['dags']['inj']['status']==0:

      # check the duration criterion
      if grb['duration']>0 and grb['duration']<cp.get('data','max-duration'):
    
        # create the name for the ligolw DAG
        dag_dict = grb['dags']['inj']
        dag_ligolw = dag_dict['basename']+'.dag'
        dag_ligolw = dag_ligolw.replace('uberdag','ligolwdag')

        # start the DAG
        peu.start_dag(grb, 'inj', dag_ligolw)
        
# -----------------------------------------------------
def update_database(opts):
  """
  This function checks for any new information
  in the GCN alert file, provided by Isabel
  """
  # update the database files (scp)
  peu.copy_exttrig_nofications()

  # read the monitor list
  monitor_list = peu.read_monitor_list()

  grbs_processed = [obj['name'] for obj in monitor_list]
  grbs_duration = [obj['duration'] for obj in monitor_list]
  counter = 0

  # open the file
  for line in file(cp.get('alerts','alert_file')):

    # leave out any empty line or any commented line
    if len(line)>1 and line[0]!="#":

      # check if we have reached the maximum number of GRBs
      # to start in this round
      if opts.check_number is not None:
        if counter>=opts.check_number:
          continue

      # extract the useful information
      words = line.split()
      grb_name = words[2]
      grb_duration = float(words[12])

      # skip if this GRB already had been processed
      if grb_name in grbs_processed:
        continue

      # check if only a certain GRB should be processed
      if opts.grb:
        if grb_name!=opts.grb:
          continue

      # we found a new GRB!!
      grb_ra = float(words[3])
      grb_dec = float(words[4])
      grb_time = words[10]
      grb_date = grb_name[:6]
      grb_gps_time = peu.get_gps_from_asc(grb_date, grb_time)
      counter += 1 

      # and prepare the call for a new analysis
      start_new_analysis(monitor_list, grb_name, grb_ra, grb_dec, grb_gps_time)

      # and add the processed GRB to the list of processed GRB's to avoid double analysis
      grbs_processed.append(grb_name)

  # check for updated duration informations for the GRBs
  peu.update_durations(monitor_list)

  # check a GRB to start the injection run
  check_start_inj_dag(monitor_list)

  # and write the list to file
  peu.write_monitor_list(monitor_list)


# -----------------------------------------------------
def start_ligolw_dag(grb, dag_key):
  """
  Execute the ligolw stage; only needed when doing injections
  @param grb: the GRB dictionary with all needed information
  @param dag_key: the keyname of the DAG itself (onoff/inj); should 
                  be only 'inj' when getting here...
  """

  # create the name for the ligolw DAG
  dag_dict = grb['dags'][dag_key]
  dag_ligolw = dag_dict['basename']+'.dag'
  dag_ligolw = dag_ligolw.replace('uberdag','ligolwdag')
  dag_dict['filename']=dag_ligolw

  # and run the command to submit this DAG
  cmd = 'cd %s; condor_submit_dag %s'  % (grb['path'], dag_ligolw)
  peu.system_call(grb['name'], cmd)
 
  # and start the not-implemented post processing
  dag_dict['status'] = 2
  peu.info(grb['name'], '  The LIGOLW stage for GRB %s has been started'%grb['name'])

# ----------------------------------------------------
def start_post(grb_dict, dag_dict, cp):
  """
  Execute the postprocessing stage for the onoff DAG
  @param dag: the DAG instance
  @param dag_dict: the DAG dictionary
  @param cp: the object of the configuration file
  """

  # create the postprocessing directory
  grb_name = grb_dict['name']

  path = "%s/GRB%s/postprocessing/"%(grb_dict['path'],grb_name)
  publishing_path = cp.get('paths','publishing_path')
  html_path = "%s/GRB%s" % (publishing_path, grb_name)

  # prepare the directory
  command = 'mkdir -p '+html_path
  peu.system_call(grb_name, command)

  # replace the in-file and create the DAG file
  f = file('sed.file','w')
  f.write("s/@GRBNAME@/GRB%s/g\n"%grb_name)
  f.write("s/@STARTTIME@/%d/g\n"%dag['starttime'])
  f.write("s/@ENDTIME@/%d/g\n"%dag['endtime'])
  f.write("s/@IFOS@/%s/g\n"%dag['ifos'])
  f.write("s=@LOGPATH@=%s=g\n"%dag['condorlogpath'])
  f.write("s/@TRIGGERTIME@/%d/g\n"%int(dag['triggertime']))
  f.write("s/@RIGHTASCENSION@/%f/g\n"%float(dag['right_ascension']))
  f.write("s/@DECLINATION@/%f/g\n"%float(dag['declination']))
  f.write("s=@OUTPUTPATH@=%s=g\n"%html_path)
  f.write("s/@LOGNAME@/%s/g\n" % os.getenv("LOGNAME"))
  f.close()

  # run the sed command
  cmd = 'sed -f sed.file %s/postproc.dag.in > %s/postproc.dag' % (path, path)
  peu.system_call(grb_name, cmd)

  # run the postprocessing stage
  cmd = 'cd %s; condor_submit_dag postproc.dag'  % (path)
  peu.system_call(grb_name, cmd)  

  # change the stage and status for this GRB
  peu.info(grb_name, '  The postprocessing stage for this GRB analysis has been started')
  dag_dict['status']=3

# ----------------------------------------------------
def start_post_inj(grb_dict, dag_dict, cp):
  """
  Execute the postprocessing stage for the inj DAG
  @param dag: the DAG instance
  @param dag_dict: the DAG dictionary
  @param cp: the object of the configuration file
  """
  raise NotImplementedError


# -----------------------------------------------------
def analysis_finalized(grb_dict, dag_key):
  """
  Finzlize the run of this DAG
  @params grb_dict: GRB dictionary with all needed information
  @param dag_key: the keyname of the DAG itself (onoff/inj)
  """

  # create the subject
  grb_name = grb_dict['name']
  subject = 'The %s analysis of GRB%s has completed' %\
       (dag_key, grb_name)

  summary_file = 'pylal_exttrig_llsummary_%s.html' % (grb_name)
  publishing_url = cp.get('paths','publishing_url')

  # open file for detailed output message
  email_msg = 'The output pages and files are located at %s/GRB%s\n' % (publishing_url, grb_name)
  email_msg += 'The summary page is %s/GRB%s/%s\n' % (publishing_url, grb_name, summary_file)
  peu.send_mail(subject, email_msg)

  # put info to the log file
  peu.info(grb_name, '  The %s analysis for GRB %s has completed successfully!' % (dag_key, grb_name))

  # set the new status so 'finished'
  dag_dict['status'] = 5


# -----------------------------------------------------
def parse_args():
    parser = optparse.OptionParser(version=git_version.verbose_msg)

    # cache input
    parser.add_option("--config-file", help="Configuration file")

    # GRBs to process
    parser.add_option("--nocheck", action="store_true", default=False, \
        help="If this flag is set, the codes does not check for a new GRB.")
    parser.add_option("--check-number", type="int", default=None, 
        help="Specifies the number of new GRBs to be processed.")
    #parser.add_option("--number-running", type="int", default=None,
    #    help="Specifies the number of running GRBs.")
    parser.add_option( "--grb", default=None,
        help="Specifies the name of a GRB to check.")


    # injection related setting
    parser.add_option( "--with-injections",default=False, \
        help="If this flag is set, an injection run is started as well.")
    parser.add_option("--injection-config",default=None,\
        help="Specifies the injection ini file (if required).")
   

    options, arguments = parser.parse_args()

    # check that mandatory switches are present
    for opt in (["config_file"]):
        if getattr(options, opt) is None:
            raise ValueError, "Option '--%s' is always required!" % opt.replace("_", "-")

    if options.check_number is not None and options.grb is not None:
      raise ValueError, "Only option '--check-number' or '--grb' can be specified, not both together!"

    if options.with_injections and not options.injection_config:
      raise ValueError, "injection ini-file must be specified with '--injection-config'"\
                        " if the option '--with-injections' is set!"

    #if options.check_number and options.number_running:
    #  raise ValueError, "As of now, you cannot specify BOTH arguments, '--check-number' and '--number-running'."

    return options, arguments

# -----------------------------------------------------
# main code
# -----------------------------------------------------

# parsing the command arguments
opts, args = parse_args()

# read the config parser and pass it to llutils
cp = ConfigParser.ConfigParser()
cp.read(opts.config_file)
peu.cp = cp

#
# First, update the internal database (llmonitor.pickle)
#
update_database(opts)

#
# Read the internal database
#
monitor_list = peu.read_monitor_list()

# put an info to the log-file
peu.info('monitor', 'pylal_exttrig_llmonitor is executed...')

# check the proxy if enough time is left
check_proxy()

#
# main loop over each entry in the monitor list
#
for grb_dict in monitor_list:

  # loop over the DAGs for this GRB
  for dag_key, dag_dict in grb_dict['dags'].iteritems():

    # skip this DAG if it is finished (5) or has not started yet
    if dag_dict['status']==0 or dag_dict['status']==5:
      continue

    # get the GRB name
    grb_name = grb_dict['name']

    # check the status on this DAG
    status = check_status(grb_dict, dag_key)
    dag_status = dag_dict['status']

    if status == 0:
      # The DAG is running
      peu.info(grb_name, ' GRB %s DAG: %s stage: %s  status: running '%\
                 (grb_name, dag_key, peu.get_stage_name(dag_status)))

    elif status == 1:
    
      if dag_status==3:
        # finalize this DAG
        analysis_finalized(grb_dict, dag_key)
      elif dag_status==2:
        # start the injection postprocessing
        start_post_inj(grb_dict, dag_dict, cp)
      elif dag_status==1:
        # start postproc (for onoff) or first the ligolw DAG before
        if dag_key == 'onoff':
          start_post(grb_dict, dag_dict, cp)
        elif dag_key == 'inj':
          #start_ligolw_dag(grb_dict, dag_key)
          start_post_inj(grb_dict, dag_dict, cp)
        else:
          raise ValueError, "There is no %s DAG in the analysis..." % dag_key
    
    else:
      # The DAG has an error
      peu.info(grb_name, ' GRB %s DAG: %s stage: %s  status: ERROR '%\
                 (grb_name, dag_key, peu.get_stage_name(dag_status)))


# write out the new status
peu.write_monitor_list(monitor_list)

#
# Update the summary page for all GRBs
#
publish_path = cp.get('paths','publishing_path')
publish_url = cp.get('paths','publishing_url')
peu.generate_summary(publish_path, publish_url)
