#!/usr/bin/python

import os
import sys
import pickle
import time
import subprocess
import optparse
import ConfigParser

import matplotlib
matplotlib.use('Agg')

from pylal import git_version
from pylal import pylal_exttrig_llutils as peu

###################################################
usage = """usage: pylal_exttrig_monitor config_file

to be executed as a cron job with e.g. the following entry:

*/5  *  *    *    *   /bin/sh /home/dietz/Work/E14/Code/pylal_exttrig_llmonitor.sh >> ~/.llmonitor.log 2>&1

The only argument is the config_file, containing some
important decleration of paths etc. Example for UWM:


paths]
gcn_file = /home/dietz/Work/E14/GCN-testing/gcn.file
pylal_exttrig_llstart = /home/dietz/Work/E14/Code/pylal_exttrig_llstart
main = /home/dietz/Work/E14/Analysis
publishing_path = /home/dietz/public_html/E14/
publishing_url = https://ldas-jobs.phys.uwm.edu/~dietz/E14/
cvs = /home/dietz/Work/E14/Pseudo_CVS
glue = /opt/lscsoft/glue
lalapps = /home/dietz/opt/Install/s5_2yr_lv_lowcbc_20090410a

[data]
h1_segments = E14_H1_science.txt
l1_segments = E14_L1_science.txt
v1_segments = E14_V1_science.txt
ini_file = S6GRB_trigger_hipe.ini

[notifications]
email = alexander.dietz@lapp.in2p3.fr


[run]
log_path = /people/dietz
ini_file = S6GRB_trigger_hipe.ini

"""


# -----------------------------------------------------
def check_proxy():
  """
  Checks the status of the grid-proxy and sends an email 
  if it runs out
  """

  my_email = 'Alexander.Dietz@lapp.in2p3.fr'
  cmd = 'grid-proxy-info > grid.info'
  peu.system_call(cmd)

  flag = False  
  for line in open('grid.info'):
    if 'timeleft' in line:
      flag = True
      time_in_hours = int(line.split()[2].split(':')[0])
      if time_in_hours<40:
        peu.send_mail('Grid Cert aging, %d hleft'%time_in_hours,'Have to renew grid certificate',[my_email])
      if time_in_hours==0:
        peu.send_mail('Grid Cert expired. Suspended further actions.',\
                      'Need renew grid certificate NOW.',[my_email])
        sys.exit(0)

  if not flag:
    peu.send_mail('Grid Cert not available','Have to init grid certificate',[my_email])

# -----------------------------------------------------
def check_status(out_name):
  """
  check the status of the given output file of a DAG
  """

  # try to open the dagman.out file
  try:
    f = open(out_name)
  except IOError:
    return -2

  # try to figure out whats the status
  for line in file(out_name).readlines()[-15:]:
    if 'Aborting DAG...' in line:
      return -1
    if 'All jobs Completed!' in line:
      return 1
    
  return 0


# -----------------------------------------------------
def start_new_analysis(monitor_list, grb_name, grb_ra, grb_dec, grb_time):
  """
  Start a new analysis of a GRB with a separate call.
  All data are given in strings.
  """

  # First check if enough time has passed
  # since the trigger-time
  gps_now = time.time() - peu.offset_gps_to_linux
  time_diff = gps_now-int(grb_time)
  if time_diff < int(cp.get('data','min_diff')):
    # do not analyze this GRB now.
    # postpone to later time
    peu.info("GRB%s: Analysis postponsed because time difference too small")
    return 

  analysis_path = cp.get('paths','main')+'/GRB'+grb_name
  cvs_path = cp.get('paths','cvs')+'/'
  seg_file = cp.get('paths','main')+'/plot_segments_grb%s.png' % grb_name

  # initialize a new DAG
  peu.info("New GRB found in the parse list: GRB %s at GPS %d"%(grb_name, grb_time))
  exttrig_dag = peu.ExttrigDag(grb_name = grb_name, grb_ra = grb_ra, \
                            grb_de=grb_dec, grb_time = grb_time)

  glue_dir = os.getenv('GLUE_LOCATION')
  lalapps_dir = os.getenv('LALAPPS_LOCATION')
  pylal_dir = os.getenv('PYLAL_LOCATION')
  exttrig_dag.set_paths(input_dir=cp.get('paths','cvs'), \
                        glue_dir= glue_dir, \
                        pylal_dir = pylal_dir, \
                        lalapps_dir= lalapps_dir,\
                        main_dir=cp.get('paths','main'),\
                        condor_log_path = cp.get('paths','condor_log_path'),\
                        log_file=log_file)
  exttrig_dag.set_ini_file(cp.get('data','ini_file'))
  exttrig_dag.set_monitor_file(monitor_file)
  exttrig_dag.set_addresses(email_adresses)

  # check if there is enough data for this GRB
  exttrig_dag.update_segment_lists(int(cp.get('data','min_diff')))
  off_source, ifo_list = exttrig_dag.get_segment_info(seg_file)
  ifo_times = "".join(ifo_list)
  exttrig_dag.calculate_optimality()

  # decide
  maindir = cp.get('paths','main') 
  if len(ifo_list)<2:
    peu.info("GRB%s: insufficient multi-IFO data to construct an "\
         "off-source segment. This GRB will be marked with NoData."%grb_name)

    # multiple notifications
    subject = "Not enough data for GRB%s " % grb_name
    email_msg = "A new GRB (%s) was found, but the analysis has not been started because "\
                "not enough data is available\n" % grb_name
    email_msg += 'The GRB was detected at GPS %s at position (%s/%s)\n' %\
                 (grb_time, grb_ra, grb_dec)    
    peu.send_mail(subject, email_msg)  

    # cleaning up
    cmd = 'mv %s/*grb%s* Files_Unused_GRB/' % (maindir, grb_name)
    peu.system_call(cmd)

    # add this GRB to the database, marked wit NoData
    dag_dict = exttrig_dag.update_database(0)
  
  else:
    # run the creation functions
    exttrig_dag.set_seg_info(off_source, ifo_times)
    exttrig_dag.create_exttrig_xml_file()
    exttrig_dag.prepare_analysis_directory()
    exttrig_dag.start_analysis()
    check_condor = exttrig_dag.check_analysis_directory()
    dag_dict = exttrig_dag.update_database(check_condor)

    # cleaning up
    cmd = 'mv %s/*grb%s* %s'%(maindir, grb_name, analysis_path)
    peu.system_call(cmd)

    # multiple notifications
    subject = "New analysis started for  GRB%s " %grb_name
    email_msg = 'The analysis of GRB%s has started\n' % grb_name
    email_msg += 'The GRB was detected at GPS %s at position (%s/%s)\n' %\
                 (grb_time, grb_ra, grb_dec)
    email_msg += 'Data enough found for detectors %s\n' % ifo_list
    email_msg += 'The DAG is located at : %s\n'% analysis_path
    peu.send_mail(subject, email_msg)

  # store the new GRB in the monitor list
  monitor_list.append(dag_dict)

# -----------------------------------------------------
def check_gcn( monitor_list, opts):
  """
  This function checks for any new information
  in the GCN alert file, provided by Isabel
  """

  # get the alert filenames and their locations 
  alert_loc = cp.get('alerts','alert_location')
  main_loc = cp.get('paths','main')
  alert_file = cp.get('alerts','alert_file')
  circular_file = cp.get('alerts','circular_file')
  
  # copy all relevant files to the working directory
  cmd = 'scp %s %s >> ~/cp.log 2>&1' % (alert_loc, main_loc)
  peu.system_call(cmd)

  grbs_processed = [obj['name'] for obj in monitor_list]
  grbs_duration = [obj['duration'] for obj in monitor_list]

  # reset the counter
  counter = 0

  # open the file
  f = open(alert_file)
  for line in f:
    # leave out any empty line or any commented line
    if len(line)>1 and line[0]!="#":
      
      # extract the useful information
      words = line.split()
      grb_name = words[2]
      grb_duration = float(words[12])

      if grb_name in grbs_processed:
        continue

      if opts.check_number:
        if counter>opts.check_number:
          continue

      if opts.grb:
        if grb_name!=opts.grb:
          continue
            
      counter += 1

      # we found a new GRB!!
      grb_ra = float(words[3])
      grb_dec = float(words[4])
      grb_time = words[10]
      grb_date = grb_name[:6]

      # convert the date to GPS
      grb_gps_time = peu.get_gps_from_asc(grb_date, grb_time)

      # and prepare the call for a new analysis
      start_new_analysis(monitor_list, grb_name, grb_ra, grb_dec, grb_gps_time)
        
      # and add the processed GRB to the list of processed GRB's to avoid double analysis
      grbs_processed.append(grb_name)
     
  f.close()
        

# -----------------------------------------------------
def start_ligolw(dag, dag_name):
  """
  Execute the ligolw stage;
  ONLY NEEDED IN CASE OF INJECTIONS!
  """

  # run the ligolw stage directly now
  dag_ligolw = dag_name+'_ligolwdag.dag'
  cmd = 'cd %s; condor_submit_dag %s'  % (dag['path'], dag_ligolw)
  peu.system_call(cmd)
 
  # and start the not-implemented post processing
  dag['status']=0
  dag['stage']='Ligolw'

  peu.info('  The LIGOLW stage for GRB %s has been started'%dag['name'])
  return dag

# ----------------------------------------------------
def start_post(dag, cp):
  """
  Execute the postprocessing stage
  @param dag: the DAG instance
  @param cp: the object of the configuration file
  """

  # create the postprocessing directory
  path = "%s/GRB%s/postprocessing/"%(dag['path'],dag['name'])
  publishing_path = cp.get('paths','publishing_path')
  html_path = "%s/GRB%s" % (publishing_path, dag['name'])

  # prepare the directory
  command = 'mkdir -p '+html_path
  peu.system_call(command)

  # replace the in-file and create the DAG file
  f = file('sed.file','w')
  f.write("s/@GRBNAME@/GRB%s/g\n"%dag['name'])
  f.write("s/@STARTTIME@/%d/g\n"%dag['starttime'])
  f.write("s/@ENDTIME@/%d/g\n"%dag['endtime'])
  f.write("s/@IFOS@/%s/g\n"%dag['ifos'])
  f.write("s=@LOGPATH@=%s=g\n"%dag['condorlogpath'])
  f.write("s/@TRIGGERTIME@/%d/g\n"%int(dag['triggertime']))
  f.write("s/@RIGHTASCENSION@/%f/g\n"%float(dag['right_ascension']))
  f.write("s/@DECLINATION@/%f/g\n"%float(dag['declination']))
  f.write("s=@OUTPUTPATH@=%s=g\n"%html_path)
  f.write("s/@LOGNAME@/%s/g\n" % os.getenv("LOGNAME"))
  f.close()

  # run the sed command
  cmd = 'sed -f sed.file %s/postproc.dag.in > %s/postproc.dag' % (path, path)
  peu.system_call(cmd)

  # run the postprocessing stage
  cmd = 'cd %s; condor_submit_dag postproc.dag'  % (path)
  peu.system_call(cmd)  

  # change the stage and status for this GRB
  peu.info('  The postprocessing stage for this GRB analysis has been started')
  dag['status']=0
  dag['stage']='Post'

  return dag

# -----------------------------------------------------
def analysis_finalized(dag):
  """
  Set the status to finished and send emails
  """

  # create the subject
  grb_name = dag_dict['name']
  subject = 'The analysis of GRB%s has completed' %\
       (grb_name)

  summary_file = 'pylal_exttrig_llsummary_%s.html' % (grb_name)
  publishing_url = cp.get('paths','publishing_url')

  # open file for detailed output message
  email_msg = 'The output pages and files are located at %s/GRB%s\n' % (publishing_url, grb_name)
  email_msg += 'The summary page is %s/GRB%s/%s\n' % (publishing_url, grb_name, summary_file)
  peu.send_mail(subject, email_msg)

  # info to the log file
  peu.info('  The analysis for GRB %s has completed successfully!' % grb_name)

  # set the new status so 'finished'
  dag['status']=2
  return dag


# -----------------------------------------------------
def parse_args():
    parser = optparse.OptionParser(version=git_version.verbose_msg)

    # cache input
    parser.add_option("--config-file", help="Configuration file")

    parser.add_option("--nocheck", action="store_true", default=False, \
        help="If this flag is set, the codes does not check for a new GRB.")
    parser.add_option("--check-number", type="int", default=None, 
        help="Specifies the number of new GRBs to be processed.")
    parser.add_option( "--grb", default=None,
        help="Specifies the name of a GRB to check.")
    parser.add_option( "--with-injections",default=False, \
        help="If this flag is set, an injection run is started as well.")
    parser.add_option("--injection-config",default=None,\
        help="Specifies the injection ini file (if required).")

    options, arguments = parser.parse_args()

    # check that mandatory switches are present
    for opt in (["config_file"]):
        if getattr(options, opt) is None:
            raise ValueError, "Option '--%s' is always required!" % opt.replace("_", "-")

    if options.check_number is not None and options.grb is not None:
      raise ValueError, "Only option '--check-number' or '--grb' can be specified, not both together!"

    if options.with_injections and not options.injection_config:
      raise ValueError, "injection ini-file must be specified with '--injection-config'"\
                        " if the option '--with-injections' is set!"

    return options, arguments

# -----------------------------------------------------
# main code
# -----------------------------------------------------

# parsing the command arguments
opts, args = parse_args()

# read the config parser
cp = ConfigParser.ConfigParser()
cp.read(opts.config_file)
peu.cp = cp

# check the command to check the time left on the proxy
check_proxy()

# read the list of DAGs to monitor
monitor_file = cp.get('paths','main')+'/llmonitor.pickle'
log_file = cp.get('paths','main')+'/llmonitor.log'
try:
  monitor_list = pickle.load(file(monitor_file))
except IOError:
  # create an empty file if it does not exist
  monitor_list = []
  pickle.dump(monitor_list, file(monitor_file,'w'))

# get the list of email adresses and dagman.out names
email_adresses = cp.get('notifications','email').replace(',',' ').split()
ini_file = cp.get('data','ini_file')
dag_name = ini_file.split('.')[0]
outfile_inspiral = dag_name + '_uberdag.dag.dagman.out'
outfile_ligolw = dag_name + '_ligolw.dag.dagman.out'
outfile_post = 'postproc.dag.dagman.out' 

# put an info to the log-file
peu.info('pylal_exttrig_llmonitor is executed with llutils at %s'%peu.__file__)

# main loop over each entry in the monitor dictionary
for dag_dict in monitor_list:

  # continue of the current DAG is marked as finished
  if dag_dict['status']==2 or dag_dict['status']==10:
    continue

  # put basic info into log file
  peu.info("Infos on the analysis of GRB "+dag_dict['name'])
  
  # check the DAG status, chose the name of the DAG
  if dag_dict['stage']=='Ligolw':
    out_name = '%s/%s' % (dag_dict['path'], outfile_ligolw)
  elif dag_dict['stage']=='Inspiral':
    out_name = '%s/%s' % (dag_dict['path'], outfile_inspiral)
  elif dag_dict['stage']=='Post':
    out_name = '%s/GRB%s/postprocessing/%s' % (dag_dict['path'], dag_dict['name'], outfile_post)
  else:
    raise ValueError, 'The choice %s is not implemented...'% dag_dict['stage']

  # check the status of this particular DAG here
  status = check_status(out_name)

  if status == 0:
    # The DAG is running
    dag_dict['status']=0
    peu.info('  DAG for %s is running' % dag_dict['name'])

  elif status<0:
    # The DAG has aborted
    if dag_dict['status']==0:
      
      # notify because of error
      dag_dict['status']=status
      if status==-1:
        peu.notify(dag_dict, 'DAG exited on error')
      elif status==-2:
        peu.notify(dag_dict, 'DAG file vanished!?')          

  elif status == 1:
    
    # The DAG has finished
    if dag_dict['status']==0:      
      # notify or make further steps: ligolw_add and postprocessing 
      peu.notify(dag_dict, 'DAG completed!')
    dag_dict['status']=1
    
    # start the next step: the ligolwadd step
    if dag_dict['stage']=='Post':
      dag_dict = analysis_finalized(dag_dict)

    # inspiral ready: start the next step: the ligolwadd step
    if dag_dict['stage']=='Inspiral':
      dag_dict = start_post(dag_dict, cp)

  peu.info('  %s: Stage: %s  Status: %s' % (dag_dict['name'], dag_dict['stage'], dag_dict['status']))


#
# Check for any new GCN alert
#
if not opts.nocheck:
  check_gcn(monitor_list, opts)


# write out the new status
pickle.dump(monitor_list, file(monitor_file,'w'))

# Update the duration information using the circular file
peu.update_duration(monitor_file, opts)

    
#
# Update the summary page of all GRBs
#
publish_path = cp.get('paths','publishing_path')
publish_url = cp.get('paths','publishing_url')
peu.generate_summary(monitor_file, publish_path, publish_url)
