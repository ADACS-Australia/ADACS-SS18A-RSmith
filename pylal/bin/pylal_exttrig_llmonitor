#!/usr/bin/python

import os
import sys
import pickle
import time
import subprocess
import optparse
import ConfigParser

import matplotlib
matplotlib.use('Agg')

from pylal import git_version
from pylal import pylal_exttrig_llutils as peu

###################################################
usage = """usage: pylal_exttrig_monitor config_file

to be executed as a cron job with e.g. the following entry:

*/5  *  *    *    *   /bin/sh /home/dietz/Work/E14/Code/pylal_exttrig_llmonitor.sh >> ~/.llmonitor.log 2>&1

The only argument is the config_file, containing some
important decleration of paths etc. Example for UWM:


paths]
gcn_file = /home/dietz/Work/E14/GCN-testing/gcn.file
pylal_exttrig_llstart = /home/dietz/Work/E14/Code/pylal_exttrig_llstart
main = /home/dietz/Work/E14/Analysis
publishing_path = /home/dietz/public_html/E14/
publishing_url = https://ldas-jobs.phys.uwm.edu/~dietz/E14/
cvs = /home/dietz/Work/E14/Pseudo_CVS
glue = /opt/lscsoft/glue
lalapps = /home/dietz/opt/Install/s5_2yr_lv_lowcbc_20090410a

[data]
h1_segments = E14_H1_science.txt
l1_segments = E14_L1_science.txt
v1_segments = E14_V1_science.txt
ini_file = S6GRB_trigger_hipe.ini

[notifications]
email = alexander.dietz@lapp.in2p3.fr


[run]
log_path = /people/dietz
ini_file = S6GRB_trigger_hipe.ini

"""

plot_hipe_template="""
../../lalapps_plot_hipe \
  --plotethinca \
  --plotthinca \
  --plotnumtemplates \
  --plotinjnum \
  --plotinspmissed \
  --plotinspinj \
  --plotsnrchi \
  --second-stage \
  --config-file ./lik_plothipe.ini \
  --log-path %s \
  --write-script"""

# -----------------------------------------------------
def check_proxy():
  """
  Checks the status of the grid-proxy and sends an email 
  if it runs out
  """

  my_email = 'Alexander.Dietz@lapp.in2p3.fr'
  cmd = 'grid-proxy-info > grid.info'
  peu.system_call('monitor',cmd, False)

  flag = False  
  for line in open('grid.info'):
    if 'timeleft' in line:
      flag = True
      time_in_hours = int(line.split()[2].split(':')[0])
      if time_in_hours<40:
        peu.send_mail('Grid Cert aging, %d hleft'%time_in_hours,'Have to renew grid certificate',[my_email])
      if time_in_hours==0:
        peu.send_mail('Grid Cert expired. Suspended further actions.',\
                      'Need renew grid certificate NOW.',[my_email])
        raise SystemError, "Grid certificate is invalid."

  if not flag:
    peu.send_mail('Grid Cert not available','Have to init grid certificate',[my_email])

# -----------------------------------------------------
def start_new_analysis(grb_name, grb_ra, grb_dec, grb_time):
  """
  Start a new analysis of a GRB with a separate call.
  All data are given in strings.
  """

  # First check if enough time has passed
  # since the trigger-time
  gps_now = time.time() - peu.offset_gps_to_linux
  time_diff = gps_now-int(grb_time)
  if time_diff < int(cp.get('data','min_diff')):
    # do not analyze this GRB now; postpone to later time
    peu.info(grb_name, "GRB%s: Analysis postponsed because time difference too small"%grb_name)
    return None
 
  if opts.check_time_window:
    if time_diff > opts.check_time_window*86400.:
      # do not analyze this GRB; only analyze online GRBs within a certain time range
      peu.info(grb_name, "GRB%s: Analysis not started because trigger is before the check-time-window"%grb_name)
      return None	



  analysis_path = cp.get('paths','main')+'/GRB'+grb_name
  cvs_path = cp.get('paths','cvs')+'/'
  seg_file = cp.get('paths','main')+'/plot_segments_grb%s.png' % grb_name
  log_file = cp.get('paths','main')+'/llmonitor.log'
  email_adresses = cp.get('notifications','email').replace(',',' ').split()

  # initialize a new DAG
  peu.info(grb_name, "New GRB found in the parse list: GRB %s at GPS %d"%(grb_name, grb_time))
  grb = peu.GRB(grb_name = grb_name, grb_ra = grb_ra, \
                grb_de = grb_dec, grb_time = grb_time)


  # set the paths to the GRB instance
  glue_dir = os.getenv('GLUE_LOCATION')
  lalapps_dir = os.getenv('LALAPPS_LOCATION')
  pylal_dir = os.getenv('PYLAL_LOCATION')
  grb.set_paths(input_dir=cp.get('paths','cvs'), \
                glue_dir= glue_dir, \
                pylal_dir = pylal_dir, \
                lalapps_dir= lalapps_dir,\
                main_dir=cp.get('paths','main'),\
                ini_file = cp.get('data','ini_file'),\
                inj_file = cp.get('data','inj_file'),\
                config_file = cp.get('paths','main')+'/'+opts.config_file,\
                condor_log_path = cp.get('paths','condor_log_path'),\
                log_file=log_file)
  grb.set_addresses(email_adresses)

  # check if there is enough data for this GRB
  grb.update_segment_lists(int(cp.get('data','min_diff')))
  grb.get_segment_info(seg_file)
  grb.calculate_optimality()

  # decide
  if grb.has_data:
    
    # run the creation functions
    grb.create_exttrig_xml_file()
    grb.prepare_analysis_directory()

    # update veto files as well
    grb.update_veto_lists(int(cp.get('data','min_diff')))

    # start the onoff analysis (condor_submit_dag)
    grb.dag['onoff'].start()
   
    # check if the DAG is running
    grb.check_analysis_directory('onoff')

    # cleaning up
    cmd = 'cp %s/*grb%s* %s'%(cp.get('paths','main'), grb.name, analysis_path)
    peu.system_call(grb.name, cmd)

    # multiple notifications
    subject = "New analysis started for  GRB%s " %grb.name
    email_msg = 'The analysis of GRB%s has started\n' % grb.name
    email_msg += 'The GRB was detected at GPS %s at position (%s/%s)\n' %\
                 (grb.time, grb.ra, grb.de)
    email_msg += 'Data enough found for detectors %s\n' % grb.ifolist
    email_msg += 'The DAG is located at : %s\n'% analysis_path
    peu.send_mail(subject, email_msg)

  else:
    peu.info(grb_name, "GRB%s: insufficient multi-IFO data to construct an "\
         "off-source segment. This GRB will be marked with NoData."%grb.name)

    # multiple notifications
    subject = "Not enough data for GRB%s " % grb.name
    email_msg = "A new GRB (%s) was found, but the analysis has not been started because "\
                "not enough data is available\n" % grb.name
    email_msg += 'The GRB was detected at GPS %s at position (%s/%s)\n' %\
                 (grb.time, grb.ra, grb.de)    
    peu.send_mail(subject, email_msg)  

  # copy the segment plot and all the other stuff
  maindir = cp.get('paths','main')
  htmldir = '%s/GRB%s' % (cp.get('paths','publishing_path'), grb.name)
  cmd = 'mkdir -p %s; cp %s/*grb%s* %s' % \
          (htmldir, maindir, grb.name, htmldir)
  peu.system_call(grb.name, cmd)

  # cleaning up
  cmd = 'mv %s/*grb%s* AuxFiles/' % (maindir, grb.name)
  peu.system_call(grb.name, cmd)

  return grb

# -----------------------------------------------------
def check_start_inj_dag(monitor_list):
  """
  Checks each GRB in the list if the injection
  run can be started for it
  """
  # loop over the GRBs
  for grb in monitor_list:
    
    # check only GRBs with data and which have not started now
    if grb.has_data and grb.dag['inj'].status == 0:

      # check the duration criterion
      if grb.duration and grb.duration<float(cp.get('data','max-duration')):

        # starting the injection DAG
        grb.prepare_injection_analysis()
        grb.dag['inj'].start()

        
# -----------------------------------------------------
def update_database(opts):
  """
  This function checks for any new information
  in the GCN alert file, provided by Isabel
  """
  # update the database files (scp)
  #print "WARNING: To save developing time, databases are not copied (monitor)"
  peu.copy_exttrig_nofications()

  # read the monitor list
  monitor_list = peu.read_monitor_list()

  # get a list with all GRBs already in the local database
  grbs_processed = [obj.name for obj in monitor_list]
  counter = 0

  # open the file
  for line in file(cp.get('alerts','alert_file')):

    # leave out any empty line or any commented line
    if len(line)>1 and line[0]!="#":

      # check if we have reached the maximum number of GRBs
      # to start in this round
      if opts.check_number is not None:
        if counter>=opts.check_number:
          continue

      # extract the useful information
      words = line.split()
      grb_name = words[2]
      grb_duration = float(words[12])

      # skip if this GRB already had been processed
      if grb_name in grbs_processed:
        continue

      # check if only a certain GRB should be processed
      if opts.grb:
        if grb_name!=opts.grb:
          continue

      # we found a new GRB!!
      grb_ra = float(words[3])
      grb_dec = float(words[4])
      grb_time = words[10]
      grb_date = grb_name[:6]
      grb_gps_time = peu.get_gps_from_asc(grb_date, grb_time)
      counter += 1 

      # and prepare the call for a new analysis
      grb = start_new_analysis(grb_name, grb_ra, grb_dec, grb_gps_time)

      # only add to the list if this GRB has been accepted...
      if grb:

        # and add the processed GRB to the list of processed GRB's to avoid double analysis
        monitor_list.append(grb)
        grbs_processed.append(grb_name)

  # check for updated duration informations for the GRBs
  peu.update_durations(monitor_list)

  # check if a GRB fullfill condition to start the injection run
  check_start_inj_dag(monitor_list)

  # and write the list to file
  peu.write_monitor_list(monitor_list)


# -----------------------------------------------------
def start_ligolw(grb, dag):
  """
  Execute the ligolw stage; only needed when doing injections
  @param grb: the GRB dictionary with all needed information
  @param dag: the DAG dictionary 
  """

  # create the name for the ligolw SH script (DAG is not working!)
  dag_name = dag.get_dagname().replace('uberdag','ligolw')
  dag.set_dagname(dag_name)
  ligolw_sh_name = dag.get_shname()

  # and run the command to submit this DAG
  cmd = 'cd %s; sh %s'  % (grb.analysis_dir, ligolw_sh_name)
  peu.system_call(grb.name, cmd)
 
  # setting status to 3
  dag.set_status(3)
  peu.info(grb.name, '  The LIGOLW stage for GRB %s has been started and ended with %s'%(grb.name, ligolw_sh_name))


# ----------------------------------------------------
def start_post(grb, dag, cp):
  """
  Execute the postprocessing stage for the onoff DAG
  @param grb: the GRB dictionary with all information
  @param dag: the DAG dictionary
  @param cp: the object of the configuration file
  """

  # get some main paths
  path = "%s/GRB%s/postprocessing/"%(grb.analysis_dir, grb.name)
  publishing_path = cp.get('paths','publishing_path')
  html_path = "%s/GRB%s" % (publishing_path, grb.name)

  # prepare the directory
  command = 'mkdir -p '+html_path
  peu.system_call(grb.name, command)

  # make the replace with the sed file
  infile = "%s/postproc.in" %path
  dagfile = "%s/postproc.dag" %path
  grb.apply_sed_file(infile, dagfile)

  # prepare the DAGname and start the DAG
  dag.set_dagname(dagfile)
  dag.start()

  # run the postprocessing stage
  #cmd = 'cd %s; condor_submit_dag postproc.dag'  % (path)
  #peu.system_call(grb.name, cmd)  

  # change the stage and status for this GRB
  peu.info(grb.name, '  The postprocessing stage for this GRB analysis has been started')
  dag.set_status(3)

# ----------------------------------------------------
def start_post_inj(grb, dag, cp):
  """
  Execute the postprocessing stage for the inj DAG
  @param grb: the GRB dictionary with all information
  @param dag: the DAG dictionary
  @param cp: the object of the configuration file
  """

  number_word = {2:'two', 3:'three',4:'four'}

  path = "%s/GRB%s/likelihood/"%(grb.analysis_dir,grb.name)
  publishing_path = cp.get('paths','publishing_path')
  html_path = "%s/GRB%s" % (publishing_path, grb.name)

  # prepare the directory, probably created before
  command = 'mkdir -p '+html_path
  peu.system_call(grb.name, command)

  # create the plot_hipe ini file
  infile = "%s/lik_plothipe.in" %path
  outfile = "%s/lik_plothipe.ini" %path
  grb.apply_sed_file(infile, outfile)

  # link the executables; cannot be set in lalapps_plot_hipe
  path_executables = path+'executables'
  command = 'mkdir -p %s; cd %s; ln -s ../../../plot* executables'%\
            (path_executables, path)
  peu.system_call(grb.name, command)


  ## create the plothipe script...
  ## run the plothipe script
  cmd = plot_hipe_template % cp.get('paths','condor_log_path')
  nifo = 0
  for ifo in grb.ifolist:
    cmd += ' --%s-data' % ifo.lower()
  cmd += ' --%s-ifo' % number_word[len(grb.ifolist)]
  peu.system_call(grb.name, cmd)

  print "plothipe call and afterwards codes: Not Implemented yet. Needs to be implemented!!!"
  #raise NotImplementedError

  # set the new status so 'finished'
  dag.set_status(3)


# -----------------------------------------------------
def analysis_finalized(grb, dag):
  """
  Finzlize the run of this DAG
  @params grb_dict: GRB dictionary with all needed information
  @param dag_key: the keyname of the DAG itself (onoff/inj)
  """

  # create the subject
  subject = 'The %s analysis of GRB%s has completed' %\
       (dag.type, grb.name)

  summary_file = 'pylal_exttrig_llsummary_%s-sanity.html' % (grb.name)
  openbox_file =  'pylal_exttrig_llsummary_%s-OPENBOX.html' % (grb.name)
  publishing_url = cp.get('paths','publishing_url')

  # open file for detailed output message
  email_msg = 'The output pages and files are located at %s/GRB%s\n' % (publishing_url, grb.name)
  email_msg += 'The summary page is %s/GRB%s/%s\n' % (publishing_url, grb.name, summary_file)
  email_msg += 'The OPENBOX page is %s/GRB%s/%s\n' % (publishing_url, grb.name, openbox_file)
  peu.send_mail(subject, email_msg)

  # put info to the log file
  peu.info(grb.name, '  The %s analysis for GRB %s has completed successfully!' % (dag.type, grb.name))

  # set the new status so 'finished'
  dag.set_status(5)


# -----------------------------------------------------
def parse_args():
    parser = optparse.OptionParser(version=git_version.verbose_msg)

    # cache input
    parser.add_option("--config-file", help="Configuration file")

    # GRBs to process
    parser.add_option("--nocheck", action="store_true", default=False, \
        help="If this flag is set, the codes does not check for a new GRB.")
    parser.add_option("--check-number", type="int", default=None, 
        help="Specifies the number of new GRBs to be processed.")
    #parser.add_option("--number-running", type="int", default=None,
    #    help="Specifies the number of running GRBs.")
    parser.add_option( "--grb", default=None,
        help="Specifies the name of a GRB to check.")
    parser.add_option("--check-time-window", type="float", default=None,
        help="Specifies the time window to be checked from the current date (in days)."\
             " Any GRB outside this range (i.e. earlier ones) and not analyzed.")


    # injection related setting
    parser.add_option( "--with-injections",default=False, \
        help="If this flag is set, an injection run is started as well.")
    parser.add_option("--injection-config",default=None,\
        help="Specifies the injection ini file (if required).")
   

    options, arguments = parser.parse_args()

    # check that mandatory switches are present
    for opt in (["config_file"]):
        if getattr(options, opt) is None:
            raise ValueError, "Option '--%s' is always required!" % opt.replace("_", "-")

    if options.check_number is not None and options.grb is not None:
      raise ValueError, "Only option '--check-number' or '--grb' can be specified, not both together!"

    if options.with_injections and not options.injection_config:
      raise ValueError, "injection ini-file must be specified with '--injection-config'"\
                        " if the option '--with-injections' is set!"

    #if options.check_number and options.number_running:
    #  raise ValueError, "As of now, you cannot specify BOTH arguments, '--check-number' and '--number-running'."

    return options, arguments

# -----------------------------------------------------
# main code
# -----------------------------------------------------

# parsing the command arguments
opts, args = parse_args()

# read the config parser and pass it to llutils
cp = ConfigParser.ConfigParser()
cp.read(opts.config_file)
peu.cp = cp

# put an info to the log-file
peu.info('monitor', 'pylal_exttrig_llmonitor is executed...')

# check the lock file
pid = peu.check_lock()
if pid:
  peu.info('monitor','A different instance of this code is running with PID= %s.'%pid)
  sys.exit(0)
peu.set_lock()

# check the proxy if enough time is left
check_proxy()

#
# First, update the internal database (llmonitor.pickle)
#
update_database(opts)

#
# Read the internal database
#
monitor_list = peu.read_monitor_list()

#
# main loop over each entry in the monitor list
#
for grb in monitor_list:

  # Skip all the rest of this GRB has not enough data
  if not grb.has_data:
    continue

  # loop over the DAGs for this GRB
  for dag_key, dag in grb.dag.iteritems():


    # skip this DAG if it is finished (5) or has not started yet
    if dag.get_status()==0 or dag.get_status()==5:
      text = "INFO: GRB%s:  DAG %s " % (grb.name, dag.type)
      if dag.get_status()==5:
        text += "finished."
      if dag.get_status()==0:
        text += "not started."
      peu.info(grb.name, text)
      continue

    # check the status on this DAG
    fstat = dag.check_status(grb, dag)
    status = dag.get_status()

    if fstat == -2:
      peu.info(grb.name, "INFO: GRB%s DAG %s is missing: %s"%(grb.name, dag.type, dag.get_dagname()))


    if fstat == 0:

      # The DAG is running
      peu.info(grb.name, 'INFO: GRB%s DAG %s is running in stage %s.'%\
                 (grb.name, dag_key, dag.get_stage_name()))

    elif fstat == 1:

      peu.info(grb.name, 'INFO: GRB%s DAG %s has completed stage %s; starting the next stage.'%\
                 (grb.name, dag_key, dag.get_stage_name()))

      if status==3:
        # finalize this DAG
        analysis_finalized(grb, dag)

      elif status==1:
        # start postproc (for onoff) or first the ligolw DAG before
        if dag_key == 'onoff':
          start_post(grb, dag, cp)
        elif dag_key == 'inj':
           
          # postprocessing of the likelihood code can be started only if
          # the onoff DAG has finished, i.e. the onoff status is at least 2
          # two steps at once are done here...
          if grb.dag['onoff'].status>=2:
            
            start_ligolw(grb, dag)
            start_post_inj(grb, dag, cp)
          else:
            peu.info(grb.name, 'INFO: GRB%s DAG %s has completed stage %s, and is waiting for the onoff DAG.'%\
                 (grb.name, dag_key, dag.get_stage_name()))
        else:
          raise ValueError, "There is no %s DAG in the analysis..." % dag_key
    
    else:
      # The DAG has (still) an error
      peu.info(grb.name, 'INFO: GRB%s DAG %s has an ERROR running stage: %s '%\
               (grb.name, dag_key, dag.get_stage_name()))


# write out the new status
peu.write_monitor_list(monitor_list)

#
# Update the summary page for all GRBs
#
publish_path = cp.get('paths','publishing_path')
publish_url = cp.get('paths','publishing_url')
peu.generate_summary(publish_path, publish_url)

# remove lock file 
# FIXME: Needs to be removed for any error exit...
peu.del_lock()
