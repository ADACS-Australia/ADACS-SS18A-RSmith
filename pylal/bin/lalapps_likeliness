#!/usr/bin/python
#
# Copyright (C) 2008  Drew Keppel
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.


#
# =============================================================================
#
#				   Preamble
#
# =============================================================================
#


import bisect
from new import instancemethod
from optparse import OptionParser
try:
	import sqlite3
except ImportError:
	# pre 2.5.x
	from pysqlite2 import dbapi2 as sqlite3
import sys


from glue import segments
from glue import iterutils
from glue.ligolw import lsctables
from glue.ligolw import table
from glue.ligolw import dbtables
from glue.ligolw import utils
from pylal import git_version
from pylal import rate
from pylal import db_thinca_rings
from pylal.xlal.datatypes.ligotimegps import LIGOTimeGPS
import scipy
import numpy
lsctables.LIGOTimeGPS = LIGOTimeGPS


__author__ = "Drew G. Keppel <drew.keppel@ligo.org>"
__version__ = "git id %s" % git_version.id
__date__ = git_version.date


#
# =============================================================================
#
#				 Command Line
#
# =============================================================================
#


def parse_command_line():
	parser = OptionParser(
		version = "Name: %%prog\n%s" % git_version.verbose_msg,
		usage = "%prog [options] [file ...]",
		description = "%prog does blah blah blah."
	)
	parser.add_option("-t", "--tmp-space", metavar = "path", help = "Path to a directory suitable for use as a work area while manipulating the database file.  The database file will be worked on in this directory, and then moved to the final location when complete.  This option is intended to improve performance when running in a networked environment, where there might be a local disk with higher bandwidth than is available to the filesystem on which the final output will reside.")
	parser.add_option("--output-dir", metavar = "path", help = "Path to directory to store output files")
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")
	options, filenames = parser.parse_args()

	return options, (filenames or [None])


#
# =============================================================================
#
#				 Book-Keeping
#
# =============================================================================
#


class Summaries(object):
	def __init__(self, Nbins):

		#
		# setting up required attributes
		#

		self.Nbins = Nbins
		self.rings = None
		self.offset_vectors = None
		self.ifos = set()
		self.dbls = set()
		self.likelihoods = {}

		#
		# here we define the single detector 2D projections
		# we will perform and how they should be binned
		#

		self.sngl_slices = [('snr', 'chisq'), ('snr', 'mtotal'), ('chisq', 'mtotal')]
		self.atanlog_atan_bins = [('snr', 'mtotal'), ('chisq', 'mtotal')]
		self.atanlog_atanlog_bins = [('snr', 'chisq')]
		self.atan_atan_bins = []
		self.sngl_bins = {}
		self.sngl_1D_bins = {}
		self.sngl_1D_count = {}
		sngls_set = set()
		for col1,col2 in self.sngl_slices:
			sngls_set.add(col1)
			sngls_set.add(col2)

		#
		# store the location of all the triggers
		#

		self.idx_sngls = {}
		for id,key in enumerate(list(sngls_set) + ['ifo', 'weight', 'coinc_event_id', 'filename']):
			self.idx_sngls[key] = id

		#
		# setup min and max values for use in the bins
		#

		self.minmax = {}
		self.minmax['snr'] = (1, 1e4)
		self.minmax['chisq'] = (1, 1e8)
		self.minmax['mtotal'] = (25, 1e2)

	def calc_delta_t_inj(self, trigger1_end_time, trigger1_end_time_ns, trigger2_end_time, trigger2_end_time_ns):
		#
		# not used yet, only using single detector statistics
		#
		try:
			return abs((trigger1_end_time - trigger2_end_time) + (trigger1_end_time_ns - trigger2_end_time)*1e-9)
		except: print "calc_delta_t_inj() failed"

	def calc_delta_t(self, trigger1_ifo, trigger1_end_time, trigger1_end_time_ns, trigger2_ifo, trigger2_end_time, trigger2_end_time_ns, time_slide_id):
		#
		# not used yet, only using single detector statistics
		#
		trigger1_true_end_time = dbtables.lsctables.LIGOTimeGPS(trigger1_end_time, trigger1_end_time_ns)
		trigger2_true_end_time = dbtables.lsctables.LIGOTimeGPS(trigger2_end_time, trigger2_end_time_ns)
		# find the instruments that were on at trigger 1's end time and
		# find the ring that contains this trigger
		try:
			[ring] = [segs[segs.find(trigger1_end_time)] for segs in self.rings.values() if trigger1_end_time in segs]
		except ValueError:
			# FIXME THERE SEEMS TO BE A BUG IN  THINCA!  Occasionally thinca records a trigger on the upper boundary
			# of its ring.  This would make it outside the ring which is very problematic.  It needs to be fixed in thinca
			# for now we'll allow the additional check that the other trigger is in the ring and use it.
			print >>sys.stderr, "trigger1 found not on a ring, trying trigger2"
			[ring] = [segs[segs.find(trigger2_end_time)] for segs in self.rings.values() if trigger2_end_time in segs]
			# now we can unslide the triggers on the ring
		try:
			trigger1_true_end_time = SnglInspiralUtils.slideTimeOnRing(trigger1_true_end_time, self.offset_vectors[time_slide_id][trigger1_ifo], ring)
			trigger2_true_end_time = SnglInspiralUtils.slideTimeOnRing(trigger2_true_end_time, self.offset_vectors[time_slide_id][trigger2_ifo], ring)
			out = abs(trigger1_true_end_time - trigger2_true_end_time)
			return float(out)
		except:
			print "calc delta t failed",trigger1_true_end_time, trigger2_true_end_time, ring
			return float(abs(trigger1_true_end_time - trigger2_true_end_time)) % 1

	def setup_binned_ratios(self):
		#
		# setting up the binned ratio objects
		#
		for ifo in self.ifos:
			self.sngl_bins[ifo] = {}
			for key in self.sngl_slices:
				if key in self.atanlog_atanlog_bins:
					self.sngl_bins[ifo][key] = rate.BinnedRatios(rate.NDBins((rate.ATanLogarithmicBins(self.minmax[key[0]][0], self.minmax[key[0]][1], self.Nbins), rate.ATanLogarithmicBins(self.minmax[key[1]][0], self.minmax[key[1]][1], self.Nbins))))
				elif key in self.atanlog_atan_bins:
					self.sngl_bins[ifo][key] = rate.BinnedRatios(rate.NDBins((rate.ATanLogarithmicBins(self.minmax[key[0]][0], self.minmax[key[0]][1], self.Nbins), rate.ATanBins(self.minmax[key[1]][0], self.minmax[key[1]][1], self.Nbins))))
				elif key in self.atan_atan_bins:
					self.sngl_bins[ifo][key] = rate.BinnedRatios(rate.NDBins((rate.ATanBins(self.minmax[key[0]][0], self.minmax[key[0]][1], self.Nbins), rate.ATanBins(self.minmax[key[1]][0], self.minmax[key[1]][1], self.Nbins))))
				else:
					print >>sys.stderr,"ERROR: binning not implemented for ",key
					sys.exit(1)

	def add_bkg_sngl(self, values):
		#
		# add a background single to its appropriate bins
		#
		for key in self.sngl_slices:
			self.sngl_bins[values[self.idx_sngls['ifo']]][key].incdenominator((values[self.idx_sngls[key[0]]], values[self.idx_sngls[key[1]]]), weight=values[self.idx_sngls['weight']])

	def add_inj_sngl(self, values):
		#
		# add an injection single to its appropriate bins
		#
		for key in self.sngl_slices:
			self.sngl_bins[values[self.idx_sngls['ifo']]][key].incnumerator((values[self.idx_sngls[key[0]]], values[self.idx_sngls[key[1]]]), weight=values[self.idx_sngls['weight']])

	def add_bkg_dbl(self, filename, ifos, values):
		#
		# add a background double to its appropriate bins
		#
		self.dbls.add(frozenset(ifos))
		self.bkg_dbls.setdefault(frozenset(ifos), []).append(tuple([filename] + list(values)))

	def add_inj_dbl(self, filename, ifos, values):
		#
		# add an injection double to its appropriate bins
		#
		self.dbls.add(frozenset(ifos))
		self.inj_dbls.setdefault(frozenset(ifos), []).append(tuple([filename] + list(values)))

	def filter_binned_ratios(self):
		#
		# smooth the binned array objects
		#
		for key1 in self.sngl_bins.keys():
			for key2 in self.sngl_bins[key1].keys():
				tablename = "".join(key1)+"_"
				if len(key2[0]) > 1 or len(key2[1]) > 1:
					tablename += "".join(key2[1]) + "_vs_" + "".join(key2[0])
				else:
					tablename += "_vs_".join(key2[::-1])
				if options.verbose:
					print >>sys.stderr, "\tsmoothing %s ..." % tablename
				rate.filter_binned_ratios(self.sngl_bins[key1][key2], rate.gaussian_window2d(self.sngl_bins[key1][key2].numerator.bins.shape[0]/30, self.sngl_bins[key1][key2].numerator.bins.shape[1]/30, sigma=60))
				self.sngl_bins[key1][key2].to_pdf()

	def output_bins(self):
		#
		# save the binned array objects to xml
		#
		xmldoc = lsctables.ligolw.Document()
		xmldoc.appendChild(lsctables.ligolw.LIGO_LW())
		# FIXME should be key2[1] vs key2[0]
		for key1 in self.sngl_bins.keys():
			for key2 in self.sngl_bins[key1].keys():
				tablename = "".join(key1)+"_"
				if len(key2[0]) > 1 or len(key2[1]) > 1:
					tablename += "".join(key2[1]) + "_vs_" + "".join(key2[0])
				else:
					tablename += "_vs_".join(key2[::-1])
				xmldoc.childNodes[-1].appendChild(rate.binned_ratios_to_xml(self.sngl_bins[key1][key2], tablename))
		for key1 in self.sngl_1D_bins.keys():
			for key2 in self.sngl_1D_bins[key1].keys():
				tablename = "".join(key1)+"_"+"".join(key2)
				xmldoc.childNodes[-1].appendChild(rate.binned_ratios_to_xml(self.sngl_1D_bins[key1][key2], tablename))
		if options.output_dir:
			utils.write_filename(xmldoc, options.output_dir + "/likelihood_snr_chisq.xml.gz", gz = True, verbose = True)
		else:
			utils.write_filename(xmldoc, "likelihood_snr_chisq.xml.gz", gz = True, verbose = True)

	def create_1D_slices(self):
		#
		# create the appropriate 1D slices from the 2D projections
		#
		for ifo in self.sngl_bins.keys():
			self.sngl_1D_bins[ifo] = {}
			self.sngl_1D_count[ifo] = {}
			for key in self.sngl_bins[ifo].keys():
				for dim,subkey in enumerate(key):
					if subkey not in self.sngl_1D_bins[ifo].keys():
						self.sngl_1D_bins[ifo][subkey] = rate.marginalize_ratios(self.sngl_bins[ifo][key], (dim+1)%2)
						self.sngl_1D_count[ifo][subkey] = 0
					else:
						self.sngl_1D_count[ifo][subkey] += 1

	def compute_sngl_likelihood(self, values):
		#
		# compute the likelihood for the single detector triggers
		#
		likelihood_2D = 1.
		likelihood_1D = 1.
		for key in self.sngl_slices:
			likelihood_2D *= self.sngl_bins[values[self.idx_sngls['ifo']]][key][(values[self.idx_sngls[key[0]]],values[self.idx_sngls[key[1]]])]
		for key in self.sngl_1D_bins[values[self.idx_sngls['ifo']]].keys():
			likelihood_1D *= (self.sngl_1D_bins[values[self.idx_sngls['ifo']]][key][(values[self.idx_sngls[key]],)])**self.sngl_1D_count[values[self.idx_sngls['ifo']]][key]
		self.likelihoods[(values[self.idx_sngls['filename']],values[self.idx_sngls['coinc_event_id']])] *= likelihood_2D / likelihood_1D

	def likelihood_map(self, filename, coinc_event_id):
		#
		# map the likehoods to the appropriate triggers
		#
		return self.likelihoods[(filename, coinc_event_id)]


#
# =============================================================================
#
#				SQLITE Queries
#
# =============================================================================
#

def single_inj(values):
	#
	# the SQL query for single detector injection triggers
	#
	query = """
SELECT"""
	keys = values.items()
	keys.sort(cmp=lambda x,y: cmp(x[1],y[1]))
	for value in keys:
		if value[0] == 'coinc_event_id' or value[0] == 'filename':
			continue
		if not value == keys[0]:
			query += ""","""
		if value[0] == 'weight':
			query += """
	-- Work out the correction factor for injection population distances
	-- FIXME this breaks if more than one inspinj job
	CASE (SELECT value FROM process_params WHERE program =="inspinj" AND param =="--d-distr")
		WHEN "log10" THEN  sim_inspiral.distance * sim_inspiral.distance * sim_inspiral.distance
		WHEN "uniform" THEN  sim_inspiral.distance * sim_inspiral.distance
		ELSE 1.0 END"""
			continue

		query += """
	snglA.""" + value[0]

	query += """,
	coinc_inspiral.coinc_event_id
FROM
	coinc_inspiral
	JOIN coinc_event_map AS mapA ON (mapA.coinc_event_id == coinc_inspiral.coinc_event_id)
	JOIN sngl_inspiral AS snglA ON (snglA.event_id == mapA.event_id)
	JOIN coinc_event_map AS mapB ON (mapB.event_id == coinc_inspiral.coinc_event_id)
	JOIN coinc_event_map AS mapC ON (mapC.coinc_event_id == mapB.coinc_event_id)
	JOIN sim_inspiral ON (sim_inspiral.simulation_id == mapC.event_id)
	JOIN coinc_event AS sim_coinc_event ON (sim_coinc_event.coinc_event_id == mapC.coinc_event_id)
	JOIN coinc_definer ON (coinc_definer.coinc_def_id == sim_coinc_event.coinc_def_id)
WHERE
	coinc_definer.search == 'inspiral'
	AND coinc_definer.search_coinc_type == 2
	AND mapA.table_name == 'sngl_inspiral'
	AND mapB.table_name == 'coinc_event'
	AND mapC.table_name == 'sim_inspiral'
	-- require coinc to not be background (= at least one of its time slide offsets is non-zero)
	-- FIXME this has to call a function to get coinc_def id
	AND NOT EXISTS (
		SELECT
			*
		FROM
			time_slide
		WHERE
			time_slide.time_slide_id == sim_coinc_event.time_slide_id
			AND time_slide.offset != 0
	)
"""

	return query

def double_inj():
	#
	# not in use, only using single detector statistics
	#
	return """
SELECT
	coinc_inspiral.coinc_event_id,
	calc_delta_t_inj(snglA.end_time, snglA.end_time_ns, snglB.end_time, snglB.end_time_ns),
	abs(2*(snglA.mchirp - snglB.mchirp)/(snglA.mchirp+snglB.mchirp)),
	abs(2*(snglA.eta - snglB.eta)/(snglA.eta+snglB.eta)),
	snglA.snr,
	snglB.snr,
	snglA.chisq,
	snglB.chisq,
	snglA.rsqveto_duration,
	snglB.rsqveto_duration,
	snglA.bank_chisq,
	snglB.bank_chisq,
	snglA.cont_chisq,
	snglB.cont_chisq,
	-- Work out the correction factor for injection population distances
	-- FIXME this breaks if more than one inspinj job
	CASE (SELECT value FROM process_params WHERE program =="inspinj" AND param =="--d-distr")
		WHEN "log10" THEN  sim_inspiral.distance * sim_inspiral.distance * sim_inspiral.distance
		WHEN "uniform" THEN  sim_inspiral.distance * sim_inspiral.distance
		ELSE 1.0 END
FROM
	coinc_inspiral
	JOIN coinc_event_map AS mapA ON (mapA.coinc_event_id == coinc_inspiral.coinc_event_id)
	JOIN coinc_event_map AS mapB ON (mapB.coinc_event_id == coinc_inspiral.coinc_event_id)
	JOIN sngl_inspiral AS snglA ON (snglA.event_id == mapA.event_id)
	JOIN sngl_inspiral AS snglB ON (snglB.event_id == mapB.event_id)
	JOIN coinc_event_map AS mapC ON (mapC.event_id == coinc_inspiral.coinc_event_id)
	JOIN coinc_event_map AS mapD ON (mapD.coinc_event_id == mapC.coinc_event_id)
	JOIN sim_inspiral ON (sim_inspiral.simulation_id == mapD.event_id)
	JOIN coinc_event AS sim_coinc_event ON (sim_coinc_event.coinc_event_id == mapD.coinc_event_id)
	JOIN coinc_event AS insp_coinc_event ON (insp_coinc_event.coinc_event_id == mapA.coinc_event_id)
	JOIN coinc_definer ON (coinc_definer.coinc_def_id == sim_coinc_event.coinc_def_id)
WHERE
	coinc_definer.search == 'inspiral'
	AND coinc_definer.search_coinc_type == 2
	AND mapA.table_name == 'sngl_inspiral'
	AND mapB.table_name == 'sngl_inspiral'
	AND mapC.table_name == 'coinc_event'
	AND mapD.table_name == 'sim_inspiral'
	AND snglA.ifo == ?
	AND snglB.ifo == ?
	-- require coinc to not be background (= at least one of its time slide offsets is non-zero)
	-- FIXME this has to call a function to get coinc_def id
	AND NOT EXISTS (
		SELECT
			*
		FROM
			time_slide
		WHERE
			time_slide.time_slide_id == coinc_event.time_slide_id
			AND time_slide.offset != 0
	)
"""

def single_bkg(values):
	#
	# the SQL query for single detector background triggers
	#
	query = """
SELECT"""
	keys = values.items()
	keys.sort(cmp=lambda x,y: cmp(x[1],y[1]))
	for value in keys:
		if value[0] in ['coinc_event_id', 'filename']:
			continue
		if not value == keys[0]:
			query += ""","""
		if value[0] == 'weight':
			query += """
	1.0"""
			continue

		query += """
	snglA.""" + value[0]

	query += """,
	coinc_inspiral.coinc_event_id,
	EXISTS (
		SELECT
			*
		FROM
			time_slide
		WHERE
			time_slide.time_slide_id == coinc_event.time_slide_id
			AND time_slide.offset != 0
	)
FROM
	coinc_inspiral
	JOIN coinc_event_map AS mapA ON (mapA.coinc_event_id == coinc_inspiral.coinc_event_id)
	JOIN sngl_inspiral AS snglA ON (snglA.event_id == mapA.event_id)
	JOIN coinc_event ON (mapA.coinc_event_id == coinc_event.coinc_event_id)
	JOIN coinc_definer ON (coinc_definer.coinc_def_id == coinc_event.coinc_def_id)
WHERE
	coinc_definer.search == 'inspiral'
	AND coinc_definer.search_coinc_type == 0
	AND mapA.table_name == 'sngl_inspiral'
"""

	return query

def double_bkg():
	#
	# not in use, only using single detector statistics
	#
	return """
SELECT
	coinc_inspiral.coinc_event_id,
	calc_delta_t(snglA.ifo, snglA.end_time, snglA.end_time_ns, snglB.ifo, snglB.end_time, snglB.end_time_ns, coinc_event.time_slide_id),
	abs(2*(snglA.mchirp - snglB.mchirp)/(snglA.mchirp+snglB.mchirp)),
	abs(2*(snglA.eta - snglB.eta)/(snglA.eta+snglB.eta)),
	snglA.snr,
	snglB.snr,
	snglA.chisq,
	snglB.chisq,
	snglA.rsqveto_duration,
	snglB.rsqveto_duration,
	snglA.bank_chisq,
	snglB.bank_chisq,
	snglA.cont_chisq,
	snglB.cont_chisq,
	EXISTS (
		SELECT
			*
		FROM
			time_slide
		WHERE
			time_slide.time_slide_id == coinc_event.time_slide_id
			AND time_slide.offset != 0
	)
FROM
	coinc_inspiral
	JOIN coinc_event_map AS mapA ON (mapA.coinc_event_id == coinc_inspiral.coinc_event_id)
	JOIN coinc_event_map AS mapB ON (mapB.coinc_event_id == coinc_inspiral.coinc_event_id)
	JOIN sngl_inspiral AS snglA ON (snglA.event_id == mapA.event_id)
	JOIN sngl_inspiral AS snglB ON (snglB.event_id == mapB.event_id)
	JOIN coinc_event ON (mapA.coinc_event_id == coinc_event.coinc_event_id)
	JOIN coinc_definer ON (coinc_definer.coinc_def_id == coinc_event.coinc_def_id)
WHERE
	coinc_definer.search == 'inspiral'
	AND coinc_definer.search_coinc_type == 0
	AND mapA.table_name == 'sngl_inspiral'
	AND mapB.table_name == 'sngl_inspiral'
	AND snglA.ifo == ?
	AND snglB.ifo == ?
"""

#
# =============================================================================
#
#				     Main
#
# =============================================================================
#


#
# command line
#


options, filenames = parse_command_line()


#
# initialize book-keeping
#


likelihood_bins = Summaries(200)


#
# iterate over database files accumulating statistics
#


if options.verbose:
	print >>sys.stderr, "collecting initial likelihood statistics ..."

for n, filename in enumerate(filenames):
	#
	# open the database
	#

	if options.verbose:
		print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
	working_filename = dbtables.get_connection_filename(filename, tmp_path = options.tmp_space, verbose = options.verbose)
	connection = sqlite3.connect(working_filename)

	if options.verbose:
		print >>sys.stderr, "\textracting all ifos ..."
	# FIXME get ifos from search summary table
	likelihood_bins.ifos |= set(ifo for ifo, in connection.cursor().execute("""SELECT DISTINCT ifo FROM sngl_inspiral"""))

	if options.verbose:
		print >>sys.stderr, "\textracting likelihood from all coinc_events ..."
	for coinc_event_id, likelihood in connection.cursor().execute("""
SELECT
	coinc_event.coinc_event_id,
	coinc_event.likelihood
FROM
	coinc_event
	"""):
		likelihood_bins.likelihoods.setdefault((filename, coinc_event_id), likelihood)

	#
	# done extracting info so close database
	#

	connection.close()
	dbtables.discard_connection_filename(filename, working_filename, verbose = options.verbose)

#
# end loop over input database files
#

if options.verbose:
	print >>sys.stderr, "initializing likelihoods to 1.0  ..."
for key in likelihood_bins.likelihoods.keys():
	likelihood_bins.likelihoods[key] = 1.0

if options.verbose:
	print >>sys.stderr, "setting up binned ratio objects ..."
likelihood_bins.setup_binned_ratios()

if options.verbose:
	print >>sys.stderr, "extracting background and injection statistics ..."
for n, filename in enumerate(filenames):
	#
	# open the database
	#

	if options.verbose:
		print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
	working_filename = dbtables.get_connection_filename(filename, tmp_path = options.tmp_space, verbose = options.verbose)
	connection = sqlite3.connect(working_filename)

	#
	# extract triggers using appropriate sqlite queries determined
	# by the existence of a sim_inspiral table in the database
	#

	if "sim_inspiral" in dbtables.get_table_names(connection):
		#
		# loop over all single ifos and add sngls to appropriate list
		#

		if options.verbose:
			print >>sys.stderr, "\textracting all sngls to populate binned ratio numerator ..."
		for values in connection.cursor().execute(single_inj(likelihood_bins.idx_sngls)):
			likelihood_bins.add_inj_sngl(values)

		#
		# loop over all double combinations of ifos and add dbls to appropriate list
		#

#		if options.verbose:
#			print >>sys.stderr, "\textracting all dbls ..."
#		connection.create_function("calc_delta_t_inj", 4, likelihood_bins.calc_delta_t_inj)
#		for ifos in list(iterutils.choices(likelihood_bins.ifos, 2)):
#			if options.verbose:
#				print >>sys.stderr, "\t\t%s ..." % ("".join(ifos))
#			for values in connection.cursor().execute(double_inj, ifos):
#				likelihood_bins.add_inj_dbl(filename, ifos, values)
	else:
		#
		# loop over all single ifos and add sngls to appropriate list
		#

		if options.verbose:
			print >>sys.stderr, "\textracting all sngls to populate binned ratio denominator ..."
		for values in connection.cursor().execute(single_bkg(likelihood_bins.idx_sngls)):
			if values[-1]:
				likelihood_bins.add_bkg_sngl(values)

		#
		# loop over all double combinations of ifos and add dbls to appropriate list
		#

#		if options.verbose:
#			print >>sys.stderr, "\textracting all dbls ..."
#		likelihood_bins.rings = db_thinca_rings.get_thinca_rings_by_available_instruments(connection)
#		likelihood_bins.offset_vectors = dbtables.lsctables.table.get_table(dbtables.get_xml(connection), dbtables.lsctables.TimeSlideTable.tableName).as_dict()
#		connection.create_function("calc_delta_t", 7, likelihood_bins.calc_delta_t)
#		for ifos in list(iterutils.choices(likelihood_bins.ifos, 2)):
#			if options.verbose:
#				print >>sys.stderr, "\t\t%s ..." % ("".join(ifos))
#			for values in connection.cursor().execute(double_bkg, ifos):
#				if values[-1]:
#					background.add_bkg_dbl(filename, ifos, values[0:-1])
#				else:
#					background.add_zero_lag_dbl(filename, ifos, values[0:-1])

	#
	# done extracting info so close database
	#

	connection.close()
	dbtables.discard_connection_filename(filename, working_filename, verbose = options.verbose)

#
# end loop over input database files
#

if options.verbose:
	print >>sys.stderr, "smoothing binned ratios ..."
likelihood_bins.filter_binned_ratios()
if options.verbose:
	print >>sys.stderr, "creating 1D binned ratios ..."
likelihood_bins.create_1D_slices()
if options.verbose:
	print >>sys.stderr, "writing out binned ratios ..."
likelihood_bins.output_bins()

sys.exit(0)

if options.verbose:
	print >>sys.stderr, "computing likelihoods ..."
for n, filename in enumerate(filenames):
	#
	# open the database
	#

	if options.verbose:
		print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
	working_filename = dbtables.get_connection_filename(filename, tmp_path = options.tmp_space, verbose = options.verbose)
	connection = sqlite3.connect(working_filename)

	#
	# extract triggers using appropriate sqlite queries determined
	# by the existence of a sim_inspiral table in the database
	#

	if "sim_inspiral" in dbtables.get_table_names(connection):
		#
		# loop over all sngls and add to appropriate list
		#

		if options.verbose:
			print >>sys.stderr, "\textracting all sngls for likelihood computation..."
		for values in connection.cursor().execute(single_inj(likelihood_bins.idx_sngls)):
			likelihood_bins.compute_sngl_likelihood(tuple(list(values) + [filename]))

		#
		# loop over all double combinations of ifos and add dbls to appropriate list
		#

#		if options.verbose:
#			print >>sys.stderr, "\textracting all dbls ..."
#		connection.create_function("calc_delta_t_inj", 4, likelihood_bins.calc_delta_t_inj)
#		for ifos in list(iterutils.choices(likelihood_bins.ifos, 2)):
#			if options.verbose:
#				print >>sys.stderr, "\t\t%s ..." % ("".join(ifos))
#			for values in connection.cursor().execute(double_inj, ifos):
#				likelihood_bins.add_inj_dbl(filename, ifos, values)
	else:
		# 
		# loop over all sngls and add to appropriate list 
		#

		if options.verbose:
			print >>sys.stderr, "\textracting all sngls for likelihood computation ..."
		for values in connection.cursor().execute(single_bkg(likelihood_bins.idx_sngls)):
			likelihood_bins.compute_sngl_likelihood(tuple(list(values[0:-1]) + [filename]))
 
		# 
		# loop over all double combinations of ifos and add dbls to appropriate list 
		#

#		if options.verbose:
#			print >>sys.stderr, "\textracting all dbls ..."
#		likelihood_bins.rings = db_thinca_rings.get_thinca_rings_by_available_instruments(connection)
#		likelihood_bins.offset_vectors = dbtables.lsctables.table.get_table(dbtables.get_xml(connection), dbtables.lsctables.TimeSlideTable.tableName).as_dict()
#		connection.create_function("calc_delta_t", 7, likelihood_bins.calc_delta_t)
#		for ifos in list(iterutils.choices(likelihood_bins.ifos, 2)):
#			if options.verbose:
#				print >>sys.stderr, "\t\t%s ..." % ("".join(ifos))
#			for values in connection.cursor().execute(double_bkg, ifos):
#				if values[-1]:
#					background.add_bkg_dbl(filename, ifos, values[0:-1])
#				else:
#					background.add_zero_lag_dbl(filename, ifos, values[0:-1])

	#
	# done extracting info so close database
	#

	connection.close()
	dbtables.discard_connection_filename(filename, working_filename, verbose = options.verbose)

#
# end loop over input database files
#

#
# iterate over database files assigning likelihoods to coincs
#

if options.verbose:
	print >>sys.stderr, "updating likelihoods in databases ..."
for n, filename in enumerate(filenames):
	#
	# open the database
	#

	if options.verbose:
		print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
	working_filename = dbtables.get_connection_filename(filename, tmp_path = options.tmp_space, verbose = options.verbose)
	connection = sqlite3.connect(working_filename)

	#
	# prepare the database
	#

	connection.create_function("likelihood_map", 2, likelihood_bins.likelihood_map)
	if options.verbose:
		print >>sys.stderr, "\trecording likelihood ..."
	connection.cursor().execute("""
UPDATE
	coinc_event
SET
	likelihood = likelihood_map(
			?,
			coinc_event.coinc_event_id
	)
	""", (filename,))
	connection.commit()

	#
	# close the database
	#

	connection.close()
	dbtables.put_connection_filename(filename, working_filename, verbose = options.verbose)
