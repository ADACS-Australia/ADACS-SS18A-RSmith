#!/usr/bin/python
#
# Copyright (C) 2006--2009  Kipp Cannon
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.


#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


"""
String cusp search final output rendering tool.
"""


import bisect
from optparse import OptionParser
import math
from matplotlib import patches
import numpy
from scipy import interpolate
from scipy import optimize
try:
	import sqlite3
except ImportError:
	# pre 2.5.x
	from pysqlite2 import dbapi2 as sqlite3
import sys
try:
	any
	all
except NameError:
	from glue.iterutils import any, all


from glue.ligolw import ligolw
from glue.ligolw import lsctables
from glue.ligolw import dbtables
from glue.ligolw import utils
from glue import iterutils
from glue import segments
from pylal import git_version
from pylal import rate
from pylal import SimBurstUtils
from pylal import SnglBurstUtils
from pylal.xlal.datatypes.ligotimegps import LIGOTimeGPS


SnglBurstUtils.matplotlib.rcParams.update({
	"font.size": 10.0,
	"axes.titlesize": 10.0,
	"axes.labelsize": 10.0,
	"xtick.labelsize": 8.0,
	"ytick.labelsize": 8.0,
	"legend.fontsize": 8.0
})


lsctables.LIGOTimeGPS = LIGOTimeGPS


__author__ = "Kipp Cannon <kipp.cannon@ligo.org>"
__version__ = "git id %s" % git_version.id
__date__ = git_version.date


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


def parse_command_line():
	parser = OptionParser(
		version = "Name: %%prog\n%s" % git_version.verbose_msg,
		usage = "%prog [options] [file ...]",
		description = "%prog performs the final, summary, stages of the upper-limit string cusp search.  Input files ending in \".gz\" are assumed to be gzip-compressed, and if no files are specified on the command line then input is read from stdin."
	)
	parser.add_option("-a", "--amplitude-factor", metavar = "factor", type = "float", default = 1e-20, help = "Multiple amplitudes in XML files by this amount (default = 1e-20).")
	parser.add_option("--cal-uncertainty", metavar = "fraction", type = "float", default = 0.0, help = "Set the fractional uncertainty in amplitude due to calibration uncertainty (eg. 0.08).")
	parser.add_option("-i", "--injections", action = "store_true", help = "Generate efficiency plots from the output files of an injection run.")
	parser.add_option("--injections-bin-size", metavar = "bins", type = "float", default = 16.7, help = "Set bin width for injection efficiency curves.")
	parser.add_option("--loudest-survivor", metavar = "likelihood-ratio", type = "float", help = "Set likelihood ratio from loudest zero-lag non-injection survivor to use as detection threshold for efficiency measurement.")
	parser.add_option("--image-formats", metavar = "ext[,ext,...]", default = "png,pdf", help = "Set list of graphics formats to produce by providing a comma-delimited list of the filename extensions.  Default = \"png,pdf\"")
	parser.add_option("-l", "--live-time-program", metavar = "program", default = "StringSearch", help = "Set the name, as it appears in the process table, of the program whose search summary entries define the search live time (default = StringSearch).")
	parser.add_option("-t", "--tmp-space", metavar = "path", help = "Path to a directory suitable for use as a work area while manipulating the database file.  The database file will be worked on in this directory, and then moved to the final location when complete.  This option is intended to improve performance when running in a networked environment, where there might be a local disk with higher bandwidth than is available to the filesystem on which the final output will reside.")
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")
	options, filenames = parser.parse_args()

	if options.injections and options.loudest_survivor is None:
		raise ValueError, "must set --loudest-survivor with --injections"

	if options.cal_uncertainty is None:
		raise ValueError, "must set --cal-uncertainty"

	options.image_formats = options.image_formats.split(",")

	return options, (filenames or [None])


#
# =============================================================================
#
#                              Zero-Lag Survivors
#
# =============================================================================
#


class Survivors(object):
	def __init__(self):
		self.xmldoc = None

	def add_contents(self, contents):
		if self.xmldoc is None:
			self.xmldoc = ligolw.Document()
			self.xmldoc.appendChild(ligolw.LIGO_LW())
			self.sngl_burst_table = lsctables.New(lsctables.SnglBurstTable, contents.sngl_burst_table.columnnames)
			self.xmldoc.childNodes[0].appendChild(self.sngl_burst_table)

		for values in contents.connection.cursor().execute("""
SELECT
	sngl_burst.*
FROM
	coinc_event
	JOIN coinc_event_map ON (
		coinc_event_map.coinc_event_id == coinc_event.coinc_event_id
	)
	JOIN sngl_burst ON (
		coinc_event_map.table_name == 'sngl_burst'
		AND coinc_event_map.event_id == sngl_burst.event_id
	)
WHERE
	coinc_event.coinc_def_id == ?
	AND NOT EXISTS (
		SELECT
			*
		FROM
			time_slide
		WHERE
			time_slide.time_slide_id == coinc_event.time_slide_id
			AND time_slide.offset != 0
	)
		""", (contents.bb_definer_id,)):
			self.sngl_burst_table.append(contents.sngl_burst_table.row_from_cols(values))

	def finish(self, filename, verbose = False):
		self.sngl_burst_table.sort(lambda a, b: cmp((a.ifo, abs(a.amplitude)), (b.ifo, abs(b.amplitude))))
		utils.write_filename(self.xmldoc, filename, verbose = verbose, gz = (filename or "stdout").endswith(".gz"))


#
# =============================================================================
#
#                              Rate vs. Threshold
#
# =============================================================================
#


def clip_offset_vector(offset_vector, instruments):
	return dict((instrument, offset) for instrument, offset in offset_vector.items() if instrument in instruments)


def clip_offset_vectors(offset_vectors, instruments):
	return dict((id, clip_offset_vector(offset_vector, instruments)) for id, offset_vector in offset_vectors.items())


def ratevsthresh_bounds(background_x, background_y, zero_lag_x, zero_lag_y):
	if len(zero_lag_x):
		if len(background_x):
			minX, maxX = min(min(zero_lag_x), min(background_x)), max(max(zero_lag_x), max(background_x))
			minY, maxY = min(min(zero_lag_y), min(background_y)), max(max(zero_lag_y), max(background_y))
		else:
			minX, maxX = min(zero_lag_x), max(zero_lag_x)
			minY, maxY = min(zero_lag_y), max(zero_lag_y)
	else:
		# don't check for background, if there's no zero-lag and no
		# background we're screwed anyway
		minX, maxX = min(background_x), max(background_x)
		minY, maxY = min(background_y), max(background_y)
	return minX, maxX, minY, maxY


def compress_ratevsthresh_curve(x, y, yerr):
	#
	# construct a background mask to retain the highest-ranked 10,000
	# elements, then every 10th until the 100,000th highest-ranked
	# element, then every 100th after that.  this is for reducing the
	# dataset size so matplotlib can handle it and vector graphics
	# output isn't ridiculous in size.
	#

	mask = numpy.arange(len(x))[::-1]
	mask = (mask < 10000) | ((mask < 100000) & (mask % 10 == 0)) | (mask % 100 == 0)

	return x.compress(mask), y.compress(mask), yerr.compress(mask)


class RateVsThreshold(SnglBurstUtils.BurstPlot):
	def __init__(self):
		SnglBurstUtils.BurstPlot.__init__(self, r"Likelihood Ratio Threshold $\Lambda$", "Event Rate (Hz)", width = 108.0)
		self.zero_lag = []
		self.background = []
		self.zero_lag_time = 0.0
		self.background_time = 0.0
		self.axes.loglog()
		self.axes.set_position([0.125, 0.15, 0.83, 0.75])

	def add_contents(self, contents):
		#
		# retrieve the offset vectors, retain only instruments that
		# are available
		#

		zero_lag_time_slides, background_time_slides = SnglBurstUtils.get_time_slides(contents.connection)
		assert len(zero_lag_time_slides) == 1
		zero_lag_time_slides = clip_offset_vectors(zero_lag_time_slides, contents.instruments)
		background_time_slides = clip_offset_vectors(background_time_slides, contents.instruments)

		#
		# compute the live time
		#

		self.zero_lag_time += SnglBurstUtils.time_slides_livetime(contents.seglists, zero_lag_time_slides.values())
		self.background_time += SnglBurstUtils.time_slides_livetime(contents.seglists, background_time_slides.values())

		#
		# count events
		#

		for likelihood_ratio, is_background in contents.connection.cursor().execute("""
SELECT
	coinc_event.likelihood,
	EXISTS (
		SELECT
			*
		FROM
			time_slide
		WHERE
			time_slide.time_slide_id == coinc_event.time_slide_id
			AND time_slide.offset != 0
	)
FROM
	coinc_event
WHERE
	coinc_event.coinc_def_id == ?
		""", (contents.bb_definer_id,)):
			if is_background:
				self.background.append(likelihood_ratio)
			else:
				self.zero_lag.append(likelihood_ratio)

	def finish(self):
		print "Zero-lag events: %d" % len(self.zero_lag)
		print "Total time in zero-lag segments: %s s" % str(self.zero_lag_time)
		print "Time-slide events: %d" % len(self.background)
		print "Total time in time-slide segments: %s s" % str(self.background_time)

		self.axes.set_title(r"Event Rate vs.\ Likelihood Ratio Threshold")

		self.background.sort()
		self.zero_lag.sort()
		self.background = numpy.array(self.background, dtype = "double")
		self.zero_lag = numpy.array(self.zero_lag, dtype = "double")

		print "Likelihood ratio for highest-ranked zero-lag survivor: %.9g" % self.zero_lag[-1]

		#
		# convert counts to rates and their uncertainties
		#

		# background count expected in zero-lag and \sqrt{N}
		# standard deviation
		background_y = numpy.arange(len(self.background), 0.0, -1.0, dtype = "double") / self.background_time * self.zero_lag_time
		background_yerr = numpy.sqrt(background_y)

		# convert to background rate and uncertainty expected in
		# zero-lag
		background_y /= self.zero_lag_time
		background_yerr /= self.zero_lag_time

		# rate observed in zero-lag
		zero_lag_y = numpy.arange(len(self.zero_lag), 0.0, -1.0, dtype = "double") / self.zero_lag_time

		#
		# determine the horizontal and vertical extent of the plot
		#

		minX, maxX, minY, maxY = ratevsthresh_bounds(self.background, background_y, self.zero_lag, zero_lag_y)
		minX = 1e-2	# FIXME:  don't hard-code

		#
		# compress the background data
		#

		background, background_y, background_yerr = compress_ratevsthresh_curve(self.background, background_y, background_yerr)

		# warning:  the error bar polygon is not *really* clipped
		# to the axes' bounding box, the result will be incorrect
		# if the density of data points is small where the polygon
		# encounters the axes' bounding box.

		poly_x = numpy.concatenate((background, background[::-1]))
		poly_y = numpy.concatenate((background_y + 1 * background_yerr, (background_y - 1 * background_yerr)[::-1]))
		self.axes.add_patch(patches.Polygon(zip(poly_x, numpy.clip(poly_y, minY, maxY)), edgecolor = "k", facecolor = "k", alpha = 0.3))
		line1, = self.axes.loglog(background.repeat(2)[:-1], background_y.repeat(2)[1:], color = "k", linestyle = "--")
		line2, = self.axes.loglog(self.zero_lag.repeat(2)[:-1], zero_lag_y.repeat(2)[1:], color = "k", linestyle = "-", linewidth = 2)

		self.axes.legend((line1, line2), (r"Expected background", r"Zero-lag"), loc = "lower left")

		self.axes.set_xlim((minX, maxX))
		self.axes.set_ylim((minY, maxY))
		self.axes.xaxis.grid(True, which="minor")
		self.axes.yaxis.grid(True, which="minor")


#
# =============================================================================
#
#                                  Efficiency
#
# =============================================================================
#


def create_recovered_likelihood_view(connection, bb_coinc_def_id):
	# Create a temporary table containing two columns:  the
	# simulation_id of an injection, and the highest likelihood ratio
	# at which that injection was recovered by a coincidence of type
	# bb_coinc_def_id.
	cursor = connection.cursor()
	cursor.execute("""
CREATE TEMPORARY TABLE recovered_likelihood (simulation_id TEXT PRIMARY KEY, likelihood REAL)
	""")
	cursor.execute("""
INSERT OR REPLACE INTO
	recovered_likelihood
SELECT
	sim_burst.simulation_id AS simulation_id,
	MAX(coinc_event.likelihood) AS likelihood
FROM
	sim_burst
	JOIN coinc_event_map AS a ON (
		a.table_name == "sim_burst"
		AND a.event_id == sim_burst.simulation_id
	)
	JOIN coinc_event_map AS b ON (
		b.coinc_event_id == a.coinc_event_id
	)
	JOIN coinc_event ON (
		b.table_name == "coinc_event"
		AND b.event_id == coinc_event.coinc_event_id
	)
WHERE
	coinc_event.coinc_def_id == ?
GROUP BY
	sim_burst.simulation_id
	""", (bb_coinc_def_id,))


def slope(x, y):
	"""
	From the x and y arrays, compute the slope at the x co-ordinates
	using a first-order finite difference approximation.
	"""
	slope = numpy.zeros((len(x),), dtype = "double")
	slope[0] = (y[1] - y[0]) / (x[1] - x[0])
	for i in xrange(1, len(x) - 1):
		slope[i] = (y[i + 1] - y[i - 1]) / (x[i + 1] - x[i - 1])
	slope[-1] = (y[-1] - y[-2]) / (x[-1] - x[-2])
	return slope


def upper_err(y, yerr, deltax):
	z = y + yerr
	deltax = int(deltax)
	upper = numpy.zeros((len(yerr),), dtype = "double")
	for i in xrange(len(yerr)):
		upper[i] = max(z[max(i - deltax, 0) : min(i + deltax, len(z))])
	return upper - y


def lower_err(y, yerr, deltax):
	z = y - yerr
	deltax = int(deltax)
	lower = numpy.zeros((len(yerr),), dtype = "double")
	for i in xrange(len(yerr)):
		lower[i] = min(z[max(i - deltax, 0) : min(i + deltax, len(z))])
	return y - lower


def write_efficiency(fileobj, bins, eff, yerr, filterwidth):
	print >>fileobj, "# ln(A)	e	D[e]"
	DlnA = bins[0].delta * filterwidth / 2.0
	for A, e, De in zip(bins.centres()[0], eff, yerr):
		print >>fileobj, math.log(A), e, De


def render_data_from_bins(dump_file, axes, efficiency, cal_uncertainty, filter_width, colour = "k", erroralpha = 0.3, linestyle = "-"):
	# extract array of x co-ordinates, and the factor by which x
	# increases from one sample to the next.
	(x,) = efficiency.centres()
	x_factor_per_sample = efficiency.bins()[0].delta

	# compute the efficiency, the slope (units = efficiency per
	# sample), the y uncertainty (units = efficiency) due to binomial
	# counting fluctuations, and the x uncertainty (units = samples)
	# due to the width of the smoothing filter.
	eff = efficiency.ratio()
	dydx = slope(numpy.arange(len(x), dtype = "double"), eff)
	yerr = numpy.sqrt(eff * (1 - eff) / efficiency.denominator.array)
	xerr = numpy.array([filter_width / 2] * len(yerr))

	# compute the net y err (units = efficiency) by (i) multiplying the
	# x err by the slope, (ii) dividing the calibration uncertainty
	# (units = percent) by the fractional change in x per sample and
	# multiplying by the slope, (iii) adding the two in quadradure with
	# the y err.
	net_yerr = numpy.sqrt((xerr * dydx)**2 + yerr**2 + (cal_uncertainty / x_factor_per_sample * dydx)**2)

	# compute net xerr (units = percent) by dividing yerr by slope and
	# then multiplying by the fractional change in x per sample.
	net_xerr = net_yerr / dydx * x_factor_per_sample

	# write the efficiency data to file
	write_efficiency(dump_file, efficiency.bins(), eff, net_yerr, filter_width)

	# plot the efficiency curve and uncertainty region
	patch = patches.Polygon(zip(numpy.concatenate((x, x[::-1])), numpy.concatenate((eff + upper_err(eff, yerr, filter_width / 2), (eff - lower_err(eff, yerr, filter_width / 2))[::-1]))), edgecolor = colour, facecolor = colour, alpha = erroralpha)
	axes.add_patch(patch)
	line, = axes.plot(x, eff, colour + linestyle)

	# compute 50% point and its uncertainty
	A50 = optimize.bisect(interpolate.interp1d(x, eff - 0.5), x[0], x[-1], xtol = 1e-40)
	A50_err = interpolate.interp1d(x, net_xerr)(A50)

	# mark 50% point on graph
	axes.axvline(A50, color = colour, linestyle = linestyle)

	# print some analysis
	num_injections = efficiency.denominator.array.sum()
	num_samples = len(efficiency.denominator)
	print "Bins were %g samples wide, ideal would have been %g" % (filter_width, (num_samples / num_injections / interpolate.interp1d(x, dydx)(A50)**2.0)**(1.0/3.0))
	print "Average number of injections in each bin = %g" % efficiency.denominator.array.mean()

	return line, A50, A50_err


class Efficiency(SnglBurstUtils.BurstPlot):
	def __init__(self, instruments, loudest_survivor_likelihood, cal_uncertainty, amplitude_factor, filter_width):
		SnglBurstUtils.BurstPlot.__init__(self, r"Injection Amplitude (\(\mathrm{s}^{-\frac{1}{3}}\))", "Detection Efficiency", width = 108.0)
		self.axes.set_title(r"Detection Efficiency vs.\ Amplitude")
		self.instruments = instruments
		self.loudest_survivor_likelihood = loudest_survivor_likelihood
		self.cal_uncertainty = cal_uncertainty
		self.amplitude_factor = amplitude_factor
		self.width = filter_width
		self.found = []
		self.all = []
		self.axes.semilogx()
		self.axes.set_position([0.10, 0.150, 0.86, 0.77])

		# set desired yticks
		self.axes.set_yticks((0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0))
		self.axes.set_yticklabels((r"\(0\)", r"\(0.1\)", r"\(0.2\)", r"\(0.3\)", r"\(0.4\)", r"\(0.5\)", r"\(0.6\)", r"\(0.7\)", r"\(0.8\)", r"\(0.9\)", r"\(1.0\)"))

	def add_contents(self, contents):
		# for each injection, retrieve the highest SNR of the burst
		# events that were found to match the injection and that
		# participated in burst<-->burst coincidences or null if no
		# such matching burst events were found
		create_recovered_likelihood_view(contents.connection, contents.bb_definer_id)
		for values in contents.connection.cursor().execute("""
SELECT
	sim_burst.*,
	recovered_likelihood.likelihood
FROM
	sim_burst
	LEFT JOIN recovered_likelihood ON (
		recovered_likelihood.simulation_id == sim_burst.simulation_id
	)
		"""):
			sim = contents.sim_burst_table.row_from_cols(values[:-1])
			likelihood_ratio = values[-1]
			found = likelihood_ratio is not None
			# were at least 2 instruments on when the injection
			# was made?
			if any(SimBurstUtils.injection_was_made(sim, contents.seglists, instruments) for instruments in iterutils.choices(contents.seglists.keys(), 2)):
				self.all.append(sim)
				if found and likelihood_ratio > self.loudest_survivor_likelihood:
					self.found.append(sim)
			elif found:
				print >>sys.stderr, "odd, injection %s was found but not injected ..." % sim.simulation_id

	def finish(self):
		# put made and found injections in the denominators and
		# numerators of the efficiency bins
		bins = rate.NDBins((rate.LogarithmicBins(min(sim.amplitude for sim in self.all) * self.amplitude_factor, max(sim.amplitude for sim in self.all) * self.amplitude_factor, 400),))
		efficiency = rate.BinnedRatios(bins)
		for sim in self.found:
			efficiency.incnumerator((sim.amplitude * self.amplitude_factor,))
		for sim in self.all:
			efficiency.incdenominator((sim.amplitude * self.amplitude_factor,))

		# regularize:  adjust unused bins so that the efficiency
		# ratio comes out to 0
		efficiency.regularize()

		# generate and plot trend curves.  adjust window function
		# normalization so that denominator array correctly
		# represents the number of injections contributing to each
		# bin:  make w(0) = 1.0.  note that this factor has no
		# effect on the efficiency because it is common to the
		# numerator and denominator arrays.  we do this for the
		# purpose of computing the Poisson error bars, which
		# requires us to know the counts for the bins
		windowfunc = rate.gaussian_window(self.width)
		windowfunc /= windowfunc[len(windowfunc) / 2 + 1]
		rate.filter_binned_ratios(efficiency, windowfunc)

		line1, A50, A50_err = render_data_from_bins(file("string_efficiency.dat", "w"), self.axes, efficiency, self.cal_uncertainty, self.width, colour = "k", linestyle = "-", erroralpha = 0.2)
		print "Pipeline's 50%% efficiency point for all detections = %g +/- %g%%\n" % (A50, A50_err * 100)

		# add a legend to the axes
		self.axes.legend((line1,), (r"\noindent Injections recovered with$\Lambda > %s$" % SnglBurstUtils.latexnumber("%.3g" % self.loudest_survivor_likelihood),), loc = "lower right")

		# adjust limits
		self.axes.set_xlim([4e-21, 1e-17])
		self.axes.set_ylim([0.0, 1.0])


#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#


options, filenames = parse_command_line()

if options.injections:
	plots = [
		Efficiency(("H1", "H2", "L1"), options.loudest_survivor, options.cal_uncertainty, options.amplitude_factor, options.injections_bin_size)
	]
	survivors = None
else:
	plots = [
		RateVsThreshold()
	]
	# FIXME:  Survivors() is busted
	#survivors = Survivors()
	survivors = None


for n, filename in enumerate(filenames):
	if options.verbose:
		print >>sys.stderr, "%d/%d:" % (n + 1, len(filenames)),
	working_filename = dbtables.get_connection_filename(filename, tmp_path = options.tmp_space, verbose = options.verbose)
	connection = sqlite3.connect(working_filename)
	dbtables.DBTable_set_connection(connection)
	database = SnglBurstUtils.CoincDatabase(connection, options.live_time_program, search = "StringCusp", verbose = options.verbose)
	for n, plot in enumerate(plots):
		if options.verbose:
			print >>sys.stderr, "adding to plot %d ..." % n
		plot.add_contents(database)
	if survivors is not None:
		if options.verbose:
			print >>sys.stderr, "storing zero-lag survivors ..."
		survivors.add_contents(database)
	connection.close()
	dbtables.discard_connection_filename(filename, working_filename, verbose = options.verbose)

n = 0
format = "%%s%%0%dd.%%s" % (int(math.log10(max(len(plots) - 1, 1))) + 1)
while plots:
	if options.verbose:
		print >>sys.stderr, "finishing plot %d ..." % n
	plots[0].finish()
	for ext in options.image_formats:
		if options.injections:
			filename = format % ("string_injections_", n, ext)
		else:
			filename = format % ("string_triggers_", n, ext)
		if options.verbose:
			print >>sys.stderr, "writing %s ..." % filename
		plots[0].fig.savefig(filename)
	del plots[0]
	n += 1
if survivors:
	survivors.finish("string_triggers_survivors.xml", verbose = options.verbose)
if options.verbose:
	print >>sys.stderr, "done."
