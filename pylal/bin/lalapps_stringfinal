#!/usr/bin/python
#
# $Id$
#
# Copyright (C) 2006  Kipp C. Cannon
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.


#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


"""
String cusp search final output rendering tool.
"""

import bisect
from optparse import OptionParser
import math
from matplotlib import patches
import numpy
try:
	import sqlite3
except ImportError:
	# pre 2.5.x
	from pysqlite2 import dbapi2 as sqlite3
import sys

from glue.ligolw import ligolw
from glue.ligolw import lsctables
from glue.ligolw import dbtables
from glue.ligolw import utils
from glue import segments
from pylal import rate
from pylal import SnglBurstUtils

__author__ = "Kipp Cannon <kipp@gravity.phys.uwm.edu>"
__version__ = "$Revision$"[11:-2]
__date__ = "$Date$"[7:-2]


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


def parse_command_line():
	parser = OptionParser(
		version="%prog CVS $Id$",
		usage = "%prog [options] [file [file ...]]",
		description = "%prog performs the final, summary, stages of the upper-limit string cusp search.  Input files ending in \".gz\" are assumed to be gzip-compressed, and if no files are specified on the command line then input is read from stdin."
	)
	parser.add_option("-a", "--amplitude-factor", metavar = "factor", default = "1e-20", help = "multiple amplitudes in XML files by this amount (default = 1e-20)")
	parser.add_option("--cal-uncertainty", metavar = "fraction", default = "0.0", help = "set the fractional uncertainty in amplitude due to calibration uncertainty (eg. 0.08)")
	parser.add_option("-i", "--injections", action = "store_true", help = "generate efficiency plots from the output files of an injection run")
	parser.add_option("--injections-bin-size", default = "16.7", help = "set bin with for injection efficiency curves")
	parser.add_option("--loudest-survivor", metavar = "instrument=amplitude", default = None, help = "set amplitude of loudest zero-lag non-injection survivor for efficiency plots")
	parser.add_option("-l", "--live-time-program", metavar = "program", default = "StringSearch", help = "set the name, as it appears in the process table, of the program whose search summary entries define the search live time (default = StringSearch)")
	parser.add_option("-v", "--verbose", action = "store_true", help = "be verbose")
	options, filenames = parser.parse_args()

	options.amplitude_factor = float(options.amplitude_factor)
	if options.injections:
		if options.loudest_survivor is None:
			raise ValueError, "must set --loudest-survivor with --injections"
		options.loudest_survivor = options.loudest_survivor.split("=")
		options.loudest_survivor = {options.loudest_survivor[0]: abs(float(options.loudest_survivor[1]))}
		options.injections_bin_size = float(options.injections_bin_size)

	if options.cal_uncertainty is None:
		raise ValueError, "must set --cal-uncertainty"
	options.cal_uncertainty = float(options.cal_uncertainty)

	return options, (filenames or [None])


#
# =============================================================================
#
#                              Zero-Lag Survivors
#
# =============================================================================
#


class Survivors(object):
	def __init__(self):
		self.xmldoc = None

	def add_contents(self, contents):
		if self.xmldoc is None:
			self.xmldoc = ligolw.Document()
			self.xmldoc.appendChild(ligolw.LIGO_LW())
			self.sngl_burst_table = lsctables.New(lsctables.SnglBurstTable, contents.sngl_burst_table.columnnames)
			self.xmldoc.childNodes[0].appendChild(self.sngl_burst_table)

		time_slide_id = [id for id in contents.time_slide_table.iterkeys() if contents.time_slide_table.is_null(id)]
		if len(time_slide_id) != 1:
			raise ValueError, "document does not contain exactly 1 zero-lag time slide (contains %d of them)" % len(time_slide_id)
		time_slide_id = time_slide_id[0]

		self.sngl_burst_table.extend(map(contents.sngl_burst_table._row_from_cols, contents.connection.cursor().execute("""
SELECT sngl_burst.* FROM
	sngl_burst
	JOIN coinc_event_map ON (
		coinc_event_map.event_id == sngl_burst.event_id
		AND coinc_event_map.table_name == 'sngl_burst'
	)
	JOIN coinc_event ON (
		coinc_event.coinc_event_id == coinc_event_map.coinc_event_id
	)
WHERE
	coinc_event.coinc_def_id == ?
	AND coinc_event.time_slide_id == ?
		""", (contents.bb_definer_id, time_slide_id))))

	def finish(self, filename, verbose = False):
		self.sngl_burst_table.sort(lambda a, b: cmp((a.ifo, abs(a.amplitude)), (b.ifo, abs(b.amplitude))))
		utils.write_filename(self.xmldoc, filename, verbose = verbose, gz = (filename or "stdout").endswith(".gz"))


#
# =============================================================================
#
#                              Rate vs. Amplitude
#
# =============================================================================
#


class RateVsAmplitude(SnglBurstUtils.BurstPlot):
	def __init__(self, instrument, amplitude_factor):
		SnglBurstUtils.BurstPlot.__init__(self, r"%s Amplitude (\(\mathrm{s}^{-\frac{1}{3}}\))" % instrument, "Coincident Event Rate (Hz)")
		self.instrument = instrument
		self.amplitude_factor = amplitude_factor
		self.foreground = []
		self.background = []
		self.foreground_time = 0.0
		self.background_time = 0.0
		self.axes.loglog()

	def add_contents(self, contents):
		for time_slide_id in contents.time_slide_table.iterkeys():
			offsets = contents.time_slide_table[time_slide_id]
			contents.seglists.offsets.update(offsets)
			live_time = float(abs(contents.seglists.intersection(offsets.keys())))
			if contents.time_slide_table.is_null(time_slide_id):
				bins = self.foreground
				self.foreground_time += live_time
			else:
				bins = self.background
				self.background_time += live_time
			for amplitude, in contents.connection.cursor().execute("""
SELECT sngl_burst.amplitude FROM
	sngl_burst
	JOIN coinc_event_map ON (
		sngl_burst.event_id == coinc_event_map.event_id
		AND coinc_event_map.table_name == 'sngl_burst'
	)
	JOIN coinc_event ON (
		coinc_event_map.coinc_event_id == coinc_event.coinc_event_id
	)
WHERE
	sngl_burst.ifo == ?
	AND coinc_event.coinc_def_id == ?
	AND coinc_event.time_slide_id == ?
			""", (self.instrument, contents.bb_definer_id, time_slide_id)):
				bins.append(abs(amplitude) * self.amplitude_factor)
		contents.seglists.offsets.clear()

	def finish(self):
		print "Total time in %s foreground segments: %s s" % (self.instrument, str(self.foreground_time))
		print "Total time in %s background segments: %s s" % (self.instrument, str(self.background_time))
		self.axes.set_title(r"\begin{center}Cummulative Coincident Event Rate vs.\ Amplitude in %s\\%d Foreground, %d Background Events\end{center}" % (self.instrument, len(self.foreground), len(self.background)))
		self.background.sort()
		self.foreground.sort()
		print "Amplitude of loudest %s zero-lag survivor: %.9g s^(-1/3)" % (self.instrument, self.foreground[-1])
		# ratio of live times
		foregrounds_per_background = self.foreground_time / self.background_time
		# cummulative number expected in foreground
		background_y = numpy.arange(len(self.background), 0.0, -1.0, "Float64") * foregrounds_per_background
		# \sqrt{N} std dev expected in foreground
		background_yerr = numpy.sqrt(numpy.array(background_y))
		# convert to rate and uncertainty expected in foreground
		background_y /= self.foreground_time
		background_yerr /= self.foreground_time
		# cummulative rate observed in foreground
		foreground_y = numpy.arange(len(self.foreground), 0.0, -1.0, "Float64") / self.foreground_time
		# plot
		# warning:  the error bar polygon is not *really* clipped
		# to the axes' bounding box, the result will be incorrect
		# if the number of sample points is small.
		xmin = 5e-21
		xmax = 1e-19
		ymin = 1e-8
		ymax = 1e-4
		poly_x = numpy.concatenate((self.background, self.background[::-1]))
		poly_y = numpy.concatenate((background_y + 2 * background_yerr, (background_y - 2 * background_yerr)[::-1]))
		self.axes.add_patch(patches.Polygon(zip(poly_x, numpy.clip(poly_y, ymin, ymax)), edgecolor = "k", facecolor = "k", alpha = 0.3))
		self.axes.plot(self.background, background_y, "ko-")

		self.axes.plot(self.foreground, foreground_y, "ro-", markeredgecolor = "r")

		self.axes.set_xlim([xmin, xmax])
		self.axes.set_ylim([ymin, ymax])
		self.axes.xaxis.grid(True, which="minor")
		self.axes.yaxis.grid(True, which="minor")


#
# =============================================================================
#
#                                  Efficiency
#
# =============================================================================
#


def slope(x, y):
	"""
	From the x and y array, compute the slope everywhere.
	"""
	slope = numpy.zeros((len(x),), numpy.Float64)
	slope[0] = (y[1] - y[0]) / (x[1] - x[0])
	for i in xrange(1, len(x) - 1):
		slope[i] = (y[i + 1] - y[i - 1]) / (x[i + 1] - x[i - 1])
	slope[-1] = (y[-1] - y[-2]) / (x[-1] - x[-2])
	return slope


def solve(x, y, xval = None, yval = None):
	"""
	From the x and y arrays, solve for the value of x where y = yval,
	or for the value of y where x = xval.  Can't do both.  A linear
	interpolation is done between (x,y) pairs.
	"""
	# FIXME: assumes x and y are both monotonically increasing!
	if xval is None and yval is not None:
		i = bisect.bisect_left(y, yval)
		return x[i - 1] + (x[i] - x[i - 1]) * (yval - y[i - 1]) / (y[i] - y[i - 1])
	elif xval is not None and yval is None:
		i = bisect.bisect_left(x, xval)
		return y[i - 1] + (y[i] - y[i - 1]) * (xval - x[i - 1]) / (x[i] - x[i - 1])
	else:
		raise ValueError, "invalid arguments"


def upper_err(y, yerr, deltax):
	z = y + yerr
	deltax = int(deltax)
	upper = numpy.zeros((len(yerr),), "Float64")
	for i in xrange(len(yerr)):
		upper[i] = max(z[max(i - deltax, 0) : min(i + deltax, len(z))])
	return upper - y


def lower_err(y, yerr, deltax):
	z = y - yerr
	deltax = int(deltax)
	lower = numpy.zeros((len(yerr),), "Float64")
	for i in xrange(len(yerr)):
		lower[i] = min(z[max(i - deltax, 0) : min(i + deltax, len(z))])
	return y - lower


def write_efficiency(fileobj, bins, eff, yerr, filterwidth):
	print >>fileobj, "# ln(A)	e	D[e]"
	DlnA = bins.bins[0].delta * filterwidth / 2.0
	for A, e, De in zip(bins.centres()[0], eff, yerr):
		print >>fileobj, math.log(A), e, De


def render_data_from_bins(dump_file, axes, efficiency, cal_uncertainty, filter_width, colour):
	# extract array of x co-ordinates, and the factor by which x
	# increases from one sample to the next.
	(x,) = efficiency.centres()
	x_factor_per_sample = efficiency.bins().bins[0].delta

	# compute the efficiency, the slope (units = efficiency per
	# sample), the y uncertainty (units = efficiency) due to binomial
	# counting fluctuations, and the x uncertainty (units = samples)
	# due to the width of the smoothing filter.
	eff = efficiency.ratio()
	dydx = slope(numpy.arange(len(x), dtype = "Float64"), eff)
	yerr = numpy.sqrt(eff * (1 - eff) / efficiency.denominator.array)
	xerr = numpy.array([filter_width / 2] * len(yerr))

	# compute the net y err (units = efficiency) by (i) multiplying the
	# x err by the slope, (ii) dividing the calibration uncertainty
	# (units = percent) by the fractional change in x per sample and
	# multiplying by the slope, (iii) adding the two in quadradure with
	# the y err.
	net_yerr = numpy.sqrt((xerr * dydx)**2 + yerr**2 + (cal_uncertainty / x_factor_per_sample * dydx)**2)

	# compute net xerr (units = percent) by dividing yerr by slope and
	# then multiplying by the fractional change in x per sample.
	net_xerr = net_yerr / dydx * x_factor_per_sample

	# write the efficiency data to file
	write_efficiency(dump_file, efficiency.bins(), eff, net_yerr, filter_width)

	# plot the efficiency curve and uncertainty region
	patch = patches.Polygon(zip(numpy.concatenate((x, x[::-1])), numpy.concatenate((eff + upper_err(eff, yerr, filter_width / 2), (eff - lower_err(eff, yerr, filter_width / 2))[::-1]))), edgecolor = colour, facecolor = colour, alpha = 0.3)
	axes.add_patch(patch)
	line = axes.plot(x, eff, colour + "-")

	# compute 50% point and its uncertainty
	A50 = solve(x, eff, yval = 0.5)
	A50_err = solve(x, net_xerr, xval = A50)

	# mark 50% point on on graph
	axes.axvline(A50, color = colour)

	# print some analysis
	num_injections = efficiency.denominator.array.sum()
	num_samples = len(efficiency.denominator)
	print "Bins were %g samples wide, ideal would have been %g" % (filter_width, (num_samples / num_injections / solve(x, dydx, xval = A50)**2.0)**(1.0/3.0))
	print "Average number of injections in each bin = %g" % efficiency.denominator.array.mean()

	return line, A50, A50_err


class Efficiency(SnglBurstUtils.BurstPlot):
	def __init__(self, loudest_survivor, cal_uncertainty, amplitude_factor, filter_width):
		SnglBurstUtils.BurstPlot.__init__(self, r"Injection Amplitude (\(\mathrm{s}^{-\frac{1}{3}}\))", "Detection Efficiency")
		self.axes.set_title(r"Detection Efficiency vs.\ Amplitude")
		self.loudest_survivor_inst, self.loudest_survivor_ampl = loudest_survivor.items()[0]
		self.cal_uncertainty = cal_uncertainty
		self.amplitude_factor = amplitude_factor
		self.width = filter_width
		self.found = []
		self.found_above_loudest = []
		self.all = []
		self.axes.semilogx()

		# set desired yticks
		self.axes.set_yticks((0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0))
		self.axes.set_yticklabels((r"\(0\)", r"\(0.1\)", r"\(0.2\)", r"\(0.3\)", r"\(0.4\)", r"\(0.5\)", r"\(0.6\)", r"\(0.7\)", r"\(0.8\)", r"\(0.9\)", r"\(1.0\)"))

	def add_contents(self, contents):
		# find all found injections:  assemble a list of sngl_burst
		# event_id's for events participating in coincidences, then
		# get a list of the sim_burst simulation_id's for
		# injections that matched one of those, then extract the
		# rows for those injections.
		self.found.extend(map(contents.sim_burst_table._row_from_cols, contents.connection.cursor().execute("""
SELECT * FROM
	sim_burst
WHERE
	simulation_id IN (
		SELECT DISTINCT a.event_id FROM
			coinc_event_map AS a
			JOIN coinc_event_map AS b ON (
				a.coinc_event_id == b.coinc_event_id
				AND a.table_name == 'sim_burst'
				AND b.table_name == 'sngl_burst'
			)
		WHERE
			b.event_id IN (
				SELECT DISTINCT event_id FROM
					coinc_event_map
					JOIN coinc_event ON (
						coinc_event_map.coinc_event_id == coinc_event.coinc_event_id
						AND coinc_event_map.table_name == 'sngl_burst'
					)
				WHERE
					coinc_event.coinc_def_id == ?
			)
	)
		""", (contents.bb_definer_id,))))
		# find all injections recovered in loudest_survivor_inst
		# with an amplitude greater than loudest_survivor_ampl:
		# assemble a list of sngl_burst event_id's for events
		# matching the conditions and participating in
		# coincidences, then get a list of the sim_burst
		# simulation_id's for injections that matched one of those,
		# then extract the rows for those injections.
		# FIXME: this query can be simplified
		self.found_above_loudest.extend(map(contents.sim_burst_table._row_from_cols, contents.connection.cursor().execute("""
SELECT * FROM
	sim_burst
WHERE
	simulation_id IN (
		SELECT DISTINCT a.event_id FROM
			coinc_event_map AS a
			JOIN coinc_event_map AS b ON (
				a.coinc_event_id == b.coinc_event_id
				AND a.table_name == 'sim_burst'
				AND b.table_name == 'sngl_burst'
			)
		WHERE
			b.event_id IN (
				SELECT DISTINCT sngl_burst.event_id FROM
					sngl_burst
					JOIN coinc_event_map ON (
						sngl_burst.event_id == coinc_event_map.event_id
						AND coinc_event_map.table_name == 'sngl_burst'
					)
					JOIN coinc_event ON (
						coinc_event_map.coinc_event_id == coinc_event.coinc_event_id
					)
				WHERE
					sngl_burst.ifo == ?
					AND abs(sngl_burst.amplitude) > ?
					AND coinc_event.coinc_def_id == ?
			)
	)
		""", (self.loudest_survivor_inst, self.loudest_survivor_ampl, contents.bb_definer_id,))))
		self.all.extend(iter(contents.sim_burst_table))

	def finish(self):
		# put made and found injections in the denominators and
		# numerators of the efficiency bins
		self.all.sort(lambda a, b: cmp(a.hpeak, b.hpeak))
		min_amplitude = self.all[0].hpeak * self.amplitude_factor
		max_amplitude = self.all[-1].hpeak * self.amplitude_factor

		bins = rate.NDBins((rate.LogarithmicBins(min_amplitude, max_amplitude, 400),))
		efficiency = rate.BinnedRatios(bins)
		efficiency_above_loudest = rate.BinnedRatios(bins)
		for sim in self.found:
			efficiency.incnumerator((sim.hpeak * self.amplitude_factor,))
		for sim in self.found_above_loudest:
			efficiency_above_loudest.incnumerator((sim.hpeak * self.amplitude_factor,))
		for sim in self.all:
			efficiency.incdenominator((sim.hpeak * self.amplitude_factor,))
			efficiency_above_loudest.incdenominator((sim.hpeak * self.amplitude_factor,))

		# regularize:  adjust unused bins so that the efficiency
		# ratio comes out to 0
		efficiency.regularize()
		efficiency_above_loudest.regularize()

		# generate and plot trend curves.  adjust window function
		# normalization so that denominator array correctly
		# represents the number of injections contributing to each
		# bin:  make w(0) = 1.0.  note that this factor has no
		# effect on the efficiency because it is common to the
		# numerator and denominator arrays.  we do this for the
		# purpose of computing the Poisson error bars, which
		# requires us to know the counts for the bins
		windowfunc = rate.gaussian_window(self.width)
		windowfunc /= windowfunc[len(windowfunc) / 2 + 1]
		rate.filter_binned_ratios(efficiency, windowfunc)
		rate.filter_binned_ratios(efficiency_above_loudest, windowfunc)

		line1, A50, A50_err = render_data_from_bins(file("string_efficiency.dat", "w"), self.axes, efficiency, self.cal_uncertainty, self.width, "k")
		print "Pipeline's 50%% efficiency point for all detections = %g +/- %g%%\n" % (A50, A50_err * 100)

		line2, A50_above_loudest, A50_above_loudest_err = render_data_from_bins(file("string_efficiency_above_loudest.dat", "w"), self.axes, efficiency_above_loudest, self.cal_uncertainty, self.width, "r")
		print "Pipeline's 50%% efficiency point for detections above loudest = %g +/- %g%%\n" % (A50_above_loudest, A50_above_loudest_err * 100)

		# add a legend to the axes
		self.axes.legend((line1, line2), (r"All recovered injections", r"\noindent Injections recovered in %s with\\\(A > %s\,\mathrm{s}^{-\frac{1}{3}}\)" % (self.loudest_survivor_inst, SnglBurstUtils.latexnumber("%g" % (self.loudest_survivor_ampl * self.amplitude_factor)))), loc = "lower right")

		# adjust vertical limits
		self.axes.set_ylim([0, 1])


#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#


options, filenames = parse_command_line()

if options.injections:
	plots = [
		Efficiency(options.loudest_survivor, options.cal_uncertainty, options.amplitude_factor, options.injections_bin_size)
	]
	survivors = None
else:
	plots = [
		RateVsAmplitude("H1", options.amplitude_factor),
		RateVsAmplitude("H2", options.amplitude_factor),
		RateVsAmplitude("L1", options.amplitude_factor),
	]
	survivors = Survivors()


for n, filename in enumerate(utils.sort_files_by_size(filenames, options.verbose, reverse = True)):
	if options.verbose:
		print >>sys.stderr, "%d/%d:" % (n + 1, len(filenames)),
	# FIXME:  this needs to be updated to work with SQLite files as input
	connection = sqlite3.connect(":memory:")
	dbtables.DBTable_set_connection(connection)
	xmldoc = utils.load_filename(filename, options.verbose, gz = filename.endswith(".gz"))
	database = SnglBurstUtils.CoincDatabase().summarize(xmldoc, options.live_time_program, options.verbose)
	for n, plot in enumerate(plots):
		if options.verbose:
			print >>sys.stderr, "adding to plot %d ..." % n
		plot.add_contents(database)
	if survivors:
		if options.verbose:
			print >>sys.stderr, "storing zero-lag survivors ..."
		survivors.add_contents(database)
	connection.close()

n = 0
format = "%%s%%0%dd.%%s" % (int(math.log10(max(len(plots) - 1, 1))) + 1)
while len(plots):
	if options.verbose:
		print >>sys.stderr, "finishing plot %d ..." % n
	plots[0].finish()
	for ext in ["png"]:
		if options.injections:
			filename = format % ("string_injections_", n, ext)
		else:
			filename = format % ("string_triggers_", n, ext)
		if options.verbose:
			print >>sys.stderr, "writing %s ..." % filename
		plots[0].fig.savefig(filename)
	del plots[0]
	n += 1
if survivors:
	survivors.finish("string_triggers_survivors.xml", verbose = options.verbose)
if options.verbose:
	print >>sys.stderr, "done."
