#!/usr/bin/python
#
# Copyright (C) 2006--2009  Kipp Cannon
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.


#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


"""
String cusp search final output rendering tool.
"""


import bisect
from optparse import OptionParser
import math
from matplotlib import patches
import numpy
from scipy import interpolate
from scipy import optimize
try:
	import sqlite3
except ImportError:
	# pre 2.5.x
	from pysqlite2 import dbapi2 as sqlite3
import sys


from glue import segments
from glue import segmentsUtils
from glue.lal import CacheEntry
from glue.ligolw import ligolw
from glue.ligolw import lsctables
from glue.ligolw import dbtables
from glue.ligolw import utils
from glue.ligolw.utils import process as ligolwprocess
from glue import segments
from pylal import git_version
from pylal import rate
from pylal import SimBurstUtils
from pylal import SnglBurstUtils
from pylal import stringutils
SnglBurstUtils.matplotlib.rcParams.update({
	"grid.linestyle": "-"
})
from pylal.xlal.datatypes.ligotimegps import LIGOTimeGPS


SnglBurstUtils.matplotlib.rcParams.update({
	"font.size": 10.0,
	"axes.titlesize": 10.0,
	"axes.labelsize": 10.0,
	"xtick.labelsize": 8.0,
	"ytick.labelsize": 8.0,
	"legend.fontsize": 8.0
})


lsctables.LIGOTimeGPS = LIGOTimeGPS


__author__ = "Kipp Cannon <kipp.cannon@ligo.org>"
__version__ = "git id %s" % git_version.id
__date__ = git_version.date


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


def parse_command_line():
	parser = OptionParser(
		version = "Name: %%prog\n%s" % git_version.verbose_msg,
		usage = "%prog [options] [file ...]",
		description = "%prog performs the final, summary, stages of the upper-limit string cusp search.  Input consists of a list of all sqlite format database files produced by all injection and non-injection runs of the analysis pipeline.  The file names can be given on the command line and/or provided in a LAL cache file."
	)
	parser.add_option("-a", "--amplitude-factor", metavar = "factor", type = "float", default = 1e-20, help = "Multiply amplitudes in XML files by this amount (default = 1e-20).")
	parser.add_option("--cal-uncertainty", metavar = "fraction", type = "float", help = "Set the fractional uncertainty in amplitude due to calibration uncertainty (eg. 0.08).  This option is required, use 0 to disable calibration uncertainty.")
	parser.add_option("--injections-bin-size", metavar = "bins", type = "float", default = 16.7, help = "Set bin width for injection efficiency curves (default = 16.7).")
	parser.add_option("-c", "--input-cache", metavar = "filename", help = "Also process the files named in this LAL cache.  See lalapps_path2cache for information on how to produce a LAL cache file.")
	parser.add_option("--image-formats", metavar = "ext[,ext,...]", default = "png,pdf", help = "Set list of graphics formats to produce by providing a comma-delimited list of the filename extensions (default = \"png,pdf\").")
	parser.add_option("-p", "--live-time-program", metavar = "program", default = "StringSearch", help = "Set the name, as it appears in the process table, of the program whose search summary entries define the search live time (default = StringSearch).")
	parser.add_option("-o", "--open-box", action = "store_true", help = "Perform open-box analysis.  In a closed-box analysis (the default), information about the events seen at zero-lag is concealed:  the rate vs. threshold plot only shows the rate of events seen in the background, the detection threshold used to measure the efficiency curves is obtained from n-th loudest background event where n is (the integer closest to) the ratio of background livetime to zero-lag livetime, and messages to stdout and stderr that contain information about event counts at zero-lag are silenced.")
	parser.add_option("-t", "--tmp-space", metavar = "path", help = "Path to a directory suitable for use as a work area while manipulating the database file.  The database file will be worked on in this directory, and then moved to the final location when complete.  This option is intended to improve performance when running in a networked environment, where there might be a local disk with higher bandwidth than is available to the filesystem on which the final output will reside.")
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")
	options, filenames = parser.parse_args()

	if options.cal_uncertainty is None:
		raise ValueError, "must set --cal-uncertainty (use 0 to ignore calibration uncertainty)"

	options.image_formats = options.image_formats.split(",")

	if options.input_cache:
		filenames += [CacheEntry(line).url for line in file(options.input_cache)]

	if not filenames:
		raise ValueError, "must provide at least one filename"

	return options, filenames


#
# =============================================================================
#
#                              Zero-Lag Survivors
#
# =============================================================================
#


class Survivors(object):
	def __init__(self):
		self.xmldoc = None

	def add_contents(self, contents):
		if self.xmldoc is None:
			self.xmldoc = ligolw.Document()
			self.xmldoc.appendChild(ligolw.LIGO_LW())
			self.sngl_burst_table = lsctables.New(lsctables.SnglBurstTable, contents.sngl_burst_table.columnnames)
			self.xmldoc.childNodes[0].appendChild(self.sngl_burst_table)

		for values in contents.connection.cursor().execute("""
SELECT
	sngl_burst.*
FROM
	coinc_event
	JOIN coinc_event_map ON (
		coinc_event_map.coinc_event_id == coinc_event.coinc_event_id
	)
	JOIN sngl_burst ON (
		coinc_event_map.table_name == 'sngl_burst'
		AND coinc_event_map.event_id == sngl_burst.event_id
	)
WHERE
	coinc_event.coinc_def_id == ?
	AND NOT EXISTS (
		SELECT
			*
		FROM
			time_slide
		WHERE
			time_slide.time_slide_id == coinc_event.time_slide_id
			AND time_slide.offset != 0
	)
		""", (contents.bb_definer_id,)):
			self.sngl_burst_table.append(contents.sngl_burst_table.row_from_cols(values))

	def finish(self, filename, verbose = False):
		self.sngl_burst_table.sort(lambda a, b: cmp((a.ifo, abs(a.amplitude)), (b.ifo, abs(b.amplitude))))
		utils.write_filename(self.xmldoc, filename, verbose = verbose, gz = (filename or "stdout").endswith(".gz"))


#
# =============================================================================
#
#                              Rate vs. Threshold
#
# =============================================================================
#


def ratevsthresh_bounds(background_x, background_y, zero_lag_x, zero_lag_y):
	if len(zero_lag_x):
		if len(background_x):
			minX, maxX = min(min(zero_lag_x), min(background_x)), max(max(zero_lag_x), max(background_x))
			minY, maxY = min(min(zero_lag_y), min(background_y)), max(max(zero_lag_y), max(background_y))
		else:
			minX, maxX = min(zero_lag_x), max(zero_lag_x)
			minY, maxY = min(zero_lag_y), max(zero_lag_y)
	else:
		# don't check for background, if there's no zero-lag and no
		# background we're screwed anyway
		minX, maxX = min(background_x), max(background_x)
		minY, maxY = min(background_y), max(background_y)
	return minX, maxX, minY, maxY


def compress_ratevsthresh_curve(x, y, yerr):
	#
	# construct a background mask to retain the highest-ranked 10,000
	# elements, then every 10th until the 100,000th highest-ranked
	# element, then every 100th after that.  this is for reducing the
	# dataset size so matplotlib can handle it and vector graphics
	# output isn't ridiculous in size.
	#

	mask = numpy.arange(len(x))[::-1]
	mask = (mask < 10000) | ((mask < 100000) & (mask % 10 == 0)) | (mask % 100 == 0)

	return x.compress(mask), y.compress(mask), yerr.compress(mask)


class RateVsThreshold(SnglBurstUtils.BurstPlot):
	def __init__(self, open_box):
		SnglBurstUtils.BurstPlot.__init__(self, r"Likelihood Ratio Threshold $\Lambda$", "Event Rate (Hz)", width = 108.0)
		self.zero_lag = []
		self.background = []
		self.zero_lag_time = 0.0
		self.background_time = 0.0
		self.open_box = open_box
		self.axes.loglog()
		self.axes.set_position([0.125, 0.15, 0.83, 0.75])
		self.axes.xaxis.grid(True, which = "major,minor")
		self.axes.yaxis.grid(True, which = "major,minor")

	def add_contents(self, contents, verbose = False):
		#
		# if the database contains a sim_burst table then it is assumed
		# to be from an injection run:  skip it
		#

		if "sim_burst" in dbtables.get_table_names(contents.connection):
			if verbose:
				print >>sys.stderr, "\tdatabase contains sim_burst table, skipping ..."
			return

		#
		# retrieve the offset vectors, retain only instruments that
		# are available
		#

		zero_lag_time_slides, background_time_slides = SnglBurstUtils.get_time_slides(contents.connection)
		assert len(zero_lag_time_slides) == 1

		#
		# compute the live time
		#

		self.zero_lag_time += stringutils.time_slides_livetime(contents.seglists, zero_lag_time_slides.values(), 2, clip = contents.coincidence_segments)
		self.background_time += stringutils.time_slides_livetime(contents.seglists, background_time_slides.values(), 2, clip = contents.coincidence_segments)

		#
		# count events
		#

		for likelihood_ratio, is_background in contents.connection.cursor().execute("""
SELECT
	coinc_event.likelihood,
	EXISTS (
		SELECT
			*
		FROM
			time_slide
		WHERE
			time_slide.time_slide_id == coinc_event.time_slide_id
			AND time_slide.offset != 0
	)
FROM
	coinc_event
WHERE
	coinc_event.coinc_def_id == ?
		""", (contents.bb_definer_id,)):
			if is_background:
				self.background.append(likelihood_ratio)
			else:
				self.zero_lag.append(likelihood_ratio)

	def finish(self):
		if self.open_box:
			self.axes.set_title(r"Event Rate vs.\ Likelihood Ratio Threshold")
		else:
			self.axes.set_title(r"Event Rate vs.\ Likelihood Ratio Threshold (Closed Box)")

		self.background.sort()
		self.zero_lag.sort()
		self.background = numpy.array(self.background, dtype = "double")
		self.zero_lag = numpy.array(self.zero_lag, dtype = "double")

		#
		# convert counts to rates and their uncertainties
		#

		# background count expected in zero-lag and \sqrt{N}
		# standard deviation
		background_y = numpy.arange(len(self.background), 0.0, -1.0, dtype = "double") / self.background_time * self.zero_lag_time
		background_yerr = numpy.sqrt(background_y)

		# convert to background rate and uncertainty expected in
		# zero-lag
		background_y /= self.zero_lag_time
		background_yerr /= self.zero_lag_time

		# rate observed in zero-lag
		zero_lag_y = numpy.arange(len(self.zero_lag), 0.0, -1.0, dtype = "double") / self.zero_lag_time

		#
		# determine the horizontal and vertical extent of the plot
		#

		if self.open_box:
			minX, maxX, minY, maxY = ratevsthresh_bounds(self.background, background_y, self.zero_lag, zero_lag_y)
		else:
			minX, maxX, minY, maxY = ratevsthresh_bounds(self.background, background_y, [], [])
		minX = 1e-2	# FIXME:  don't hard-code

		#
		# compress the background data
		#

		background, background_y, background_yerr = compress_ratevsthresh_curve(self.background, background_y, background_yerr)

		# warning:  the error bar polygon is not *really* clipped
		# to the axes' bounding box, the result will be incorrect
		# if the density of data points is small where the polygon
		# encounters the axes' bounding box.

		poly_x = numpy.concatenate((background, background[::-1]))
		poly_y = numpy.concatenate((background_y + 1 * background_yerr, (background_y - 1 * background_yerr)[::-1]))
		self.axes.add_patch(patches.Polygon(zip(poly_x, numpy.clip(poly_y, minY, maxY)), edgecolor = "k", facecolor = "k", alpha = 0.3))
		line1, = self.axes.loglog(background.repeat(2)[:-1], background_y.repeat(2)[1:], color = "k", linestyle = "--")
		if self.open_box:
			line2, = self.axes.loglog(self.zero_lag.repeat(2)[:-1], zero_lag_y.repeat(2)[1:], color = "k", linestyle = "-", linewidth = 2)
			self.axes.legend((line1, line2), (r"Expected background", r"Zero-lag"), loc = "lower left")
		else:
			self.axes.legend((line1,), (r"Expected background",), loc = "lower left")

		self.axes.set_xlim((minX, maxX))
		self.axes.set_ylim((minY, maxY))


#
# =============================================================================
#
#                                  Efficiency
#
# =============================================================================
#


def create_recovered_likelihood_view(connection, bb_coinc_def_id):
	# Create a temporary table containing two columns:  the
	# simulation_id of an injection, and the highest likelihood ratio
	# at which that injection was recovered by a coincidence of type
	# bb_coinc_def_id.
	cursor = connection.cursor()
	cursor.execute("""
CREATE TEMPORARY TABLE recovered_likelihood (simulation_id TEXT PRIMARY KEY, likelihood REAL)
	""")
	cursor.execute("""
INSERT OR REPLACE INTO
	recovered_likelihood
SELECT
	sim_burst.simulation_id AS simulation_id,
	MAX(coinc_event.likelihood) AS likelihood
FROM
	sim_burst
	JOIN coinc_event_map AS a ON (
		a.table_name == "sim_burst"
		AND a.event_id == sim_burst.simulation_id
	)
	JOIN coinc_event_map AS b ON (
		b.coinc_event_id == a.coinc_event_id
	)
	JOIN coinc_event ON (
		b.table_name == "coinc_event"
		AND b.event_id == coinc_event.coinc_event_id
	)
WHERE
	coinc_event.coinc_def_id == ?
GROUP BY
	sim_burst.simulation_id
	""", (bb_coinc_def_id,))


def slope(x, y):
	"""
	From the x and y arrays, compute the slope at the x co-ordinates
	using a first-order finite difference approximation.
	"""
	slope = numpy.zeros((len(x),), dtype = "double")
	slope[0] = (y[1] - y[0]) / (x[1] - x[0])
	for i in xrange(1, len(x) - 1):
		slope[i] = (y[i + 1] - y[i - 1]) / (x[i + 1] - x[i - 1])
	slope[-1] = (y[-1] - y[-2]) / (x[-1] - x[-2])
	return slope


def upper_err(y, yerr, deltax):
	z = y + yerr
	deltax = int(deltax)
	upper = numpy.zeros((len(yerr),), dtype = "double")
	for i in xrange(len(yerr)):
		upper[i] = max(z[max(i - deltax, 0) : min(i + deltax, len(z))])
	return upper - y


def lower_err(y, yerr, deltax):
	z = y - yerr
	deltax = int(deltax)
	lower = numpy.zeros((len(yerr),), dtype = "double")
	for i in xrange(len(yerr)):
		lower[i] = min(z[max(i - deltax, 0) : min(i + deltax, len(z))])
	return y - lower


def write_efficiency(fileobj, bins, eff, yerr, filterwidth):
	print >>fileobj, "# ln(A)	e	D[e]"
	DlnA = bins[0].delta * filterwidth / 2.0
	for A, e, De in zip(bins.centres()[0], eff, yerr):
		print >>fileobj, math.log(A), e, De


def render_data_from_bins(dump_file, axes, efficiency, cal_uncertainty, filter_width, colour = "k", erroralpha = 0.3, linestyle = "-"):
	# extract array of x co-ordinates, and the factor by which x
	# increases from one sample to the next.
	(x,) = efficiency.centres()
	x_factor_per_sample = efficiency.bins()[0].delta

	# compute the efficiency, the slope (units = efficiency per
	# sample), the y uncertainty (units = efficiency) due to binomial
	# counting fluctuations, and the x uncertainty (units = samples)
	# due to the width of the smoothing filter.
	eff = efficiency.ratio()
	dydx = slope(numpy.arange(len(x), dtype = "double"), eff)
	yerr = numpy.sqrt(eff * (1 - eff) / efficiency.denominator.array)
	xerr = numpy.array([filter_width / 2] * len(yerr))

	# compute the net y err (units = efficiency) by (i) multiplying the
	# x err by the slope, (ii) dividing the calibration uncertainty
	# (units = percent) by the fractional change in x per sample and
	# multiplying by the slope, (iii) adding the two in quadradure with
	# the y err.
	net_yerr = numpy.sqrt((xerr * dydx)**2 + yerr**2 + (cal_uncertainty / x_factor_per_sample * dydx)**2)

	# compute net xerr (units = percent) by dividing yerr by slope and
	# then multiplying by the fractional change in x per sample.
	net_xerr = net_yerr / dydx * x_factor_per_sample

	# write the efficiency data to file
	write_efficiency(dump_file, efficiency.bins(), eff, net_yerr, filter_width)

	# plot the efficiency curve and uncertainty region
	patch = patches.Polygon(zip(numpy.concatenate((x, x[::-1])), numpy.concatenate((eff + upper_err(eff, yerr, filter_width / 2), (eff - lower_err(eff, yerr, filter_width / 2))[::-1]))), edgecolor = colour, facecolor = colour, alpha = erroralpha)
	axes.add_patch(patch)
	line, = axes.plot(x, eff, colour + linestyle)

	# compute 50% point and its uncertainty
	A50 = optimize.bisect(interpolate.interp1d(x, eff - 0.5), x[0], x[-1], xtol = 1e-40)
	A50_err = interpolate.interp1d(x, net_xerr)(A50)

	# mark 50% point on graph
	axes.axvline(A50, color = colour, linestyle = linestyle)

	# print some analysis
	num_injections = efficiency.denominator.array.sum()
	num_samples = len(efficiency.denominator)
	print >>sys.stderr, "Bins were %g samples wide, ideal would have been %g" % (filter_width, (num_samples / num_injections / interpolate.interp1d(x, dydx)(A50)**2.0)**(1.0/3.0))
	print >>sys.stderr, "Average number of injections in each bin = %g" % efficiency.denominator.array.mean()

	return line, A50, A50_err


class Efficiency(SnglBurstUtils.BurstPlot):
	def __init__(self, detection_threshold, cal_uncertainty, amplitude_factor, filter_width):
		SnglBurstUtils.BurstPlot.__init__(self, r"Injection Amplitude (\(\mathrm{s}^{-\frac{1}{3}}\))", "Detection Efficiency", width = 108.0)
		self.axes.set_title(r"Detection Efficiency vs.\ Amplitude")
		self.detection_threshold = detection_threshold
		self.cal_uncertainty = cal_uncertainty
		self.amplitude_factor = amplitude_factor
		self.width = filter_width
		self.seglists = segments.segmentlistdict()
		self.found = []
		self.missed = []
		self.all = []
		self.axes.semilogx()
		self.axes.set_position([0.10, 0.150, 0.86, 0.77])

		# set desired yticks
		self.axes.set_yticks((0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0))
		self.axes.set_yticklabels((r"\(0\)", r"\(0.1\)", r"\(0.2\)", r"\(0.3\)", r"\(0.4\)", r"\(0.5\)", r"\(0.6\)", r"\(0.7\)", r"\(0.8\)", r"\(0.9\)", r"\(1.0\)"))
		self.axes.xaxis.grid(True, which = "major,minor")
		self.axes.yaxis.grid(True, which = "major,minor")

	def add_contents(self, contents, verbose = False):
		#
		# if the database does not contain a sim_burst table then
		# it is assumed to be from a non-injection run:  skip it
		#

		if "sim_burst" not in dbtables.get_table_names(contents.connection):
			if verbose:
				print >>sys.stderr, "\tdatabase does not contain sim_burst table, skipping ..."
			return

		#
		# update segment information
		#

		self.seglists |= contents.seglists

		# for each injection, retrieve the highest likelihood ratio
		# of the burst coincs that were found to match the
		# injection or null if no such matching burst events were
		# found
		create_recovered_likelihood_view(contents.connection, contents.bb_definer_id)
		for values in contents.connection.cursor().execute("""
SELECT
	sim_burst.*,
	recovered_likelihood.likelihood
FROM
	sim_burst
	LEFT JOIN recovered_likelihood ON (
		recovered_likelihood.simulation_id == sim_burst.simulation_id
	)
		"""):
			sim = contents.sim_burst_table.row_from_cols(values[:-1])
			likelihood_ratio = values[-1]
			found = likelihood_ratio is not None
			# were at least 2 instruments on when the injection
			# was made?
			if len(SimBurstUtils.on_instruments(sim, contents.seglists)) >= 2:
				# yes
				self.all.append(sim)
				if found and likelihood_ratio > self.detection_threshold:
					self.found.append(sim)
				else:
					self.missed.append((sim, contents.filename))
			elif found:
				# no
				print >>sys.stderr, "odd, injection %s was found but not injected ..." % sim.simulation_id

	def finish(self):
		# put made and found injections in the denominators and
		# numerators of the efficiency bins
		bins = rate.NDBins((rate.LogarithmicBins(min(sim.amplitude for sim in self.all) * self.amplitude_factor, max(sim.amplitude for sim in self.all) * self.amplitude_factor, 400),))
		efficiency = rate.BinnedRatios(bins)
		for sim in self.found:
			efficiency.incnumerator((sim.amplitude * self.amplitude_factor,))
		for sim in self.all:
			efficiency.incdenominator((sim.amplitude * self.amplitude_factor,))

		# regularize:  adjust unused bins so that the efficiency is
		# 0, not NaN
		efficiency.regularize()

		# generate and plot trend curves.  adjust window function
		# normalization so that denominator array correctly
		# represents the number of injections contributing to each
		# bin:  make w(0) = 1.0.  note that this factor has no
		# effect on the efficiency because it is common to the
		# numerator and denominator arrays.  we do this for the
		# purpose of computing the Poisson error bars, which
		# requires us to know the counts for the bins
		windowfunc = rate.gaussian_window(self.width)
		windowfunc /= windowfunc[len(windowfunc) / 2 + 1]
		rate.filter_binned_ratios(efficiency, windowfunc)

		line1, A50, A50_err = render_data_from_bins(file("string_efficiency.dat", "w"), self.axes, efficiency, self.cal_uncertainty, self.width, colour = "k", linestyle = "-", erroralpha = 0.2)
		print >>sys.stderr, "Pipeline's 50%% efficiency point for all detections = %g +/- %g%%\n" % (A50, A50_err * 100)

		# add a legend to the axes
		self.axes.legend((line1,), (r"\noindent Injections recovered with $\Lambda > %s$" % SnglBurstUtils.latexnumber("%.3g" % self.detection_threshold),), loc = "lower right")

		# adjust limits
		self.axes.set_xlim([1e-21, 2e-18])
		self.axes.set_ylim([0.0, 1.0])

		# dump some information about the highest-amplitude missed
		# injections
		self.missed.sort(lambda (a, f1), (b, f2): cmp(a.amplitude, b.amplitude))
		f = file("string_missed_injections.txt", "w")
		print >>f, "Highest Amplitude Missed Injections"
		print >>f, "==================================="
		for sim, filename in self.missed[-100:]:
			print >>f
			print >>f, "%s in %s:" % (str(sim.simulation_id), filename)
			for instrument in self.seglists:
				print >>f, "%s:" % instrument
				print >>f, "\tAmplitude:\t%.16g" % SimBurstUtils.string_amplitude_in_instrument(sim, instrument)
				print >>f, "\tTime:\t%s s" % SimBurstUtils.time_at_instrument(sim, instrument)
			print >>f, "Amplitude in waveframe:\t%.16g" % sim.amplitude
			t = sim.get_time_geocent()
			print >>f, "Time at geocentre:\t%s s" % t
			print >>f, "Segments within 60 seconds:\t%s" % segmentsUtils.segmentlistdict_to_short_string(self.seglists & segments.segmentlistdict((instrument, segments.segmentlist([segments.segment(t-60, t+60)])) for instrument in self.seglists))


#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#


def process_files(filenames, products, live_time_program, tmp_path = None, verbose = False):
	for n, filename in enumerate(filenames):
		if verbose:
			print >>sys.stderr, "%d/%d:" % (n + 1, len(filenames)),
		#
		# connect to database and summarize contents
		#

		working_filename = dbtables.get_connection_filename(filename, tmp_path = tmp_path, verbose = verbose)
		connection = sqlite3.connect(working_filename)
		dbtables.DBTable_set_connection(connection)
		contents = SnglBurstUtils.CoincDatabase(connection, live_time_program, search = "StringCusp", verbose = verbose)

		#
		# augment summary with extra stuff we need.  the filename
		# is recorded for dumping debuggin information related to
		# missed injections.  if burca was run with the
		# --coincidence-segments option then the value is copied
		# into a segmentlistdict to facilitate the computation of
		# livetime, otherwise a dummy value is created spanning all
		# time
		#

		contents.filename = filename

		# as a side-effect, this enforces the rule that burca has
		# been run on the input file exactly once
		contents.coincidence_segments = ligolwprocess.get_process_params(contents.xmldoc, "ligolw_burca", "--coincidence-segments")
		if contents.coincidence_segments:
			contents.coincidence_segments = segments.segmentlistdict.fromkeys(contents.seglists, segmentsUtils.from_range_strings(coincidence_segments.split(","), boundtype = LIGOTimeGPS).coalesce())
		else:
			contents.coincidence_segments = None

		#
		# process contents
		#

		for n, plot in enumerate(products):
			if verbose:
				print >>sys.stderr, "adding to plot %d ..." % n
			plot.add_contents(contents, verbose = verbose)

		#
		# close
		#

		connection.close()
		dbtables.discard_connection_filename(filename, working_filename, verbose = verbose)


def write_products(products, prefix, image_formats, verbose = False):
	format = "%%s%%0%dd.%%s" % (int(math.log10(max(len(products) - 1, 1))) + 1)
	n = 0
	while products:
		if verbose:
			print >>sys.stderr, "finishing plot %d ..." % n
		products[0].finish()
		for ext in image_formats:
			filename = format % (prefix, n, ext)
			if verbose:
				print >>sys.stderr, "writing %s ..." % filename
			products[0].fig.savefig(filename)
		del products[0]
		n += 1


options, filenames = parse_command_line()


if options.open_box:
	print >>sys.stderr, """

---=== !! BOX IS OPEN !! ===---

PRESS CTRL-C SOON IF YOU DIDN'T MEAN TO OPEN THE BOX

"""


if options.verbose:
	print >>sys.stderr, "Collecting background and zero-lag statistics ..."
rate_vs_threshold = RateVsThreshold(options.open_box)
process_files(filenames, [rate_vs_threshold], options.live_time_program, tmp_path = options.tmp_space, verbose = options.verbose)
write_products([rate_vs_threshold], "string_triggers_", options.image_formats, verbose = False)


if options.open_box:
	print >>sys.stderr, "Zero-lag events: %d" % len(rate_vs_threshold.zero_lag)
print >>sys.stderr, "Total time in zero-lag segments: %s s" % str(rate_vs_threshold.zero_lag_time)
print >>sys.stderr, "Time-slide events: %d" % len(rate_vs_threshold.background)
print >>sys.stderr, "Total time in time-slide segments: %s s" % str(rate_vs_threshold.background_time)
if options.open_box:
	detection_threshold = rate_vs_threshold.zero_lag[-1]
	print >>sys.stderr, "Likelihood ratio for highest-ranked zero-lag survivor: %.9g" % detection_threshold
else:
	detection_threshold = rate_vs_threshold.background[-int(round(rate_vs_threshold.background_time / rate_vs_threshold.zero_lag_time))]
	print >>sys.stderr, "Simulated likelihood ratio for highest-ranked zero-lag survivor: %.9g" % detection_threshold


if options.verbose:
	print >>sys.stderr, "Collecting efficiency statistics ..."
# FIXME:  don't hard-code instrument names
efficiency = Efficiency(detection_threshold, options.cal_uncertainty, options.amplitude_factor, options.injections_bin_size)
process_files(filenames, [efficiency], options.live_time_program, tmp_path = options.tmp_space, verbose = options.verbose)
write_products([efficiency], "string_injections_", options.image_formats, verbose = False)


if options.verbose:
	print >>sys.stderr, "done."
