#!/usr/bin/python
#
# $Id$
#
# Copyright (C) 2007  Kipp C. Cannon
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.


# Information extracted from data set:
#
# - from zero lag obtain zero lag event rate vs. amplitude threshold curve,
# and amplitude of loudest event to use as final threshold
#
# - from time slides, obtain background event rate vs. amplitude threshold
# curve.  this is \mu_{0}(x) in Brady et al. upper limit paper.
#
# - numerically differentiate that curve to obtain \mu_{0}'(x).  this is
# done by first computing a spline approximation of the \mu_{0}(x) curve,
# and then differentiating that curve.
#
# - from histogram of the times between events, confirm that background is
# a Poisson process.  but should this be of all background events, or only
# background events with amplitudes above the final threshold?  again, the
# latter could be a problem if there aren't enough events in the time
# slides above threshold
#
# - in each M_{sun} vs. centre frequency bin, measure the injection
# recovery probability at the final threshold.  this is \epsilon(x) in
# Brady et al., and required two 2D M_{sun} vs. frequency binnings to
# compute.  all made injections go into the first 2D binning, and all
# injections recovered with an amplitude above threshold go into the second
# 2D binning.  the ratio of the second binning's counts to those in the
# first yields the detection efficiency (found / made).
#
# - in each M_{sun} vs. centre frequency bin, measure the injection
# recovery probability density at the final threshold.  this is
# \epsilon'(x) in Brady et al., and requires one additional 2D M_{sun} vs.
# frequency binning to compute.  all recovered injections regardless of
# amplitude go into this binning weighted by an amplitude window function.
# the ratio of this third binnings' counts to those in the first above
# yields the efficiency density if the amplitude window has been normalized
# properly (so as to yield count per amplitude interval).


#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


import bisect
import glob
import math
from matplotlib import patches
import numpy
from scipy import interpolate
from optparse import OptionParser
try:
	import sqlite3
except ImportError:
	# pre 2.5.x
	from pysqlite2 import dbapi2 as sqlite3
import sys


from glue import iterutils
from glue import segments
from glue.ligolw import dbtables
from pylal import ligolw_tisi
from pylal import rate
from pylal import SimBurstUtils
from pylal import SnglBurstUtils
from pylal.date import LIGOTimeGPS


__author__ = "Kipp Cannon <kipp@gravity.phys.uwm.edu>"
__version__ = "$Revision$"[11:-2]
__date__ = "$Date$"[7:-2]


#
# =============================================================================
#
#                                 Speed Hacks
#
# =============================================================================
#


dbtables.lsctables.LIGOTimeGPS = LIGOTimeGPS


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


def parse_command_line():
	parser = OptionParser(
		version="%prog CVS $Id$",
		usage = "%prog [options] -i|--injections-glob pattern -b|--background-glob pattern",
		description = "%prog performs the final, summary, stages of an upper-limit excess power search for burst-like gravitational waves."
	)
	parser.add_option("-b", "--background-glob", metavar = "pattern", default = [], action = "append", help = "Shell filename pattern for non-injection files (required).")
	parser.add_option("-i", "--injections-glob", metavar = "pattern", default = [], action = "append", help = "Shell filename pattern for injection files (required).")
	parser.add_option("-l", "--live-time-program", metavar = "program", default = "lalapps_power", help = "Set the name, as it appears in the process table, of the program whose search summary \"out\" segments define the search live time (default = \"lalapps_power\")")
	parser.add_option("--zero-lag-survivors", metavar = "number", type = "int", help = "Set the final confidence-likelihood joint threshold so that this many events survive at zero lag.  If n survivors are requested, then the threshold is set at exactly the amplitude of the n+1 event, and events are discarded if their amplitudes are <= the threshold.  A \"loudest-event\" upper limit is obtained by setting this to 0.  The default is 0.")
	parser.add_option("--confidence-contour-slope", metavar = "slope", type = "float", default = -12.0, help = "Set the slope of the confidence-likelihood joint threshold contours (default = -12.0).")
	parser.add_option("--dump-scatter-data", action = "store_true", help = "Instead of computing upper limit, dump data files for confidence-likelihood scatter diagnostic scatter plot.")
	parser.add_option("--plot-scatter-data", action = "store_true", help = "Instead of computing upper limit, load data files for confidence-likelihood scatter diagnostic scatter plot and generate scatter plot.")
	parser.add_option("-t", "--tmp-space", metavar = "path", help = "Path to a directory suitable for use as a work area while manipulating the database file.  The database file will be worked on in this directory, and then moved to the final location when complete.  This option is intended to improve performance when running in a networked environment, where there might be a local disk with higher bandwidth than is available to the filesystem on which the final output will reside.")
	parser.add_option("-v", "--verbose", action = "store_true", help = "be verbose")
	options, filenames = parser.parse_args()

	if not options.plot_scatter_data:
		if not options.background_glob:
			raise ValueError, "missing required --background-glob argument"
		if not options.injections_glob:
			raise ValueError, "missing required --injections-glob argument"

	return options, (filenames or [None])


#
# =============================================================================
#
#                              Re-usable Queries
#
# =============================================================================
#


#
# An iterator that returns (likelihood, confidence, is_background) tuples
# for all sngl_burst <--> sngl_burst coincs.
#


def bb_id_likelihood_confidence_background(database):
	return database.connection.cursor().execute("""
SELECT
	coinc_event.coinc_event_id,
	coinc_event.likelihood,
	multi_burst.confidence,
	EXISTS (
		SELECT
			*
		FROM
			time_slide
		WHERE
			time_slide.time_slide_id == coinc_event.time_slide_id
			AND time_slide.offset != 0
	)
FROM
	coinc_event
	JOIN multi_burst ON (
		multi_burst.coinc_event_id == coinc_event.coinc_event_id
	)
WHERE
	coinc_event.coinc_def_id == ?
	""", (database.bb_definer_id,))


#
# Construct the temporary sim_coinc_map view that allows for quick mapping
# from sim_burst to sngl_burst<-->sngl_burst coincs
#


def create_sim_coinc_map_view(database):
	database.connection.cursor().execute("""
CREATE TEMPORARY VIEW
	sim_coinc_map
AS
	SELECT
		sim_burst.simulation_id AS simulation_id,
		sim_coinc_event.coinc_def_id AS sim_coinc_def_id,
		burst_coinc_event.coinc_event_id AS burst_coinc_event_id,
		burst_coinc_event.likelihood AS burst_coinc_likelihood,
		multi_burst.confidence AS burst_coinc_confidence,
		amplitude(burst_coinc_event.likelihood, multi_burst.confidence) AS burst_coinc_amplitude
	FROM
		sim_burst
		JOIN coinc_event_map AS a ON (
			a.table_name == 'sim_burst'
			AND a.event_id == sim_burst.simulation_id
		)
		JOIN coinc_event AS sim_coinc_event ON (
			sim_coinc_event.coinc_event_id == a.coinc_event_id
		)
		JOIN coinc_event_map AS b ON (
			b.coinc_event_id == a.coinc_event_id
		)
		JOIN coinc_event AS burst_coinc_event ON (
			b.table_name == 'coinc_event'
			AND b.event_id == burst_coinc_event.coinc_event_id
		)
		JOIN multi_burst ON (
			multi_burst.coinc_event_id == burst_coinc_event.coinc_event_id
		)
	""")


#
# =============================================================================
#
#            Measuring Confidence-Likelihood Density Contour Slope
#
# =============================================================================
#


#
# Phase 1:  dump data files.
#


def dump_confidence_likelihood_scatter_data(globs, live_time_program = "lalapps_power", tmp_path = None, verbose = False):
	#
	# Collect file names.
	#

	if verbose:
		print >>sys.stderr, "building file list ..."
	filenames = [filename for g in globs for filename in glob.glob(g)]
	filenames.sort()

	#
	# Initialize storage.
	#

	injections = iterutils.Highest(max = 1e6)
	background = iterutils.Highest(max = 1e6)
	zero_lag = iterutils.Highest(max = 1e6)

	#
	# Iterate over files.
	#

	for n, filename in enumerate(filenames):
		#
		# Open the database file.
		#

		if verbose:
			print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
		working_filename = dbtables.get_connection_filename(filename, tmp_path = tmp_path, verbose = verbose)
		connection = sqlite3.connect(working_filename)
		connection.create_function("amplitude", 2, coinc_amplitude)
		dbtables.DBTable_set_connection(connection)
		database = SnglBurstUtils.CoincDatabase(live_time_program, verbose)

		#
		# Process database contents.  Assume all files with
		# sim_burst tables are the outputs of injection runs, and
		# others aren't.
		#

		if database.sim_burst_table is None:
			# non-injections
			background_buffer = []
			zero_lag_buffer = []
			for id, l, c, is_background in bb_id_likelihood_confidence_background(database):
				if is_background:
					background_buffer.append((coinc_amplitude(l, c), l, c))
				else:
					zero_lag_buffer.append((coinc_amplitude(l, c), l, c))
			background.extend(background_buffer)
			zero_lag.extend(zero_lag_buffer)
		else:
			# injections
			injections_buffer = []
			create_sim_coinc_map_view(database)
			for a, l, c in database.connection.cursor().execute("""
SELECT
	burst_coinc_amplitude,
	burst_coinc_likelihood,
	burst_coinc_confidence
FROM
	sim_coinc_map
WHERE
	sim_coinc_def_id == ?
			""", (database.sce_definer_id,)):
				injections_buffer.append((-a, l, c))
			injections.extend(injections_buffer)

		#
		# Done with this file.
		#

		connection.close()
		dbtables.discard_connection_filename(filename, working_filename, verbose = verbose)

	#
	# Dump scatter plot data.
	#

	if verbose:
		print >>sys.stderr, "writing scatter plot data ..."

	f = file("lalapps_excesspowerfinal_background_scatter.dat", "w")
	for a, l, c in background:
		print >>f, "%.16g %.16g" % (l, c)

	f = file("lalapps_excesspowerfinal_zero_lag_scatter.dat", "w")
	for a, l, c in zero_lag:
		print >>f, "%.16g %.16g" % (l, c)

	f = file("lalapps_excesspowerfinal_injections_scatter.dat", "w")
	for a, l, c in injections:
		print >>f, "%.16g %.16g" % (l, c)

	if verbose:
		print >>sys.stderr, "done."


#
# Phase 2:  generate scatter plot from data files.
#


def plot_confidence_likelihood_scatter_data(slope, verbose = False):
	#
	# Start plot
	#

	fig = SnglBurstUtils.figure.Figure()
	SnglBurstUtils.FigureCanvas(fig)
	fig.set_size_inches(6.5, 6.5 / ((1 + math.sqrt(5)) / 2))
	axes = fig.gca()
	axes.grid(True)
	axes.loglog()
	axes.set_xlabel(r"Confidence")
	axes.set_ylabel(r"Likelihood Ratio")
	axes.set_title(r"Likelihood Ratio vs.\ Confidence Scatter Plot")

	#
	# Plot scatter data
	#

	def read_and_plot(filename, colour, verbose = False):
		if verbose:
			print >>sys.stderr, "reading '%s' ..." % filename
		X = []
		Y = []
		for line in file(filename):
			if line[0] == "#":
				continue
			y, x = map(float, line.strip().split())
			X.append(x)
			Y.append(y)
		if verbose:
			print >>sys.stderr, "plotting ..."
		return axes.plot(X, Y, colour)

	set1 = read_and_plot("lalapps_excesspowerfinal_injections_scatter.dat", "r+", verbose = verbose)
	set2 = read_and_plot("lalapps_excesspowerfinal_background_scatter.dat", "k+", verbose = verbose)
	#set3 = read_and_plot("lalapps_excesspowerfinal_zero_lag_scatter.dat", "b+", verbose = verbose)

	#axes.legend((set1, set2, set3), (r"Injections", r"Background (time slides)", r"Zero lag"), loc = "lower right")
	axes.legend((set1, set2), (r"Injections", r"Background (time slides)"), loc = "lower right")

	#
	# Plot threshold contours
	#

	def fit(x, lnb):
		return numpy.exp(slope * numpy.log(x) + lnb)

	if verbose:
		print >>sys.stderr, "plotting contours ..."
	ymin, ymax = axes.get_ylim()
	for lnb in range(30, 90, 10):
		x = 10**numpy.arange(0.85, 3.0, 0.01)
		y = fit(x, lnb)
		x = numpy.compress((ymin <= y) & (y <= ymax), x)
		y = numpy.compress((ymin <= y) & (y <= ymax), y)
		axes.plot(x, y, "g-")

	#
	# Adjust plot range and add likelihood = 1.0 marker
	#

	axes.plot(axes.get_xlim(), (1, 1), "k-")
	axes.set_xlim(10**0.85, 10**3)

	#
	# Write plot
	#

	if verbose:
		print >>sys.stderr, "writing 'lalapps_excesspowerfinal_scatter.png' ..."
	fig.savefig("lalapps_excesspowerfinal_scatter.png")

	#
	# Done
	#

	if verbose:
		print >>sys.stderr, "done."


#
# =============================================================================
#
#                               Live Time Tools
#
# =============================================================================
#


def get_background_time_slides(contents):
	"""
	Query the database for the IDs and offsets of non-zero-lag time
	slides.
	"""
	time_slides = {}
	for id, instrument, offset in contents.connection.cursor().execute("""
SELECT
	time_slide_id,
	instrument,
	offset
FROM
	time_slide
WHERE
	EXISTS (
		SELECT
			*
		FROM
			time_slide AS a
		WHERE
			a.time_slide_id == time_slide.time_slide_id
			AND a.offset != 0
	)
	"""):
		if id not in time_slides:
			time_slides[id] = {}
		time_slides[id][instrument] = offset
	return time_slides


def zero_lag_livetime(seglists):
	"""
	Return the total live time in the zero lag time slide.
	"""
	seglists.offsets.clear()
	return float(abs(seglists.intersection(seglists.keys())))


def time_slides_livetime(seglists, time_slides, verbose = False):
	"""
	Given a sequence of time slides (each of which is an instrument -->
	offset dictionary), use the segmentlistdict dictionary of segment
	lists to compute the live time in each time slide.  Return the sum
	of the live times from all slides.
	"""
	livetime = 0.0
	old_offsets = seglists.offsets.copy()
	N = len(time_slides)
	if verbose:
		print >>sys.stderr, "Computing the live time for %d time slides:" % N
	for n, time_slide in enumerate(time_slides):
		if verbose:
			print >>sys.stderr, "\t%.1f%%\r" % (100.0 * n / N),
		seglists.offsets.update(time_slide)
		livetime += float(abs(seglists.intersection(time_slide.keys())))
	seglists.offsets.update(old_offsets)
	if verbose:
		print >>sys.stderr, "\t100.0%"
	return livetime


#
# =============================================================================
#
#                              Rate vs. Threshold
#
# =============================================================================
#


#
# Book-keeping class
#


class RateVsThresholdData(object):
	def __init__(self):
		self.background_amplitudes = iterutils.Highest(max = 1e7)
		self.zero_lag_amplitudes = []
		self.background_time_slides = []
		self.out_seglists = segments.segmentlistdict()


	def update_from(self, contents, filename = ""):
		#
		# Record the "out" segment lists.
		#

		self.out_seglists |= contents.seglists

		#
		# Add the background time slides from this file to the
		# total list.
		#

		self.background_time_slides = ligolw_tisi.time_slide_list_merge(self.background_time_slides, get_background_time_slides(contents).values())

		#
		# Iterate over burst<-->burst coincidences.  Assume there
		# are no injections in this file.  Accumulate confidences
		# in a separate buffer to avoid slow calls to the Highest
		# class' append() method.
		#

		buffer = []
		for id, likelihood, confidence, is_background in bb_id_likelihood_confidence_background(contents):
			if is_background:
				buffer.append(coinc_amplitude(likelihood, confidence))
			else:
				self.zero_lag_amplitudes.append((coinc_amplitude(likelihood, confidence), filename, id))
		self.background_amplitudes.extend(buffer)


	def finish(self, zero_lag_survivors, verbose = False):
		# compute the live time at zero lag and in time slides.

		self.zero_lag_live_time = zero_lag_livetime(self.out_seglists)
		self.background_live_time = time_slides_livetime(self.out_seglists, self.background_time_slides, verbose = verbose)

		# sort the zero lag amplitudes from highest to lowest.

		self.zero_lag_amplitudes.sort(reverse = True)

		# given the desired number of survivors at zero lag, find
		# the "amplitude" threshold to cut coincs on.  The
		# interpretation is that only coincidences with an
		# "amplitude" greater than (not equal to) this value are to
		# be retained.

		self.amplitude_threshold = self.zero_lag_amplitudes[zero_lag_survivors][0]

		# compute the mean event rate vs. amplitude threshold as
		# observed in the time slides.  xcoords are the background
		# event amplitudes, ycoords are the are the corresponding
		# rates.  have to not trick array() with the Highest class'
		# fake element count.  this is \mu_{0}(x) in the Brady et
		# al. paper, but the actual value at threshold is extracted
		# from a smoothed approximation below.  note that the x
		# co-ordinates come out in decreasing order, but it's more
		# convenient for them to be in increasing order so we
		# reverse the two arrays.

		self.background_rate_x = numpy.fromiter(self.background_amplitudes, dtype = "double", count = list.__len__(self.background_amplitudes))
		self.background_rate_y = numpy.arange(1, len(self.background_rate_x) + 1, dtype = "double") / self.background_live_time
		self.background_rate_x = self.background_rate_x[::-1]
		self.background_rate_y = self.background_rate_y[::-1]

		# compute the 1-sigma vertical error bars.  ycoords is the
		# expected event rate measured from the background, times
		# zero_lag_live_time is the expected event count at zero
		# lag, the square root of which is the 1 sigma Poisson
		# fluctuation in the zero lag event count, divided by
		# zero_lag_live_time gives the 1 sigma Poisson fluctuation
		# in the expected zero lag event rate.

		self.background_rate_dy = numpy.sqrt(self.background_rate_y * self.zero_lag_live_time) / self.zero_lag_live_time

		# compute cubic spline approximation of background event
		# rate curve in the vacinity of the amplitude threshold.
		# this is used to obtain a smoothed approximation of the
		# background event rate and its first derivative, which are
		# \mu_{0}(x) and \mu_{0}'(x) in the Brady et al. paper.
		# because of the shape of the curve, it is more natural to
		# approximate ln rate(threshold) instead of
		# rate(threshold).  the curve samples used to construct the
		# spline are chosen by identifying the sample corresponding
		# to the amplitude threshold, and then taking the samples
		# on either side of that corresponding to +/- 1 order of
		# magnitude in rate (which is a symmetric interval in
		# ln(rate)).  to obtain \mu_{0}(x) and \mu_{0}'(x) from the
		# spline, if
		#
		#	y = ln rate(threshold)
		#
		# then
		#
		#	dy/dthreshold = 1/rate drate/dthreshold
		#
		# so
		#
		#	drate/dthreshold = rate * dy/dthreshold

		index = bisect.bisect(self.background_rate_x, self.amplitude_threshold)
		lo = len(self.background_rate_x) - (len(self.background_rate_x) - index) * 10
		hi = len(self.background_rate_x) - (len(self.background_rate_x) - index) / 10
		spline = interpolate.UnivariateSpline(self.background_rate_x[lo:hi], numpy.log(self.background_rate_y[ho:hi]), k = 3)
		self.background_rate_approximant = lambda x: numpy.exp(spline(x))

		self.mu0 = math.exp(spline(self.amplitude_threshold))
		self.mu0primed = self.mu0 * spline(self.amplitude_threshold, nu = 1)

		# compute the mean event rate vs. amplitude threshold as
		# observed at zero-lag.  to make the plots nicer, don't
		# include zero lag events whose amplitudes are below the
		# lowest amplitude recorded for a background event.  for
		# convenience, we reverse the arrays so that the x
		# co-ordinates are in increasing order.

		self.zero_lag_rate_x = numpy.array([zero_lag_amplitude[0] for zero_lag_amplitude in self.zero_lag_amplitudes], dtype = "double")
		self.zero_lag_rate_y = numpy.arange(1, len(self.zero_lag_rate_x) + 1, dtype = "double") / self.zero_lag_live_time
		self.zero_lag_rate_x = self.zero_lag_rate_x[::-1]
		self.zero_lag_rate_y = self.zero_lag_rate_y[::-1]
		index = bisect.bisect(self.zero_lag_rate_x, self.background_rate_x[0])
		self.zero_lag_rate_x = self.zero_lag_rate_x[index:]
		self.zero_lag_rate_y = self.zero_lag_rate_y[index:]


#
# Measure threshold
#


def measure_threshold(filenames, n_survivors, live_time_program = "lalapps_power", tmp_path = None, verbose = False):
	#
	# Initialize the book-keeping object.
	#

	rate_vs_threshold_data = RateVsThresholdData()

	#
	# Iterate over non-injection files.
	#

	for n, filename in enumerate(filenames):
		#
		# Open the database file.
		#

		if verbose:
			print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
		working_filename = dbtables.get_connection_filename(filename, tmp_path = tmp_path, verbose = verbose)
		connection = sqlite3.connect(working_filename)
		dbtables.DBTable_set_connection(connection)
		database = SnglBurstUtils.CoincDatabase(live_time_program, verbose)

		#
		# Process database contents.
		#

		rate_vs_threshold_data.update_from(database, filename = filename)

		#
		# Done with this file.
		#

		connection.close()
		dbtables.discard_connection_filename(filename, working_filename, verbose = verbose)

	#
	# Determine likelihood threshold.
	#

	if verbose:
		print >>sys.stderr, "finishing rate vs. threshold measurement ..."
	rate_vs_threshold_data.finish(n_survivors, verbose = verbose)

	#
	# Done.
	#

	return rate_vs_threshold_data


#
# Plot
#


def plot_rate_vs_threshold(data):
	"""
	Input is a RateVsThresholdData instance.  Output is a matplotlib
	Figure instance.
	"""
	# initialize the plot

	plot = SnglBurstUtils.BurstPlot(r"Coincident $n$-tuple ``amplitude'' (see text)", r"Mean Event Rate (Hz)")
	plot.axes.semilogy()

	# draw the background rate 1-sigma error bars as a shaded polygon
	# clipped to the plot.  warning:  the error bar polygon is not
	# *really* clipped to the axes' bounding box, the result will be
	# incorrect if the number of sample points is small.

	ymin = 1e-9
	ymax = 1e0
	poly_x = numpy.concatenate((data.background_rate_x, data.background_rate_x[::-1]))
	poly_y = numpy.concatenate((data.background_rate_y + 1 * data.background_rate_dy, (data.background_rate_y - 1 * data.background_rate_dy)[::-1]))
	plot.axes.add_patch(patches.Polygon(numpy.column_stack((poly_x, numpy.clip(poly_y, ymin, ymax))), edgecolor = "k", facecolor = "k", alpha = 0.3))

	# draw the mean background event rate

	line1 = plot.axes.plot(data.background_rate_x, data.background_rate_y, "k-")

	# draw the smoothed approximation to the mean background event rate

	index = bisect.bisect(data.background_rate_x, data.amplitude_threshold)
	x = data.background_rate_x[index - 100 : index + 100]
	line2 = plot.axes.plot(x, data.background_rate_approximant(x), "b-")

	# draw the mean zero-lag event rate

	line3 = plot.axes.plot(data.zero_lag_rate_x, data.zero_lag_rate_y, "r-")

	# draw a vertical marker indicating the threshold

	plot.axes.axvline(data.amplitude_threshold, color = "k")

	#plot.axes.set_ylim((ymin, ymax))
	#plot.axes.xaxis.grid(True, which="minor")
	#plot.axes.yaxis.grid(True, which="minor")

	# add a legend and a title

	plot.axes.legend((line1, line3), (r"Background (time slides)", r"Zero lag"))
	plot.axes.set_title(r"Mean Event Rate vs.\ Amplitude Threshold")

	# done

	return plot.fig


#
# =============================================================================
#
#                                  Efficiency
#
# =============================================================================
#


#
# Book-keeping class
#


class EfficiencyData(SimBurstUtils.Efficiency_hrss_vs_freq):
	def __init__(self, *args):
		SimBurstUtils.Efficiency_hrss_vs_freq.__init__(self, *args)
		self.recovered_x = []
		self.recovered_y = []
		self.recovered_z = []

	def add_contents(self, contents, amplitude_threshold):
		if self.instruments != contents.instruments:
			raise ValueError, "this document contains instruments %s, but we want %s" % ("+".join(contents.instruments), "+".join(self.instruments))

		# NOTE:  seglist must be computed the same way the segment
		# list used to measure live time is computed, otherwise the
		# detection efficiency is not being measured in the same
		# data as the live time is being claimed.  that means it
		# must be the simple intersection, with no attempt to
		# contract it on the grounds that it's difficult to detect
		# things at the edges of segments.

		seglist = contents.seglists.intersection(self.instruments)

		# iterate over all sims

		create_sim_coinc_map_view(contents)
		for values in contents.connection.cursor().execute("""
SELECT
	sim_burst.*,
	(
		SELECT
			MAX(sim_coinc_map.burst_coinc_amplitude)
		FROM
			sim_coinc_map
		WHERE
			sim_coinc_map.simulation_id == sim_burst.simulation_id
			AND sim_coinc_map.sim_coinc_def_id == ?
	)
FROM
	sim_burst
		""", (contents.scn_definer_id,)):
			sim = contents.sim_burst_table._row_from_cols(values)
			amplitude = values[-1]
			found = (amplitude is not None) and (amplitude > amplitude_threshold)
			# FIXME:  this assumes all injections are done at
			# zero lag (which is correct for now, but watch out
			# for this).  in general, a time_slide_id needs to
			# be retrieved from the sim_coinc_event, the
			# corresponding offsets retrieved from the
			# time_slide_table and applied to the segment
			# lists, and *then* the segmentlist intersection
			# taken and the _was_made() test performed on the
			# injection.
			if SimBurstUtils.injection_was_made(sim, seglist, self.instruments):
				# injection was made
				self.injected_x.append(sim.frequency)
				self.injected_y.append(sim.egw_over_rsquared)
				if amplitude is not None:
					# injection was recovered (found at
					# all)
					self.recovered_x.append(sim.frequency)
					self.recovered_y.append(sim.egw_over_rsquared)
					self.recovered_z.append(amplitude)
					if amplitude > amplitude_threshold:
						# injection was found above
						# threshold
						self.found_x.append(sim.frequency)
						self.found_y.append(sim.egw_over_rsquared)
			elif found:
				print >>sys.stderr, "odd, injection %s was found but not injected ..." % sim.simulation_id


	def finish(self, amplitude_threshold):
		SimBurstUtils.Efficiency_hrss_vs_freq.finish(self)

		# use the same binning for the efficiency density as was
		# constructed for the efficiency

		self.efficiency_density = rate.BinnedRatios(self.efficiency.denominator.bins)

		# share denominators (saves computing it twice)

		self.efficiency_density.denominator = self.efficiency.denominator

		# construct the amplitude weighting function

		# FIXME

		#amplitude_weight = rate.BinnedArray(NDBins((LinearBins(amplitude_threshold - 5, amplitude_threshold + 5, 101),)))

		# gaussian window's width is the number of bins
		# corresponding to 1 unit of amplitude, which is determined
		# by computing the inverse of the "volume" of the bin
		# corresponding to amplitude_threshold

		#window = rate.gaussian_window(1.0 / amplitude_weight.bins.volumes()[amplitude_weight.bins[(amplitude_threshold,)]])

		# store the recovered injections in the efficiency density
		# numerator bins weighted by amplitude

		#for x, y, z in zip(self.recovered_x, self.recovered_y, self.recovered_z):
		#	try:
		#		weight = amplitude_weight[(z,)]
		#	except IndexError:
		#		# beyond the edge of the window
		#		weight = 0.0
		#	self.efficiency_density.incnumerator((x, y), weight)

		# smooth the efficiency density numerator using the same 2D
		# window as was used for the efficiency numerator and
		# denominators

		#rate.filter_array(self.efficiency_density.numerator, rate.gaussian_window2d(self.window_size_x, self.window_size_y))


#
# Measure efficiency
#


def measure_efficiency(filenames, amplitude_threshold, live_time_program = "lalapps_power", tmp_path = None, verbose = False):
	# FIXME:  instrument is hard-coded.  bad bad bad.  sigh...
	efficiency = EfficiencyData(("H1", "H2", "L1"), (lambda sim, instrument: sim.egw_over_rsquared), r"$M_{\odot} / \mathrm{pc}^{2}$", 0.1)

	#
	# Iterate over injection files.
	#

	for n, filename in enumerate(filenames):
		#
		# Open the database file.
		#

		if verbose:
			print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
		working_filename = dbtables.get_connection_filename(filename, tmp_path = tmp_path, verbose = verbose)
		connection = sqlite3.connect(working_filename)
		connection.create_function("amplitude", 2, coinc_amplitude)
		dbtables.DBTable_set_connection(connection)
		database = SnglBurstUtils.CoincDatabase(live_time_program, verbose)

		#
		# Process database contents.
		#

		efficiency.add_contents(database, amplitude_threshold)

		#
		# Done with this file.
		#

		connection.close()
		dbtables.discard_connection_filename(filename, working_filename, verbose = verbose)

	return efficiency


#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#


#
# Command line.
#


options, filenames = parse_command_line()


#
# The "amplitude" of a coinc, the statistic on which we threshold.  This
# function is defined here, like this, so that the slope of the
# confidence-likelihood isodensity contours doesn't have to be carried
# around.  For example, we can export this function to sqlite3 as a
# two-argument function so that we do not have to pass the slope parameter
# into SQL queries.
#


def coinc_amplitude(likelihood, confidence, m = options.confidence_contour_slope):
	# In the 2-D confidence--likelihood parameter space, the background
	# isodensity contours for high confidence, high likelihood,
	# coincident n-tuples are found to be approximated by the family
	# of curves given by
	#
	#	likelihood = b * confidence^m
	#
	# or
	#
	#	ln likelihood = m ln confidence + ln b
	#
	# where m is a constant and b parameterizes the family of curves.
	# Injections are found to have high "b" values, and noise low "b"
	# values.  We use b (actually ln b) as the final, combined,
	# statistic measuring the "goodness" of a coincident n-tuple.
	# Given the likelihood ratio and confidence of a coincident
	# n-tuple, this function computes and returns ln b, which we refer
	# to as the "amplitude" of a coinc throughout this code following
	# the language of Brady et al.

	if likelihood <= 0:
		# log() doesn't like 0, so we handle this case separately.
		return float("-inf")

	if 1.0 / likelihood <= 0:
		# this time likelihood == +inf, which can happen when there
		# are regions of parameter space where no noise is ever,
		# *ever*, seen.
		return float("+inf")

	return math.log(likelihood) - m * math.log(confidence)


#
# Preparatory work?
#


if options.dump_scatter_data:
	print >>sys.stderr, "=== Confidence-Likelihood Scatter Dump ==="
	print >>sys.stderr
	dump_confidence_likelihood_scatter_data(options.background_glob + options.injections_glob, live_time_program = options.live_time_program, tmp_path = options.tmp_space, verbose = options.verbose)
	print >>sys.stderr
	print >>sys.stderr, "=== Done ==="
	sys.exit(0)


if options.plot_scatter_data:
	print >>sys.stderr, "=== Confidence-Likelihood Scatter Plot ==="
	print >>sys.stderr
	plot_confidence_likelihood_scatter_data(options.confidence_contour_slope, verbose = options.verbose)
	print >>sys.stderr
	print >>sys.stderr, "=== Done ==="
	sys.exit(0)


#
# Accumulate the statistics required to extract rate vs. threshold
# information, and measure the amplitude of the n_survivors+1 loudest
# event.
#


print >>sys.stderr, "=== Threshold ==="
print >>sys.stderr


if options.verbose:
	print >>sys.stderr, "building file list ..."
filenames = [filename for g in options.background_glob for filename in glob.glob(g)]
filenames.sort()
if not filenames:
	raise ValueError, "no background/zero lag files found"


rate_vs_threshold_data = measure_threshold(filenames, options.zero_lag_survivors, live_time_program = options.live_time_program, tmp_path = options.tmp_space, verbose = options.verbose)


print >>sys.stderr
print >>sys.stderr, "=== Threshold Summary ==="
print >>sys.stderr
print >>sys.stderr, "threshold definition:  ln likelihood > %.16g ln confidence + %.16g" % (options.confidence_contour_slope, rate_vs_threshold_data.amplitude_threshold)
print >>sys.stderr, "total live time in background = %.16g s" % rate_vs_threshold_data.background_live_time
print >>sys.stderr, "total live time at zero lag = %.16g s" % rate_vs_threshold_data.zero_lag_live_time
print >>sys.stderr, "number of coincs in background = %d" % len(rate_vs_threshold_data.background_amplitudes)
print >>sys.stderr, "average number of background coincs per zero lag live time = %.16g" % (len(rate_vs_threshold_data.background_amplitudes) / rate_vs_threshold_data.background_live_time * rate_vs_threshold_data.zero_lag_live_time)
print >>sys.stderr, "number of coincs at zero lag = %d" % len(rate_vs_threshold_data.zero_lag_amplitudes)
print >>sys.stderr, "at threshold, \\mu_{0} = %.16g Hz" % rate_vs_threshold_data.mu0
print >>sys.stderr, "at threshold, \\mu_{0}' = %.16g Hz / unit of amplitude" % rate_vs_threshold_data.mu0primed

print >>sys.stderr
print >>sys.stderr, "100 Highest-Ranked Zero Lag Events"
print >>sys.stderr, "----------------------------------"
print >>sys.stderr
print >>sys.stderr, "Amplitude\tFilename\tID"
for amplitude, filename, id in rate_vs_threshold_data.zero_lag_amplitudes[:100]:
	print >>sys.stderr, "%.16g\t%s\t%s" % (amplitude, filename, id)
print >>sys.stderr


print >>sys.stderr
print >>sys.stderr, "plotting event rate ..."
fig = plot_rate_vs_threshold(rate_vs_threshold_data)

#print >>sys.stderr, "writing lalapps_excesspowerfinal_rate_vs_threshold.pdf ..."
#fig.savefig("lalapps_excesspowerfinal_rate_vs_threshold.pdf")

print >>sys.stderr, "writing lalapps_excesspowerfinal_rate_vs_threshold.png ..."
fig.savefig("lalapps_excesspowerfinal_rate_vs_threshold.png")

print >>sys.stderr, "done."
print >>sys.stderr


#
# Efficiency
#


print >>sys.stderr
print >>sys.stderr, "=== Efficiency =="
print >>sys.stderr


amplitude_threshold = rate_vs_threshold_data.amplitude_threshold
#amplitude_threshold = 49.35564477090189


if options.verbose:
	print >>sys.stderr, "building file list ..."
filenames = [filename for g in options.injections_glob for filename in glob.glob(g)]
filenames.sort()
if not filenames:
	raise ValueError, "no injection files found"


efficiency = measure_efficiency(filenames, amplitude_threshold, live_time_program = options.live_time_program, tmp_path = options.tmp_space, verbose = options.verbose)
efficiency.finish(amplitude_threshold)


print >>sys.stderr
print >>sys.stderr, "=== Efficiency Summary ==="
print >>sys.stderr


print >>sys.stderr, "plotting efficiency curves ..."
fig = SimBurstUtils.plot_Efficiency_hrss_vs_freq(efficiency)

#print >>sys.stderr, "writing lalapps_excesspowerfinal_efficiency.pdf ..."
#fig.savefig("lalapps_excesspowerfinal_efficiency.pdf")

print >>sys.stderr, "writing lalapps_excesspowerfinal_efficiency.png ..."
fig.savefig("lalapps_excesspowerfinal_efficiency.png")


#
# Done.
#


print >>sys.stderr
print >>sys.stderr, "=== Done ==="
print >>sys.stderr
