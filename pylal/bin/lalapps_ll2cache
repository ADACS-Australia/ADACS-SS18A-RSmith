#!/usr/bin/python

"""
Build a LAL cache from a list of LIGO LW XML files containing search
summary tables.
"""

__author__ = "Kipp Cannon <kipp@gravity.phys.uwm.edu>"
__date__ = "$Date$"
__version__ = "$Revision$"


#
# Preamble.
#

import glob
from optparse import OptionParser
import os
import sys
from xml import sax

from glue import lal
from glue import segments
from glue.ligolw import docutils
from glue.ligolw import ligolw
from glue.ligolw import lsctables
from glue.ligolw import metaio


#
# Filter function for partial LIGO LW document loading.
#

def SearchSummaryFilter(name, attrs):
	if name != ligolw.Table.tagName:
		return False
	return attrs["Name"] == lsctables.SearchSummaryTable.tableName


#
# Parse command line.
#

parser = OptionParser(version = "%prog CVS $Id$")
parser.add_option("-v", "--verbose", action = "store_true", help = "be verbose")
parser.add_option("-o", "--output", metavar = "FILENAME", help = "write output to FILENAME (default = stdout")
options, patterns = parser.parse_args()
del parser

if options.output:
	output = file(options.output)
else:
	output = sys.stdout


#
# Process files one-by-one.
#

def all_identical(table, column):
	vals = table.getColumnByName(column).asarray()
	for i in range(1, len(vals)):
		if vals[i] != vals[0]:
			raise Exception, "not all identical"
	return vals[0]


cwd = os.getcwd().strip()


for pattern in patterns:
	for name in glob.glob(pattern):
		# Load and merge search summary tables
		doc = ligolw.Document()
		sax.parse(file(name), docutils.PartialLIGOLWContentHandler(doc, SearchSummaryFilter))
		docutils.MergeCompatibleTables(doc)
		if len(doc.childNodes) != 1:
			raise Exception, "Missing or incompatible search summary tables in document %s" % name
		searchsumm = doc.childNodes[0]

		# initialize cache entry info
		cache = lal.CacheEntry()

		# extract segment
		cache.segment = searchsumm.get_outlist().extent()

		# extract observatory
		try:
			cache.observatory = all_identical(searchsumm, "ifos")
		except:
			raise Exception, "Not all search summary entries in %s list the same ifo(s)" % name

		# extract description
		try:
			cache.description = all_identical(searchsumm, "comment")
		except:
			raise Exception, "Not all search summary entries in %s list the same comment" % name

		# set URL
		cache.url = "file://localhost" + cwd + "/" + name.strip()

		# write cache entry
		print >>output, str(cache)
