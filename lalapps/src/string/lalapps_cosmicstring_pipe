#!/usr/bin/python

#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


"""
Standalone ring pipeline driver script


$Id$

This script produces the condor submit and dag files to run
the standalone ring code on LIGO data
"""


import sys, os
import tempfile
import ConfigParser
from optparse import OptionParser


from glue import iterutils
from glue import pipeline
from glue import segments
from glue import segmentsUtils
from glue.lal import CacheEntry
from pylal import ligolw_tisi
from lalapps import cosmicstring
from lalapps import power


__author__ = 'Xavier Siemens<siemens@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


def parse_command_line():
	parser = OptionParser(
		usage = "%prog [options] ...",
		description = "FIXME"
	)
	parser.add_option("-f", "--config-file", metavar = "filename", help = "Use this configuration file (required).")
	parser.add_option("-l", "--log-path", metavar = "path", help = "Make condor put log files in this directory (required).")
	parser.add_option("--background-time-slides", metavar = "filename", action = "append", help = "Set file from which to obtain the time slide table for use in the background branch of the pipeline (required).  This option can be given multiple times to parallelize the background analysis across time slides.  You will want to make sure the time slide files have distinct vectors to not repeat the same analysis multiple times, and in particular you'll want to make sure only one of them has a zero-lag vector in it.") 
	parser.add_option("--injection-time-slides", metavar = "filename", help = "Set file from which to obtain the time slide table for use in the injection branch of the pipeline (required).")
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")

	options, filenames = parser.parse_args()

	required_options = ["log_path", "config_file", "background_time_slides", "injection_time_slides"]
	missing_options = [option for option in required_options if getattr(options, option) is None]
	if missing_options:
		raise ValueError, "missing required options %s" % (sorted("--%s" % option.replace("_", "-") for option in missing_options))

	options.injection_time_slides = [options.injection_time_slides]

	return options, filenames

options, filenames = parse_command_line()


#
# =============================================================================
#
#                                  Initialize
#
# =============================================================================
#


#
# start a log file for this script
#

basename = os.path.splitext(os.path.basename(options.config_file))[0]
log_fh = open(basename + '.pipeline.log', 'w')
print >>log_fh, "$Id$\nInvoked with arguments:"
for name_value in options.__dict__.items():
	print >>log_fh, "%s %s" % name_value
print >>log_fh

#
# create the config parser object and read in the ini file
#

config_parser = ConfigParser.ConfigParser()
config_parser.read(options.config_file)

#
# initialize lalapps.power and lalapps.cosmicstring modules
#

power.init_job_types(config_parser, job_types = ("datafind", "binj", "lladd", "binjfind", "burca", "sqlite"))
cosmicstring.init_job_types(config_parser, job_types = ("string", "meas_likelihood", "calc_likelihood"))

#
# make directories to store the cache files, job logs, and trigger output
#

def make_dag_directories(top_level_directory, config_parser):
	cwd = os.getcwd()
	power.make_dir_if_not_exists(top_level_directory)
	os.chdir(top_level_directory)
	power.make_dag_directories(config_parser)
	# FIXME:  move this into make_dag_directories().  requires update
	# of excess power and gstlal dags
	power.make_dir_if_not_exists(power.get_triggers_dir(config_parser))
	os.chdir(cwd)

power.make_dag_directories(config_parser)
injection_folders = []
for i in range(config_parser.getint('pipeline', 'injection-runs')):
	injection_folders.append(os.path.abspath("injections%d" % i))
	make_dag_directories(injection_folders[-1], config_parser)
noninjection_folders = []
noninjection_folders.append(os.path.abspath("noninjections"))
make_dag_directories(noninjection_folders[-1], config_parser)

#
# create a log file that the Condor jobs will write to
#

logfile = tempfile.mkstemp(prefix = basename, suffix = '.log', dir = options.log_path)[1]

#
# create the DAG writing the log to the specified directory
#

dag = pipeline.CondorDAG(logfile)
dag.set_dag_file(basename)

#
# get chunk lengths from the values in the ini file
#

short_segment_duration = config_parser.getint('lalapps_StringSearch', 'short-segment-duration')
pad = config_parser.getint('lalapps_StringSearch', 'pad')
min_segment_length = config_parser.getint('pipeline', 'segment-length') # not including pad at each end
trig_overlap = config_parser.getint('pipeline', 'trig_overlap')
overlap = short_segment_duration / 2 + 2 * pad	# FIXME:  correct?

#
# get the instruments and raw segments
#

# FIXME:  use ligolw_segments to convert segwizard files to XML format and
# use that as input for this script
seglists = segments.segmentlistdict((instrument, segmentsUtils.fromsegwizard(file(config_parser.get('input','segments_%s' % instrument))).coalesce()) for instrument in set(s.strip() for s in config_parser.get('pipeline','ifos').split(',')))

#
# Using time slide information, construct segment lists describing times
# requiring trigger construction.
#

if options.verbose:
	print >>sys.stderr, "Computing segments for which lalapps_StringSearch jobs are required ..."

background_time_slides = {}
background_seglists = segments.segmentlistdict()
for filename in options.background_time_slides:
	cache_entry = CacheEntry(None, None, None, "file://localhost" + os.path.abspath(filename))
	background_time_slides[cache_entry] = ligolw_tisi.load_time_slides(filename, verbose = options.verbose, gz = filename.endswith(".gz")).values()
	background_seglists |= cosmicstring.compute_segment_lists(seglists, ligolw_tisi.time_slide_component_vectors(background_time_slides[cache_entry], 2), min_segment_length, pad)

injection_time_slides = {}
injection_seglists = segments.segmentlistdict()
for filename in options.injection_time_slides:
	cache_entry = CacheEntry(None, None, None, "file://localhost" + os.path.abspath(filename))
	injection_time_slides[cache_entry] = ligolw_tisi.load_time_slides(filename, verbose = options.verbose, gz = filename.endswith(".gz")).values()
	injection_seglists |= cosmicstring.compute_segment_lists(seglists, ligolw_tisi.time_slide_component_vectors(injection_time_slides[cache_entry], 2), min_segment_length, pad)


#
# =============================================================================
#
#                                  Build DAG
#
# =============================================================================
#


#
# datafinds
#


datafinds = power.make_datafind_stage(dag, injection_seglists | background_seglists, verbose = options.verbose)


#
# trigger production, coincidence, injection identification, likelihood
# ratio probability density data collection
#


def make_coinc_branch(dag, datafinds, seglists, time_slides, min_segment_length, pad, overlap, short_segment_duration, tag, do_injections = False, verbose = False):
	#
	# injection job
	#

	if do_injections:
		binjnodes = power.make_binj_fragment(dag, seglists.extent_all(), tag, 0.0)

		# artificial parent-child relationship to force binj jobs
		# to run before all datafinds.  makes dag run faster
		# because it allows string search jobs to start moving onto
		# the cluster without waiting for all the datafinds
		# complete

		for binjnode in binjnodes:
			for datafindnode in datafinds:
				datafindnode.add_parent(binjnode)
	else:
		binjnodes = set()

	#
	# trigger generator jobs
	#

	trigger_nodes = cosmicstring.make_single_instrument_stage(dag, datafinds, seglists, tag, min_segment_length, pad, overlap, short_segment_duration, binjnodes = binjnodes, verbose = verbose)

	#
	# coincidence analysis
	#

	coinc_nodes = []
	binj_cache = set(cache_entry for node in binjnodes for cache_entry in node.get_output_cache())
	for n, (time_slides_cache_entry, these_time_slides) in enumerate(time_slides.items()):
		if verbose:
			print >>sys.stderr, "%s %d/%d (%s):" % (tag, n + 1, len(time_slides), time_slides_cache_entry.path())

		#
		# ligolw_cafe & ligolw_add
		#

		tisi_cache = set([time_slides_cache_entry])
		nodes = set()
		for seg, parents, cache in power.group_coinc_parents(trigger_nodes, these_time_slides, verbose = verbose):
			nodes |= power.make_lladd_fragment(dag, parents | binjnodes, "%s_%d" % (tag, n), segment = seg, input_cache = cache | binj_cache, extra_input_cache = tisi_cache, remove_input = do_injections, preserve_cache = binj_cache | tisi_cache)

		#
		# ligolw_burca
		#

		if verbose:
			print >>sys.stderr, "building burca jobs ..."
		coinc_nodes.append(power.make_burca_fragment(dag, nodes, "%s_%d" % (tag, n), verbose = verbose))
		if verbose:
			print >>sys.stderr, "done %s %d/%d" % (tag, n + 1, len(time_slides))

	#
	# ligolw_binjfind
	#

	if do_injections:
		if verbose:
			print >>sys.stderr, "building binjfind jobs ..."
		coinc_nodes = [power.make_binjfind_fragment(dag, these_coinc_nodes, "%s_%d" % (tag, n), verbose = verbose) for n, these_coinc_nodes in enumerate(coinc_nodes)]

	#
	# ligolw_sqlite
	#

	if verbose:
		print >>sys.stderr, "building sqlite jobs ..."
	coinc_nodes = [power.make_sqlite_fragment(dag, these_coinc_nodes, "%s_%d" % (tag, n), verbose = verbose) for n, these_coinc_nodes in enumerate(coinc_nodes)]

	#
	# lalapps_string_meas_likelihood
	#

	if verbose:
		print >>sys.stderr, "building lalapps_string_meas_likelihood jobs ..."
	likelihood_nodes = [cosmicstring.make_meas_likelihood_fragment(dag, these_coinc_nodes, "%s_%d" % (tag, n)) for n, these_coinc_nodes in enumerate(coinc_nodes)]

	#
	# write output cache
	#

	if verbose:
		print >>sys.stderr, "writing output cache ..."
	for n, (these_coinc_nodes, these_likelihood_nodes) in enumerate(zip(coinc_nodes, likelihood_nodes)):
		power.write_output_cache(these_coinc_nodes | these_likelihood_nodes, "%s_%s_output.cache" % (os.path.splitext(dag.get_dag_file())[0], "%s_%d" % (tag, n)))

	#
	# done
	#

	return coinc_nodes, likelihood_nodes


user_tag = config_parser.get('pipeline', 'user_tag')


injection_coinc_nodes = []
injection_likelihood_nodes = []
for i, folder in enumerate(injection_folders):
	cwd = os.getcwd()
	os.chdir(folder)
	# note:  unpacking enforces rule that each injection run uses a
	# single time-slides file
	[these_coinc_nodes], [these_likelihood_nodes] = make_coinc_branch(dag, datafinds, injection_seglists, injection_time_slides, min_segment_length, pad, overlap, short_segment_duration, "%s_INJ_%d" % (user_tag, i), do_injections = True, verbose = options.verbose)
	os.chdir(cwd)
	injection_coinc_nodes.append(these_coinc_nodes)
	injection_likelihood_nodes.append(these_likelihood_nodes)


for i, folder in enumerate(noninjection_folders):
	assert i == 0	# loop only works for one iteration
	cwd = os.getcwd()
	os.chdir(folder)
	background_coinc_nodes, background_likelihood_nodes = make_coinc_branch(dag, datafinds, background_seglists, background_time_slides, min_segment_length, pad, overlap, short_segment_duration, "%s_%d" % (user_tag, i), do_injections = False, verbose = options.verbose)
	os.chdir(cwd)


def flatten_node_groups(node_groups):
	return reduce(lambda a, b: a | b, node_groups, set())


all_background_likelihood_nodes = flatten_node_groups(background_likelihood_nodes)
all_injection_likelihood_nodes = flatten_node_groups(injection_likelihood_nodes)


#
# round-robin parameter distribution density data for likelihood ratio
# computation.
#


if options.verbose:
	print >>sys.stderr, "building lalapps_string_calc_likelihood jobs ..."

def round_robin_and_flatten(injection_coinc_node_groups, injection_likelihood_node_groups):
	# see the documentation for glue.iterutils.choices() for an
	# explanation of the procedure used here to round-robing the node
	# lists
	A = list(iterutils.choices(injection_coinc_node_groups, 1))
	B = list(iterutils.choices(injection_likelihood_node_groups, len(injection_likelihood_node_groups) - 1))
	B.reverse()
	A = [flatten_node_groups(node_groups) for node_groups in A]
	B = [flatten_node_groups(node_groups) for node_groups in B]
	return zip(A, B)

coinc_nodes = set()
for n, (these_coinc_nodes, these_likelihood_nodes) in enumerate(round_robin_and_flatten(injection_coinc_nodes, injection_likelihood_nodes)):
	coinc_nodes |= cosmicstring.make_calc_likelihood_fragment(dag, these_coinc_nodes, these_likelihood_nodes | all_background_likelihood_nodes, "%s_INJ_%d" % (user_tag, n), files_per_calc_likelihood = cosmicstring.calc_likelihoodjob.files_per_calc_likelihood, verbose = options.verbose)
coinc_nodes |= cosmicstring.make_calc_likelihood_fragment(dag, flatten_node_groups(background_coinc_nodes), all_injection_likelihood_nodes | all_background_likelihood_nodes, user_tag, files_per_calc_likelihood = 1, verbose = options.verbose)



#
# =============================================================================
#
#                                     Done
#
# =============================================================================
#

#
# write DAG
#

if options.verbose:
	print >>sys.stderr, "writing dag ..."
dag.write_sub_files()
dag.write_dag()

#
# write a message telling the user that the DAG has been written
#

print """Created a DAG file which can be submitted by executing

$ condor_submit_dag %s

from a condor submit machine (e.g. hydra.phys.uwm.edu)

Do not forget to initialize your grid proxy certificate on the condor
submit machine by running the commands

$ unset X509_USER_PROXY
$ grid-proxy-init -hours $((24*7)):00

""" % dag.get_dag_file()
