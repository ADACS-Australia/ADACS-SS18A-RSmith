#!/usr/bin/python
"""
Standalone ring pipeline driver script


$Id$

This script produces the condor submit and dag files to run
the standalone ring code on LIGO data
"""

__author__ = 'Xavier Siemens<siemens@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]


# import standard modules and append the lalapps prefix to the python path
import sys, os
import tempfile
import ConfigParser
from optparse import OptionParser

# import the modules we need to build the pipeline
from glue import pipeline
from glue import segments
from glue import segmentsUtils
from glue.ligolw import lsctables
from glue.ligolw import utils
from lalapps import cosmicstring
from lalapps import power


def parse_command_line():
  parser = OptionParser(
    usage = "%prog [options] ...",
    description = "FIXME"
  )
  parser.add_option("-i", "--injections", action = "store_true", help = "Add simulated bursts.")
  parser.add_option("-f", "--config-file", metavar = "filename", help = "Use this configuration file (required).")
  parser.add_option("-l", "--log-path", metavar = "path", help = "Make condor put log files in this directory (required).")
  parser.add_option("-p", "--playground", action = "store_true", help = "Run on playground data only.")
  parser.add_option("-t", "--time-slides", metavar = "filename", help = "Load time-slide vectors from this XML file (required).")
  parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")

  options, filenames = parser.parse_args()

  required_options = ["log_path", "config_file", "time_slides"]
  missing_options = [option for option in required_options if getattr(options, option) is None]
  if missing_options:
  	raise ValueError, "missing required options %s" % (sorted("--%s" % option.replace("_", "-") for option in missing_options))

  return options, filenames

options, filenames = parse_command_line()


# start a log file for this script
basename = os.path.splitext(os.path.basename(options.config_file))[0]
log_fh = open(basename + '.pipeline.log', 'w')
print >>log_fh, "$Id$\nInvoked with arguments:"
for name_value in options.__dict__.items():
  print >>log_fh, "%s %s" % name_value
print >>log_fh


# create the config parser object and read in the ini file
config_parser = ConfigParser.ConfigParser()
config_parser.read(options.config_file)

# initialize lalapps.power and lalapps.cosmicstring modules
power.init_job_types(config_parser, job_types = ("datafind", "binj"))
cosmicstring.init_job_types(config_parser, job_types = ("string",))

# try and make a directory to store the cache files and job logs
power.make_dag_directories(config_parser)
try: os.mkdir('triggers')
except: pass

# create a log file that the Condor jobs will write to
tempfile.tempdir = options.log_path
tempfile.template = basename + '.log'
logfile = tempfile.mktemp()
file(logfile, "w")	# create/truncate the file

# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
dag.set_dag_file(basename)

# create the Condor jobs that will be used in the DAG
string_job = cosmicstring.StringJob(config_parser)
string_job.set_sub_file( basename + '.string.sub' )

# get chunk lengths from the values in the ini file
short_segment_duration = config_parser.getint('lalapps_StringSearch', 'short-segment-duration')
pad = config_parser.getint('lalapps_StringSearch', 'pad')
min_segment_length = config_parser.getint('pipeline', 'segment-length') # not including pad at each end
trig_overlap = config_parser.getint('pipeline', 'trig_overlap')
overlap = short_segment_duration / 2 + 2 * pad	# FIXME:  correct?

# get the instruments
ifos = set(s.strip() for s in config_parser.get('pipeline','ifos').split(','))

# get the segments and time-slide offset vectors
seglists = segments.segmentlistdict((ifo, segmentsUtils.fromsegwizard(file(config_parser.get('input','segments_%s' % ifo))).coalesce()) for ifo in ifos)
offset_vectors = lsctables.table.get_table(utils.load_filename(options.time_slides, gz = (options.time_slides or "stdin").endswith(".gz"), verbose = options.verbose), lsctables.TimeSlideTable.tableName).as_dict()


# create injection job
if options.injections:
  binj_fragment = power.make_binj_fragment(dag, seglists.extent_all(), config_parser.get('pipeline', 'user_tag'), 0.0)
else:
  binj_fragment = set()


# loop over instruments, constructing trigger generator jobs
for ifo in ifos:
  # loop over segments
  for segment_number, seg in enumerate(seglists[ifo]):
    # clip segment to string job's requirements
    string_seg = cosmicstring.clip_segment(seg, pad, short_segment_duration)
    if(abs(string_seg) - 2 * pad < min_segment_length):
      # segment too small
      continue

    # find all the data for ifo
    datafind_fragment = power.make_datafind_fragment(dag, ifo, seg)
    # artificial parent-child relationship to force binj job to run before
    # all datafinds.  makes dag run faster because it allows string search
    # jobs to move onto the cluster before all the datafinds complete
    for parent in binj_fragment:
      for child in datafind_fragment:
        child.add_parent(parent)

    # loop over string jobs for this segment
    while abs(string_seg) >= min_segment_length + 2 * pad:
      # try to use 5* min_segment_length each time (must be an odd
      # multiple!).
      if abs(string_seg) >= 5 * min_segment_length + 2 * pad:
        job_seg = segments.segment(string_seg[0], string_seg[0] + 5 * min_segment_length + 2 * pad)
      else:
        job_seg = cosmicstring.clip_segment(string_seg, pad, short_segment_duration)

      # string job for this segment
      if binj_fragment:
        string_fragment = cosmicstring.make_string_fragment(dag, datafind_fragment | binj_fragment, ifo, job_seg, config_parser.get('pipeline', 'user_tag'), [node for node in datafind_fragment][0].get_output(), injargs = {'injection-file': [cache_entry.path() for node in binj_fragment for cache_entry in node.get_output_cache()][0]})
      else:
        string_fragment = cosmicstring.make_string_fragment(dag, datafind_fragment | binj_fragment, ifo, job_seg, config_parser.get('pipeline', 'user_tag'), [node for node in datafind_fragment][0].get_output())
      # dummy code to force output files to get set
      for node in string_fragment:
        node.get_output_cache()

      # advance segment
      string_seg = segments.segment(job_seg[1] - overlap, string_seg[1])

# write out the DAG
dag.write_sub_files()
dag.write_dag()

# write a message telling the user that the DAG has been written
print """Created a DAG file which can be submitted by executing

   condor_submit_dag %s

from a condor submit machine (e.g. hydra.phys.uwm.edu)

Do not forget to initialize your grid proxy certificate on the condor
submit machine by running the commands

  unset X509_USER_PROXY
  grid-proxy-init -hours 177
  export LSC_DATAFIND_SERVER=dataserver.phys.uwm.edu

Contact the administrator of your cluster to find the hostname and port of
the LSCdataFind server.""" % dag.get_dag_file()
