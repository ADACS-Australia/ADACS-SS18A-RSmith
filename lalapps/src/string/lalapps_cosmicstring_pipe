#!/usr/bin/python

#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


"""
Standalone ring pipeline driver script


$Id$

This script produces the condor submit and dag files to run
the standalone ring code on LIGO data
"""


import sys, os
import tempfile
import ConfigParser
from optparse import OptionParser


from glue import pipeline
from glue import segments
from glue import segmentsUtils
from glue.lal import CacheEntry
from pylal import ligolw_tisi
from lalapps import cosmicstring
from lalapps import power


__author__ = 'Xavier Siemens<siemens@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


def parse_command_line():
	parser = OptionParser(
		usage = "%prog [options] ...",
		description = "FIXME"
	)
	parser.add_option("-i", "--injections", action = "store_true", help = "Add simulated bursts.")
	parser.add_option("-f", "--config-file", metavar = "filename", help = "Use this configuration file (required).")
	parser.add_option("-l", "--log-path", metavar = "path", help = "Make condor put log files in this directory (required).")
	parser.add_option("-p", "--playground", action = "store_true", help = "Run on playground data only.")
	parser.add_option("-t", "--time-slides", metavar = "filename", help = "Load time-slide vectors from this XML file (required).")
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")

	options, filenames = parser.parse_args()

	required_options = ["log_path", "config_file", "time_slides"]
	missing_options = [option for option in required_options if getattr(options, option) is None]
	if missing_options:
		raise ValueError, "missing required options %s" % (sorted("--%s" % option.replace("_", "-") for option in missing_options))

	return options, filenames

options, filenames = parse_command_line()


#
# =============================================================================
#
#                                  Initialize
#
# =============================================================================
#


#
# start a log file for this script
#

basename = os.path.splitext(os.path.basename(options.config_file))[0]
log_fh = open(basename + '.pipeline.log', 'w')
print >>log_fh, "$Id$\nInvoked with arguments:"
for name_value in options.__dict__.items():
	print >>log_fh, "%s %s" % name_value
print >>log_fh

#
# create the config parser object and read in the ini file
#

config_parser = ConfigParser.ConfigParser()
config_parser.read(options.config_file)

#
# initialize lalapps.power and lalapps.cosmicstring modules
#

power.init_job_types(config_parser, job_types = ("datafind", "binj", "lladd", "binjfind", "burca", "sqlite"))
cosmicstring.init_job_types(config_parser, job_types = ("string",))

#
# make directories to store the cache files, job logs, and trigger output
#

power.make_dag_directories(config_parser)
try:
	os.mkdir('triggers')
except:
	pass

#
# create a log file that the Condor jobs will write to
#

logfile = tempfile.mkstemp(prefix = basename, suffix = '.log', dir = options.log_path)[1]

#
# create the DAG writing the log to the specified directory
#

dag = pipeline.CondorDAG(logfile)
dag.set_dag_file(basename)

#
# get chunk lengths from the values in the ini file
#

short_segment_duration = config_parser.getint('lalapps_StringSearch', 'short-segment-duration')
pad = config_parser.getint('lalapps_StringSearch', 'pad')
min_segment_length = config_parser.getint('pipeline', 'segment-length') # not including pad at each end
trig_overlap = config_parser.getint('pipeline', 'trig_overlap')
overlap = short_segment_duration / 2 + 2 * pad	# FIXME:  correct?

#
# get the instruments and raw segments
#

seglists = segments.segmentlistdict((instrument, segmentsUtils.fromsegwizard(file(config_parser.get('input','segments_%s' % instrument))).coalesce()) for instrument in set(s.strip() for s in config_parser.get('pipeline','ifos').split(',')))

#
# Using time slide information, construct segment lists describing times
# requiring trigger construction.
#

if options.verbose:
	print >>sys.stderr, "Computing segments for which lalapps_StringSearch jobs are required ..."

background_time_slides = {}
background_seglists = segments.segmentlistdict()
for filename in [options.time_slides]:
	cache_entry = CacheEntry(None, None, None, "file://localhost" + os.path.abspath(filename))
	background_time_slides[cache_entry] = ligolw_tisi.load_time_slides(filename, verbose = options.verbose, gz = filename.endswith(".gz")).values()
	background_seglists |= cosmicstring.compute_segment_lists(seglists, ligolw_tisi.time_slide_component_vectors(background_time_slides[cache_entry], 2), min_segment_length, pad)


#
# =============================================================================
#
#                                  Build DAG
#
# =============================================================================
#


#
# datafinds
#

datafinds = power.make_datafind_stage(dag, background_seglists, verbose = options.verbose)

#
# main analysis
#

def make_coinc_branch(dag, datafinds, seglists, time_slides, min_segment_length, pad, overlap, short_segment_duration, tag, do_injections = False, verbose = False):
	#
	# injection job
	#

	if do_injections:
		binjnodes = power.make_binj_fragment(dag, seglists.extent_all(), tag, 0.0)
	else:
		binjnodes = set()

	# artificial parent-child relationship to force binj jobs to run before all
	# datafinds.  makes dag run faster because it allows string search jobs to
	# move onto the cluster before all the datafinds complete

	for binjnode in binjnodes:
		for datafindnode in datafinds:
			datafindnode.add_parent(binjnode)

	#
	# trigger generator jobs
	#

	trigger_nodes = cosmicstring.make_single_instrument_stage(dag, datafinds, seglists, tag, min_segment_length, pad, overlap, short_segment_duration, binjnodes = binjnodes, verbose = verbose)

	#
	# coincidence analysis
	#

	coinc_nodes = set()
	binj_cache = set(cache_entry for node in binjnodes for cache_entry in node.get_output_cache())
	for n, (time_slides_cache_entry, these_time_slides) in enumerate(time_slides.items()):
		if verbose:
			print >>sys.stderr, "%s %d/%d (%s):" % (tag, n + 1, len(time_slides), time_slides_cache_entry.path())

		#
		# ligolw_cafe & ligolw_add
		#

		tisi_cache = set([time_slides_cache_entry])
		nodes = set()
		for seg, parents, cache in power.group_coinc_parents(trigger_nodes, these_time_slides, verbose = verbose):
			preserve_cache = binj_cache | tisi_cache
			if not do_injections:
				preserve_cache |= cache
			nodes |= power.make_lladd_fragment(dag, parents | binjnodes, "%s_%d" % (tag, n), segment = seg, input_cache = cache | binj_cache, extra_input_cache = tisi_cache, preserve_cache = preserve_cache)

		#
		# ligolw_burca
		#

		if verbose:
			print >>sys.stderr, "building burca jobs ..."
		coinc_nodes |= power.make_burca_fragment(dag, nodes, "%s_%d" % (tag, n), verbose = verbose)
		if verbose:
			print >>sys.stderr, "done %s %d/%d" % (tag, n + 1, len(time_slides))

	#
	# ligolw_binjfind
	#

	if do_injections:
		if verbose:
			print >>sys.stderr, "building binjfind jobs ..."
		coinc_nodes = power.make_binjfind_fragment(dag, coinc_nodes, tag, verbose = verbose)

	#
	# ligolw_sqlite
	#

	if verbose:
		print >>sys.stderr, "building sqlite jobs ..."
	coinc_nodes = power.make_sqlite_fragment(dag, coinc_nodes, tag, verbose = verbose)

	#
	# done
	#

	power.write_output_cache(coinc_nodes, "%s_%s_output.cache" % (os.path.splitext(dag.get_dag_file())[0], tag))
	return coinc_nodes


coinc_nodes = make_coinc_branch(dag, datafinds, background_seglists, background_time_slides, min_segment_length, pad, overlap, short_segment_duration, config_parser.get('pipeline', 'user_tag'), do_injections = options.injections, verbose = options.verbose)


#
# =============================================================================
#
#                                     Done
#
# =============================================================================
#

#
# write DAG
#

dag.write_sub_files()
dag.write_dag()

#
# write a message telling the user that the DAG has been written
#

print """Created a DAG file which can be submitted by executing

$ condor_submit_dag %s

from a condor submit machine (e.g. hydra.phys.uwm.edu)

Do not forget to initialize your grid proxy certificate on the condor
submit machine by running the commands

$ unset X509_USER_PROXY
$ grid-proxy-init -hours 177
$ export LSC_DATAFIND_SERVER=dataserver.phys.uwm.edu

Contact the administrator of your cluster to find the hostname and port of
the LSCdataFind server.""" % dag.get_dag_file()
