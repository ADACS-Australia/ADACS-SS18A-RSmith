#!/usr/bin/python
"""
Standalone ring pipeline driver script


$Id$

This script produces the condor submit and dag files to run
the standalone ring code on LIGO data
"""

__author__ = 'Xavier Siemens<siemens@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]


# import standard modules and append the lalapps prefix to the python path
import sys, os, random
import re, string
import tempfile
import ConfigParser
from optparse import OptionParser

# import the modules we need to build the pipeline
from glue import pipeline
from glue import segments
from glue import segmentsUtils
from lalapps import cosmicstring


def parse_command_line():
  parser = OptionParser(
    usage = "%prog [options] ...",
    description = "FIXME"
  )
  parser.add_option("-i", "--injections", action = "store_true", help = "Add simulated bursts.")
  parser.add_option("-f", "--config-file", metavar = "filename", help = "Use this configuration file.")
  parser.add_option("-l", "--log-path", metavar = "path", help = "Make condor put log files in this directory.")
  parser.add_option("-p", "--playground", action = "store_true", help = "Run on playground data only.")
  parser.add_option("-d", "--datafind", action = "store_true", help = "Do datafind step.")
  parser.add_option("-b", "--burst-search", action = "store_true", help = "Do the burst search.")
  parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")

  options, filenames = parser.parse_args()

  required_options = ["log_path", "config_file"]
  missing_options = [option for option in required_options if getattr(options, option) is None]
  if missing_options:
  	raise ValueError, "missing required options %s" % (sorted("--%s" % option.replace("_", "-") for option in missing_options))

  return options, filenames

options, filenames = parse_command_line()


# try and make a directory to store the cache files and job logs
try: os.mkdir('logs')
except: pass

try: os.mkdir('cache')
except: pass

try: os.mkdir('triggers')
except: pass

# create the config parser object and read in the ini file
cp = ConfigParser.ConfigParser()
cp.read(options.config_file)

# create a log file that the Condor jobs will write to
basename = os.path.splitext(os.path.basename(options.config_file))[0]
tempfile.tempdir = options.log_path

tempfile.template = basename + '.log'
logfile = tempfile.mktemp()
fh = open( logfile, "w" )
fh.close()

# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
dag.set_dag_file(basename)

# create the Condor jobs that will be used in the DAG
df_job = pipeline.LSCDataFindJob('cache','logs',cp)
string_job = cosmicstring.StringJob(cp)
if options.injections: 
  injection_job = cosmicstring.InjJob(cp)

# set better submit file names than the default
subsuffix = '.sub'
df_job.set_sub_file( basename + '.datafind'+ subsuffix )
string_job.set_sub_file( basename + '.string' + subsuffix )
if options.injections: 
  injection_job.set_sub_file( basename + '.binj' + subsuffix ) 

# get chunk lengths from the values in the ini file
short_segment_duration = cp.getint('string', 'short-segment-duration')
pad = cp.getint('string', 'pad')
min_segment_length = cp.getint('pipeline', 'segment-length') # not including pad at each end
trig_overlap = cp.getint('pipeline', 'trig-overlap')
overlap = short_segment_duration / 2 + 2 * pad	# FIXME:  correct?

# get the instruments and snr thresholds
ifos = set(s.strip() for s in cp.get('pipeline','ifos').split(','))
snr = dict((ifo, cp.get('pipeline','threshold_%s' % ifo)) for ifo in ifos)

# get the segments
seglists = segments.segmentlistdict((ifo, segmentsUtils.fromsegwizard(file(cp.get('input','segments_%s' % ifo))).coalesce()) for ifo in ifos)

# write out a log file for this script
log_fh = open(basename + '.pipeline.log', 'w')

print >>log_fh, "$Id$\n"
print >>log_fh, "Invoked with arguments:"
for name, value in options.__dict__.items():
  print >>log_fh, "%s %s" % (name, value)
print >>log_fh


def clip_segment(seg, pad, short_segment_duration):
  # clip segment to the length required by lalapps_StringSearch.  if
  #
  #   duration = segment length - padding
  #
  # then
  #
  #   duration / short_segment_duration - 0.5
  #
  # must be an odd integer, therefore
  #
  #   2 * duration + short_segment_duration
  #
  # must be divisble by (4 * short_segment_duration)
  duration = float(abs(seg)) - 2 * pad
  extra = (2 * duration + short_segment_duration) % (4 * short_segment_duration)
  extra /= 2

  # clip segment
  seg = segments.segment(seg[0], seg[1] - extra)

  # bounds must be integers
  if int(seg[0]) != seg[0] or int(seg[1]) != seg[1]:
    raise ValueError, "segment %s does not have integer boundaries" % str(seg)
  seg = segments.segment(int(seg[0]), int(seg[1]))

  # done
  return seg


# create injection job
if options.injections:
  seg = seglists.extent_all()
  inject = cosmicstring.InjNode(injection_job)
  inject.set_start(int(seg[0]))
  inject.set_end(int(seg[1]))
  seed = int(1e6 * random.uniform(0.0,1.0))
  inject.add_var_opt('seed',seed)
  inject.set_name('inj')
  dag.add_node(inject)


# loop over instruments, constructing trigger generator jobs
for ifo in ifos:
  # create all the LSCdataFind jobs to run in sequence
  datatype = cp.get('input', 'type_%s' % ifo) 
  channel = cp.get('input', 'channel_%s' % ifo) 

  # loop over segments
  for segment_number, seg in enumerate(seglists[ifo]):
    # clip segment to string job's requirements
    string_seg = clip_segment(seg, pad, short_segment_duration)
    if(abs(string_seg) - 2 * pad < min_segment_length):
      # segment too small
      continue

    # find all the data for ifo
    df = pipeline.LSCDataFindNode(df_job)
    df.set_start(seg[0])
    df.set_end(seg[1])
    df.set_observatory(ifo[0])
    df.set_type(datatype)
    df.set_name("df_%s_%d_%d" % (ifo, int(seg[0]), int(seg[1])))
    if options.datafind:
      dag.add_node(df)

    # loop over string jobs for this segment
    while abs(string_seg) >= min_segment_length + 2 * pad:
      # try to use 5* min_segment_length each time (must be an odd
      # multiple!).
      if abs(string_seg) >= 5 * min_segment_length + 2 * pad:
        job_seg = segments.segment(string_seg[0], string_seg[0] + 5 * min_segment_length + 2 * pad)
      else:
        job_seg = clip_segment(string_seg, pad, short_segment_duration)

      # string job for this segment
      string = cosmicstring.StringNode(string_job)
      string.set_start(job_seg[0])
      string.set_end(job_seg[1])
      string.set_ifo(ifo)
      string.set_cache(df.get_output())
      string.add_var_opt('channel', channel)
      string.add_var_opt('threshold', snr[ifo])
      string.add_var_opt('trig-start-time', job_seg[0])
      string.add_var_opt('outfile', string.get_output())
      if options.datafind:
        string.add_parent(df)
      string.set_name("string_%s_%d_%d" % (ifo, int(job_seg[0]), int(job_seg[1])))
      if options.injections:
        string.add_parent(inject)
        string.add_var_opt('injection-file', inject.get_output())
      if options.burst_search:
        dag.add_node(string)

      # advance segment
      string_seg = segments.segment(job_seg[1] - overlap, string_seg[1])

# write out the DAG
dag.write_sub_files()
dag.write_dag()

# write a message telling the user that the DAG has been written
print "\nCreated a DAG file which can be submitted by executing"
print "\n   condor_submit_dag -maxjobs 100 ", dag.get_dag_file()
print """\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
If you are running LSCdataFind jobs, do not forget to initialize your grid 
proxy certificate on the condor submit machine by running the commands

  unset X509_USER_PROXY
  grid-proxy-init -hours 177
  export LSC_DATAFIND_SERVER=dataserver.phys.uwm.edu

Contact the administrator of your cluster to find the hostname and port of the
LSCdataFind server.
"""
