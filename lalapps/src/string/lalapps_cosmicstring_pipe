#!/usr/bin/python
"""
Standalone ring pipeline driver script


$Id$

This script produces the condor submit and dag files to run
the standalone ring code on LIGO data
"""

__author__ = 'Xavier Siemens<siemens@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]


# import standard modules and append the lalapps prefix to the python path
import sys, os, random
import getopt, re, string
import tempfile
import ConfigParser

# import the modules we need to build the pipeline
from glue import pipeline
from lalapps import cosmicstring

def usage():
  msg = """\
Usage: lalapps_ring_pipe [options]

  -h, --help               display this message
  -v, --version            print version information and exit
  -i, --injections         add simulated bursts
  -f, --config-file FILE   use configuration file FILE
  -l, --log-path PATH      directory to write condor log file
  -p, --playground         run on playground data only 
  -d, --datafind           perform the datafind step
  -b, --burst-search       perform the burst search
"""
  print >> sys.stderr, msg

# pasrse the command line options to figure out what we should do
shortop = "hvipf:l:db"
longop = [
  "help",
  "version",
  "injections",
  "playground",
  "config-file=",
  "log-path=",
  "datafind",
  "burst-search",
  ]

try:
  opts, args = getopt.getopt(sys.argv[1:], shortop, longop)
except getopt.GetoptError:
  usage()
  sys.exit(1)

config_file = None
do_inj = 0
config_file = None
log_path = None
do_slides = None
playground = 0
do_datafind = None
do_burstsearch = None
do_coincidence = None

for o, a in opts:
  if o in ("-v", "--version"):
    print "$Id$"
    sys.exit(0)
  elif o in ("-h", "--help"):
    usage()
    sys.exit(0)
  elif o in ("-i", "--injections"):
    do_inj = 1
  elif o in ("-p", "--playground"):
    playground = 1
  elif o in ("-f", "--config-file"):
    config_file = a
  elif o in ("-l", "--log-path"):
    log_path = a
  elif o in ("-d", "--datafind"):
    do_datafind = 1
  elif o in ("-b", "--burst-search"):
    do_burstsearch = 1
  else:
    print >> sys.stderr, "Unknown option:", o
    usage()
    sys.exit(1)

if not config_file:
  print >> sys.stderr, "No configuration file specified."
  print >> sys.stderr, "Use --config-file FILE to specify location."
  sys.exit(1)

if not log_path:
  print >> sys.stderr, "No log file path specified."
  print >> sys.stderr, "Use --log-path PATH to specify a location."
  sys.exit(1)

# try and make a directory to store the cache files and job logs
try: os.mkdir('logs')
except: pass

try: os.mkdir('cache')
except: pass

try: os.mkdir('triggers')
except: pass

# create the config parser object and read in the ini file
cp = ConfigParser.ConfigParser()
cp.read(config_file)

# create a log file that the Condor jobs will write to
basename = os.path.splitext(os.path.basename(config_file))[0]
tempfile.tempdir = log_path

tempfile.template = basename + '.log'
logfile = tempfile.mktemp()
fh = open( logfile, "w" )
fh.close()

# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
dag.set_dag_file(basename)

# create the Condor jobs that will be used in the DAG
df_job = pipeline.LSCDataFindJob('cache','logs',cp)
string_job = cosmicstring.StringJob(cp)
if do_inj: 
  injection_job = cosmicstring.InjJob(cp)

# set better submit file names than the default
subsuffix = '.sub'
df_job.set_sub_file( basename + '.datafind'+ subsuffix )
string_job.set_sub_file( basename + '.string' + subsuffix )
if do_inj: 
  injection_job.set_sub_file( basename + '.binj' + subsuffix ) 

# get chunk lengths from the values in the ini file
short_seg_lenth = int(cp.get('string', 'short-segment-duration'))
pad = int(cp.get('string', 'pad'))
length = int(cp.get('pipeline', 'segment-length'))+2*pad
trig_overlap = int(cp.get('pipeline', 'trig-overlap'))
overlap = 2*(short_seg_lenth/4+pad)

# read science segs that are greater or equal to a chunk from the input file
data = pipeline.ScienceData()
data.read(cp.get('input','segments'),length)

# create the chunks from the science segments
data.make_chunks(length,overlap+trig_overlap,playground,0,0)
#make_chunks(self, length, overlap=0, play=0, sl=0, excl_play=0)
data.make_chunks_from_unused(length,overlap/2+trig_overlap,playground,0,0,0)
#make_chunks_from_unused(self, length, trig_overlap, play=0, min_length=0, sl=0, excl_play=0)

# get the order of the ifos to filter
ifo1 = cp.get('pipeline','ifo1')
ifo2 = cp.get('pipeline','ifo2')
ifo3 = cp.get('pipeline','ifo3')
snrH1 = cp.get('pipeline','thresholdH1')
snrH2 = cp.get('pipeline','thresholdH2')
snrL1 = cp.get('pipeline','thresholdL1')

datatype1 = cp.get('input','type1') 
datatype2 = cp.get('input','type2') 
datatype3 = cp.get('input','type3')

# create all the LSCdataFind jobs to run in sequence
prev_df1 = None
prev_df2 = None
prev_df3 = None
prev_inject = None

segment_number = -1
for seg in data:

  segment_number=segment_number+1


  # find all the data for first ifo
  df1 = pipeline.LSCDataFindNode(df_job)
  df1.set_start(seg.start())
  df1.set_end(seg.end())
  df1.set_observatory(ifo1[0])
  df1.set_type(datatype1)
  df1.set_name("df_"+ifo1+"_"+str(segment_number))
  if do_datafind and prev_df1: 
    df1.add_parent(prev_df1)
  if do_datafind:
    dag.add_node(df1)
  prev_df1 = df1

  # find data for second ifo
  df2 = pipeline.LSCDataFindNode(df_job)
  df2.set_start(seg.start())
  df2.set_end(seg.end())
  df2.set_observatory(ifo2[0])
  df2.set_type(datatype2)
  df2.set_name("df_"+ifo2+"_"+str(segment_number))
  if do_datafind and prev_df2: 
    df2.add_parent(prev_df2)
  if do_datafind:
    dag.add_node(df2)
  prev_df2 = df2

  # find data for third ifo
  df3 = pipeline.LSCDataFindNode(df_job)
  df3.set_start(seg.start())
  df3.set_end(seg.end())
  df3.set_observatory(ifo3[0])
  df3.set_type(datatype3)
  df3.set_name("df_"+ifo3+"_"+str(segment_number))
  if do_datafind and prev_df3: 
    df3.add_parent(prev_df3)
  if do_datafind:
    dag.add_node(df3)
  prev_df3 = df3

  if do_inj:
    # create injection jobs
    inject1  = cosmicstring.InjNode(injection_job)
    injstart=int(seg.start())+overlap
    injend=int(seg.end())-overlap
    inject1.set_start(injstart)
    inject1.set_end(injend)
    seed = int(1e6 * random.uniform(0.0,1.0))
    inject1.add_var_opt('seed',seed)
    inject1.set_name("inj_"+str(segment_number))
    if prev_inject: 
      inject1.add_parent(prev_inject)
    dag.add_node(inject1)
    prev_inject = inject1

  chunk_number=-1    
  for chunk in seg:
    
    chunk_number=chunk_number+1    

    #string job for first ifo
    string1 = cosmicstring.StringNode(string_job)
    string1.set_start(chunk.start())
    string1.set_end(chunk.end())
    string1.set_ifo(ifo1)
    string1.set_cache(df1.get_output())
    string1.add_var_opt('threshold',snrH1)
    string1.add_var_opt('trig-start-time',chunk.trig_start())
    string1.add_var_opt('outfile',string1.get_output())
    if do_datafind:
      string1.add_parent(df1)
    string1.set_name("string_"+ifo1+"_"+str(segment_number)+"_"+str(chunk_number))
    if do_inj:
      string1.add_parent(inject1)
      string1.add_var_opt('injection-file',inject1.get_output())
    if do_burstsearch:
      dag.add_node(string1)

    #string job for second ifo  
    string2 = cosmicstring.StringNode(string_job)
    string2.set_start(chunk.start())
    string2.set_end(chunk.end())
    string2.set_ifo(ifo2)
    string2.set_cache(df2.get_output())
    string2.add_var_opt('threshold',snrL1)
    string2.add_var_opt('trig-start-time',chunk.trig_start())
    string2.add_var_opt('outfile',string2.get_output())
    if do_datafind:
      string2.add_parent(df2)
    string2.set_name("string_"+ifo2+"_"+str(segment_number)+"_"+str(chunk_number))
    if do_inj:
      string2.add_parent(inject1)
      string2.add_var_opt('injection-file',inject1.get_output())
    if do_burstsearch:
      dag.add_node(string2)

    #string job for second ifo  
    string3 = cosmicstring.StringNode(string_job)
    string3.set_start(chunk.start())
    string3.set_end(chunk.end())
    string3.set_ifo(ifo3)
    string3.set_cache(df3.get_output())
    string3.add_var_opt('threshold',snrH2)
    string3.add_var_opt('trig-start-time',chunk.trig_start())
    string3.add_var_opt('outfile',string3.get_output())
    if do_datafind:
      string3.add_parent(df3)
    string3.set_name("string_"+ifo3+"_"+str(segment_number)+"_"+str(chunk_number))
    if do_inj:
      string3.add_parent(inject1)
      string3.add_var_opt('injection-file',inject1.get_output())
    if do_burstsearch:
      dag.add_node(string3)

# write out the DAG
dag.write_sub_files()
dag.write_dag()

# write a message telling the user that the DAG has been written
print "\nCreated a DAG file which can be submitted by executing"
print "\n   condor_submit_dag -maxjobs 100 ", dag.get_dag_file()
print """\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
If you are running LSCdataFind jobs, do not forget to initialize your grid 
proxy certificate on the condor submit machine by running the commands

  unset X509_USER_PROXY
  grid-proxy-init -hours 177
  export LSC_DATAFIND_SERVER=dataserver.phys.uwm.edu

Contact the administrator of your cluster to find the hostname and port of the
LSCdataFind server.
"""

# write out a log file for this script
log_fh = open(basename + '.pipeline.log', 'w')
  
log_fh.write( "$Id$" + "\n\n" )
log_fh.write( "Invoked with arguments:\n" )
for o, a in opts:
  log_fh.write( o + ' ' + a + '\n' )
log_fh.write( "\n" )
log_fh.write( "Parsed " + str(len(data)) + " science segments\n" )
total_data = 0
for seg in data:
  for chunk in seg:
    total_data += len(chunk)
print >> log_fh, "total data =", total_data

print >> log_fh, "\n===========================================\n"
print >> log_fh, data
for seg in data:
  print >> log_fh, seg
  for chunk in seg:
    print >> log_fh, chunk

sys.exit(0)

