#!/usr/bin/env tclshexe
#
# File: buildStackSlideSimpleCondorDag.tclsh Started: 01/03/05
# Author: Greg Mendell
#

if { $argc < 20 } {
    puts " ";
    puts "Create Condor DAG that runs ComputeStackSlideSums to find Loudest Event and Monte Carlo Simulation in parallel as parent jobs; child job runs designated matlab script on xml output of parent jobs.\n";
    puts "Usage:";
    puts " ";
    puts " ";
    puts "$argv0 <base_name> <eff_or_ul> <ifo> <start_time> <duration> <start_freq> <full_band_width> <num_sub_bands> <sft_time> <num_sfts> <sft_path_pattern> <LEparam_file> <numLEjobs> <MCparam_file> <numMCjobs> <sun_edat_file> <earth_edat_file> <computeStackSlideSumsAndPath> <compiledMatlabScriptAndPath> <dagWorkingDir>";
    puts " ";
    puts "base_name:                    the base filename from which to create all .dag, .sub, .xml, and .txt filenames."
    puts "eff_or_ul:                    if > 0 then find ULs with this confidence (e.g., 0.95); else find efficiencies."
    puts "ifo:                          the IFO for which to create jobs.";
    puts "start_time:                   start time of first SFT to use.";
    puts "duration:                     duration of the search.";
    puts "start_freq:                   start frequency to run on.";
    puts "full_band_width:              full frequency band width to run on.";
    puts "num_sub_bands:                find the Efficiency or UL for this number of sub-bands";
    puts "sft_time:                     time baseline of the SFTs used.";
    puts "num_sfts:                     number of sfts used in job."
    puts "sft_path_pattern:             path to SFTs plus glob style pattern to match to find SFT files." 
    puts "LEparam_file:                 parameter file to use to set up job that finds the Loudest Event."
    puts "numLEjobs:                    number of parallel jobs to run to find the Loudest Event."
    puts "MCparam_file:                 parameter file to use to set up Monte Carlo simulation jobs."
    puts "numMCjobs:                    number of parallel jobs to run perform Monte Carlo simulation."
    puts "sun_edat_file:                path and name of sun ephemeris file to use.";
    puts "earth_edat_file:              path and name of earth ephemeris file to use.";
    puts "computeStackSlideSumsAndPath: path and name of ComputeStackSlideSums executable."
    puts "matlabScriptAndPath:          path and name of compiled Matlab script to run after parent jobs finish.";
    puts "dagWorkingDir:                the working directory from where all condor dag and submit files will run.";    
    puts " "; 
    return;
}

# Names of parameters used in the script
set base_name [lindex $argv 0];
set eff_or_ul [lindex $argv 1]
set ifo [lindex $argv 2];
set start_time [lindex $argv 3];
set duration [lindex $argv 4];
set start_freq [lindex $argv 5];
set full_band_width [lindex $argv 6];
set num_sub_bands [lindex $argv 7];
set sft_time [lindex $argv 8];
set num_sfts [lindex $argv 9];
set sft_path_pattern [lindex $argv 10];
set LEparam_file [lindex $argv 11];
set numLEjobs [lindex $argv 12];
set MCparam_file [lindex $argv 13];
set numMCjobs [lindex $argv 14];
set sun_edat_file [lindex $argv 15];
set earth_edat_file [lindex $argv 16];
set computeStackSlideSumsAndPath [lindex $argv 17];
set matlabScriptAndPath [lindex $argv 18];
set dagWorkingDir [lindex $argv 19];

# Create submit files for DAG:
# A  B
# \ /
#  C

# Write submit file for parent job A.
set submitFileParentA $base_name;
append submitFileParentA "_parentA.sub";
set pAFid [open $submitFileParentA "w"];
puts $pAFid "universe   = standard";
puts $pAFid "executable = $computeStackSlideSumsAndPath";
puts $pAFid "output     = parentA.out.\$(JobID).\$(subJobID)";
puts $pAFid "error      = parentAerr.\$(JobID).\$(subJobID)";
puts $pAFid "log        = parentA.log";
puts $pAFid "";
puts $pAFid "arguments = \$(argList)";
puts $pAFid "queue";
close $pAFid;

# Write submit file for parent job B.
set submitFileParentB $base_name;
append submitFileParentB "_parentB.sub";
set pBFid [open $submitFileParentB "w"];
puts $pBFid "universe   = standard";
puts $pBFid "executable = $computeStackSlideSumsAndPath";
puts $pBFid "output     = parentB.out.\$(JobID).\$(subJobID)";
puts $pBFid "error      = parentBerr.\$(JobID).\$(subJobID)";
puts $pBFid "log        = parentB.log";
puts $pBFid "";
puts $pBFid "arguments = \$(argList)";
puts $pBFid "queue";
close $pBFid;

# Create condor submit file for child job C that finds and outputs final result, e.g., estimated UL or Efficiency.
# Need to source the injected amplitude used in the Monte Carlo Simulation:
set submitFileChildC $base_name;
append submitFileChildC "_childC.sub";
source $MCparam_file;
set amplitudeMC $searchThreshold4; # MUST BE SET IN MCparam_file!
set matlabScriptPath [file dirname $matlabScriptAndPath ];
if {$eff_or_ul > 0.0} {
  set outputFileChildC "Confidence_";
} else {
  set outputFileChildC "Efficiency_";
}
append outputFileChildC $base_name;
set cFid [open $submitFileChildC "w"];
puts $cFid "universe   = vanilla";
puts $cFid "environment = PATH=\$(PATH):$matlabScriptPath";
puts $cFid "getenv     = True";
puts $cFid "executable = $matlabScriptAndPath";
puts $cFid "output     = childC.out.\$(JobID)";
puts $cFid "error      = childCerr.\$(JobID)";
puts $cFid "log        = childC.log";
puts $cFid "";
if {$eff_or_ul > 0.0} {
   puts $cFid "arguments  = $dagWorkingDir/\$(nameFileListA) $dagWorkingDir/\$(nameFileListB) $amplitudeMC $eff_or_ul $num_sfts 0 $outputFileChildC.\$(JobID) 0";
} else {
   puts $cFid "arguments  = $dagWorkingDir/\$(nameFileListA) $dagWorkingDir/\$(nameFileListB) $amplitudeMC 0 $outputFileChildC.\$(JobID) 0";
}
puts $cFid "queue";
close $cFid;

# Open condor dag file.
set dagFile $base_name;
append  dagFile "_submitAll.dag";
set dagFid [open $dagFile "w"];
puts $dagFid "# DAG for base_name = $base_name";

# find band_width for each job
set band_width [expr $full_band_width/$num_sub_bands]

# loop over jobs:
for {set j 0} {$j<$num_sub_bands} {incr j} {
   set freq [expr $start_freq + ($j*$band_width)];    
   # Create parent job A that finds Loudest Event.
   set outputParentA $base_name;        # Name of files with output from parent job A
   append outputParentA  "_outputA";
   set outputFileListA $base_name;      # file with list of files with output from parent job A
   append outputFileListA "_fileListA.txt.$j";
   set parentListA($j) "";                  # list of parent jobs that use submitFileParentA
   # Open the file with the list of output files for parent job A.
   set pListAFid [open $outputFileListA "w"];
   # Divide the band_width up and create jobs for each sub-band.
   set sub_band_width [expr $band_width/$numLEjobs];
   for {set i 0} {$i<$numLEjobs} {incr i} {
      set sub_start_freq [expr $freq + ($sub_band_width*$i)];
      set mid_freq [expr $sub_start_freq + ($sub_band_width/2.0)];
      set jobID_subJobID "$j";
      append jobID_subJobID "_";
      append jobID_subJobID "$i";
      set outputFileName "$outputParentA$jobID_subJobID";
      #set cmd "exec ./runComputeStackSlideSUM.tclsh $ifo $start_time $duration $mid_freq $sub_band_width $sft_time $num_sfts \"$sft_path_pattern\" $LEparam_file runStackSlideSearch.job $sun_edat_file $earth_edat_file $outputFileName condor >> $submitFileParentA";
      set cmd "exec ./runComputeStackSlideSUM.tclsh $ifo $start_time $duration $mid_freq $sub_band_width $sft_time $num_sfts \"$sft_path_pattern\" $LEparam_file runStackSlideSearch.job $sun_edat_file $earth_edat_file $outputFileName condor";
      #puts $cmd;
      set nodeName "A$jobID_subJobID";
      set argList [lindex [split [lindex [split [eval $cmd] "\n"] 1] "="] 1];
      puts $dagFid "JOB $nodeName $submitFileParentA";
      puts $dagFid "VARS $nodeName JobID=\"$j\" subJobID=\"$i\" argList=\"$argList\"";
      append parentListA($j) "PARENT $nodeName CHILD C$j\n";
      append outputFileName ".xml";
      puts $pListAFid "$dagWorkingDir/$outputFileName";
   } 
   close $pListAFid;
   puts $dagFid "";

   # Create parent job B that runs Monte Carlo Simulation
   set outputParentB $base_name;        # Name of files with output from parent job B
   append outputParentB  "_outputB";
   set outputFileListB $base_name;      # file with list of files with output from parent job B
   append outputFileListB "_fileListB.txt.$j";
   set parentListB($j) "";                  # list of parent jobs that use submitFileParentB
   # Open the file with the list of output files for parent job B.
   set pListBFid [open $outputFileListB "w"];
   # Divide the band_width up and create jobs for each sub-band.
   set sub_band_width [expr $band_width/$numMCjobs];
   for {set i 0} {$i<$numMCjobs} {incr i} {
      set sub_start_freq [expr $freq + ($sub_band_width*$i)];
      set mid_freq [expr $sub_start_freq + ($sub_band_width/2.0)];
      set jobID_subJobID "$j";
      append jobID_subJobID "_";
      append jobID_subJobID "$i";
      set outputFileName "$outputParentB$jobID_subJobID";
      #set cmd "exec ./runComputeStackSlideSUM.tclsh $ifo $start_time $duration $mid_freq $sub_band_width $sft_time $num_sfts \"$sft_path_pattern\" $MCparam_file runStackSlideSearch.job $sun_edat_file $earth_edat_file $outputFileName condor >> $submitFileParentB";
      set cmd "exec ./runComputeStackSlideSUM.tclsh $ifo $start_time $duration $mid_freq $sub_band_width $sft_time $num_sfts \"$sft_path_pattern\" $MCparam_file runStackSlideSearch.job $sun_edat_file $earth_edat_file $outputFileName condor";
      #puts $cmd;
      set nodeName "B$jobID_subJobID";
      set argList [lindex [split [lindex [split [eval $cmd] "\n"] 1] "="] 1];
      puts $dagFid "JOB $nodeName $submitFileParentB";
      puts $dagFid "VARS $nodeName JobID=\"$j\" subJobID=\"$i\" argList=\"$argList\"";
      append parentListB($j) "PARENT $nodeName CHILD C$j\n";
      append outputFileName ".xml";
      puts $pListBFid "$dagWorkingDir/$outputFileName";
  } 
  close $pListBFid;
  puts $dagFid "";

  # create child job C for parents A and B above.
  puts $dagFid "JOB C$j $submitFileChildC";
  puts $dagFid "VARS C$j JobID=\"$j\" nameFileListA=\"$outputFileListA\" nameFileListB=\"$outputFileListB\"";
  puts $dagFid "";
}
# END first loop over jobs: for {set j 0} {$j<$num_sub_bands} {incr j}

# write dependencies into condor dag file and close.
for {set j 0} {$j<$num_sub_bands} {incr j} {
   puts $dagFid $parentListA($j);
   puts $dagFid $parentListB($j);
}
# END second loop over jobs: for {set j 0} {$j<$num_sub_bands} {incr j}
close $dagFid;

exit 0;
