#!/usr/bin/env python
"""

MakeSFTDAG.py - Creates DAGs to run jobs that generates SFTs; can act as a dag generator for use with onasys.
Loosely based on lalapps_strain_pipe

$Id$

"""

__author__ = 'Greg Mendell<gmendell@ligo-wa.caltech.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]


# import standard modules and append the lalapps prefix to the python path
import sys, os
import getopt, re, string
import tempfile
#import ConfigParser
#sys.path.append('')

# import the modules we need to build the pipeline
#from glue import pipeline
#import strain

#
# USAGE FUNCTION
#
def usage():
  msg = """\
Usage: MakeSFTDAG [options]

  -h, --help                 display this message
  -s, --gps-start-time       (optional) GPS start time of data segment (unused except by onasys)
  -e, --gps-end-time         (optional) GPS end time of data segment (unused except by onasys)
  -a, --analysis-start-time  GPS start time of data from which to generate SFTs (optional and unused if a segment file is given)
  -b, --analysis-end-time    GPS end time of data from which to generate SFTs (optional and unused if a segment file is given)
  -f, --dag-file             filename for .dag file (excluding the .dag)
  -t, --aux-path             (optional) path to auxiliary data files (unused except by onasys)
  -G, --tag-string           tag string used in names of various files unique to jobs that will run under the DAG
  -d, --input-data-type      input data type for use with the LSCdataFind --type option
  -x, --extra-datafind-time  (optional) extra time to subtract/add from/to start/end time arguments of LSCdataFind (default is 0)
  -k  --filter-knee-freq     high pass filter knee frequency used on time domain data before generating SFTs
  -T  --time-baseline        time baseline of SFTs  (e.g., 60 or 1800 seconds)
  -p  --output-sft-path      path to output SFTs
  -C  --cache-path           (optional) path to cache files that will be produced by LSCdataFind (default is $PWD/cache; this directory is created if it does not exist and must agree with that given in .sub files)
  -O  --log-path             (optional) path to some log and error files (default is $PWD/logs; this directory is created if it does not exist and should agree with that given in .sub files)
  -N  --channel-name         name of input time-domain channel to read from frames
  -r  --sample-rate          (optional) sample rate of input time-domain channel (default is 16384; currently unused)
  -w  --window-type          (optional) type of windowing of time-domain to do before generating SFTs (1 = Tukey, 2 = Hann; default is 1; currently MakeSFT code defaults to option 1 regardless)
  -v, --sft-ver              (optional) sft version to output (1 or 2; default is 1; currently MakeSFT code defaults to option 1 regardless)
  -m, --max-num-per-node     maximum number of SFTs to generate on one node
  -L, --max-length-all-jobs  maximum total amount of data to process, in seconds (optional and unused if a segment file is given)
  -g, --segment-file         alternative file with segments to use, rather than the input times.
  -l, --min-seg-length       (optional) minimum length segments to process in seconds (used only if a segment file is given; default is 0)
  -H  --use-hot              input data is from h(t) calibrated frames (h of t = hot!) (0 or 1).
"""
  print >> sys.stderr, msg

#
# FUNCTION THAT WRITE ONE JOB TO DAG FILE
#
def writeToDag(dagFID, nodeCount, filterKneeFreq, timeBaseline, outputSFTPath, cachePath, startTimeThisNode, endTimeThisNode, channelName, site, inputDataType, extraDatafindTime, useHoT, tagString):
  LSCdataFind = 'LSCdataFind_%i' % nodeCount
  MakeSFTs    = 'MakeSFTs_%i' % nodeCount
  startTimeDatafind = startTimeThisNode - extraDatafindTime
  endTimeDatafind = endTimeThisNode + extraDatafindTime
  tagStringOut = '%s_%i' % (tagString, nodeCount)
  cacheFile   = '%s/%s-%s-%s.cache' % (cachePath, site,startTimeDatafind,endTimeDatafind)
  dagFID.write('JOB %s datafind.sub\n' % LSCdataFind)
  dagFID.write('RETRY %s 10\n' % LSCdataFind)
  dagFID.write('VARS %s gpsstarttime="%s" gpsendtime="%s" observatory="%s" inputdatatype="%s" tagstring="%s"\n' % (LSCdataFind, startTimeDatafind, endTimeDatafind, site, inputDataType, tagStringOut))
  dagFID.write('JOB %s MakeSFTs.sub\n' % MakeSFTs)
  dagFID.write('RETRY %s 1\n' % MakeSFTs)
  argList = '-f %s -t %s -p %s -C %s -s %s -e %s -N %s' %(filterKneeFreq, timeBaseline, outputSFTPath, cacheFile, startTimeThisNode, endTimeThisNode, channelName)
  if useHoT: argList = argList + ' -H'
  dagFID.write('VARS %s argList="%s" tagstring="%s"\n'%(MakeSFTs,argList, tagStringOut))
  dagFID.write('PARENT %s CHILD %s\n'%(LSCdataFind,MakeSFTs))
  
#
# MAIN CODE START HERE 
#

# parse the command line options
shortop = "s:e:a:b:f:t:G:d:x:k:T:p:C:O:N:r:w:v:m:L:g:l:hH"
longop = [
  "help",
  "gps-start-time=",
  "gps-end-time=",
  "analysis-start-time=",
  "analysis-end-time=",
  "dag-file=",
  "aux-path=",
  "tag-string=",
  "input-data-type=",
  "extra-datafind-time=",
  "filter-knee-freq=",
  "time-baseline=",
  "output-sft-path=",
  "cache-path=",
  "log-path=",
  "channel-name=",
  "sample-rate=",
  "window-type=",
  "sft-ver=",
  "max-num-per-node=",
  "max-length-all-jobs=",
  "segment-file=",
  "min-seg-length=",
  "use-hot",
  ]

try:
  opts, args = getopt.getopt(sys.argv[1:], shortop, longop)
except getopt.GetoptError:
  usage()
  sys.exit(1)

# initialize with default values or with None
gpsStartTime = None
gpsEndTime = None
analysisStartTime = None
analysisEndTime = None
dagFileName = None
auxPath = None
tagString = None
inputDataType = None
extraDatafindTime = 0L
filterKneeFreq = -1
timeBaseline = None
outputSFTPath = None
cachePath = "cache"
logPath = "logs"
channelName = None
sampleRate = 16384
windowType = 1
sftVersion = 1
maxNumPerNode = None
maxLengthAllJobs = None
segmentFile = None
minSegLength = 0L
useHoT = False

for o, a in opts:
  if o in ("-h", "--help"):
    usage()
    sys.exit(0)
  elif o in ("-s", "--gps-start-time"):
    gpsStartTime = long(a)
  elif o in ("-e", "--gps-end-time"):
    gpsEndTime = long(a)
  elif o in ("-a", "--analysis-start-time"):
    analysisStartTime = long(a)
  elif o in ("-b", "--analysis-end-time"):
    analysisEndTime = long(a)    
  elif o in ("-f", "--dag-file"):
    dagFileName = a   
  elif o in ("-t", "--aux-path"):
    auxPath = a
  elif o in ("-G", "--tag-string"):
    tagString = a
  elif o in ("-d", "--input-data-type"):
    inputDataType = a
  elif o in ("-x", "--extra-datafind-time"):
    extraDatafindTime = long(a)
  elif o in ("-k", "--filter-knee-freq"):
    filterKneeFreq = int(a)
  elif o in ("-T", "--time-baseline"):
    timeBaseline = long(a)
  elif o in ("-p", "--output-sft-path"):
    outputSFTPath = a
  elif o in ("-C", "--cache-path"):
    cachePath = a
  elif o in ("-O", "--log-path"):
    logPath = a
  elif o in ("-N", "--channel-name"):
    channelName = a
  elif o in ("-r", "--sample-rate"):
    sampleRate = int(a)
  elif o in ("-w", "--window-type"):
    windowType = int(a)
  elif o in ("-v", "--sft-ver"):
    sftVersion = int(a)
  elif o in ("-m", "--max-num-per-node"):
    maxNumPerNode = long(a)
  elif o in ("-L", "--max-length-all-jobs"):
    maxLengthAllJobs = long(a)
  elif o in ("-g", "--segment-file"):
    segmentFile = a
  elif o in ("-l", "--min-seg-length"):
    minSegLength = long(a)
  elif o in ("-H", "--use-hot"):
    useHoT = True
  else:
    print >> sys.stderr, "Unknown option:", o
    usage()
    sys.exit(1)

if not dagFileName:
  print >> sys.stderr, "No dag filename specified."
  print >> sys.stderr, "Use --help for usage details."
  sys.exit(1)

if not tagString:
  print >> sys.stderr, "No tag string specified."
  print >> sys.stderr, "Use --help for usage details."
  sys.exit(1)

if not inputDataType:
  print >> sys.stderr, "No input data type specified."
  print >> sys.stderr, "Use --help for usage details."
  sys.exit(1)

if extraDatafindTime < 0L:
  print >> sys.stderr, "Invalid extra datafind time specified."
  print >> sys.stderr, "Use --help for usage details."
  sys.exit(1)
  
if filterKneeFreq < 0:
  print >> sys.stderr, "No filter knee frequency specified."
  print >> sys.stderr, "Use --help for usage details."
  sys.exit(1)

if not timeBaseline:
  print >> sys.stderr, "No time baseline specified."
  print >> sys.stderr, "Use --help for usage details."
  sys.exit(1)

if not outputSFTPath:
  print >> sys.stderr, "No output SFT path specified."
  print >> sys.stderr, "Use --help for usage details."
  sys.exit(1)
  
if not cachePath:
  print >> sys.stderr, "No cache path specified."
  print >> sys.stderr, "Use --help for usage details."
  sys.exit(1)

if not logPath:
  print >> sys.stderr, "No log path specified."
  print >> sys.stderr, "Use --help for usage details."
  sys.exit(1)

if not channelName:
  print >> sys.stderr, "No channel name specified."
  print >> sys.stderr, "Use --help for usage details."
  sys.exit(1)

if sampleRate < 1:
  print >> sys.stderr, "No sample rate specified."
  print >> sys.stderr, "Use --help for usage details."
  sys.exit(1)
  
if (windowType != 1) and (windowType != 2):
  print >> sys.stderr, "Invalid window type specified."
  print >> sys.stderr, "Use --help for usage details."
  sys.exit(1)

if (sftVersion != 1) and (sftVersion != 2):
  print >> sys.stderr, "Invalid SFT version specified."
  print >> sys.stderr, "Use --help for usage details."
  sys.exit(1)

if not maxNumPerNode:
  print >> sys.stderr, "No maximum number of SFTs per node specified."
  print >> sys.stderr, "Use --help for usage details."
  sys.exit(1)

# try and make a directory to store the cache files and job logs
try: os.mkdir(logPath)
except: pass
try: os.mkdir(cachePath)
except: pass

# Check if segment file was given, else set up one segment from the command line
segList = [];
adjustSegExtraTime = False
if (segmentFile != None):

    if minSegLength < 0L:
      print >> sys.stderr, "Invalid minimum segment length specified."
      print >> sys.stderr, "Use --help for usage details."
      sys.exit(1)

    # the next flag causes extra time that cannot be processes to be trimmed from the start and end of a segment
    adjustSegExtraTime = True
    try:
         for line in open(segmentFile):
             try: 
                 splitLine = line.split();
                 try: 
                     oneSeg = [];
                     oneSeg.append(long(splitLine[0]));
                     oneSeg.append(long(splitLine[1]));
                     if ((oneSeg[1] - oneSeg[0]) >= minSegLength):
                         segList.append(oneSeg)
                     else:
                         pass
                 except:
                     pass
             except:
                 pass
         # End for line in open(segmentFile)
         if (len(segList) < 1):
             print >> sys.stderr, "No segments found in segment file: %s." % segmentFile
             sys.exit(1)
    except:
         print >> sys.stderr, "Error reading or parsing segment file: %s." % segmentFile
         sys.exit(1)
else:
    if not analysisStartTime:
      print >> sys.stderr, "No GPS analysis start time specified."
      print >> sys.stderr, "Use --help for usage details."
      sys.exit(1)

    if not analysisEndTime:
      print >> sys.stderr, "No GPS analysis end time specified."
      print >> sys.stderr, "Use --help for usage details."
      sys.exit(1)

    if not maxLengthAllJobs:
      print >> sys.stderr, "No maximum length of all jobs specified."
      print >> sys.stderr, "Use --help for usage details."
      sys.exit(1)

    # Make sure not to exceed maximum allow analysis
    if (analysisEndTime > analysisStartTime + maxLengthAllJobs):
       analysisEndTime = analysisStartTime + maxLengthAllJobs

    try:
        oneSeg = [];
        oneSeg.append(analysisStartTime);
        oneSeg.append(analysisEndTime);
        segList.append(oneSeg);
    except:
        print >> sys.stderr, "There was a problem setting up the segment to run on: [%s, %s)." % (analysisStartTime,analysisEndTime)
        sys.exit(1)
    
# END if (segmentFile != None)

# Get the IFO site, which is the first letter of the channel name.
site = channelName[0]

# initialize count of nodes
nodeCount         = 0L

# create the DAG file with the jobs to run
dagFileName = dagFileName + '.dag'
dagFID = file(dagFileName,'w')

startTimeAllNodes = None
for seg in segList:
    # Each segment in the segList runs on one or more nodes; initialize the number SFTs produced by the current node:
    numThisNode = 0L
    if (adjustSegExtraTime):
       segStartTime = seg[0]
       segEndTime = seg[1]
       segExtraTime = (segEndTime - segStartTime) % timeBaseline
       segExtraStart =  long(segExtraTime / 2)
       segExtraEnd = segExtraTime - segExtraStart
       #print segStartTime,segEndTime, segExtraTime, segExtraStart, segExtraEnd
       analysisStartTime = segStartTime + segExtraStart
       if analysisStartTime > segEndTime: analysisStartTime = segEndTime
       analysisEndTime = segEndTime - segExtraEnd
       if analysisEndTime < segStartTime: analysisEndTime = segStartTime
    else:
       analysisStartTime = seg[0]
       analysisEndTime = seg[1]     
    #print analysisStartTime, analysisEndTime
    # Loop through the analysis time; make sure no more than maxNumPerNode SFTs are produced by any one node
    startTimeThisNode = analysisStartTime
    endTimeThisNode   = analysisStartTime
    endTimeAllNodes   = analysisStartTime
    while (endTimeAllNodes < analysisEndTime):
         # increment endTimeAllNodes by the timeBaseline until we get past the analysisEndTime
         endTimeAllNodes = endTimeAllNodes + timeBaseline
         if (endTimeAllNodes <= analysisEndTime):
            # increment the number of SFTs output from this node, and update the end time this node.
            numThisNode = numThisNode + 1
            endTimeThisNode = endTimeAllNodes
            if (numThisNode < maxNumPerNode):
               continue
            else:
               # write jobs to dag for this node
               nodeCount = nodeCount + 1
               if (nodeCount == 1): startTimeAllNodes = startTimeThisNode
               writeToDag(dagFID,nodeCount, filterKneeFreq, timeBaseline, outputSFTPath, cachePath, startTimeThisNode, endTimeThisNode, channelName, site, inputDataType, extraDatafindTime, useHoT, tagString)
               # Update for next node
               startTimeThisNode = endTimeThisNode
               numThisNode       = 0L
    else:
         # we are at or past the analysisEndTime; output job for last node if needed.
         if (numThisNode > 0L):
            # write jobs to dag for this node
            nodeCount = nodeCount + 1
            if (nodeCount == 1): startTimeAllNodes = startTimeThisNode
            writeToDag(dagFID,nodeCount, filterKneeFreq, timeBaseline, outputSFTPath, cachePath, startTimeThisNode, endTimeThisNode, channelName, site, inputDataType, extraDatafindTime, useHoT, tagString)
    # END while (endTimeAllNodes < analysisEndTime)
# END for seg in segList

# Close the DAG file
dagFID.close

# Update actual end time of the last job and print out the times all jobs will run on:
endTimeAllNodes = endTimeThisNode

if not startTimeAllNodes:
  print >> sys.stderr, "The startTimeAllNodes == none; the DAG file contains no jobs!"
  sys.exit(1)

if (endTimeAllNodes <= startTimeAllNodes):
  print >> sys.stderr, "The endTimeAllNodes <= startTimeAllNodes; the DAG file contains no jobs!"
  sys.exit(1)

print >> sys.stdout, startTimeAllNodes, endTimeAllNodes

sys.exit(0)
