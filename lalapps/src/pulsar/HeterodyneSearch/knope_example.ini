# Example configuration file for the full known pulsar search pipeline

; general inputs for the whole analysis
[analysis]
# a list of the inteferometers to analyse (REQUIRED)
ifos = ['H1', 'L1']

# a GPS start time (if a single value is set then this is used for all detectors), or a dictionary of GPS start times for the analysis; one for each detector (e.g. {'H1': 1129136415, 'L1': 1129137415})
starttime = 1129136415

# a GPS end time (if a single value is set then this is used for all detectors), or a dictionary of GPS end times for the analysis; one for each detector (e.g. {'H1': 1129136415, 'L1': 1129137415})
endtime = 1129284015

# choose whether to use lalapps_heterodyne_pulsar (heterodyne) or lalapps_SplInter (splinter) (value is case insensitive)
preprocessing_engine = heterodyne

# a flag to set if just wanting to do the data processing (e.g. heterodyne or splinter) and not parameter estimation
preprocessing_only = False

# a flag to set if just wanting to do the postprocessing (parameter estimation and webpage page creation)
postprocessing_only = False

# if just doing the postprocessing (i.e. 'postprocessing_only = True') a pickled version of the class used for the
# preprocessing (or entire previous run is required) to obtain information on the pre-processed data
preprocessed_pickle_object =

# flag to set whether to run the analysis for individual detectors only (default is to run on individual detectors AND coherently with all detectors)
incoherent_only = False

# flag to set whether to only run the coherent multi-detector analysis
coherent_only = False

# set the number of background odds ratio studies when doing parameter estimation
num_background = 0

# a list of multiplicative factors of the pulsar's rotation frequency to analyse (e.g. [1., 2.] for analysing both the 1f and 2f modes)
# This can be a list if either 1 or 2 values, but if it is a list of two values they can currently only [1., 2.] (or [2., 1.]),
# whereas if there is only one value it can take any positive number.
freq_factors = [2.0]

# the path for timing and solar system ephemeris file
ephem_path = /usr/share/lalpulsar

# the directory in which the DAGs are created and run from
run_dir = /home/matthew/analysis/dags

# the name of the DAG file to create (if not set this will be generated automatically)
dag_name =

# set this flag to automatically submit the Condor DAG created by the script
submit_dag = True

# this flag sets whether running in autonomous mode
# If running in autonomous mode then information on the current state of the run for each pulsar must be kept
# (e.g. a json file must be created for each pulsar containing the end time that has currently been run until,
# it will also contain the previously used segment list as cache file). If no file is found to exist for a
# particular pulsar, e.g. that pulsar has newly been added to the pulsar directory, then it will start from
# scratch for that pulsar (based on the start time in this file). The script (run e.g. with cron once per week)
# that uses this file for the automation should update the endtime on each run.
# A script will be required to produce the the cache files for each pulsar.
autonomous = True

# the initial start time of any autonomous run (as starttime will be updated each time the code is run)
autonomous_initial_start =

# a base directory (for each detector) for preprocessing outputs (structure for the parameter estimation is still required).
# Within each the following structure structure is assumed:
# -> base_dir/PSRname
#           |       '-> /segments.txt (science segments file for that pulsar)
#           |       '-> /data (directory for processed data files)
#           |               '-> /coarse (directory for coarse heterodyned data)
#           |               |         '-> /freqfactor (directory for heterodyne at freq factor times rotation frequency - if required)
#           |               '-> /fine (directory for fine heterodyned data)
#           |               |       '-> /freqfactor (directory for heterodyne at rotation frequency - if required)
#           |               '-> /splinter (directory for spectrally interpolated data)
#           |                           '-> /freqfactor (directory for heterodyne at rotation frequency - if required)
#           '-> /splinter (a directory to output files from splinter before moving them into the above structure)
#           '-> /cache.lcf (a general frame/SFT cache file used by all pulsar if their directory does not containing there own cache)
preprocessing_base_dir = {'H1': '/home/matthew/analysis/H1', 'L1': '/home/matthew/analysis/L1'}

# path to directory containing pulsar parameter (.par) files, or an individual parameter file
# (once the analysis script has been run at least once each .par file will have an associated file (.mod_parfilename) with
# it modification time - if the file is updated, and therefore has a different modification time, then
# the full analysis will be re-run for that pulsar)
pulsar_param_dir = /home/matthew/analysis/pulsar

# path to log files
log_dir = /home/matthew/analysis/log

# set to true if running on software/hardware injections
injections = False

# file to output a pickled version of the KnownPulsarPipelineDAG class
pickle_file = /home/matthew/analysis/run.p

# email address for job completion notication (if no email is given the no notications will be sent)
email = matthew.pitkin@ligo.org


; Condor information
[condor]
# Condor accounting group
accounting_group = ligo.prod.o1.cw.targeted.bayesian

# Condor accounting group user
accounting_group_user = matthew.pitkin

# the data find executable (e.g. /usr/bin/gw_data_find), or a dictionary of pre-found cache files (one entry for each detector) if not wanting to use gw_data_find e.g. {'H1': '/home/matthew/analysis/H1cache.txt'}
datafind = /usr/bin/gw_data_find


; inputs for running a data find job (NOTE: this can be used to search for frame if using the heterodyne, or SFTs if using spectral interpolation)
[datafind]

# a dictionary of frame types to be returned; one for each detector
type = {'H1': 'H1_HOFT_C00', 'L1': 'L1_HOFT_C00'}

# a string to match in the URL paths returned
match = localhost


; Placeholder for information about Max's pulsar database (this could be used instead of requiring local copies of .par files)
[pulsar_database]


; inputs for running a science segment finding job
[segmentfind]
# path to segment database query script, or a dictionary of pre-found segment files (one entry for each detector) if not wanting to use segment finding script e.g. {'H1': '/home/matthew/analysis/H1segments.txt'}
segfind = /usr/bin/ligolw_segment_query_dqsegdb

# path to ligolw_print
ligolw_print = /usr/bin/ligolw_print

# URL of segment database server
server = https://segments.ligo.org

# a dictionary of the required segment types
segmenttype = {'H1': 'H1:DMT-ANALYSIS_READY:1', 'L1': 'L1:DMT-ANALYSIS_READY:1'}


; inputs for running the lalapps_heterodyne_pulsar code
[heterodyne]
# condor universe
universe = vanilla

# path to lalapps_heterodyne_pulsar
heterodyne_exec = /usr/bin/lalapps_heterodyne_pulsar

# path to directory containing pulsar parameter file (.par) updates, or an individual parameter file
# pulsar_update_dir = /home/matthew/analysis/pulsar_update # DONT USE FINE HETERODYNE UPDATES - JUST REDO WHOLE HETERODYNE SO THAT THERE AREN'T MULTIPLE PAR FILES/HETERODYNE FILES

# low-pass filter (9th order Butterworth) knee frequency
filter_knee = 0.25

# the frame data sample rate (for the coarse heterodyne)
coarse_sample_rate = 16384

# the re-sampling rate for the coarse heterodyne
coarse_resample_rate = 1

# a dictionary of frame channel names; one for each detector
channels = {'H1': 'H1:GDS-CALIB_STRAIN', 'L1': 'L1:GDS-CALIB_STRAIN'}

# the fine heterodyne re-sampling rate (the sample rate is taken from coarse_resample_rate)
fine_resample_rate = 1/60

# the standard deviation threshold for removing outliers
stddev_thresh = 3.5

# set to output the coarse heterodyne data in binary files (if true then a binary input will be assumed for the fine heterodyne)
binary_output = True

# gzip the coarse output files rather than outputting as binary
gzip_coarse_output = False

# gzip the fine output files
gzip_fine_output = True


; inputs for running the lalapps_SplInter code
[splinter]
# condor universe
universe = vanilla

# path to SplInter executable
splinter_exec = /usr/bin/lalapps_SplInter

# list with the start and end frequency ranges for the SFTs
freq_range = [30., 2000.]

# the standard deviation threshold for removing outliers
stddev_thresh = 3.5

# the bandwidth (Hz) around the signal frequency to use in interpolation
bandwidth = 0.3 # this is the default value from the code

# minimum length (seconds) of science segments to use
min_seg_length = 1800 # this is the default value (half an hour) from the code


; inputs for running the parameter estimation code lalapps_pulsar_parameter_estimation_nested
[pe]
# condor universe
universe = vanilla

# path to the parameter estimation executable
pe_exec = /usr/bin/lalapps_pulsar_parameter_estimation_nested

# the base output directory for the nested samples (directories for each detector/joint analysis will be created)
pe_output_dir = /home/matthew/analyses/nested_samples

# a pre-made prior file for the analysis (if this is not set then the prior file will be generated for each source based in the par file and/or pre-processed data) - this has priority over any other options in this configuration file
premade_prior_file =

# a set of prior options in dictionary form e.g.
# prior_options = {'H0': {'priortype': 'uniform', 'ranges': [0., 1e-22]}, 'PHI0': {'priortype': 'uniform', 'ranges': [0., 3.14]}}
prior_options =

# if 'derive_amplitude_prior' is true (and the 'premade_prior_file' is not set) then amplitude priors will be derived
# from the heterodyned/spectrally interpolated data based on the either previous upper limits or those
# estimated from ASDs of previous runs.
# The 'amplitude_prior_type' can also be set to either 'fermidirac' (default) or 'uniform'. All other parameters
# will have their ranges taken from 'predefined_prior_options'.
derive_amplitude_prior = True

amplitude_prior_scale = 5

amplitude_prior_type = 'fermidirac'

amplitude_prior_model_type = 'waveform'

# a JSON file with pulsar name keys associated with previous posterior samples files for use as priors in current analysis
previous_posteriors_file = path_to_file_of_previous_posterior_files

# a JSON file with amplitude upper limits from previous runs
amplitude_prior_file = path_to_file_of_previous_upper_limits

# a file, or dictionary or files, containing paths to amplitude spectral density files
amplitude_prior_asds =

# a value, or dictionary of values, containing the observation times (in days) for use with the above ASD files
amplitude_prior_obstimes =

# go through the pulsar parameter file and use the errors to set Gaussian priors in the prior file (also create a correlation coefficient file for these, setting everything to be uncorrelated except the extremely highly correlated binary parameters)
use_parameter_errors = False

# the number of parallel runs for a given
n_runs = 5

# the number of live points for each run
n_live = 2048

# the number of MCMC samples for each nested sample update (if not set this will be automatically calculated)
n_mcmc =

# the number of MCMC samples for initial shuffling of the prior points (shouldn't be needed for pulsar code)
n_mcmc_initial =

# the tolerance (stopping criterion) for the runs
tolerance = 0.1

# a random seed for the RNG (if not set then this will default to it's standard method)
random_seed =

# flag to set whether running with non-GR parameterisation
non_gr = False

# flag to say whether to use the 'waveform' or 'source' parameterisation (defaults to 'waveform')
model_type = 'waveform'

# flag to set whether using a Gaussian likelihood, or the default Student's t-likelihood (if using Splinter for the pre-processing engine then a Gaussian likelihood will automatically be used, and override anything set here)
gaussian_like = False

# flag to set whether the model under consideration is a biaxial model
biaxial = False

; placeholder tags to remind me to implement marginalisation over phase and amplitude calibration uncertainties (for each detector independently)
; cal_marg_amp = {'H1': 10., 'L1': 12.} # 1-sigma percentage amplitude uncertainty
; cal_marg_phase = {'H1': 0.2, 'L1': 0.2} # 1-sigma phase uncertainty (rads)

# path to lalapps_nest2pos for nested sample -> posterior conversion
n2p_exec = /usr/bin/lalapps_nest2pos

# the base output directory for posteriors
n2p_output_dir = /home/matthew/analyses/posterior_samples

## information for background runs
# the number of live points for background analyses
n_live_background = 1024

# the number of of parallel runs for each
n_runs_background = 2

# the base output directory for the background nested samples (directories for each detector/joint analysis will be created)
pe_output_dir_background = /home/matthew/analyses/background/nested_samples

# the base output directory for the background posteriors
n2p_output_dir_background = /home/matthew/analyses/background/posterior_samples

# flag to set whether to clean (i.e. remove) all the nested sample files (keeping the posteriors)
clean_nest_samples = False

# set to true if wanting the output to use the l=m=2 gravitational wave phase as the initial phase, rather than the default rotational phase
use_gw_phase = False

# flags for use of Reduced Order Quadrature (ROQ)
# Any generated ROQ interpolant files will be placed in structure of the pe_output_dir
use_roq = False

# set the number of training sets for using the in reduced basis and interpolant generation
roq_ntraining = 2500

# set the maximum data chunk length for when using ROQ
roq_chunkmax = 1440

# set the tolerance to producing the ROQ bases
roq_tolerance = 5e-12

# set if wanting the bases produced over a uniform parameter space even for (phase) parameters with Gaussian priors
roq_uniform = False

; inputs for creating results pages
[results_page]
# condor universe
universe = local

# results page (Condor) log directory
log_dir = /usr1/matthew/logs

# results page creation executable
results_exec = /usr/bin/lalapps_knope_result_page.py

# results collation executable
collate_exec = /usr/bin/lalapps_knope_collate_results.py

# the output base web directory for the results
web_dir = /home/matthew/public_html/results

# the equivalent output base URL for the above path
base_url = https://ldas-grid.ligo-wa.caltech.edu/~matthew/results

# the upper limit credible interval to use (default to 95%)
upper_limit = 95

# value on which to sort the results table
sort_value = name

# direction on which to sort the results table
sort_direction = ascending

# list upper limits to show in the results table
results = ['h0ul', 'ell', 'sdrat', 'q22', 'bsn']

# list of source values to output
parameters = ['f0rot', 'f1rot', 'ra', 'dec', 'dist', 'sdlim']

# set whether to show posterior plots for all parameters
show_all_posteriors = False

# set whether to subtract the true/heterodyned value from any phase parameters in a search for plotting
subtract_truths = False

# set whether to show the priors on the 1D posterior plots
show_priors = True