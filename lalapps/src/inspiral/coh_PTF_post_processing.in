#!/usr/bin/env @PYTHONPROG@
"""
coh_PTF_post_processing.in - post processing driver script for coh_PTF externally triggered pipeline

$Id$

This script generated the condor DAG necessary to run the post processing for the coh_PTF GRB pipeline.
"""

__author__ = 'Duncan Macleod <duncan.macleod@ligo.org>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

# =============================================================================
# import standard modules and append the lalapps prefix to the python path
# =============================================================================

from __future__ import division

import os,sys,ConfigParser,glob,shutil
from glue import pipeline,lal
from optparse import OptionParser

sys.path.append('@PYTHONLIBDIR@')

# =============================================================================
# Initialise subDAG
# =============================================================================

def init_subDAG(tag, outdir, logdir, cp):

  # generate job
  dag  = pipeline.CondorDAG('%s/%s.log' % (logdir,tag))
  dag.set_dag_file(os.path.join(outdir,tag))
  # set max jobs args
  if cp.has_option('condor-max-jobs', tag):
    dag.add_maxjobs_category(tag, cp.getint('condor-max-jobs', tag))

  return dag

# =============================================================================
# Write arguments for CondorJobs
# =============================================================================

def init_job(exe, universe, tag, subdir, logdir, cp, memory=None):

  logtag = '$(cluster)-$(process)'

  subdir = os.path.abspath(subdir)
  logdir = os.path.abspath(logdir)
  nfslogdir = '%s/logs' % subdir
  if not os.path.isdir(nfslogdir):
    os.mkdir(nfslogdir)

  job = pipeline.CondorDAGJob(universe,exe)
  job.set_sub_file(os.path.join(subdir,'%s.sub' % (tag)))
  job.set_log_file(os.path.join(logdir,'%s-%s.log' % (tag,logtag)))
  job.set_stderr_file(os.path.join(nfslogdir,'%s-%s.err' % (tag,logtag)))
  job.set_stdout_file(os.path.join(nfslogdir,'%s-%s.out' % (tag,logtag)))
  job.add_condor_cmd('getenv','True')

  if cp.has_section(tag):
    job.add_ini_opts(cp, tag)
  if memory:
    job.add_condor_cmd('requirements', 'memory > %s' % memory)

  return job

# =============================================================================
# Write arguments for trig_combiner job
# =============================================================================

def trig_combiner_setup(job, category, ifotag, usertag, grb, onoffcache,\
                        grbdir, numtrials, outdir):

  # setup node
  node = pipeline.CondorDAGNode(job)
  node.set_category(category)

  node.add_var_opt('ifo-tag', ifotag)
  node.add_var_opt('user-tag', usertag)
  node.add_var_opt('grb-name', grb)
  node.add_var_opt('segment-dir', os.path.abspath(grbdir))
  node.add_var_opt('cache', os.path.abspath(onoffcache))
  node.add_var_opt('num-trials', numtrials)
  node.add_var_opt('output-dir', os.path.abspath(outdir))

  return node

# =============================================================================
# Write arguments for trig_cluster job
# =============================================================================

def trig_cluster_setup(job, category, trigfile, outdir):

  node = pipeline.CondorDAGNode(job)
  node.set_category(category)
  node.add_var_opt('trig-file', trigfile)
  node.add_var_opt('output-dir', os.path.abspath(outdir))


  return node

# =============================================================================
# Write arguments for injfind job
# =============================================================================

def injfind_setup(job, category, injdir, injrun, ifotag, grb,\
                  datastart, dataduration):

  # construct cache file
  injcachefile = '%s/HL-INJECTION_GRB%s_%s-%s-%s.cache'\
                 % (injdir, grb, injrun, datastart, dataduration)
  # if cache does not exist, make one
  if not os.path.isfile(injcachefile):
    injfiles = glob.glob('%s/HL-INJECTION_GRB%s_%s_*-%s-%s.xml'\
                         % (injdir, grb, injrun, datastart, dataduration))
    injcache = lal.Cache.from_urls(injfiles)
    injcache.tofile(open(injcachefile, 'w'))

  # construct trigger cache file
  trigcachefile = '%s/%s-INSPIRAL_HIPE_GRB%s_%s-%s-%s.cache'\
                  % (injdir, ifotag, grb, injrun, datastart, dataduration)

  # initialise node
  node = pipeline.CondorDAGNode(job)
  node.set_category(category)
  node.add_var_opt('cache', trigcachefile)
  node.add_var_opt('inj-cache', injcachefile)

  return node

# =============================================================================
# write sbv_plotter args
# =============================================================================

def sbv_setup(job, category, trigfile, grb, outdir, grbdir, injfile=None):

  node = pipeline.CondorDAGNode(job)
  node.set_category(category)
  node.add_var_opt('trig-file', trigfile)
  node.add_var_opt('grb-name', grb)
  node.add_var_opt('segment-dir', grbdir)
  node.add_var_opt('output-path', os.path.abspath(outdir))
  if injfile:
    node.add_var_arg('--inj-file %s ' % injfile)

  return node

# =============================================================================
# write efficiency args
# =============================================================================

def onoff_efficiency_setup(job, category, outdir, segdir, offsource, onsource):

  node = pipeline.CondorDAGNode(job)
  node.set_category(category)
  node.add_var_opt('output-path', outdir)
  node.add_var_opt('offsource-file', offsource)
  node.add_var_opt('onsource-file', onsource)
  node.add_var_opt('segment-dir', segdir)

  return node

def injection_efficiency_setup(job, category, outdir, segdir, offsource,\
                               onsource, injrun, cp, found, missed): 

  node = pipeline.CondorDAGNode(job)
  node.set_category(category)
  node.add_var_opt('output-path', outdir)
  node.add_var_opt('upper-inj-dist', cp.getfloat(injrun, 'max-distance'))
  node.add_var_opt('lower-inj-dist', cp.getfloat(injrun, 'min-distance'))
  node.add_var_opt('offsource-file', offsource)
  node.add_var_opt('onsource-file', onsource)
  node.add_var_opt('found-file', found)
  node.add_var_opt('missed-file', missed)
  node.add_var_opt('segment-dir', segdir)

  return node

# =============================================================================
# Finalise DAG object
# =============================================================================

def finalise_DAG(dag, parents=[]):

  dag.write_sub_files()
  dag.write_dag()
  dagfile    = os.path.split(dag.get_dag_file())
  DAGManJob  = pipeline.CondorDAGManJob(dagfile[1],dagfile[0])
  DAGManNode = pipeline.CondorDAGManNode(DAGManJob)
  for node in parents:
    DAGManNode.add_parent(node)

  return DAGManNode

# =============================================================================
# parse command line
# =============================================================================

def parse_command_line():

  usage = """usage: %prog [options]

lalapps_coh_PTF_post_processing will set up a DAG to run post processing for the coh_PTF triggered pipeline end to end. It will set up five sub DAGs for each of the following jobs:

1) trig_combiner: combines triggers from onoff coh_PTF_inspiral jobs and
   separates into 'ONSOURCE' 'OFF_TRIAL_X' etc

2) trig_cluster: clusters each of the files from trig_combiner

3) injfinder: tests found/missed status of all injections

4) sbv_plotter: calculates signal-based-vetoes and final detection statistic,
   and plots a whole bunch of stuff

5) efficiency: calculates inverse false-alarm rate (IFAR) for all injections
   and detection candidate events, and calculates search 90% efficiency
   distances. This is split into two submit files, separating onoff jobs and
   injection jobs.

The default is to run the full post processing pipeline, various steps can be skipp by specifying the --skip options. The requires arguments are:

--run-dir
--grb-name
--config-file
"""

  parser = OptionParser(usage=usage, version= "%prog CVS\n$Id$\n$Name$\n")

  parser.add_option( "--verbose", action="store_true", default=False,\
                    help="verbose output")

  parser.add_option("-r", "--run-dir", type="string", action="store",\
                    default=None,\
                    help="run directory, parent of the GRBXXXXXX directory")

  parser.add_option("-o", "--output-dir", type="string", action="store",\
                    default=os.getcwd(),\
                    help="output directory for post processing, "+\
                         "default: %default")

  parser.add_option("-i", "--ifo-tag", type="string", action="store",\
                    default="H1L1V1", help="IFO tag, default: %default")

  parser.add_option("-n", "--grb-name", type="string", action="store",\
                    default=None, help="GRB identifier, e.g. 090802")

  parser.add_option("-f", "--config-file", type="string", action="store",\
                    default=None, help="post processing inifile for analsis")

  parser.add_option("-a", "--inj-config-file", type="string", action="store",\
                    default=None, help="injection inifile for analysis")

  parser.add_option("-p", "--log-path", action="store", type="string",\
                    default=None, help="directory to write condor log file")

  parser.add_option("-T", "--skip-trig-combiner", action="store_true",\
                    default=False,\
                    help="skip trip combiner, default: %default")

  parser.add_option("-C", "--skip-clustering", action="store_true",\
                    default=False, help="skip clustering, default: %default")

  parser.add_option("-I", "--skip-injfind", action="store_true",\
                    default=False,\
                    help="skip injection finding, default: %default")

  parser.add_option("-S", "--skip-sbv-plotter", action="store_true",\
                    default=False, help="skip SBV plotter, default: %default")

  parser.add_option("-E", "--skip-efficiency", action="store_true",\
                    default=False, help="skip efficiency, default: %default") 

  (opts, args) = parser.parse_args()

  if not opts.run_dir:
    parser.error('Must give --run-dir')

  if not opts.grb_name:
    parser.error('Must give --grb-name') 

  if not opts.config_file:
    parser.error('Must give --config-file')

  #if not opts.inj_config_file:
  #  parser.error('Must give --inj-config-file')

  return opts, args

# =============================================================================
# Main function
# =============================================================================

def main(rundir, outdir, ifotag, grb, inifile, injfile, verbose=False,\
         logdir=None, run_combiner=True, run_clustering=True, run_injfind=True,\
         run_sbvplotter=True, run_efficiency=True):

  # load ini files
  if verbose:
    print >>sys.stdout
    print >>sys.stdout, 'Initialising post processing driver, '+\
                        'loading configuration files...'

  # get directory
  grbdir = os.path.abspath('%s/GRB%s' % (rundir, grb))
  if not os.path.isdir(grbdir):
    raise ValueError, 'Cannot find directory GRB%s in %s' % (grb, rundir)

  # generate post processing directory
  if not os.path.isdir(outdir):
    os.makedirs(outdir)

  os.chdir(outdir)
  plotdir = 'output'
  exedir = 'executables'
  if not os.path.isdir(exedir):
    os.mkdir(exedir)

  if not logdir:
    logdir = '%s/%s' % (outdir, 'logs')
  if not os.path.isdir(logdir):
    os.makedirs(logdir)

  # load ini file
  cp = ConfigParser.ConfigParser()
  cp.optionxform = str
  cp.read(inifile)

  # load inj file
  if injfile:
    injcp = ConfigParser.ConfigParser()
    injcp.optionxform = str
    injcp.read(injfile)

    # find injection runs
    injruns = injcp.sections()
  else:
    injruns = []

  usertag = cp.get('input', 'user-tag')

  # =========
  # get times
  # =========

  # get times from datafind cache
  datafindstr = '%s/datafind/%s-INSPIRAL_HIPE_GRB%s_DATAFIND-*-*.cache'\
                % (grbdir, ifotag, grb)
  datafindglob = glob.glob(datafindstr)
  if len(datafindglob)!=1:
    raise ValueError, 'Cannot find single datafind cache matching %s' % datafindstr
  datafindcache = datafindglob[0]

  datastart, dataduration = map(int, os.path.splitext(datafindcache)[0]\
                                          .split('-')[-2:])
  
  pad = cp.getint('data', 'pad-data') 
  start = datastart+pad
  duration = dataduration-2*pad

  # ================
  # find onoff cache
  # ================

  onoffcache = '%s/onoff/' % grbdir
  onoffcache += '%s-INSPIRAL_HIPE_GRB%s_ZERO_LAG_CATEGORY_1-%s-%s.cache'\
                % (ifotag, grb, datastart, dataduration)

  # ==============
  # set parameters
  # ==============

  universe = cp.get('condor', 'universe')

  for (job, executable) in cp.items('condor'):
    if job=='universe':  continue
    # replace tilde in executable
    executable = os.path.expanduser(executable)
    # replace environment variables in executable
    executable = os.path.expandvars(executable)
    # copy executable to exedir
    executable2 = os.path.join(outdir, exedir, os.path.basename(executable))
    if not os.path.isfile(executable2) or\
       not os.path.samefile(executable, executable2):
      shutil.copy(executable, executable2)
    cp.set('condor', job, executable2)

  # ==========
  # write dags
  # ==========

  if verbose:
    print >>sys.stdout
    print >>sys.stdout, "Generating dag..."

  # initialise uberdag
  dagtag  = os.path.splitext(os.path.basename(inifile))[0]
  uberdag = pipeline.CondorDAG("%s/%s_uberdag.log" % (logdir,dagtag))
  uberdag.set_dag_file('%s_uberdag' % (dagtag))

  DAGManNode = {}

  # ==================
  # generate time tags
  # ==================

  numtrials = cp.getint('input', 'num-trials')
  timetags = [ 'ALL_TIMES', 'ONSOURCE', 'OFFSOURCE' ] +\
             [ 'OFFTRIAL_%d' % t for t in xrange(1,numtrials+1) ]

  minmemory = 0
  if cp.has_option('pipeline', 'minimum-memory'):
    minmemory = cp.getfloat('pipeline', 'minimum-memory')

  # =============
  # trig_combiner
  # =============

  if run_combiner:

    tag    = 'trig_combiner'
    exe    = cp.get('condor', tag)
  
    # generate job
    dag = init_subDAG(tag, outdir, logdir, cp)
    job = init_job(exe, universe, tag, outdir, logdir, cp, minmemory)

    # setup single node
    node = trig_combiner_setup(job, tag, ifotag, usertag, grb, onoffcache,\
                               grbdir, numtrials, outdir)
    dag.add_node(node)
  
    # finalise DAG
    DAGManNode[tag] = finalise_DAG(dag)
    uberdag.add_node(DAGManNode[tag])
  
  # ============
  # trig_cluster
  # ============

  trigfile = {}
  clstfile = {}
  for timetype in timetags:
    trigfile[timetype] = '%s-%s_GRB%s_%s-%s-%s.xml.gz'\
                         % (ifotag, usertag, grb, timetype, start, duration)
    clstfile[timetype] = trigfile[timetype].replace(timetype,\
                                                     '%s_CLUSTERED' % timetype)

  if run_clustering:

    tag = 'trig_cluster'
    exe    = cp.get('condor', tag)
  
    # generate job
    dag = init_subDAG(tag, outdir, logdir, cp)
    job = init_job(exe, universe, tag, outdir, logdir, cp, minmemory)

    # loop time tags
    for timetype in timetags:
      node = trig_cluster_setup(job, tag, trigfile[timetype], outdir)
      dag.add_node(node)

    # finalise DAG
    parents = []
    if run_combiner: parents.append(DAGManNode['trig_combiner'])
    DAGManNode[tag] = finalise_DAG(dag, parents)
    uberdag.add_node(DAGManNode[tag])

  # =========
  # injfinder
  # =========

  if run_injfind:

    # find buffer segments
    buffseg = '%s/%s' % (grbdir, 'bufferSeg.txt')
    if not os.path.isfile(buffseg):
      raise ValueError, 'Cannot find buffer segment file as %s' % buffseg
  
    tag = 'injfinder'
    exe = cp.get('condor', tag)
  
    # generate job
    dag = init_subDAG(tag, outdir, logdir, cp)
    job = init_job(exe, universe, tag, outdir, logdir, cp, minmemory)

    # construct arguments
    job.add_opt('output-dir', outdir)
    job.add_opt('exclude-segments', buffseg)

    for injrun in injruns:
      node = injfind_setup(job, tag, '%s/%s' %(grbdir,injrun), injrun,\
                           ifotag, grb, datastart, dataduration)
      dag.add_node(node)

    # finalise DAG
    parents = []
    DAGManNode[tag] = finalise_DAG(dag, parents)
    uberdag.add_node(DAGManNode[tag])
    
  # ===========
  # sbv_plotter
  # ===========

  if run_sbvplotter:

    tag = 'sbv_plotter'
    exe = cp.get('condor', tag)

    # generate job
    dag = init_subDAG(tag, outdir, logdir, cp)
    job = init_job(exe, universe, tag, outdir, logdir, cp, minmemory)

    for timetype in [ 'ALL_TIMES', 'OFFSOURCE' ]:
      # setup SBV plotting job
      sbvoutpath = '%s/%s/%s/plots' % (outdir, plotdir, timetype)
      node = sbv_setup(job, tag, trigfile[timetype], grb, sbvoutpath, grbdir)
      dag.add_node(node)
      # setup SBV clustered plotting job
      sbvoutpath = '%s/%s/%s/plots_clustered' % (outdir, plotdir, timetype)
      node = sbv_setup(job, tag, clstfile[timetype], grb, sbvoutpath, grbdir)
      dag.add_node(node)

    for injrun in injruns:
      # setup SBV injection plots
      injfile = '%s/%s-INJECTION_GRB%s_%s_FOUND-%d-%d.xml'\
                % (outdir, ifotag, grb, injrun, start, duration)
      sbvoutpath = '%s/%s/%s/plots' % (outdir, plotdir, injrun)
      node = sbv_setup(job, tag, trigfile[timetype], grb, sbvoutpath, grbdir,\
                       injfile)
      dag.add_node(node)
      # setup SBV clusterd injection plots
      sbvoutpath = '%s/%s/%s/plots_clustered' % (outdir, plotdir, injrun)
      node = sbv_setup(job, tag, clstfile[timetype], grb, sbvoutpath, grbdir,\
                       injfile)
      dag.add_node(node)

    # finalise DAG
    parents = []
    if run_clustering: parents.append(DAGManNode['trig_cluster'])
    if run_injfind:    parents.append(DAGManNode['injfinder'])
    DAGManNode[tag] = finalise_DAG(dag, parents)
    uberdag.add_node(DAGManNode[tag])
 
  # ==========
  # efficiency
  # ==========

  if run_efficiency:

    tag = 'efficiency'
    exe = cp.get('condor', tag)

    # generate job
    dag = init_subDAG(tag, outdir, logdir, cp)
    job = init_job(exe, universe, tag, outdir, logdir, cp, minmemory)

    # construct arguments
    job.add_opt('segment-dir', grbdir)

    for timetype in timetags:
      if timetype in [ 'OFFSOURCE', 'ALL_TIMES' ]:  continue
      # setup onoff efficiency jobs
      effoutdir = '%s/%s/%s/efficiency' % (outdir, plotdir, timetype)
      node = onoff_efficiency_setup(job, tag, effoutdir, grbdir,\
                                    clstfile['OFFSOURCE'], clstfile[timetype])
      dag.add_node(node)

    # define new job for inejction efficiency
    tag = 'injection-efficiency'
    job = init_job(exe, universe, tag, outdir, logdir, cp, minmemory)

    for timetype in timetags:
      if timetype in [ 'OFFSOURCE', 'ALL_TIMES' ]:  continue

      for injrun in injruns:
        # setup injection efficiency jobs
        found = '%s/%s-INJECTION_GRB%s_%s_FOUND-%d-%d.xml'\
                % (outdir, ifotag, grb, injrun, start, duration)
        missed = found.replace('FOUND', 'MISSED')
        effoutdir = '%s/%s/%s/efficiency' % (outdir, plotdir, injrun)
        node = injection_efficiency_setup(job, tag, effoutdir, grbdir,\
                                          clstfile['OFFSOURCE'],\
                                          clstfile[timetype], injrun, injcp,\
                                          found, missed)
        dag.add_node(node)

    # finalise DAG
    parents = []
    if run_clustering: parents.append(DAGManNode['trig_cluster'])
    if run_injfind:    parents.append(DAGManNode['injfinder'])
    DAGManNode[tag] = finalise_DAG(dag, parents)
    uberdag.add_node(DAGManNode[tag])

  # =============
  # write uberdag
  # =============

  uberdag.write_sub_files()
  uberdag.write_dag()

  # print message
  print >>sys.stdout
  print >>sys.stdout, '------------------------------------'
  print >>sys.stdout, 'Ready. To submit, run:'
  print >>sys.stdout
  subcmd = 'condor_submit_dag '
  if cp.has_option('pipeline', 'maxjobs'):
    subcmd += '-maxjobs %s ' % cp.getint('pipeline', 'maxjobs')
  subcmd += os.path.abspath(uberdag.get_dag_file())
  print >>sys.stdout, subcmd
  print >>sys.stdout

  print >>sys.stdout, 'Once submitted, to monitor status, run:'
  print >>sys.stdout
  print >>sys.stdout, 'lalapps_ihope_status --dag-file %s'\
                      % (os.path.abspath(uberdag.get_dag_file()))
  print >>sys.stdout, '------------------------------------'
  print >>sys.stdout


if __name__=='__main__':

  opts, args = parse_command_line()
  verbose = opts.verbose
  rundir  = os.path.abspath(opts.run_dir)
  outdir  = os.path.abspath(opts.output_dir)
  logdir  = opts.log_path
  ifotag  = opts.ifo_tag
  grb     = opts.grb_name
  inifile = os.path.abspath(opts.config_file)
  injfile = opts.inj_config_file
  if injfile:
    injfile = os.path.abspath(injfile)

  main(rundir, outdir, ifotag, grb, inifile, injfile, verbose=verbose,\
       logdir=logdir,\
       run_combiner=not opts.skip_trig_combiner,\
       run_clustering=not opts.skip_clustering,\
       run_injfind=not opts.skip_injfind,\
       run_sbvplotter=not opts.skip_sbv_plotter,\
       run_efficiency=not opts.skip_efficiency)
