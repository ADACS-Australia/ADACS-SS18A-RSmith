#!/usr/bin/env python
"""
plot_inspiral_hipe.in - generates plots from ihope output

$Id$

This script generates a condor DAG to make plots that summarize the
results of an inspiral search
"""

__author__ = 'Patrick Brady <patrick@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import sys, os, copy, math
import socket, time
import re, string
from optparse import *
import tempfile
import ConfigParser
import urlparse
from pylal import CoincInspiralUtils
from pylal import itertools
sys.path.append('/scratch2/rahul/opt/lscsoft/lalapps//lib/python2.4/site-packages/lalapps')

##############################################################################
# import the modules we need to build the pipeline
from glue import pipeline

from lalapps import inspiral

#############################################################################
#
#  MAIN PROGRAM
#
##############################################################################
usage = """usage: %prog [options] 
"""

parser = OptionParser( usage )

parser.add_option("-V", "--version",action="store_true",default=False,\
    help="print version information and exit")
    
parser.add_option("-u", "--user-tag",action="store",type="string",\
    default=None,metavar=" USERTAG",\
    help="tag the jobs with USERTAG (overrides value in ini file)")

parser.add_option("-g", "--g1-data",action="store_true",default=False,\
    help="analyze g1 data")

parser.add_option("-a", "--h1-data",action="store_true",default=False,\
    help="analyze h1 data")

parser.add_option("-b", "--h2-data",action="store_true",default=False,\
    help="analyze h2 data")

parser.add_option("-l", "--l1-data",action="store_true",default=False,\
    help="analyze l1 data")

parser.add_option("-v", "--v2-data",action="store_true",default=False,\
    help="analyze v2 data")

parser.add_option("-S", "--one-ifo",action="store_true",default=False,\
    help="analyze single ifo data (not usable for GEO)")

parser.add_option("-D", "--two-ifo",action="store_true",default=False,\
    help="analyze two interferometer data")

parser.add_option("-T", "--three-ifo",action="store_true",default=False,\
    help="analyze three interferometer data")

parser.add_option("-Q", "--four-ifo",action="store_true",default=False,\
    help="analyze four intereferometer data")
  
parser.add_option("-R", "--five-ifo",action="store_true",default=False,\
    help="analyze five intereferometer data")

parser.add_option("-A", "--analyze-all",action="store_true",default=False,\
    help="analyze all ifos and all data (over-rides above)")

parser.add_option("-i", "--plotinspiral" ,action="store_true",default=False,\
    help="run plotinspiral to summarize filtering output")

parser.add_option("-t", "--plotthinca" ,action="store_true",default=False,\
    help="run plotthina to summarize coincidence")

parser.add_option("-n", "--plotnumtemplates" ,action="store_true",default=False,\
    help="run plotnumtemplates to plot trigbanks and templtbanks")

parser.add_option("-I", "--plotinjnum" ,action="store_true",default=False,\
    help="run plotinj")

parser.add_option("-e", "--plotethinca" ,action="store_true",default=False,\
    help="run plotethinca parameter for a pair of ifo combos")

parser.add_option("-M", "--plotinspmissed" ,action="store_true",default=False,\
    help="run plotinspmissed to plot missed triggers")

parser.add_option("-j", "--plotinspinj" ,action="store_true",default=False,\
    help="run plotinspinj to plot inspiral injections.")

parser.add_option("-s", "--plotsnrchi" ,action="store_true",default=False,\
    help="run plotsnrchi to plot  snr vs chisq for a glob of triggers.")

parser.add_option("-f", "--config-file",action="store",type="string",\
    metavar=" FILE",help="use configuration file FILE")

parser.add_option("-p", "--log-path",action="store",type="string",\
    metavar=" PATH",help="directory to write condor log file")

command_line = sys.argv[1:]
(opts,args) = parser.parse_args()

#################################
#ALPHABETS USED IN THIS CODE.
# b, e, f, g, i, I, j, l, M, n, p, s, t, u, v, A, D, S, T, Q

#################################
# if --version flagged
if opts.version:
  print "$Id$"
  sys.exit(0)

#################################
# Sanity check of input arguments
if not opts.config_file:
  print >> sys.stderr, "No configuration file specified."
  print >> sys.stderr, "Use --config-file FILE to specify location."
  sys.exit(1)

if not opts.log_path:
  print >> sys.stderr, "No log file path specified."
  print >> sys.stderr, "Use --log-path PATH to specify a location."
  sys.exit(1)

  
if not opts.v2_data and not opts.h1_data and not opts.h2_data and \
    not opts.l1_data and not opts.analyze_all:
  print >> sys.stderr, "No ifos specified.  Please specify at least one of"
  print >> sys.stderr, "--g1-data, --h1-data, --h2-data, --l1-data"
  print >> sys.stderr, "or use --analyze-all to analyze all ifos all data"
  sys.exit(1)

if opts.plotthinca and opts.plotinspiral and opts.plotinspmissed and opts.plotinspinj and opts.plotsnrchi and not opts.one_ifo \
    and not opts.two_ifo and not opts.three_ifo and not opts.four_ifo \
    and not opts.analyze_all:
  print >> sys.stderr, "No number of ifos given. Please specify at least one of"
  print >> sys.stderr, "--one-ifo, --two-ifo, --three-ifo, --four-ifo"
  print >> sys.stderr, "or use --analyze-all to analyze all ifos all data"
  sys.exit(1)
 
#if not opts.plotinspiral: 
#  print >> sys.stderr, """  No steps of the pipeline specified.
#  Please specify at least one of
#  --plotinspiral"""
#  sys.exit(1)

#if not opts.plotthinca:
#  print >> sys.stderr, """  No steps of the pipeline specified.
#  Please specify at least one of
#  --plotthinca"""
#  sys.exit(1)


###############################
# This list constructs doubles, triples and quadraple 

ifolist = [ifo for ifo in ('G1','H1', 'H2', 'L1', 'V2') \
            if getattr(opts, "%s_data" % ifo.lower())]

  
ifo_combos=CoincInspiralUtils.get_ifo_combos(ifolist)
if opts.one_ifo:
  ifo_combos=ifolist
else:
  ifo_combo=list(itertools.choices(ifolist,2))
  ifo_combos = []
  for ifo in ifo_combo:
    ifo_combos.append(ifo[0]+ifo[1])



#################################
   
ifo_list = ['G1','H1','H2','L1','V2']

ifotag = None
#################################
# store the values
do = {}
do['G1'] = opts.g1_data
do['H1'] = opts.h1_data
do['H2'] = opts.h2_data
do['L1'] = opts.l1_data
do['V2'] = opts.v2_data

#################################
# analyze everything if --analyze-all set
if opts.analyze_all:
  for ifo in ifo_list:
    do[ifo] = True
  opts.one_ifo   = True
  opts.two_ifo   = True
  opts.three_ifo = True
  opts.four_ifo  = True
  opts.five_ifo  = True
################################
# a list of the ifos to do plots for
ifos = []
for ifo in ifo_list:
  if do[ifo]:
    ifos.append(ifo)

##############################################################################
# try to make a directory to store the cache files and job logs
try: os.mkdir('logs')
except: pass

##############################################################################
# create the config parser object and read in the ini file
cp = ConfigParser.ConfigParser()
cp.read(opts.config_file)

##############################################################################
# if a usertag has been specified, override the config file
if opts.user_tag:
  usertag = opts.user_tag
  cp.set('pipeline','user-tag',usertag)
else:
  try:
    usertag = string.strip(cp.get('pipeline','user-tag'))
  except:
    usertag = None
  
##############################################################################
# create a log file that the Condor jobs will write to
basename = re.sub(r'\.ini',r'',opts.config_file)
tempfile.tempdir = opts.log_path
if usertag:
  tempfile.template = basename + '.' + usertag + '.dag.log.'
else:
  tempfile.template = basename + '.dag.log.'
logfile = tempfile.mktemp()
fh = open( logfile, "w" )
fh.close()

##############################################################################
# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
if usertag:
  dag.set_dag_file(basename + '.' + usertag )
else:
  dag.set_dag_file(basename )

# set better submit file names than the default
if usertag:
  subsuffix = '.' + usertag + '.sub'
else:
  subsuffix = '.sub'

##############################################################################
# create the Condor jobs that will be used in the DAG

# inspiral:
if opts.plotinspiral:

  plotinspiral_jobs = {}

  for ifo in ifo_list:
    cp.set('plotinspiral','figure-name','plotinspiral_' + ifo)
    cp.set('plotinspiral','ifo-type',ifo)
    plotinspiral_jobs[ifo] = inspiral.PlotInspiralJob(cp)
    plotinspiral_jobs[ifo].set_sub_file( basename + '.plotinspiral_' + ifo + subsuffix )

  all_jobs = []
  all_jobs.extend(plotinspiral_jobs.values())

#plotthinca
if opts.plotthinca:
   
  plotthinca_jobs = {}

  for ifo in ifo_combos:
    cp.set('plotthinca','figure-name','plotthinca_' + ifo)
    plotthinca_jobs[ifo] = inspiral.PlotThincaJob(cp)
    plotthinca_jobs[ifo].set_sub_file( basename + '.plotthinca_' + ifo + subsuffix )

  all_jobs = []
  all_jobs.extend(plotthinca_jobs.values())

#plotnumtemplates
if opts.plotnumtemplates:

  plotnumtemplates_jobs = {}
  plotnumtemplates_jobs = inspiral.PlotNumtemplatesJob(cp)
  plotnumtemplates_jobs.set_sub_file( basename + '.plotnumtemplates' + subsuffix)

  all_jobs = []
  all_jobs.append(plotnumtemplates_jobs)

#plotinjnum
if opts.plotinjnum:
  
  if not opts.two_ifo and not opts.three_ifo and \
    not opts.four_ifo and not opts.analyze_all:

    print >> sys.stderr, "No number of ifos given. Please specify at least one of"
    print >> sys.stderr, "--two-ifo, --three-ifo, --four-ifo"
    print >> sys.stderr, "or use --analyze-all to analyze all ifos all data"
    sys.exit(1)
  
  plotinjnum_jobs = {}
  
  for ifo in ifo_combos:
    cp.set('plotinjnum','figure-name','plotinjnum_' + ifo)
    plotinjnum_jobs[ifo] = inspiral.PlotInjnumJob(cp) 
    plotinjnum_jobs[ifo].set_sub_file( basename + '.plotinjnum_' + ifo + subsuffix )
  
  all_jobs = []
  all_jobs.extend(plotinjnum_jobs.values())  

# plotethinca
if opts.plotethinca:

  plotethinca_jobs = {}

  for ifo in ifo_combos:
    cp.set('plotethinca','figure-name','plotethinca_' + ifo)
    cp.set('plotethinca','ifo-type', ifo)
    cp.set('plotethinca','ifo-times',ifo)
    plotethinca_jobs[ifo] = inspiral.PlotEthincaJob(cp)
    plotethinca_jobs[ifo].set_sub_file( basename + '.plotethinca_' + ifo + subsuffix )

  all_jobs = []
  all_jobs.extend(plotethinca_jobs.values())

#plotinspmissed
if opts.plotinspmissed:

  plotinspmissed_jobs = {}

  for ifo in ifo_combos:
    cp.set('plotinspmissed','figure-name','plotinspmissed_' + ifo)
    plotinspmissed_jobs[ifo] = inspiral.PlotInspmissedJob(cp)
    plotinspmissed_jobs[ifo].set_sub_file( basename + '.plotinspmissed_' + ifo + subsuffix )

  all_jobs = []
  all_jobs.extend(plotinspmissed_jobs.values())

#plotinspmissed
if opts.plotinspinj:

  plotinspinj_jobs = {}

  for ifo in ifo_list:
    cp.set('plotinspinj','figure-name','plotinspinj_' + ifo)
    plotinspinj_jobs[ifo] = inspiral.PlotInspinjJob(cp)
    plotinspinj_jobs[ifo].set_sub_file( basename + '.plotinspinj_' + ifo + subsuffix )

  all_jobs = []
  all_jobs.extend(plotinspinj_jobs.values())

#plotsnrchi
if opts.plotsnrchi:

  plotsnrchi_jobs = {}

  for ifo in ifo_list:
    cp.set('plotsnrchi','figure-name','plotsnrchi_' + ifo)
    plotsnrchi_jobs[ifo] = inspiral.PlotSnrchiJob(cp)
    plotsnrchi_jobs[ifo].set_sub_file( basename + '.plotsnrchi_' + ifo + subsuffix )

  all_jobs = []
  all_jobs.extend(plotsnrchi_jobs.values())


#############################################################################
# set the usertag in the jobs
if usertag:
  for job in all_jobs:
    job.add_opt('user-tag',usertag)

##############################################################################
#  The meat of the DAG generation comes below
#
##############################################################################

if opts.plotinspiral:
  for ifo in ifos:
  # add an plotinspiral job
    plotinspiral_node=inspiral.PlotInspiralNode(plotinspiral_jobs[ifo])
    dag.add_node(plotinspiral_node)


if opts.plotthinca:
  for ifo in ifo_combos:
  # add an plotthinca job
    plotthinca_node=inspiral.PlotThincaNode(plotthinca_jobs[ifo])
    dag.add_node(plotthinca_node)


if opts.plotnumtemplates:
   # add an plotnumtemplates job
   plotnumtemplates_node=inspiral.PlotNumtemplatesNode(plotnumtemplates_jobs)
   dag.add_node(plotnumtemplates_node)

if opts.plotinjnum:
  for ifo in ifo_combos:
  # add an plotthinca job
    plotinjnum_node=inspiral.PlotInjnumNode(plotinjnum_jobs[ifo])
    dag.add_node(plotinjnum_node)


if opts.plotethinca:
  for ifo in ifo_combos:
  # add an plotethinca job
    plotethinca_node =inspiral.PlotEthincaNode(plotethinca_jobs[ifo])
    dag.add_node(plotethinca_node)

if opts.plotinspmissed:
   for ifo in ifo_combos:
  # add an plotinspmissed job
    plotinspmissed_node =inspiral.PlotInspmissedNode(plotinspmissed_jobs[ifo])
    dag.add_node(plotinspmissed_node)

if opts.plotinspinj:
   for ifo in ifo_list:
  # add an plotinspinj job
    plotinspinj_node =inspiral.PlotInspinjNode(plotinspinj_jobs[ifo])
    dag.add_node(plotinspinj_node)

if opts.plotsnrchi:
   for ifo in ifo_list:
  # add an plotinspinj job
    plotsnrchi_node =inspiral.PlotSnrchiNode(plotsnrchi_jobs[ifo])
    dag.add_node(plotsnrchi_node)


##############################################################################
# Step 10: Write out the DAG, help message and log file
dag.write_sub_files()
dag.write_dag()

##############################################################################  
# write a message telling the user that the DAG has been written
print "\nCreated a DAG file which can be submitted by executing"
print "\n   condor_submit_dag", dag.get_dag_file()
print """\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
If you are running LSCdataFind jobs, do not forget to initialize your grid 
proxy certificate on the condor submit machine by running the commands

  unset X509_USER_PROXY
  grid-proxy-init -hours 72

Enter your pass phrase when promted. The proxy will be valid for 72 hours. 
If you expect the LSCdataFind jobs to take longer to complete, increase the
time specified in the -hours option to grid-proxy-init. You can check that 
the grid proxy has been sucessfully created by executing the command:

  grid-cert-info -all -file /tmp/x509up_u`id -u`

This will also give the expiry time of the proxy. 
"""

##############################################################################
# write out a log file for this script
if usertag:
  log_fh = open(basename + '.plotter.' + usertag + '.log', 'w')
else:
  log_fh = open(basename + '.plotter.log', 'w')
  
log_fh.write( "$Id$" + "\n" )
log_fh.write( "$Name$" + "\n\n" )
log_fh.write( "Invoked with arguments:" )
for arg in command_line:
  if arg[0] == '-':
    log_fh.write( "\n" )
  log_fh.write( arg + ' ')

log_fh.write( "\n" )
log_fh.write( "Config file has CVS strings:\n" )
#log_fh.write( cp.get('pipeline','version') + "\n" )
#log_fh.write( cp.get('pipeline','cvs-tag') + "\n\n" )

log_fh.close()

sys.exit(0)

