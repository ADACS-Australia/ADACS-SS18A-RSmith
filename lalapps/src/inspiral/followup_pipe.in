#!/usr/bin/env @PYTHONPROG@
"""
Something

$Id$

This program creates cache files for the output of inspiral hipe
"""

__author__ = 'Chad Hanna <channa@phys.lsu.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import sys, os, copy, math
import socket, time
import re, string
from optparse import *
import tempfile
import ConfigParser
import urlparse
from UserDict import UserDict
sys.path.append('@PYTHONLIBDIR@')

##############################################################################
# import the modules we need to build the pipeline
from glue import pipeline
from glue import lal
#from pylab import *
from glue import segments 
from glue import segmentsUtils
from glue.ligolw import ligolw
from glue.ligolw import table
from glue.ligolw import lsctables
from glue.ligolw import utils
from pylal import CoincInspiralUtils
from pylal.fu_utils import *
from pylal.fu_writeXMLparams import * 
from pylal import Fr      
##############################################################################
# redefine the columns of interest
##############################################################################
lsctables.SimInspiralTable.loadcolumns = ["waveform","geocent_end_time",
    "geocent_end_time_ns","h_end_time","h_end_time_ns","l_end_time",
    "l_end_time_ns","source","mass1","mass2","mchirp","eta","distance",
    "spin1x","spin1y","spin1z","spin2x","spin2y","spin2z","eff_dist_h",
    "eff_dist_l","eff_dist_g","eff_dist_t","eff_dist_v"]

lsctables.SnglInspiralTable.loadcolumns = ["ifo","end_time","end_time_ns",
    "eff_distance","mass1","mass2","mchirp","eta","snr","chisq","chisq_dof",
    "sigmasq","event_id"]

################ TRIG BANK FROM SIRE FILE CONDOR DAG JOB ######################

class trigBankFollowUpJob(pipeline.CondorDAGJob):
  """
  A followup trig bank job
  """
  def __init__(self, options, cp, tag_base='TRIGBANK_FOLLOWUP'):
    """
    """
    self.__executable = string.strip(cp.get('trig-bank','executable'))
    self.__universe = "standard"
    pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
#    self.tag_base = tag_base
#    self.options = options
    self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")

    self.set_stdout_file('logs/futrigbank-$(macrogpsstarttime)-$(cluster)-$(process).out')
    self.set_stderr_file('logs/futrigbank-$(macrogpsstarttime)-$(cluster)-$(process).err')
    self.set_sub_file('futrigbank.sub')


class trigBankFollowUpNode(pipeline.CondorDAGNode):
  """
  Runs an instance of a trig bank followup job
  """
  def __init__(self,job,time,proc,opts,xml_glob):
    """
    job = A CondorDAGJob that can run an instance of trigbank followup.
    """
    pipeline.CondorDAGNode.__init__(self,job)
    self.output_file_name = ""
    self.output_user_tag = "FOLLOWUP_" + str(time)

    for row in proc:
      self.add_var_opt(row.param[2:],row.value)
      if row.param[2:] == 'gps-start-time':
        self.add_macro("macrogpsstarttime",row.value)
        self.start_time = row.value
      if row.param[2:] == 'input-ifo':
        self.input_ifo = row.value
      if row.param[2:] == 'ifo-tag':
        self.ifo_tag = row.value
      if row.param[2:] == 'gps-end-time':
        self.end_time = row.value

    self.add_var_opt("user-tag",self.output_user_tag)
    self.add_file_arg(xml_glob)
    self.output_file_name = self.input_ifo + "-TRIGBANK_" + self.ifo_tag + \
      "_" + self.output_user_tag + "-" + self.start_time + "-" + \
      str(eval(self.end_time)-eval(self.start_time)) + ".xml"

class inspiralFollowUpJob(pipeline.CondorDAGJob):
  """
  A followup trig bank job
  """
  def __init__(self, options, cp, tag_base='INSPIRAL_FOLLOWUP'):
    """
    """
    self.__executable = string.strip(cp.get('inspiral','executable'))
    self.__universe = "standard"
    pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
    self.tag_base = tag_base
    self.options = options
    self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")

    self.set_stdout_file('logs/fuinspiral-$(macrogpsstarttime)-$(cluster)-$(process).out')
    self.set_stderr_file('logs/fuinspiral-$(macrogpsstarttime)-$(cluster)-$(process).err')
    self.set_sub_file('fuinspiral.sub')


class inspiralFollowUpNode(pipeline.CondorDAGNode):
  """
  Runs an instance of a trig bank followup job
  """
  def __init__(self,job,time,proc,opts,bankFile):
    """
    job = A CondorDAGJob that can run an instance of trigbank followup.
    """
    pipeline.CondorDAGNode.__init__(self,job)
    self.output_file_name = ""
    self.output_user_tag = "FOLLOWUP_" + str(time)
    self.add_rsq_veto = 1
    for row in proc:
      if row.param[2:] == 'bank-file':
        continue
      if row.param[2:] == 'trig-start-time':
        continue
      if row.param[2:] == 'trig-end-time':
        continue
      self.add_var_opt(row.param[2:],row.value)
      if row.param[2:] == 'gps-start-time':
        self.add_macro("macrogpsstarttime",row.value)
        self.start_time = row.value
      if row.param[2:] == 'channel-name':
        self.input_ifo = row.value[0:2]
      if row.param[2:] == 'ifo-tag':
        self.ifo_tag = row.value
      if row.param[2:] == 'gps-end-time':
        self.end_time = row.value
      if row.param[2:] == 'enable-rsq-veto':
        self.add_rsq_veto = 0
      if row.param[2:] == 'disable-rsq-veto':
        self.add_rsq_veto = 0
    self.add_var_opt("user-tag",self.output_user_tag)
    self.add_var_opt("write-snrsq","")
    self.add_var_opt("write-chisq","")
    self.add_var_opt("write-spectrum","")
    self.add_var_opt("bank-file",bankFile)
    self.add_var_opt("trig-start-time", str( eval(str(time))-1.0) )
    self.add_var_opt("trig-end-time", str( eval(str(time))+1.0) )
    if self.add_rsq_veto:
      self.add_var_opt("enable-rsq-veto","")
    self.output_file_name = self.input_ifo + "-INSPIRAL_" + self.ifo_tag + \
      "_" + self.output_user_tag + "-" + self.start_time + "-" +\
      str(eval(self.end_time)-eval(self.start_time)) + ".xml"
    
class plotSNRCHISQJob(pipeline.CondorDAGJob):
  """
  A followup trig bank job
  """
  def __init__(self, options, cp, tag_base='PLOT_FOLLOWUP'):
    """
    """
    self.__executable = string.strip(cp.get('plots','executable'))
    self.__universe = "vanilla"
    pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
    self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")
    self.set_stdout_file('logs/fuplotsnrchisq-$(macrogpsstarttime)-$(cluster)-$(process).out')
    self.set_stderr_file('logs/fuplotsnrchisq-$(macrogpsstarttime)-$(cluster)-$(process).err')
    self.set_sub_file('fuplotsnrchisq.sub')


class plotSNRCHISQNode(pipeline.CondorDAGNode):
  """
  Runs an instance of a trig bank followup job
  """
  def __init__(self,job,time,fileName,trig,page):
    """
    job = A CondorDAGJob that can run an instance of trigbank followup.
    """
    pipeline.CondorDAGNode.__init__(self,job)
    self.output_file_name = ""
    self.add_var_opt("frame-file",fileName.replace(".xml",".gwf"))
    self.add_var_opt("gps",time)
    self.add_var_opt("inspiral-xml-file",fileName)
    self.container = HTMLcontainer(trig,"SNR_and_CHISQ_plots")
    self.add_var_opt("output-html-file",self.container.locallink)
    self.add_var_opt("output-path","followuptrigs/"+self.container.name+"/")
    self.add_var_opt("image-file", self.container.localimage)
    self.add_var_opt("page", page)
    tableFile = open(self.container.locallink,'w')
    writeIULHeader(tableFile)
    tableFile.write('<h3>Follow up of trigger [' +str(trig.eventID) +']</h3>\n')
    self.container.text = "click here for SNR and CHISQ plots"
    tableFile.close()
  def getContainer(self):
    return self.container

##############################################################################
# qscan class for qscan jobs

class qscanJob(pipeline.CondorDAGJob):
  """
  A qscan job
  """
  def __init__(self, opts, cp, tag_base='QSCAN'):
    """
    """
    self.__executable = string.strip(cp.get('qscan','executable'))
    self.__universe = "vanilla"
    pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
    self.tag_base = tag_base
    self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")
    self.set_stdout_file('logs/qscan-$(macrogpsstarttime)-$(cluster)-$(process).out')     
    self.set_stderr_file('logs/qscan-$(macrogpsstarttime)-$(cluster)-$(process).err')
    self.set_sub_file('qscan.sub')

##############################################################################
# qscan class for qscan Node

class qscanNode(pipeline.CondorDAGNode):
  """
  Runs an instance of a qscan job
  """
  def __init__(self,job,time,cp,dir,trig,ifo):
    """
    job = A CondorDAGJob that can run an instance of qscan.
    """
    pipeline.CondorDAGNode.__init__(self,job)
    self.add_var_arg(str(time))
    self.add_macro("macrogpsstarttime",str(time))
    if (ifo == 'H1') or (ifo == 'H2'):
      self.add_file_arg(string.strip(cp.get('qscan','Hconfig-file')))
      self.add_file_arg(string.strip(cp.get('qscan','Hcache-file')))
    if (ifo == 'L1'):
      self.add_file_arg(string.strip(cp.get('qscan','Lconfig-file')))
      self.add_file_arg(string.strip(cp.get('qscan','Lcache-file')))
    self.container = HTMLcontainer(trig,"QSCAN")
#   self.add_var_arg(dir+"/"+self.container.name+"/")
    self.add_var_arg(string.strip(cp.get('qscan','output')))
    tableFile = open(self.container.locallink,'a')
    writeIULHeader(tableFile)
    tableFile.write('<h3>Follow up of trigger [' +str(trig.eventID) +']</h3>\n')
    tableFile.write('<a href="http://ldas-jobs.ligo.caltech.edu/~channa/' + \
                    str(time) + '/">' + str(ifo) + \
                    ' QScan of trigger [' +str(trig.eventID) +']</a>\n')
    self.container.text = "click here for QScans"
    tableFile.close()

  def getContainer(self):
    return self.container

##############################################################################
#
#  MAIN PROGRAM
#
##############################################################################

###### OPTION PARSING AND SANITY CHECKS #####################################
usage = """usage: %prog [options]
"""

parser = OptionParser( usage )

parser.add_option("-v", "--version",action="store_true",default=False,\
    help="print version information and exit")

parser.add_option("-l", "--log-path",action="store",type="string",\
    metavar=" PATH",help="directory to write condor log file")

parser.add_option("-f", "--config-file",action="store",type="string",\
    metavar=" FILE",help="ini file")

parser.add_option("-g", "--generate-cache",action="store_true",\
    default=False, help="write LAL cache")

parser.add_option("-r","--read-triggers",action="store_true",default=False,\
    help="Read the followup trigger XML (COIRE FILE)" )

parser.add_option("-q", "--qscan",action="store_true",default=False,\
    help="do qscans")

parser.add_option("-t", "--trig-bank",action="store_true",default=False,\
    help="do trigbanks for single triggers")

parser.add_option("-i", "--inspiral",action="store_true",default=False,\
    help="do inspirals for single triggers - output SNR/CHISQ/PSD")

parser.add_option("-p", "--plots",action="store_true",default=False,\
    help="plot SNR/CHISQ from inspiral stage")

parser.add_option("-w", "--write-to-iulgroup",action="store_true", \
    default=False, help="publish the page to the iulgroup")

parser.add_option("-c", "--cache-path",action="store",type="string",\
    default="./.",metavar=" PATH",help="directory to find  all hipe XML files")

parser.add_option("-s", "--science-run", action="store",type="string",\
    default="S5", metavar=" RUN", help="name of science run")

command_line = sys.argv[1:]
(opts,args) = parser.parse_args()

if opts.version:
  print "$Id$"
  sys.exit(0)

if not opts.cache_path:
  print >> sys.stderr, "No cache path specified."
  print >> sys.stderr, "Use --cache-path PATH to specify a location."
  sys.exit(1)

if not opts.science_run:
  print >> sys.stderr, "No science run specified."
  print >> sys.stderr, "Use --science-run RUN to specify a run."
  sys.exit(1)

#if not opts.xml_glob:
#  print >> sys.stderr, "Must specify a GLOB of xmls to read"
#  sys.exit(1)

#if not opts.statistic:
#  print >> sys.stderr, "Must specify a statistic to use"
#  sys.exit(1)

## READ IN THE CONFIG (.ini) FILE
cp = ConfigParser.ConfigParser()
cp.read(opts.config_file)

## set up the local directories
setupdirs()

############# TURN THE HIPE OUTPUT INTO LAL CACHE FILES #######################
if opts.generate_cache:
  cache = getCache(opts)
  cache.getCacheAll(cp)
  cache.writeCacheAll()
  print >> sys.stderr, "\nHIPE CACHE FILES WRITTEN TO:"
  for n, t in cache.nameMaps:
    print >> sys.stderr, " * " + n + " [" + str(len(cache[t])) + "]"

##############################################################################
# create a log file that the Condor jobs will write to
basename = 'followup'
tempfile.tempdir = opts.log_path
tempfile.template = basename + '.dag.log.'
logfile = tempfile.mktemp()
fh = open( logfile, "w" )
fh.close()

##############################################################################
# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
dag.set_dag_file( basename )
subsuffix = '.sub'

##############################################################################
#Decide if we are actually reading any trigger files
page = string.strip(cp.get('output','page'))

if not opts.read_triggers and not opts.write_to_iulgroup:
  sys.exit(0)
  # We are done here
if not opts.read_triggers and opts.write_to_iulgroup:
  publishToIULGroup(page)
  # don't overwrite anything, just publish what we have
  sys.exit(0)

# Since we are continuing get useful stuff from the ini file.
xml_glob = string.strip(cp.get('triggers','xml-glob'))
numtrigs = string.strip(cp.get('triggers','num-trigs'))
statistic =  string.strip(cp.get('triggers','statistic'))
bla =  string.strip(cp.get('triggers','bitten-l-a'))
blb =  string.strip(cp.get('triggers','bitten-l-b'))


print string.strip(cp.get('hipe-cache','TMPLTBANK')) 
############# READ IN THE COIRE FILES #########################################

found, coincs = readFiles(xml_glob,getstatistic(statistic,bla,blb))
missed = None
followuptrigs = getfollowuptrigs(numtrigs,page,coincs,missed)

print "\n.......Found " + str(len(coincs)) + " trigs to follow up" 

############ SET UP THE REQUESTED JOBS ########################################
trigJob = trigBankFollowUpJob(opts,cp)
inspJob = inspiralFollowUpJob(opts,cp)
plotJob = plotSNRCHISQJob(opts,cp)
qscanJob = qscanJob(opts,cp)

trigJobCnt = 0
inspJobCnt = 0
plotJobCnt = 0
qscanJobCnt = 0
summaryHTMLlist = []

print "\n.......Setting up pipeline jobs"

for trig in followuptrigs:
  # Set up HTML structs for web page
  summaryHTML = summaryHTMLTable(trig)
  summaryHTML.containers.append(writeXMLparams(trig))

  # TRY GETTING TRIG BANK PROCESS PARAMS
  try: 
    trig_process_params = cache.getProcessParamsFromCache( \
                     cache.filesMatchingGPS(trig.gpsTime,'TRIGBANK'), \
                     trig.gpsTime) 
  except: 
    print "couldn't get trigbank process params for " + str(trig.eventID)
    trig_process_params = []

  # TRY GETTING INSPIRAL PROCESS PARAMS
  try: 
    inspiral_process_params = cache.getProcessParamsFromCache( \
                     cache.filesMatchingGPS(trig.gpsTime,'INSPIRAL_'), \
                     trig.gpsTime)
  except: 
    print "couldn't get inspiral process params for " + str(trig.eventID)
    inspiral_process_params = []

  # currently this assumes you will be getting trig bank jobs...fix this!
  qFlag = True
  for ifo in trig_process_params:
    try: trig.gpsTime[ifo]
    except: continue

    # SETUP QSCAN JOBS
    if trig.gpsTime[ifo] and opts.qscan:
      try: 
        qNode = qscanNode(qscanJob,trig.gpsTime[ifo],cp,trig.summarydir,trig,ifo)
        dag.add_node(qNode)
        qFlag = False
        qscanJobCnt+=1
        #summaryHTML.containers.append( qNode.getContainer() )
      except: print "couldn't add qscan job for " + str(ifo) + "@ "+ str(trig.gpsTime[ifo])
      
    # SETUP TRIGBANK JOBS
    if trig.gpsTime[ifo]:
      try:
        trigNode = trigBankFollowUpNode(trigJob,trig.gpsTime[ifo],trig_process_params[ifo],opts,xml_glob)
        if opts.trig_bank:
          dag.add_node(trigNode)
          trigJobCnt+=1
      except:
        trigNode = 0
        print "couldn't add trigbank job for " + str(ifo) + "@ "+ str(trig.gpsTime[ifo])

    # SETUP INSPIRAL JOBS
    if trig.gpsTime[ifo] and trigNode:
      try:
        inspiralNode = inspiralFollowUpNode(inspJob,trig.gpsTime[ifo],\
           inspiral_process_params[ifo],opts,trigNode.output_file_name)
        if trigNode and opts.trig_bank: # only add a parent if it exists
          inspiralNode.add_parent(trigNode)
        if opts.inspiral:
          dag.add_node(inspiralNode)
          inspJobCnt+=1
      except: 
        inspiralNode = 0
        print "couldn't add inspiral job for " + str(ifo) + "@ "+ str(trig.gpsTime[ifo])
    else: 
      inspiralNode = 0

    # SETUP PLOT JOBS
    if trig.gpsTime[ifo] and inspiralNode:
      plotNode = plotSNRCHISQNode(plotJob,trig.gpsTime[ifo],inspiralNode.output_file_name,trig,page)
      try:
        plotNode = plotSNRCHISQNode(plotJob,trig.gpsTime[ifo],inspiralNode.output_file_name,trig,page)
        if inspiralNode and opts.inspiral:
          plotNode.add_parent(inspiralNode)
        if opts.plots:
          dag.add_node(plotNode)
          plotJobCnt+=1
        #summaryHTML.containers.append( plotNode.getContainer() )
      except:
        print "couldn't add plot job for " + str(ifo) + "@ "+ str(trig.gpsTime[ifo])
  
  if qNode: summaryHTML.containers.append( qNode.getContainer() )
  if plotNode: summaryHTML.containers.append( plotNode.getContainer() )
  summaryHTMLlist.append(summaryHTML)
    

print "\nFound " + str(trigJobCnt) + " trig bank jobs" 
print "\nFound " + str(inspJobCnt) + " inspiral Jobs" 
print "\nFound " +  str(plotJobCnt) + " plot jobs"
print "\nFound " +  str(qscanJobCnt) + " qscan jobs"
print "\n.......Writing DAG"

dag.write_sub_files()
dag.write_dag()

writeHTMLTables(summaryHTMLlist)
if opts.write_to_iulgroup:
  publishToIULGroup(page)

sys.exit(0)



sys.exit(0)

#print coincs.getsngls('H1')

#for coinc in coincs:
#  print getattr(coinc, 'H1').snr

#    print cache.filesMatchingGPS(trig.gpsTime,'TRIGBANK')

