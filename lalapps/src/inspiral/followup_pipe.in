#!/usr/bin/env @PYTHONPROG@
"""
Something

$Id$

This program creates cache files for the output of inspiral hipe
"""

__author__ = 'Chad Hanna <channa@ligo.caltech.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import sys, os, copy, math, random
from subprocess import *
import socket, time
import re, string
from optparse import *
import tempfile
import ConfigParser
import urlparse
import urllib
from UserDict import UserDict
sys.path.append('@PYTHONLIBDIR@')

##############################################################################
# import the modules we need to build the pipeline
from glue import pipeline
from glue import lal
from glue import segments 
from glue import segmentsUtils
from glue.ligolw import ligolw
from glue.ligolw import table
from glue.ligolw import lsctables
from glue.ligolw import utils
from pylal import CoincInspiralUtils
from pylal.fu_utils import *
from pylal.fu_writeXMLparams import * 
from pylal.fu_Condor import *
from pylal.webUtils import *
from pylal import Fr
from lalapps import inspiral
from lalapps import inspiralutils

##############################################################################
#
#  MAIN PROGRAM
#
##############################################################################

######################## OPTION PARSING  #####################################
usage = """usage: %prog [options]
"""

parser = OptionParser( usage )

parser.add_option("-v", "--version",action="store_true",default=False,\
    help="print version information and exit")

parser.add_option("-l", "--log-path",action="store",type="string",\
    metavar=" PATH",help="directory to write condor log file")

parser.add_option("-f", "--config-file",action="store",type="string",\
    metavar=" FILE",help="ini file")

parser.add_option("-g", "--generate-fu-cache",action="store_true",\
    default=False, help="create hipe caches")

parser.add_option("-D", "--disable-followup",action="store_true",\
    default=False, help="disable candidate followup (for running qscans only)")

parser.add_option("-m", "--datafind",action="store_true",\
    default=False, help="use datafind to get qscan/trends data")

parser.add_option("-M", "--hoft-datafind",action="store_true",\
    default=False, help="use datafind to get hoft data (for qscan)")

parser.add_option("", "--single-qevent",action="store_true",default=False,\
    help="do single qevent")

parser.add_option("", "--H1H2-qevent",action="store_true",default=False,\
    help="do coherent qevent")

parser.add_option("-q", "--qscan",action="store_true",default=False,\
    help="do qscans")

parser.add_option("-G", "--generate-segments",action="store_true",\
    default=False, help="generate the science segments for background qscans")

parser.add_option("-Q", "--background-qscan",action="store_true",\
    default=False, help="do qscans over a list of times")

parser.add_option("-n", "--hoft-qscan",action="store_true",\
    default=False, help="do hoft qscans")

parser.add_option("-N", "--background-hoft-qscan",action="store_true",\
    default=False, help="do hoft qscans over a list of times")

parser.add_option("-s", "--seis-qscan",action="store_true",\
    default=False, help="do seismic qscans")

parser.add_option("-S", "--background-seis-qscan",action="store_true",\
    default=False, help="do seismic qscans over a list of times")

parser.add_option("", "--distrib-remote-q", action="store_true",\
    default=False, help="this argument needs to be called when you want to \
    analyse the V1 qscans and seismic qscans that have been run remotely at  \
    CC-Lyon. After the V1 qscans have been done, you have received a \
    \"V1_qscans_results.tar.gz\" file that contains all the qscan results. \
    Before launching the \"analyseQscan\" jobs, you first need to copy these \
    results at their expected location, ie the \"xxouput\" fields defined in \
    the qscan sections of your \"followup_pipe.ini\" file. This is taken care \
    of by the argument \"--distrib-remote-q\"")

parser.add_option("-a", "--analyse-qscan",action="store_true",\
    default=False, help="run the analyseQscan script to interprete the qscan")

parser.add_option("-b", "--analyse-seis-qscan",action="store_true",\
    default=False, help="run the analyseQscan script to interprete the seismic qscan")

parser.add_option("-e", "--analyse-hoft-qscan",action="store_true",\
    default=False, help="run the analyseQscan script to interprete the hoft qscan")

parser.add_option("-d", "--data-quality",action="store_true",default=False,\
    help="do data quality lookup - CURRENTLY BROKEN, DO NOT USE IT")

parser.add_option("-t", "--trig-bank",action="store_true",default=False,\
    help="generate a pseudo trigbank xml file for single triggers")

parser.add_option("", "--inspiral-datafind",action="store_true",default=False,\
    help="do datafind jobs to update the old cache files." + \
         "This option affects the way the inspiral, frame-check, and mcmc jobs" + \
         "accessing to the data.")

parser.add_option("-i", "--inspiral",action="store_true",default=False,\
    help="do inspirals for single triggers - output SNR/CHISQ/PSD")

parser.add_option("-F", "--frame-check",action="store_true",default=False,\
    help="do FrCheck jobs on the frames used by the inspiral code")

parser.add_option("-I", "--ifo-status-check",action="store_true",default=False,\
    help="download IFO summary plots")

parser.add_option("-p", "--plots",action="store_true",default=False,\
    help="plot SNR/CHISQ from inspiral stage")

parser.add_option("-C", "--mcmc",action="store_true",default=False,\
    help="Do MCMC on the followup trigs (experimental)")

parser.add_option("-P", "--plot-mcmc",action="store_true",default=False,\
    help="Plot MCMC results (experimental)")

parser.add_option("-H", "--inspiral-head",action="store_true",default=False,\
    help="Run a job using inspiral from HEAD (to get bank and cont. chisq) (experimental)")

parser.add_option("", "--disable-ifarsorting",action="store_true",\
    default=False, help="this option disables the sorting of the candidates \
    according to their IFAR value. Instead the candidates will be sorted \
    according to their combined snr, if this option is used.")

parser.add_option("", "--convert-eventid",action="store_true",\
    default=False, help="For the analysis of the second year branch triggers \
    the event_id was defined as an \"int_8s\" instead of an \"ilwd:char\" \
    Therefore when analysing S5 second year data, one must activate this \
    option. The followup code will make a copy of the original xml files in \
    the local directory and use the \"ligolw_conv_inspid\" to convert the \
    event_id into the correct type.")

parser.add_option("", "--add-trig-columns",action="store_true",\
    default=False, help="For the analysis of teh second year branch, \
    we want to be able to use the followup code of the TRUNK. This requires to \
    handle the discrepancies between the column definitions in the \
    sngl_inspiral table. This option can be called when the \"--trig-bank\" \
    argument is also specified. When the option \"--add-trig-columns\" is \
    specified the code check for the existence of extra columns in the \
    sngl_inspiral table definition of the installed version of glue. If extra \
    columns are detected, they are added to the followup template bank xml \
    files.")

parser.add_option("-w", "--write-to-iulgroup",action="store_true", \
    default=False, help="publish the page to the iulgroup")

parser.add_option("", "--sky-map",action="store_true",default=False,\
    help="generate sky map data for an event")

parser.add_option("", "--sky-map-plot",action="store_true",default=False,\
    help="plot sky map data for an event")

parser.add_option("", "--coh-inspiral",action="store_true",default=False,\
    help="run a lalapps_inspiral job much the same way as the coherent code")

command_line = sys.argv[1:]
(opts,args) = parser.parse_args()

if opts.version:
  print "$Id$"
  sys.exit(0)

####################### SANITY CHECKS #####################################

if not opts.config_file:
  print >> sys.stderr, "No configuration file specified."
  print >> sys.stderr, "Use --config-file FILE to specify location" 
  sys.exit(1)

if not opts.log_path and not opts.write_to_iulgroup:
  print >> sys.stderr, "No log file path specified"
  print >> sys.stderr, "Use --log-path PATH to specify a location"
  sys.exit(1)

if not opts.write_to_iulgroup and not opts.generate_fu_cache and \
  not opts.datafind and not opts.qscan and not opts.background_qscan \
  and not opts.trig_bank and not opts.inspiral and not opts.plots and not \
  opts.hoft_qscan and not opts.seis_qscan and not opts.background_hoft_qscan \
  and not opts.background_seis_qscan and not opts.hoft_datafind and not \
  opts.generate_segments and not opts.analyse_qscan and not \
  opts.analyse_seis_qscan and not opts.analyse_hoft_qscan and not \
  opts.mcmc and not opts.frame_check and not opts.inspiral_head and not \
  opts.ifo_status_check and not opts.single_qevent and not opts.H1H2_qevent \
  and not opts.plot_mcmc and not opts.inspiral_datafind:
  print >> sys.stderr, "No steps of the pipeline specified."
  print >> sys.stderr, "Please specify at least one of"
  print >> sys.stderr, "--generate-fu-cache, --trig-bank, --inspiral, --plots,"
  print >> sys.stderr, "--datafind, --qscan, --hoft-qscan, --seis-qscan,"
  print >> sys.stderr, "--background-qscan, --background-hoft-qscan,"
  print >> sys.stderr, "--background-seis-qscan, --hoft-datafind,"
  print >> sys.stderr, "--generate-segments, --frame-check, --inspiral-datafind,"
  print >> sys.stderr, "--analyse-qscan, --analyse-seis-qscan,"
  print >> sys.stderr, "--analyse-hoft-qscan, --mcmc, --plot-mcmc,"
  print >> sys.stderr, "--ifo-status-check, --single-qevent, --H1H2-qevent"
  print >> sys.stderr, "or --write-to-iulgroup (use this option alone for now)"  
  sys.exit(1)

if opts.disable_followup:
  print >> sys.stderr, "Warning: this option disables any followup jobs, only qscan datafind and background qscan jobs will be run..."

#################### READ IN THE CONFIG (.ini) FILE ########################
cp = ConfigParser.ConfigParser()
cp.read(opts.config_file)

## set the option to make remote calculations for some Virgo qscans
depIfo = 'V1'
cp.set('q-datafind','remote-ifo',depIfo)
cp.set('foreground-qscan','remote-ifo',depIfo)
cp.set('background-qscan','remote-ifo',depIfo)
cp.set('foreground-seismic-qscan','remote-ifo',depIfo)
cp.set('background-seismic-qscan','remote-ifo',depIfo)

## get the directory where the code is run
currentPath = os.path.abspath('.')

############# TURN THE HIPE OUTPUT INTO LAL CACHE FILES #######################

if not opts.disable_followup:
  cache = getCache(opts,cp,currentPath)

##############################################################################
# create the DAG writing the log to the specified directory
#page = string.strip(cp.get('output','page'))
page = string.strip(cp.get('output','url'))
publish_path = string.strip(cp.get('output','page'))
dag = followUpDAG(opts.config_file, opts.log_path)
dag.setupDAGWeb('followup web page','index.html',page,publish_path)

############# READ IN THE COIRE FILES #########################################

if not opts.disable_followup:

  # if the option convert-eventid is called, we need to prepare the directory 
  # where the .xml files will be copied
  if opts.convert_eventid:
    if not os.access('LOCAL_XML_COPY',os.F_OK):
      os.mkdir('LOCAL_XML_COPY')
    else: pass

  numtrigs, found, coincs, search = cache.readTriggerFiles(cp,opts)
  missed = None

  if opts.trig_bank: trigbank_test = 1
  else: trigbank_test = 0
  if opts.disable_ifarsorting: ifar = False
  else: ifar = True
  followuptrigs = getfollowuptrigs(numtrigs,page,coincs,missed,search,trigbank_test,ifar,opts.add_trig_columns)

  print "\n.......Found " + str(len(followuptrigs)) + " trigs to follow up" 

############ SET UP THE REQUESTED JOBS ########################################

if not opts.disable_followup:
  inspDataJob     = followupDataFindJob(cp,'inspiral')
  inspJob         = followUpInspJob(cp)
  inspJobNotrig   = followUpInspJob(cp,'notrig')
  plotJob         = plotSNRCHISQJob(opts,cp)
  qscanFgJob      = qscanJob(opts,cp)
  distribQJob     = distributeQscanJob(cp)
  anaQscanJob     = analyseQscanJob(opts,cp)
  frcheckJob      = FrCheckJob(opts,cp)
  statusJob       = IFOstatus_checkJob(opts,cp)
  MCMCJob 	  = followupmcmcJob(opts,cp)
  PLOTMCMCJob     = plotmcmcJob(opts,cp)
  headInspJob     = followUpInspJob(cp, 'head')
  cohInspJob      = followUpInspJob(cp, 'coh')
  h1h2QJob        = h1h2QeventJob(opts,cp)   
  skyMapJob       = lalapps_skyMapJob(opts,cp)
  skyPlotJob      = pylal_skyPlotJob(opts,cp)


dataJob         = followupDataFindJob(cp,'futrig')
qscanBgJob      = qscanJob(opts,cp,'QSCANLITE')

prev_dNode = None

print "\n.......Setting up pipeline jobs"

dq_url_pattern = "http://ldas-cit.ligo.caltech.edu/segments/S5/%s/dq_segments.txt"

segFile = {}
ifos_list = ['H1','H2','L1','G1','V1','T1']

############ PREPARE FILES FOR REMOTE (DEPORTED) VIRGO QSCANS #################
###############################################################################

if not opts.disable_followup:
  
  depIfoIniConfig = depIfo+'config-file'
  depIfoIniWeb = depIfo+'web'
  depIfoDir = depIfo+'_qscans_config'
  depQscanList = ['foreground-qscan', 'background-qscan', 'foreground-seismic-qscan', 'background-seismic-qscan']
  
  # Build directories
  os.system('mkdir -p '+depIfoDir)
  os.system('mkdir -p '+depIfoDir+'/CONFIG')
  os.system('mkdir -p '+depIfoDir+'/RESULTS')
  for depQscan in depQscanList:
     os.system('mkdir -p '+depIfoDir+'/RESULTS/results_'+depQscan)
  os.system('mkdir -p '+depIfoDir+'/SCRIPTS')
  os.system('mkdir -p '+depIfoDir+'/TIMES')

  # Copy the qscan configuration files
  for depQscan in depQscanList:
    if cp.has_option(depQscan, depIfoIniConfig):
      qscanConfig = string.strip(cp.get(depQscan, depIfoIniConfig))
      if qscanConfig!='':
        print 'copy '+qscanConfig+' -----> '+depIfoDir+'/CONFIG/'+depQscan+'_config.txt'
        os.system('cp '+qscanConfig+' '+depIfoDir+'/CONFIG/'+depQscan+'_config.txt')
  
  # Copy the scripts used in the remote computing center
  #   first, get the path to the scripts
  scriptPath = Popen(["which", "analyseQscan.py"], stdout=PIPE).communicate()[0]

  scriptPath = scriptPath.strip('\n')
  scriptPath = scriptPath.replace('analyseQscan.py','followup_virgo_scripts')
  os.system('cp '+scriptPath+'/qsub_wscan.sh '+depIfoDir+'/SCRIPTS/')
  os.system('cp '+scriptPath+'/qsub_wscanlite.sh '+depIfoDir+'/SCRIPTS/')
  os.system('cp '+scriptPath+'/wscan_in2p3.sh '+depIfoDir+'/SCRIPTS/')
  os.system('cp '+scriptPath+'/wscanlite_in2p3.sh '+depIfoDir+'/SCRIPTS/')
  os.system('cp '+scriptPath+'/prepare_sendback.py '+depIfoDir)
  os.system('cp '+scriptPath+'/virgo_qscan_in2p3.py '+depIfoDir)
  depIfoWebForeground = string.strip(cp.get('foreground-qscan', depIfoIniWeb))
  if not depIfoWebForeground.startswith('http://virgo.in2p3.fr/followups/'):
    print('\nWARNING for foreground qscans:')
    print('   wrong web address : '+depIfoWebForeground)
    print('   The web address for Virgo qscans should start with \"http://virgo.in2p3.fr/followups/\"')
    print('   followed by the name of the submitter of the jobs')
  else:
    depIfoWebForeground = depIfoWebForeground.replace('http://virgo.in2p3.fr/followups/','')
    os.system('sed -e \'s|@foreground@|'+depIfoWebForeground+'|\' '+depIfoDir+'/virgo_qscan_in2p3.py > '+depIfoDir+'/virgo_qscan_in2p3.py.tmp')
    os.system('mv -f '+depIfoDir+'/virgo_qscan_in2p3.py.tmp '+depIfoDir+'/virgo_qscan_in2p3.py')
    os.system('chmod +x '+depIfoDir+'/virgo_qscan_in2p3.py')
  depIfoWebForegroundSeismic = string.strip(cp.get('foreground-seismic-qscan', depIfoIniWeb))
  if not depIfoWebForegroundSeismic.startswith('http://virgo.in2p3.fr/followups/'):
    print('\nWARNING for foreground-seismic qscans:')
    print('   wrong web address : '+depIfoWebForegroundSeismic)
    print('   The web address for Virgo qscans should start with \"http://virgo.in2p3.fr/followups/\"')
    print('   followed by the name of the submitter of the jobs')
  else:
    depIfoWebForegroundSeismic = depIfoWebForegroundSeismic.replace('http://virgo.in2p3.fr/followups/','')
    os.system('sed -e \'s|@foreground-seismic@|'+depIfoWebForegroundSeismic+'|\' '+depIfoDir+'/virgo_qscan_in2p3.py > '+depIfoDir+'/virgo_qscan_in2p3.py.tmp')
    os.system('mv -f '+depIfoDir+'/virgo_qscan_in2p3.py.tmp '+depIfoDir+'/virgo_qscan_in2p3.py')
    os.system('chmod +x '+depIfoDir+'/virgo_qscan_in2p3.py')

############# LOOP OVER RANDOM BACKGROUND TIMES ###############################
###############################################################################
# Prepare the qscan background
for ifo in ifos_list:
  times, timeListFile = getQscanBackgroundTimes(cp,opts,ifo, dq_url_pattern,segFile)
  for time in times:
    # SETUP DATAFIND JOBS FOR BACKGROUND QSCANS (REGULAR DATA SET)
    dNode = followupDataFindNode(dataJob,'futrig','q-datafind',cp,time,ifo,opts,dag,prev_dNode,'datafind')
    if dNode.validNode: prev_dNode = dNode

    # SETUP DATAFIND JOBS FOR BACKGROUND QSCANS (HOFT)
    dHoftNode = followupDataFindNode(dataJob,'futrig','q-hoft-datafind',cp,time,ifo,opts,dag,prev_dNode,'hoft_datafind')
    if dHoftNode.validNode: prev_dNode = dHoftNode
    
    # SETUP BACKGROUND QSCAN JOBS
    qBgNode = qscanNode(qscanBgJob,time,cp,dHoftNode.outputFileName,ifo,'background-hoft-qscan',opts,dHoftNode,dag,'hoft_datafind','background_hoft_qscan')
    qBgNode = qscanNode(qscanBgJob,time,cp,dNode.outputFileName,ifo,'background-qscan',opts, dNode, dag,'datafind','background_qscan')
    qBgNode = qscanNode(qscanBgJob,time,cp,dNode.outputFileName,ifo,'background-seismic-qscan',opts, dNode, dag,'datafind','background_seis_qscan')
  
  # WRITE TIMES FOR REMOTE (DEPORTED) CALCULATIONS
  if not opts.disable_followup:
    if ifo == depIfo and len(times)!=0:
      os.system('cp '+timeListFile+' '+depIfoDir+'/TIMES/background_qscan_times.txt')

################# LOOP OVER FOLLOWUP TRIGGERS #################################
###############################################################################

if not opts.disable_followup:
    
  # Initialize qscan times text file for deported qscans
  depQscanTFile = open(depIfoDir+'/TIMES/qscan_times.txt','w')
  
  # LOOP ON TRIGGERS (=CANDIDATES)
  for trig in followuptrigs:
 
    # Initialization of local variables
    dHoftNode = {}
    
    # We need a source localization node at the coincident trigger level
    # it takes many ifos as input. 
    skyNode = lalapps_skyMapNode(skyMapJob,trig)
 
    #Lets make a new section for this event
    dag.appendSection('Trigger ID = '+str(trig.eventID)+ ' Stat = '+str(trig.statValue))  
    
    # TRY GETTING INSPIRAL PROCESS PARAMS... 
    # currently done even when inspiral jobs are not requested... fix?
    inspiral_process_params = cache.processFollowupCache(cp, opts, trig)
    bank_process_params, bank = cache.processFollowupCache(cp, opts, trig, 'TMPLTBANK_*') 

    # loop over ifos found in coincidence
    for ifo in trig.ifolist_in_coinc:
      try: trig.gpsTime[ifo]
      except: continue
    
      # Write time in file for deported qscans
      if ifo==depIfo:
         depQscanTFile.write(repr(trig.gpsTime[ifo])+'\n')
	 
      #Lets append a new subsection to the last section
      dag.appendSubSection(str(ifo)+' @ GPS '+str(trig.gpsTime[ifo]))

      # SETUP DATAFIND JOBS FOR HOFT QCANS
      dHoftNode[ifo] = followupDataFindNode(dataJob,'futrig','q-hoft-datafind',cp,trig.gpsTime[ifo],ifo,opts,dag,prev_dNode,'hoft_datafind')
      if dHoftNode[ifo].validNode: prev_dNode = dHoftNode[ifo]

      # SETUP DATAFIND JOBS FOR REGULAR QSCANS
      dNode = followupDataFindNode(dataJob,'futrig','q-datafind',cp,trig.gpsTime[ifo],ifo,opts,dag,prev_dNode,'datafind')
      if dNode.validNode: prev_dNode = dNode
    
      # SETUP FOREGROUND QSCAN JOBS
      qFgNode = qscanNode(qscanFgJob,trig.gpsTime[ifo],cp,dHoftNode[ifo].outputFileName,ifo,'foreground-hoft-qscan',opts,dHoftNode[ifo],dag,'hoft_datafind','hoft_qscan')
      qFgNode = qscanNode(qscanFgJob,trig.gpsTime[ifo],cp,dNode.outputFileName,ifo,'foreground-qscan',opts,dNode,dag,'datafind','qscan')
      qFgNode = qscanNode(qscanFgJob,trig.gpsTime[ifo],cp,dNode.outputFileName,ifo,'foreground-seismic-qscan',opts,dNode,dag,'datafind','seis_qscan')

      # SETUP DATAFIND JOBS FOR INSPIRAL, FRAME CHECKS, AND MCMC JOBS (REQUIRED 
      # ONLY IF THE INSPIRAL_HIPE CACHE FILES ARE OBSOLETE) 
      inspiralDataFindNode = followupDataFindNode(inspDataJob,'inspiral','inspiral-datafind',cp,trig.gpsTime[ifo],ifo,opts,dag,prev_dNode,'inspiral_datafind',inspiral_process_params[ifo])
      if inspiralDataFindNode.validNode: prev_dNode = inspiralDataFindNode

      # SETUP INSPIRAL JOBS
      inspiralNode = followUpInspNode(inspJob,inspiral_process_params[ifo], ifo, trig, cp, opts, dag, inspiralDataFindNode.outputFileName,inspiralDataFindNode,'inspiral_datafind')
      cohInspNode = followUpInspNode(cohInspJob,inspiral_process_params[ifo], ifo, trig, cp, opts, dag, inspiralDataFindNode.outputFileName,inspiralDataFindNode,'inspiral_datafind',type='coh')
      #skyNode.append_insp_node(inspiralNode,ifo)
      skyNode.append_insp_node(cohInspNode,ifo)      
      headInspiralNode = followUpInspNode(headInspJob,inspiral_process_params[ifo], ifo, trig, cp, opts, dag, inspiralDataFindNode.outputFileName,inspiralDataFindNode,'inspiral_datafind', 'head', bank)


      # SETUP PLOT JOBS
      plotNode = plotSNRCHISQNode(plotJob,ifo,inspiralNode.output_file_name,trig,page,dag,inspiralNode,opts)

      # SETUP FRAME CHECK JOBS
      frcheckNode = FrCheckNode(frcheckJob,inspiral_process_params[ifo], ifo, trig, cp, opts, dag, inspiralDataFindNode.outputFileName, inspiralDataFindNode, 'inspiral_datafind')

      # MCMC JOBS
      # run multiple MCMC chains for each trigger (1 chain = 1 node)
      chainNumber = string.strip(cp.get('mcmc','chain_nb'))
      mcmcIdList = []
      for k in range(int(chainNumber)):
        randomseed = [str(trig.gpsTime[ifo] + k).split('.')[0][5:9],str(trig.gpsTime[ifo]).split('.')[1]]
        MCMCNode = followupmcmcNode(MCMCJob,inspiral_process_params[ifo], ifo, trig, randomseed, cp, opts, dag)
        mcmcIdList.append(MCMCNode.id)

      # PLOT MCMC JOBS
      PLOTMCMCNode = plotmcmcNode(PLOTMCMCJob,ifo,trig,mcmcIdList,cp,opts,dag)

      # SETUP STATUS SUMMARY PLOT JOBS
      statusNode = IFOstatus_checkNode(statusJob, ifo, trig, cp, opts, dag)


    # SETUP H1H2 SPECIFIC JOBS
    # first make sure that the coincident trigger is found in at least one Hanford ifo and that both Hanford ifos are in the analysed times
    lho_flag = trig.ifolist_in_coinc.count('H1') + trig.ifolist_in_coinc.count('H2')
    if lho_flag == 2:
      #Lets append a new subsection to the last section
      dag.appendSubSection('H1H2 followup' +' @ GPS '+str(trig.gpsTime['H1']))
      h1h2Qevent = h1h2QeventNode(h1h2QJob,dHoftNode,trig.gpsTime,['H1','H2'],'qevent',cp,opts,dag,'H1H2_qevent')


    # LOOP OVER IFOS NOT FOUND IN COINCIDENCE (THOUGH IN THE ANALYSED TIMES)
    for j in range(0,len(trig.ifoTag)-1,2):
      ifo = trig.ifoTag[j:j+2]
      if not trig.ifolist_in_coinc.count(ifo):

        #Lets append a new subsection to the last section
        dag.appendSubSection(str(ifo)+" (non coincident ifo)")

        # SETUP DATAFIND JOBS FOR HOFT QCANS
        dHoftNode = followupDataFindNode(dataJob,'futrig','q-hoft-datafind',cp,trig.gpsTime[trig.ifolist_in_coinc[0]],ifo,opts,dag,prev_dNode,'hoft_datafind')
        if dHoftNode.validNode: prev_dNode = dHoftNode

        # SETUP DATAFIND JOBS FOR REGULAR QCANS
        dNode = followupDataFindNode(dataJob,'futrig','q-datafind',cp,trig.gpsTime[trig.ifolist_in_coinc[0]],ifo,opts,dag,prev_dNode,'datafind')
        if dNode.validNode: prev_dNode = dNode

        # SETUP FOREGROUND QSCAN JOBS
        qFgNode = qscanNode(qscanFgJob,trig.gpsTime[trig.ifolist_in_coinc[0]],cp,dHoftNode.outputFileName,ifo,'foreground-hoft-qscan',opts,dHoftNode,dag,'hoft_datafind','hoft_qscan')
        qFgNode = qscanNode(qscanFgJob,trig.gpsTime[trig.ifolist_in_coinc[0]],cp,dNode.outputFileName,ifo,'foreground-qscan',opts,dNode,dag,'datafind','qscan')

        # SETUP DATAFIND JOBS FOR INSPIRAL JOBS 
        # (REQUIRED ONLY IF THE INSPIRAL_HIPE CACHE FILES ARE OBSOLETE)
        inspiralDataFindNode = followupDataFindNode(inspDataJob,'inspiral','inspiral-datafind',cp,trig.gpsTime[trig.ifolist_in_coinc[0]],ifo,opts,dag,prev_dNode,'inspiral_datafind',inspiral_process_params[ifo])
        if inspiralDataFindNode.validNode: prev_dNode = inspiralDataFindNode

        # generate the snr/chisq time series for each triggered template
        for itf in trig.ifolist_in_coinc:
          # SETUP INSPIRAL JOBS
          inspiralNode = followUpInspNode(inspJobNotrig,inspiral_process_params[ifo], itf, trig, cp, opts, dag, inspiralDataFindNode.outputFileName,inspiralDataFindNode,'inspiral_datafind','notrig',None)
          # Setup sky map node, this will cause a warning because it will try
          # to add multiple ifos at once.  What we really want is the same 
          # template filtered through all ifos.  This needs to be fixed.  For
          # now the skymap is wedged in to test the plumbing.
          #skyMapNode.append(inspiralNode,ifo)
          # SETUP PLOT JOBS
          plotNode = plotSNRCHISQNode(plotJob,ifo,inspiralNode.output_file_name,trig,page,dag,inspiralNode,opts,itf)

    # SOURCE LOCALIZATION.  This requires the complex frame data from the above
    # inspiral jobs for all ifos that are on.  This should be replaced with the
    # output of the coherent inspiral stages which use the same template
    # Set up a sky map node.  These take inputs from all the triggers and
    # must appear outside the loop over ifos.  But we need the output file 
    # names from several inspiral jobs since they are only run over one ifo
    # we could walk the graph and find this (maybe) but instead I'll just track
    # the names we need. 
    skyNode.add_node_to_dag(dag,opts,trig)
    skyPlotNode = pylal_skyPlotNode(skyPlotJob,trig,skyNode,dag,page,opts)
  


  # Prepare for moving the deported qscan directory (tar-gzip)
  depQscanTFile.close()
  os.system('tar zcvf '+depIfoDir+'.tar.gz '+depIfoDir)

  # Distribute the results of the deported qscans to the expected output directories
  distribQNode = distributeQscanNode(distribQJob,'QSCAN/QSCAN.cache','QSCANLITE/QSCANLITE.cache',depIfo,depIfo+"_qscans_results.tar.gz",opts,dag)
  
  # Loop for analyseQscan jobs (which must be children of all qscan jobs)
  for trig in followuptrigs:
    for ifo in trig.ifolist_in_coinc:
      anaQscanNode = analyseQscanNode(anaQscanJob,trig.gpsTime[ifo],ifo,'foreground-qscan','QSCAN/QSCAN.cache','QSCANLITE/QSCANLITE.cache',cp,opts,dag,'analyse_qscan')
      anaQscanNode = analyseQscanNode(anaQscanJob,trig.gpsTime[ifo],ifo,'foreground-seismic-qscan','QSCAN/QSCAN.cache','QSCANLITE/QSCANLITE.cache',cp,opts,dag,'analyse_seis_qscan')
      anaQscanNode = analyseQscanNode(anaQscanJob,trig.gpsTime[ifo],ifo,'foreground-hoft-qscan','QSCAN/QSCAN.cache','QSCANLITE/QSCANLITE.cache',cp,opts,dag,'analyse_hoft_qscan')
    
##########################################################################
dag.writeAll()
if opts.write_to_iulgroup:
  dag.publishToHydra(publish_path)

sys.exit(0)
