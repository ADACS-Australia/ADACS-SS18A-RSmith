#!/usr/bin/env @PYTHONPROG@
"""
Something

$Id$

This program creates cache files for the output of inspiral hipe
"""

__author__ = 'Chad Hanna <channa@phys.lsu.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import sys, os, copy, math
import socket, time
import re, string
from optparse import *
import tempfile
import ConfigParser
import urlparse
from UserDict import UserDict
sys.path.append('@PYTHONLIBDIR@')

##############################################################################
# import the modules we need to build the pipeline
from glue import pipeline
from glue import lal
from glue import segments 
from glue import segmentsUtils
from glue.ligolw import ligolw
from glue.ligolw import table
from glue.ligolw import lsctables
from glue.ligolw import utils
from pylal import CoincInspiralUtils
from pylal.fu_utils import *
from pylal.fu_writeXMLparams import * 
from pylal import Fr      

##############################################################################
# redefine the columns of interest
##############################################################################
lsctables.SearchSummaryTable.loadcolumns = ["ifos","in_start_time","in_start_time_ns","in_end_time","in_end_time_ns","out_start_time","out_start_time_ns","out_end_time","out_end_time_ns"]

lsctables.SimInspiralTable.loadcolumns = ["waveform","geocent_end_time",
    "geocent_end_time_ns","h_end_time","h_end_time_ns","l_end_time",
    "l_end_time_ns","source","mass1","mass2","mchirp","eta","distance",
    "spin1x","spin1y","spin1z","spin2x","spin2y","spin2z","eff_dist_h",
    "eff_dist_l","eff_dist_g","eff_dist_t","eff_dist_v"]

#lsctables.SnglInspiralTable.loadcolumns = ["ifo","end_time","end_time_ns",
#    "eff_distance","mass1","mass2","mchirp","eta","snr","chisq","chisq_dof",
#    "sigmasq","event_id"]

################ TRIG BANK FROM SIRE FILE CONDOR DAG JOB ######################

class trigBankFollowUpJob(pipeline.CondorDAGJob):
  """
  A followup trig bank job
  """
  def __init__(self, options, cp, tag_base='TRIGBANK_FOLLOWUP'):
    """
    """
    self.__executable = string.strip(cp.get('condor','trigtotmplt'))
    self.__universe = "standard"
    pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
    self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")

    self.set_stdout_file('logs/futrigbank-$(macrogpsstarttime)-$(cluster)-$(process).out')
    self.set_stderr_file('logs/futrigbank-$(macrogpsstarttime)-$(cluster)-$(process).err')
    self.set_sub_file('futrigbank.sub')


class trigBankFollowUpNode(pipeline.CondorDAGNode):
  """
  Runs an instance of a trig bank followup job
  """
  def __init__(self,job,time,proc,opts,xml_glob):
    """
    job = A CondorDAGJob that can run an instance of trigbank followup.
    """
    pipeline.CondorDAGNode.__init__(self,job)
    self.output_file_name = ""
    self.output_user_tag = "FOLLOWUP_" + str(time)

    for row in proc:
      self.add_var_opt(row.param[2:],row.value)
      if row.param[2:] == 'gps-start-time':
        pass
        #self.add_macro("macrogpsstarttime",row.value)
        #self.start_time = row.value
      if row.param[2:] == 'input-ifo': 
        self.input_ifo = row.value
      if row.param[2:] == 'ifo-tag':
        self.ifo_tag = row.value
      if row.param[2:] == 'gps-end-time':
        pass
        #self.end_time = row.value
        
    # be careful that the clustering time must be larger than 2 s, we should fix this.
    self.start_time = str( eval(str(time)) - 1 )
    self.end_time = str( eval(str(time)) + 1 )
    self.add_var_opt("gps-start-time",self.start_time)
    self.add_var_opt("gps-end-time",self.end_time)
    self.add_macro("macrogpsstarttime",self.start_time)
    
    self.add_var_opt("user-tag",self.output_user_tag)
    fStr = os.popen('ls ' + xml_glob).readlines() 
    for inFile in fStr:
      self.add_file_arg(string.strip(inFile))
    self.output_file_name = self.input_ifo + "-TRIGBANK_" + self.ifo_tag + \
      "_" + self.output_user_tag + "-" + str(int(eval(self.start_time))) + "-"\
      + "2" + ".xml"

class inspiralFollowUpJob(pipeline.CondorDAGJob):
  """
  A followup inspiral job
  """
  def __init__(self, options, cp, tag_base='INSPIRAL_FOLLOWUP'):
    """
    """
    self.__executable = string.strip(cp.get('condor','inspiral'))
    self.__universe = "standard"
    pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
    self.tag_base = tag_base
    self.options = options
    self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")

    self.set_stdout_file('logs/fuinspiral-$(macrogpsstarttime)-$(cluster)-$(process).out')
    self.set_stderr_file('logs/fuinspiral-$(macrogpsstarttime)-$(cluster)-$(process).err')
    self.set_sub_file('fuinspiral.sub')


class inspiralFollowUpNode(pipeline.CondorDAGNode):
  """
  Runs an instance of an inspiral followup job
  """
  def __init__(self,job,time,proc,opts,bankFile):
    """
    job = A CondorDAGJob that can run an instance of inspiral followup.
    """
    pipeline.CondorDAGNode.__init__(self,job)
    self.output_file_name = ""
    self.output_user_tag = "FOLLOWUP_" + str(time)
    self.add_rsq_veto = 1
    for row in proc:
      if row.param[2:] == 'minimal-match':
        continue
      if row.param[2:] == 'bank-file':
        continue
      if row.param[2:] == 'trig-start-time':
        continue
      if row.param[2:] == 'trig-end-time':
        continue
      if (row.param[2:] == 'injection-file') and ( len(string.strip(cp.get('triggers','injection-file'))) > 0 ):
        row.value = string.strip(cp.get('triggers','injection-file'))
      self.add_var_opt(row.param[2:],row.value)
      if row.param[2:] == 'gps-start-time':
        self.add_macro("macrogpsstarttime",row.value)
        self.start_time = row.value
      if row.param[2:] == 'channel-name':
        self.input_ifo = row.value[0:2]
      if row.param[2:] == 'ifo-tag':
        self.ifo_tag = row.value
      if row.param[2:] == 'gps-end-time':
        self.end_time = row.value
      if row.param[2:] == 'enable-rsq-veto':
        self.add_rsq_veto = 0
      if row.param[2:] == 'disable-rsq-veto':
        self.add_rsq_veto = 0
    self.add_var_opt("user-tag",self.output_user_tag)
    self.add_var_opt("write-snrsq","")
    self.add_var_opt("write-chisq","")
    self.add_var_opt("write-spectrum","")
    self.add_var_opt("bank-file",bankFile)
    self.add_var_opt("trig-start-time", str( eval(str(time))-1.0) )
    self.add_var_opt("trig-end-time", str( eval(str(time))+1.0) )
    if self.add_rsq_veto:
      self.add_var_opt("enable-rsq-veto","")
    self.output_file_name = self.input_ifo + "-INSPIRAL_" + self.ifo_tag + \
      "_" + self.output_user_tag + "-" + self.start_time + "-" +\
      str(eval(self.end_time)-eval(self.start_time)) + ".xml"
    
class plotSNRCHISQJob(pipeline.CondorDAGJob):
  """
  A followup plotting job for snr and chisq time series 
  """
  def __init__(self, options, cp, tag_base='PLOT_FOLLOWUP'):
    """
    """
    self.__executable = string.strip(cp.get('condor','plotsnrchisq'))
    self.__universe = "vanilla"
    pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
    self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")
    self.add_condor_cmd('getenv','True')
    self.set_stdout_file('logs/fuplotsnrchisq-$(macrogpsstarttime)-$(cluster)-$(process).out')
    self.set_stderr_file('logs/fuplotsnrchisq-$(macrogpsstarttime)-$(cluster)-$(process).err')
    self.set_sub_file('fuplotsnrchisq.sub')


class plotSNRCHISQNode(pipeline.CondorDAGNode):
  """
  Runs an instance of a plotSNRCHISQ followup job
  """
  def __init__(self,job,time,fileName,trig,page,plotFlag):
    """
    job = A CondorDAGJob that can run an instance of plotSNRCHISQ followup.
    """
    pipeline.CondorDAGNode.__init__(self,job)
    self.output_file_name = ""
    self.add_var_opt("frame-file",fileName.replace(".xml",".gwf"))
    self.add_var_opt("gps",time)
    self.add_var_opt("inspiral-xml-file",fileName)
    self.container = HTMLcontainer(trig,"SNR_and_CHISQ_plots")
    self.container2 = HTMLcontainer(trig,"Calibrated_Spectrum")
    self.add_var_opt("output-html-file",self.container.locallink)
    self.add_var_opt("output-path",self.container.localdetailpath)
    self.add_var_opt("image-file", self.container.localimage)
    self.add_var_opt("output-html-file2",self.container2.locallink)
    self.add_var_opt("output-path2",self.container2.localdetailpath)
    self.add_var_opt("image-file2", self.container2.localimage)
    self.add_var_opt("page", page)
    if plotFlag: self.initializeHTML()
#    tableFile = open(self.container.locallink,'w')
#    writeIULHeader(tableFile)
#    tableFile.write('<h3>Follow up of trigger [' +str(trig.eventID) +']</h3>\n')
    self.container.text = "click here for SNR and CHISQ plots"
    self.container2.text = "click here for calibrated spectra"
#    tableFile.close()
  def getContainer(self):
    return self.container
  def getContainer2(self):
    return self.container2
  def initializeHTML(self):
    tableFile = open(self.container.locallink,'w')
    writeIULHeader(tableFile)
    tableFile.write('<h3>Follow up of trigger [' +str(trig.eventID) +']</h3>\n')
    tableFile.close()
    tableFile2 = open(self.container2.locallink,'w')
    writeIULHeader(tableFile2)
    tableFile2.write('<h3>Follow up of trigger [' +str(trig.eventID) +']</h3>\n')
    tableFile2.close()


##############################################################################
# qconfigureJob class for hoft qscan

class qconfigureJob(pipeline.CondorDAGJob):
  """
  A qconfigure job
  """
  def __init__(self, options, cp, tag_base='QCONFIGURE_FOLLOWUP'):
    """
    """
    self.__executable = string.strip(cp.get('condor','qconfigure'))
    self.__universe = "vanilla"
    pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
    self.tag_base = tag_base
    self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")
    #self.add_condor_cmd('getenv','True')
    self.set_stdout_file('logs/fuqconfigure-$(cluster)-$(process).out')
    self.set_stderr_file('logs/fuqconfigure-$(cluster)-$(process).err')
    self.set_sub_file('fuqconfigure.sub')

##############################################################################
# qconfigureNode (for hoft qscans)

#class qconfigureNode(pipeline.CondorDAGNode):
#  """
#  Runs an instance of a qconfigure job
#  """
#  def __init__(self,job,inputFile,ifo):
#    """
#    job = A CondorDAGJob that can run an instance of qconfigureNode.
#    """
#    pipeline.CondorDAGNode.__init__(self,job)
#    configuration = '$(ifo)_hoft.txt'
#    self.add_var_arg(configuration)
#    self.add_var_arg(inputFile)

##############################################################################
# convertCacheJob class for convertlalcache jobs (for regular qscan)

class convertCacheJob(pipeline.CondorDAGJob):
  """
  A convertlalcache job
  """
  def __init__(self, options, cp, tag_base='CVCACHE_FOLLOWUP'):
    """
    """
    self.__executable = string.strip(cp.get('condor','convertcache'))
    self.__universe = "vanilla"
    pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
    self.tag_base = tag_base
    self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")
    self.set_stdout_file('logs/fuconvertcache-$(macrogpsstarttime)-$(cluster)-$(process).out')
    self.set_stderr_file('logs/fuconvertcache-$(macrogpsstarttime)-$(cluster)-$(process).err')
    self.set_sub_file('fuconvertcache.sub')

##############################################################################
# convertCacheNode class for convertlalcache Node (for regular qscans)

class convertCacheNode(pipeline.CondorDAGNode):
  """
  Runs an instance of a convertlalcache job
  """
  def __init__(self,job):
    """
    job = A CondorDAGJob that can run an instance of convertCacheNode.
    datafindNode = the datafind node whose output we want to convert
    """
    self.__output = None
    pipeline.CondorDAGNode.__init__(self,job)

  def set_output(self,lalcache):
    self.add_file_arg(lalcache)
    qcache = lalcache.rstrip("cache") +  "qcache"
    self.add_file_arg(qcache)
    self.__output = qcache

  def get_output(self):
    return self.__output

##############################################################################
# qscan class for qscan jobs

class qscanJob(pipeline.CondorDAGJob):
  """
  A qscan job
  """
  def __init__(self, opts, cp, tag_base='QSCAN'):
    """
    """
    self.__executable = string.strip(cp.get('condor','qscan'))
    self.__universe = "vanilla"
    pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
    self.tag_base = tag_base
    self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")
    self.set_stdout_file('logs/qscan-$(macrogpsstarttime)-$(cluster)-$(process).out')     
    self.set_stderr_file('logs/qscan-$(macrogpsstarttime)-$(cluster)-$(process).err')
    self.set_sub_file('qscan.sub')

##############################################################################
# qscan class for qscan Node

class qscanNode(pipeline.CondorDAGNode):
  """
  Runs an instance of a qscan job
  """
  def __init__(self,job,time,cp,trig,qcache,ifo,name,qFlag):
    """
    job = A CondorDAGJob that can run an instance of qscan.
    """
    pipeline.CondorDAGNode.__init__(self,job)
    self.add_var_arg(repr(time))
    self.container = HTMLcontainer(trig,name)
    if qFlag: self.initializeHTML(trig)
    tableFile = open(self.container.locallink,'a')
    if name == 'HOFT_QSCAN':
      sectionName = 'qscan-hoft'
      qscanConfig = string.strip(cp.get(sectionName, ifo + 'config-file'))
    if name == 'SEIS_QSCAN':
      sectionName = 'qscan-seismic'
      qscanConfig = string.strip(cp.get(sectionName, ifo[0] + 'config-file'))
    if name == 'QSCAN':
      sectionName = 'qscan'
      qscanConfig = string.strip(cp.get(sectionName, ifo[0] + 'config-file'))
    self.add_file_arg(qscanConfig)
    self.add_file_arg(qcache)
    if (ifo == 'H1') or (ifo == 'H2'):
      self.add_var_arg(string.strip(cp.get(sectionName,'Houtput')))
      tableFile.write('<a href="' + string.strip(cp.get(sectionName,'Hweb')) + \
                    str(time) + '/">' + str(ifo) + \
                    ' QScan of trigger [' +str(trig.eventID) +']</a><br>\n')
    if (ifo == 'L1'):
      self.add_var_arg(string.strip(cp.get(sectionName,'Loutput')))
      tableFile.write('<a href="' + string.strip(cp.get(sectionName,'Lweb')) + \
                    str(time) + '/">' + str(ifo) + \
                    ' QScan of trigger [' +str(trig.eventID) +']</a><br>\n')
#   self.add_var_arg(dir+"/"+self.container.name+"/")
#   writeIULHeader(tableFile)
    self.container.text = "click here for QScans"
    tableFile.close()

  def getContainer(self):
    return self.container
  def initializeHTML(self,trig):
    tableFile = open(self.container.locallink,'w')
    writeIULHeader(tableFile)   
    tableFile.write('<h3>Follow up of trigger [' +str(trig.eventID) +']</h3>\n')
    tableFile.close()

class segJob(pipeline.CondorDAGJob):
  """
  A segwizard job
  """
  def __init__(self, opts, cp, tag_base='SEG'):
    """
    """
    self.__executable = string.strip(cp.get('condor','dataqual'))
    self.__universe = "local"
    pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
    self.tag_base = tag_base
    self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")
    self.add_condor_cmd('getenv','True')
    self.set_stdout_file('followuptrigs/seg-$(macrogpsstarttime).out')
    self.set_stderr_file('logs/seg-$(macrogpsstarttime)-$(cluster)-$(process).err')
    self.set_sub_file('seg.sub')
  #  self.add_condor_cmd("should_transfer_files","YES")
  #  self.add_condor_cmd("when_to_transfer_output","ON_EXIT")
  #  self.add_condor_cmd("transfer_input_files",string.strip(cp.get('seg','tclshexe')) )

class segNode(pipeline.CondorDAGNode):
  """
  Runs an instance of a qscan job
  """
  def __init__(self,job,time,cp,dir,trig,ifo,sFlag):
    """
    job = A CondorDAGJob that can run an instance of qscan.
    """
    pipeline.CondorDAGNode.__init__(self,job)
    scirun = string.strip(cp.get('hipe-cache','science-run'))
    self.add_var_opt('run', scirun )
    self.add_var_opt('interferometer', str(ifo).lower())
    window =  string.strip(cp.get('seg','window'))
    self.add_var_opt('gps-end-time', str( int(eval(str(time))) + int(eval(window)) ))
    self.add_var_opt('gps-start-time', str( int(eval(str(time))) - int(eval(window)) ))
    self.add_macro("macrogpsstarttime",str(time))
    self.container = HTMLcontainer(trig,"DATA_QUALITY")
    if sFlag: self.initializeHTML(trig)
    tableFile = open(self.container.locallink,'a')
    tableFile.write('<a href="http://www.lsc-group.phys.uwm.edu/iulgroup/' + \
              string.strip(cp.get("output","page")) + "/followuptrigs/" + \
              'seg-' + str(time) + '.out">' + str(ifo) + \
              ' Data Quality of trigger [' +str(trig.eventID) +']</a><br>\n' )
    self.container.text = "click here for Data Quality"
    tableFile.close()

  def getContainer(self):
    return self.container
  def initializeHTML(self,trig):
    tableFile = open(self.container.locallink,'w')
    writeIULHeader(tableFile)
    tableFile.write('<h3>Follow up of trigger [' +str(trig.eventID) +']</h3>\n')


##############################################################################
#
#  MAIN PROGRAM
#
##############################################################################

######################## OPTION PARSING  #####################################
usage = """usage: %prog [options]
"""

parser = OptionParser( usage )

parser.add_option("-v", "--version",action="store_true",default=False,\
    help="print version information and exit")

parser.add_option("-l", "--log-path",action="store",type="string",\
    metavar=" PATH",help="directory to write condor log file")

parser.add_option("-f", "--config-file",action="store",type="string",\
    metavar=" FILE",help="ini file")

parser.add_option("-g", "--generate-cache",action="store_true",\
    default=False, help="write LAL cache")

parser.add_option("-m", "--datafind",action="store_true",\
    default=False, help="use datafind to get qscan/trends data")

#parser.add_option("-r","--read-triggers",action="store_true",default=False,\
#    help="Read the followup trigger XML (COIRE FILE)" )

parser.add_option("-q", "--qscan",action="store_true",default=False,\
    help="do qscans")

parser.add_option("-H", "--hoft-qscan",action="store_true",\
    default=False, help="do hoft qscans")

parser.add_option("-s", "--seis-qscan",action="store_true",\
    default=False, help="do seismic qscans")

parser.add_option("-d", "--data-quality",action="store_true",default=False,\
    help="do data quality lookup - CURRENTLY BROKEN, DO NOT USE IT")

parser.add_option("-t", "--trig-bank",action="store_true",default=False,\
    help="generate a pseudo trigbank xml file for single triggers")

parser.add_option("-i", "--inspiral",action="store_true",default=False,\
    help="do inspirals for single triggers - output SNR/CHISQ/PSD")

parser.add_option("-p", "--plots",action="store_true",default=False,\
    help="plot SNR/CHISQ from inspiral stage")

parser.add_option("-w", "--write-to-iulgroup",action="store_true", \
    default=False, help="publish the page to the iulgroup")

#parser.add_option("-c", "--cache-path",action="store",type="string",\
#    default="./.",metavar=" PATH",help="directory to find  all hipe XML files")

#parser.add_option("-s", "--science-run", action="store",type="string",\
#    default="S5", metavar=" RUN", help="name of science run")

command_line = sys.argv[1:]
(opts,args) = parser.parse_args()

if opts.version:
  print "$Id$"
  sys.exit(0)

####################### SANITY CHECKS #####################################

if not opts.config_file:
  print >> sys.stderr, "No configuration file specified."
  print >> sys.stderr, "Use --config-file FILE to specify location" 
  sys.exit(1)

if not opts.log_path and not opts.write_to_iulgroup:
  print >> sys.stderr, "No log file path specified"
  print >> sys.stderr, "Use --log-path PATH to specify a location"
  sys.exit(1)

if not opts.write_to_iulgroup and not opts.generate_cache and \
  not opts.datafind and not opts.qscan \
  and not opts.trig_bank and not opts.inspiral and not opts.plots and not \
  opts.hoft_qscan and not opts.seis_qscan:
  print >> sys.stderr, "No steps of the pipeline specified."
  print >> sys.stderr, "Please specify at least one of"
  print >> sys.stderr, "--generate-cache, --trig-bank, --inspiral, --plots,"
  print >> sys.stderr, "--datafind, --qscan, --hoft-qscan, --seis-qscan"
  print >> sys.stderr, "or --write-to-iulgroup (use this option alone for now)"  
  sys.exit(1)


#################### READ IN THE CONFIG (.ini) FILE ########################
cp = ConfigParser.ConfigParser()
cp.read(opts.config_file)

## set up the local directories
setupdirs()

############# TURN THE HIPE OUTPUT INTO LAL CACHE FILES #######################

cache = getCache(opts)
if opts.generate_cache:
  cache.getCacheAll(cp)
  cache.writeCacheAll()
  print >> sys.stderr, "\nHIPE CACHE FILES WRITTEN TO:"
  for n, t in cache.nameMaps:
    print >> sys.stderr, " * " + n + " [" + str(len(cache[t])) + "]"

  command = "ln -s " + string.strip(cp.get('hipe-cache','hipe-cache-path')) + \
          " cache"
  # link datafind for inspiral jobs...
  try: os.system(command)
  except: pass
else:
  print >> sys.stderr, "\nOption --generate-cache NOT specified, it will be assumed that the hipe cache files already exists... "

##############################################################################
# create a log file that the Condor jobs will write to
basename = re.sub(r'\.ini',r'',opts.config_file)
tempfile.tempdir = opts.log_path
tempfile.template = basename + '.dag.log.'
logfile = tempfile.mktemp()
fh = open( logfile, "w" )
fh.close()

##############################################################################
# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
dag.set_dag_file( basename )
subsuffix = '.sub'

##############################################################################
#Decide if we are actually reading any trigger files
page = string.strip(cp.get('output','page'))

#if not opts.read_triggers and not opts.write_to_iulgroup:
#  sys.exit(0)
  # We are done here

#if not opts.read_triggers and opts.write_to_iulgroup:
if opts.write_to_iulgroup:
  publishOnHydra(page)
  # don't overwrite anything, just publish what we have
  sys.exit(0)

# Since we are continuing get useful stuff from the ini file.
xml_glob = string.strip(cp.get('triggers','xml-glob'))
numtrigs = string.strip(cp.get('triggers','num-trigs'))
statistic =  string.strip(cp.get('triggers','statistic'))
bla =  string.strip(cp.get('triggers','bitten-l-a'))
blb =  string.strip(cp.get('triggers','bitten-l-b'))
 
############# READ IN THE COIRE FILES #########################################

found, coincs, search = readFiles(xml_glob,getstatistic(statistic,bla,blb))
missed = None

if opts.trig_bank: trigbank_test = 1
else: trigbank_test = 0  
followuptrigs = getfollowuptrigs(numtrigs,page,coincs,missed,search,trigbank_test)

print "\n.......Found " + str(len(followuptrigs)) + " trigs to follow up" 

################ DO DATA FINDING ##############################################
#if opts.datafind:
#  times = []
#  for trigs in followuptrigs:
#    time = 0
#    try: time = trigs.gpsTime['H1']
#    except: pass
#    try: time = trigs.gpsTime['H2']
#    except: pass
#    try: time = trigs.gpsTime['L1']
#    except: pass
#    if time: times.append(time)
#  times.sort()
  # 1s is substracted to the expected startTime to make sure the window will be large enough. This is to be sure to handle the rouding to the next sample.
#  startTime = int( times[0] - eval(string.strip(cp.get('datafind','qscan-searchTimeRange')))/2 ) - 1
#  endTime =  int( times[-1] + eval(string.strip(cp.get('datafind','qscan-searchTimeRange')))/2 ) + 2
#  command = string.strip(cp.get('condor','datafind')) + \
#          " --observatory=L" + " --type=" + \
#          string.strip(cp.get('datafind','qscan-level')) + \
#          " --gps-start-time=" + str(startTime) + " --gps-end-time=" + \
#          str(endTime) + " --url-type=file" + " --lal-cache > Lq.cache.tmp"

#  print "....Finding Qscan LLO data ["+str(startTime)+"-"+str(endTime)+"]\n"
#  os.system(command)
#  Lq = open('Lq.cache.tmp','r')
#  lines = Lq.readlines()
#  Lqout = open('Lq.cache','w')
#  for line in lines:
#    total = line.replace("file://localhost","").split("/")[0:-1]
#    first = total[0].split()
#    first.insert(-1,str(eval(first[-1]) + eval(first[-2])))
#    total[0] = " ".join(first)
#    total[0] += " "
#    Lqout.write("/".join(total)+'\n')

#  command = string.strip(cp.get('condor','datafind')) + \
#          " --observatory=H" + " --type=" + \
#          string.strip(cp.get('datafind','qscan-level')) + \
#          " --gps-start-time=" + str(startTime) + " --gps-end-time=" + \
#          str(endTime) + " --url-type=file" + " --lal-cache > Hq.cache.tmp" 
#  print "....Finding Qscan LHO data ["+str(startTime)+"-"+str(endTime)+"]\n"
#  os.system(command)
#  Hq = open('Hq.cache.tmp','r')
#  lines = Hq.readlines()
#  Hqout = open('Hq.cache','w')
#  for line in lines:
#    total = line.replace("file://localhost","").split("/")[0:-1]
#    first = total[0].split()
#    first.insert(-1,str(eval(first[-1]) + eval(first[-2])))
#    total[0] = " ".join(first)
#    total[0] += " "
#    Hqout.write("/".join(total)+'\n')


############ SET UP THE REQUESTED JOBS ########################################
# trigJob      = trigBankFollowUpJob(opts,cp)
inspJob        = inspiralFollowUpJob(opts,cp)
plotJob        = plotSNRCHISQJob(opts,cp)
dataJob        = pipeline.LSCDataFindJob('datafind_cache','logs',cp)
#qconfJob       = qconfigureJob(opts,cp) 
cvcacheJob     = convertCacheJob(opts,cp)
qscanJob       = qscanJob(opts,cp)
segJob         = segJob(opts,cp)

# trigJobCnt      = 0
inspJobCnt        = 0
plotJobCnt        = 0
dataJobCnt        = 0
dataHoftJobCnt    = 0
cvcacheJobCnt     = 0
cvcacheHoftJobCnt = 0
#qconfigureJobCnt  = 0
qscanJobCnt       = 0
qscanHoftJobCnt   = 0
qscanSeisJobCnt   = 0
segJobCnt         = 0
summaryHTMLlist = []

prev_dNode = None

# define dictionary for qconfigure jobs
#qconfigureTest = {'H1':1,'H2':1,'L1':1,'G1':1,'V1':1,'T1':1}

print "\n.......Setting up pipeline jobs"

for trig in followuptrigs:
  # Set up HTML structs for web page
  summaryHTML = summaryHTMLTable(trig)
  summaryHTML.containers.append(writeXMLparams(trig))

  print trig.ifoTag
  print trig.gpsTime

  # TRY GETTING INSPIRAL PROCESS PARAMS... currently done even if the inspiral jobs are not requested... should be fixed
  try: 
    inspiral_process_params = cache.getProcessParamsFromCache( \
                     cache.filesMatchingGPSinCache(trig.gpsTime,\
                     "second_inspiral.cache"), \
                     trig.ifoTag, trig.gpsTime)
    #inspiral_process_params = cache.getProcessParamsFromCache( \
    #                 cache.filesMatchingGPS(trig.gpsTime,'INSPIRAL_'), \
    #                 trig.gpsTime)
  except: 
    print "couldn't get inspiral process params for " + str(trig.eventID)
    inspiral_process_params = []

  # flag initialization: for html page purposes...
  if opts.qscan: qFlag = 1
  else: qFlag = 0
  if opts.hoft_qscan: hoftqFlag = 1
  else: hoftqFlag = 0
  if opts.seis_qscan: seisqFlag = 1
  else: seisqFlag = 0
  if opts.plots: plotsFlag = 1
  else: plotsFlag = 0
  if opts.data_quality: sFlag = 1
  else: sFlag = 0

  # Build the list of ifos in which this trigger was found - this is assuming that an ifo name is made of 2 characters, such as "L1"...  
  ifo_list = [trig.ifoList[i:i+2] for i in range(0,len(trig.ifoList),2)]

  # If hoft-qscan is requested, get the cache file
#  if opts.hoft_qscan:
#    try:
#      cachesMatchingGps = cache.filesMatchingGPSinDir(trig.gpsTime[ifo_list[0]],'cache')
#      hoftCache = 'hoft_' + str(trig.gpsTime[ifo_list[0]]) + '.cache.tmp'
#      inputFiles = ''
#      for file in cachesMatchingGps:
#        inputFiles = inputFiles + file + ' '
#        cacheFile = open("cache/" + file,"r")
#        cacheContent = cacheFile.readlines()
        # this is a hack, try to find a way to use the path function !
#        pathForConfigTemp = cacheContent[1].split()[4].strip().split('/')
#        pathForConfigList = pathForConfigTemp[3:len(pathForConfigTemp)]
#        pathForConfig = ''
#        for string in pathForConfigList:
#          pathForConfig = pathForConfig + '/' + string
        # end of the hack
#        inputFiles.append(pathForConfig)

#      os.chdir("cache")
#      os.system('cat ' + inputFiles + ' > ../hoft_qscan_cache/' + hoftCache)
#      os.chdir("..") 
#    except:
#      print "could not get the list of cache files for hoft qscan jobs, for trigger" + str(trig.eventID)

  # loop over ifos
  for ifo in ifo_list:
    try: trig.gpsTime[ifo]
    except: continue

    # SETUP DATA QUALITY JOBS
    if 0:
      try:
        sNode = segNode(segJob,trig.gpsTime[ifo],cp,trig.summarydir,trig,ifo,sFlag)
        if opts.data_quality: 
          dag.add_node(sNode)
          segJobCnt+=1
        sFlag = False
      except:
        print "couldn't add data quality job for " + str(ifo) + "@ "+ str(trig.gpsTime[ifo])
        sNode = 0
    else: sNode = 0

    # SETUP DATAFIND JOBS FOR HOFT QCANS
    if trig.gpsTime[ifo]:

      # 1s is substracted to the expected startTime to make sure the window
      # will be large enough. This is to be sure to handle the rouding to the
      # next sample done by qscan.
      q_time = cp.getint('q-hoft-datafind','search-time-range')/2 
      dNode = pipeline.LSCDataFindNode(dataJob)
      dNode.set_observatory(ifo[0])
      dNode.set_start(int( trig.gpsTime[ifo] - q_time - 1))
      dNode.set_end(int( trig.gpsTime[ifo] + q_time + 1))
      dNode.set_type( cp.get('q-hoft-datafind', ifo + '_type' ))
      if prev_dNode and opts.datafind: 
        dNode.add_parent(prev_dNode)
      if opts.datafind:
        dag.add_node(dNode)
        dataHoftJobCnt+=1
      prev_dNode = dNode
    else: dNode = None

    # SETUP CONVERTLALCACHE JOBS FOR HOFT QSCANS
    if trig.gpsTime[ifo] and dNode:
      cvHoftNode = convertCacheNode(cvcacheJob)
      cvHoftNode.set_output( dNode.get_output() )
      if dNode and opts.datafind: # only add a parent if it exists
        cvHoftNode.add_parent(dNode)
      if opts.datafind:
        dag.add_node(cvHoftNode)
        cvcacheHoftJobCnt+=1
    else: cvHoftNode = 0

    # SETUP QCONFIGURE JOBS FOR HOFT QSCANS
#    if qconfigureTest[ifo]: # set up only one job per ifo 
#      if trig.gpsTime[ifo] and dHoftNode:
#        try:
#
#          cacheFile = open("datafind_cache/" + ifo[0:1] + "-" + df_type + "-" + str(df_start_time) + "-" + str(df_end_time) + ".cache.tmp","r")
#          print "the cache file has been opened"
#          cacheContent = cacheFile.readlines()
          # this is a hack, try to find a way to use the path function !
#          pathForConfigTemp = cacheContent[0].split()[4].strip().split('/')
#          pathForConfigList = pathForConfigTemp[3:len(pathForConfigTemp)]
#          inputFile = ''
#          for string in pathForConfigList:
#            inputFile = pathForConfig + '/' + string
          # end of the hack
#          qconfNode = qconfigureNode(qconfJob,inputFile,ifo)
#          qconfigureTest[ifo] = 0

#         if dHoftNode and opts.datafind: # only add a parent if it exists
#            qconfNode.add_parent(dHoftNode)
#          if hoft_qscan:
#            dag.add_node(qconfNode)
#            qconfigureJobCnt+=1
#        except:
#          print "couldn't add qconfigure job for hoft qscan  @ "+ str(trig.gpsTime[ifo])
#          qconfNode = 0
#      else: qconfNode = 0

    # SETUP HOFT QSCAN JOBS
    if trig.gpsTime[ifo] and cvHoftNode:
      try:
        qHoftNode = qscanNode(qscanJob,trig.gpsTime[ifo],cp,trig,
            cvHoftNode.get_output(),ifo,"HOFT_QSCAN",hoftqFlag)
        if cvHoftNode and opts.datafind: # only add a parent if it exists
          qHoftNode.add_parent(cvHoftNode)
        if opts.hoft_qscan:
          dag.add_node(qHoftNode)
          qscanHoftJobCnt+=1
        hoftqFlag = False
        #summaryHTML.containers.append( qNode.getContainer() )
      except:
        print "couldn't add hoft qscan job for " + str(ifo) + "@ "+ str(trig.gpsTime[ifo])
        qHoftNode = 0
    else: qHoftNode = 0


    # SETUP DATAFIND JOBS FOR REGULAR QCANS
    if trig.gpsTime[ifo]:


      # 1s is substracted to the expected startTime to make sure the window
      # will be large enough. This is to be sure to handle the rouding to the
      # next sample done by qscan.
      q_time = cp.getint('q-datafind','search-time-range')/2 

      dNode = pipeline.LSCDataFindNode(dataJob)
      dNode.set_observatory(ifo[0])
      dNode.set_start(int( trig.gpsTime[ifo] - q_time - 1))
      dNode.set_end(int( trig.gpsTime[ifo] + q_time + 1))
      dNode.set_type( cp.get('q-datafind','type' ))
      if prev_dNode and opts.datafind: 
        dNode.add_parent(prev_dNode)
      if opts.datafind:
        dag.add_node(dNode)
        dataJobCnt+=1
      prev_dNode = dNode
    else: dNode = None 
      
    # SETUP CONVERTLALCACHE JOBS
    if trig.gpsTime[ifo] and dNode:
      cvNode = convertCacheNode(cvcacheJob)
      cvNode.set_output( dNode.get_output() )
      if dNode and opts.datafind: # only add a parent if it exists
        cvNode.add_parent(dNode)
      if opts.datafind:
        dag.add_node(cvNode)
        cvcacheJobCnt+=1
    else: cvNode = 0

    # SETUP QSCAN JOBS
    if trig.gpsTime[ifo] and cvNode:
      qNode = qscanNode(qscanJob,trig.gpsTime[ifo],cp,trig, 
          cvNode.get_output(),ifo,"QSCAN",qFlag)
      if cvNode and opts.datafind: # only add a parent if it exists
        qNode.add_parent(cvNode)          
      if opts.qscan:
        dag.add_node(qNode)
        qscanJobCnt+=1
      qFlag = False
    else: qNode = 0

    # SETUP SEISMIC QSCAN JOBS
    if trig.gpsTime[ifo] and cvNode:
      try:
        qSeisNode = qscanNode(qscanJob,trig.gpsTime[ifo],cp,trig,
            cvNode.get_output(),ifo,"SEIS_QSCAN",seisqFlag)
        if cvNode and opts.datafind: # only add a parent if it exists
          qSeisNode.add_parent(cvNode)
        if opts.seis_qscan:
          dag.add_node(qSeisNode)
          qscanSeisJobCnt+=1
        seisqFlag = False
        #summaryHTML.containers.append( qNode.getContainer() )
      except:
        print "couldn't add seis qscan job for " + str(ifo) + "@ "+ str(trig.gpsTime[ifo])
        qSeisNode = 0
    else: qSeisNode = 0


    # TRY GETTING THE TRIG BANK PROCESS PARAMS
#    if trig.gpsTime[ifo]:
      # TRY TO FIND THE --BANK-FILE ARGUMENT IN THE INSPIRAL PROCESS PARAMS TABLE
#      trigBankFile = 0
#      try:
#        for row in inspiral_process_params[ifo]:
#          if row.param[2:] == 'bank-file':
#            trigBankFile = row.value
#            break
#      except:
#        print "could not find the --bank-file argument in the inspiral process params table for " + str(ifo) + "@"+ str(trig.gpsTime[ifo])
      # THEN GET THE TRIG BANK PROCESS PARAMS
#      if trigBankFile:
#        try: 
#          trig_process_params_ifo = cache.getProcessParamsFromMatchingFileInCache(trigBankFile,'trigbank.cache')
#          foundTrigBankTable = 1
#        except:
#          print "couldn't get trigbank process params for " + str(trig.eventID)
#          foundTrigBankTable = 0
#      else:
#        print "couldn't get trigbank process params for " + str(trig.eventID)
#        foundTrigBankTable = 0
#        print "passed in the else loop"
 
    # SETUP TRIGBANK JOBS
#    if trig.gpsTime[ifo] and foundTrigBankTable:
#      try:
        # trigNode = trigBankFollowUpNode(trigJob,trig.gpsTime[ifo],inspiral_process_params[ifo],opts,xml_glob)
#        trigNode = trigBankFollowUpNode(trigJob,trig.gpsTime[ifo],trig_process_params_ifo,opts,xml_glob)
#        if opts.trig_bank:
#          dag.add_node(trigNode)
#          trigJobCnt+=1
#      except:
#        trigNode = 0
#        print "couldn't add trigbank job for " + ifo + "@ "+ str(trig.gpsTime[ifo])
#    else:
#      trigNode = 0

    # SETUP INSPIRAL JOBS
    if trig.gpsTime[ifo]:
      # build the string of the relevant trigbank file name
      trigbank_file_name = ifo + '-TRIGBANK_FOLLOWUP_' + str(trig.gpsTime[ifo]) + '.xml'
      try:
        inspiralNode = inspiralFollowUpNode(inspJob,trig.gpsTime[ifo],\
           inspiral_process_params[ifo],opts,trigbank_file_name)
        #if trigNode and opts.trig_bank: # only add a parent if it exists
        #  inspiralNode.add_parent(trigNode)
        if opts.inspiral:
          dag.add_node(inspiralNode)
          inspJobCnt+=1
      except: 
        inspiralNode = 0
        print "couldn't add inspiral job for " + str(ifo) + "@ "+ str(trig.gpsTime[ifo])
    else: 
      inspiralNode = 0

    # SETUP PLOT JOBS
    if trig.gpsTime[ifo] and inspiralNode:
      try:
        plotNode = plotSNRCHISQNode(plotJob,trig.gpsTime[ifo],inspiralNode.output_file_name,trig,page,plotsFlag)
        plotsFlag = 0
        if inspiralNode and opts.inspiral:
          plotNode.add_parent(inspiralNode)
        if opts.plots:
          dag.add_node(plotNode)
          plotJobCnt+=1
        #summaryHTML.containers.append( plotNode.getContainer() )
      except:
        plotNode = 0
        print "couldn't add plot job for " + str(ifo) + "@ "+ str(trig.gpsTime[ifo])
    else: plotNode = 0  

  if sNode:      summaryHTML.containers.append( sNode.getContainer() )
  if qHoftNode : summaryHTML.containers.append( qHoftNode.getContainer() )
  if qSeisNode : summaryHTML.containers.append( qSeisNode.getContainer() )
  if qNode:      summaryHTML.containers.append( qNode.getContainer() )
  if plotNode: 
    summaryHTML.containers.append( plotNode.getContainer() )
    summaryHTML.containers.append( plotNode.getContainer2() )
  summaryHTMLlist.append(summaryHTML)
    

#print "\nFound " +  str(trigJobCnt)          + " trig bank jobs" 
print "\nFound " +  str(inspJobCnt)           + " inspiral Jobs" 
print "\nFound " +  str(plotJobCnt)           + " plot jobs"
print "\nFound " +  str(dataJobCnt)           + " datafind jobs for regular qscans"
print "\nFound " +  str(dataHoftJobCnt)       + " datafind jobs for hoft qscans"
#print "\nFound " +  str(qconfigureJobCnt)     + " qconfigure jobs for hoft qscans"
print "\nFound " +  str(cvcacheJobCnt)        + " convertlalcache jobs for regular qscans"
print "\nFound " +  str(cvcacheHoftJobCnt)    + " convertlalcache jobs for hoft qscans"
print "\nFound " +  str(qscanJobCnt)          + " regular qscan jobs"
print "\nFound " +  str(qscanHoftJobCnt)      + " hoft qscan jobs"
print "\nFound " +  str(qscanSeisJobCnt)      + " seismic qscan jobs"
print "\nFound " +  str(segJobCnt)            + " data quality jobs"
print "\n.......Writing DAG"

dag.write_sub_files()
dag.write_dag()

writeHTMLTables(summaryHTMLlist)
if opts.write_to_iulgroup:
  publishOnHydra(page)

sys.exit(0)

