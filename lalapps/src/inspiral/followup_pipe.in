#!/usr/bin/env @PYTHONPROG@
"""
Something

$Id$

This program creates cache files for the output of inspiral hipe
"""

__author__ = 'Chad Hanna <channa@phys.lsu.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import sys, os, copy, math, random
import socket, time
import re, string
from optparse import *
import tempfile
import ConfigParser
import urlparse
import urllib
from UserDict import UserDict
sys.path.append('@PYTHONLIBDIR@')

##############################################################################
# import the modules we need to build the pipeline
from glue import pipeline
from glue import lal
from glue import segments 
from glue import segmentsUtils
from glue.ligolw import ligolw
from glue.ligolw import table
from glue.ligolw import lsctables
from glue.ligolw import utils
from pylal import CoincInspiralUtils
from pylal.fu_utils import *
from pylal.fu_writeXMLparams import * 
from pylal.fu_Condor import *
from pylal.webUtils import *
from pylal import Fr
from lalapps import inspiral
from lalapps import inspiralutils

##############################################################################
#
#  MAIN PROGRAM
#
##############################################################################

######################## OPTION PARSING  #####################################
usage = """usage: %prog [options]
"""

parser = OptionParser( usage )

parser.add_option("-v", "--version",action="store_true",default=False,\
    help="print version information and exit")

parser.add_option("-l", "--log-path",action="store",type="string",\
    metavar=" PATH",help="directory to write condor log file")

parser.add_option("-f", "--config-file",action="store",type="string",\
    metavar=" FILE",help="ini file")

parser.add_option("-g", "--generate-fu-cache",action="store_true",\
    default=False, help="create hipe caches")

parser.add_option("-D", "--disable-followup",action="store_true",\
    default=False, help="disable candidate followup (for running qscans only)")

parser.add_option("-m", "--datafind",action="store_true",\
    default=False, help="use datafind to get qscan/trends data")

parser.add_option("-M", "--hoft-datafind",action="store_true",\
    default=False, help="use datafind to get hoft data (for qscan)")

parser.add_option("", "--single-qevent",action="store_true",default=False,\
    help="do single qevent")

parser.add_option("", "--H1H2-qevent",action="store_true",default=False,\
    help="do coherent qevent")

parser.add_option("-q", "--qscan",action="store_true",default=False,\
    help="do qscans")

parser.add_option("-G", "--generate-segments",action="store_true",\
    default=False, help="generate the science segments for background qscans")

parser.add_option("-Q", "--background-qscan",action="store_true",\
    default=False, help="do qscans over a list of times")

parser.add_option("-n", "--hoft-qscan",action="store_true",\
    default=False, help="do hoft qscans")

parser.add_option("-N", "--background-hoft-qscan",action="store_true",\
    default=False, help="do hoft qscans over a list of times")

parser.add_option("-s", "--seis-qscan",action="store_true",\
    default=False, help="do seismic qscans")

parser.add_option("-S", "--background-seis-qscan",action="store_true",\
    default=False, help="do seismic qscans over a list of times")

parser.add_option("-a", "--analyse-qscan",action="store_true",\
    default=False, help="run the analyseQscan script to interprete the qscan")

parser.add_option("-b", "--analyse-seis-qscan",action="store_true",\
    default=False, help="run the analyseQscan script to interprete the seismic qscan")

parser.add_option("-e", "--analyse-hoft-qscan",action="store_true",\
    default=False, help="run the analyseQscan script to interprete the hoft qscan")

parser.add_option("-d", "--data-quality",action="store_true",default=False,\
    help="do data quality lookup - CURRENTLY BROKEN, DO NOT USE IT")

parser.add_option("-t", "--trig-bank",action="store_true",default=False,\
    help="generate a pseudo trigbank xml file for single triggers")

parser.add_option("-i", "--inspiral",action="store_true",default=False,\
    help="do inspirals for single triggers - output SNR/CHISQ/PSD")

parser.add_option("-F", "--frame-check",action="store_true",default=False,\
    help="do FrCheck jobs on the frames used by the inspiral code")

parser.add_option("-I", "--ifo-status-check",action="store_true",default=False,\
    help="download IFO summary plots")

parser.add_option("-p", "--plots",action="store_true",default=False,\
    help="plot SNR/CHISQ from inspiral stage")

parser.add_option("-C", "--mcmc",action="store_true",default=False,\
    help="Do MCMC on the followup trigs (experimental)")

parser.add_option("-P", "--plot-mcmc",action="store_true",default=False,\
    help="Plot MCMC results (experimental)")

parser.add_option("-H", "--inspiral-head",action="store_true",default=False,\
    help="Run a job using inspiral from HEAD (to get bank and cont. chisq) (experimental)")

parser.add_option("-w", "--write-to-iulgroup",action="store_true", \
    default=False, help="publish the page to the iulgroup")

command_line = sys.argv[1:]
(opts,args) = parser.parse_args()

if opts.version:
  print "$Id$"
  sys.exit(0)

####################### SANITY CHECKS #####################################

if not opts.config_file:
  print >> sys.stderr, "No configuration file specified."
  print >> sys.stderr, "Use --config-file FILE to specify location" 
  sys.exit(1)

if not opts.log_path and not opts.write_to_iulgroup:
  print >> sys.stderr, "No log file path specified"
  print >> sys.stderr, "Use --log-path PATH to specify a location"
  sys.exit(1)

if not opts.write_to_iulgroup and not opts.generate_fu_cache and \
  not opts.datafind and not opts.qscan and not opts.background_qscan \
  and not opts.trig_bank and not opts.inspiral and not opts.plots and not \
  opts.hoft_qscan and not opts.seis_qscan and not opts.background_hoft_qscan \
  and not opts.background_seis_qscan and not opts.hoft_datafind and not \
  opts.generate_segments and not opts.analyse_qscan and not \
  opts.analyse_seis_qscan and not opts.analyse_hoft_qscan and not \
  opts.mcmc and not opts.frame_check and not opts.inspiral_head and not \
  opts.ifo_status_check and not opts.single_qevent and not opts.H1H2_qevent \
  and not opts.plot_mcmc :
  print >> sys.stderr, "No steps of the pipeline specified."
  print >> sys.stderr, "Please specify at least one of"
  print >> sys.stderr, "--generate-fu-cache, --trig-bank, --inspiral, --plots,"
  print >> sys.stderr, "--datafind, --qscan, --hoft-qscan, --seis-qscan,"
  print >> sys.stderr, "--background-qscan, --background-hoft-qscan,"
  print >> sys.stderr, "--background-seis-qscan, --hoft-datafind,"
  print >> sys.stderr, "--generate-segments, --frame-check,"
  print >> sys.stderr, "--analyse-qscan, --analyse-seis-qscan,"
  print >> sys.stderr, "--analyse-hoft-qscan, --mcmc, --plot-mcmc,"
  print >> sys.stderr, "--ifo-status-check, --single-qevent, --H1H2-qevent"
  print >> sys.stderr, "or --write-to-iulgroup (use this option alone for now)"  
  sys.exit(1)

if opts.disable_followup:
  print >> sys.stderr, "Warning: this option disables any followup jobs, only qscan datafind and background qscan jobs will be run..."

#################### READ IN THE CONFIG (.ini) FILE ########################
cp = ConfigParser.ConfigParser()
cp.read(opts.config_file)

## get the directory where the code is run
currentPath = os.path.abspath('.')

############# TURN THE HIPE OUTPUT INTO LAL CACHE FILES #######################

if not opts.disable_followup:
  cache = getCache(opts,cp,currentPath)

##############################################################################
# create the DAG writing the log to the specified directory
page = string.strip(cp.get('output','page'))
dag = followUpDAG(opts.config_file, opts.log_path)
dag.setupDAGWeb('followup web page','index.html',page)

############# READ IN THE COIRE FILES #########################################

if not opts.disable_followup:
  numtrigs, found, coincs, search = cache.readTriggerFiles(cp)
  missed = None

  if opts.trig_bank: trigbank_test = 1
  else: trigbank_test = 0  
  followuptrigs = getfollowuptrigs(numtrigs,page,coincs,missed,search,trigbank_test)

  print "\n.......Found " + str(len(followuptrigs)) + " trigs to follow up" 

############ SET UP THE REQUESTED JOBS ########################################

if not opts.disable_followup:
  inspJob         = followUpInspJob(cp)
  plotJob         = plotSNRCHISQJob(opts,cp)
  qscanFgJob      = qscanJob(opts,cp)
  anaQscanJob     = analyseQscanJob(opts,cp)
  frcheckJob      = FrCheckJob(opts,cp)
  statusJob       = IFOstatus_checkJob(opts,cp)
  MCMCJob 	  = mcmcJob(opts,cp)
  PLOTMCMCJob     = plotmcmcJob(opts,cp)
  headInspJob     = followUpInspJob(cp, 'head')
  h1h2QJob        = h1h2QeventJob(opts,cp)   

dataJob         = qscanDataFindJob(cp,'futrig')
qscanBgJob      = qscanLiteJob(opts,cp)

prev_dNode = None

print "\n.......Setting up pipeline jobs"

dq_url_pattern = "http://ldas-cit.ligo.caltech.edu/segments/S5/%s/dq_segments.txt"

segFile = {}
ifos_list = ['H1','H2','L1','G1','V1','T1']


############# LOOP OVER RANDOM BACKGROUND TIMES ###############################
###############################################################################
# Prepare the qscan background
for ifo in ifos_list:
  times = getQscanBackgroundTimes(cp,opts,ifo, dq_url_pattern,segFile)
  for time in times:
    # SETUP DATAFIND JOBS FOR BACKGROUND QSCANS (REGULAR DATA SET)
    dNode = qscanDataFindNode(dataJob,'futrig','q-datafind',cp,time,ifo,opts,dag,prev_dNode,'datafind')
    if dNode.validNode: prev_dNode = dNode

    # SETUP DATAFIND JOBS FOR BACKGROUND QSCANS (HOFT)
    dHoftNode = qscanDataFindNode(dataJob,'futrig','q-hoft-datafind',cp,time,ifo,opts,dag,prev_dNode,'hoft_datafind')
    if dHoftNode.validNode: prev_dNode = dHoftNode
    
    # SETUP BACKGROUND QSCAN JOBS
    qBgNode = qscanNode(qscanBgJob,time,cp,dHoftNode.outputFileName,ifo,'background-hoft-qscan',opts,dHoftNode,dag,'hoft_datafind','background_hoft_qscan')
    qBgNode = qscanNode(qscanBgJob,time,cp,dNode.outputFileName,ifo,'background-qscan',opts, dNode, dag,'datafind','background_qscan')
    qBgNode = qscanNode(qscanBgJob,time,cp,dNode.outputFileName,ifo,'background-seismic-qscan',opts, dNode, dag,'datafind','background_seis_qscan')


################# LOOP OVER FOLLOWUP TRIGGERS #################################
###############################################################################

if not opts.disable_followup:

  # LOOP ON TRIGGERS (=CANDIDATES)
  for trig in followuptrigs:
 
    # Initialization of local variables
    dHoftNode = {}

    #Lets make a new section for this event
    dag.appendSection('Trigger ID = '+str(trig.eventID)+ ' Stat = '+str(trig.statValue))  

    # TRY GETTING INSPIRAL PROCESS PARAMS... 
    # currently done even when inspiral jobs are not requested... fix?
    inspiral_process_params = cache.processFollowupCache(cp, opts, trig)
    bank_process_params, bank = cache.processFollowupCache(cp, opts, trig, 'TMPLTBANK') 

    # loop over ifos found in coincidence
    for ifo in trig.ifolist_in_coinc:
    
      try: trig.gpsTime[ifo]
      except: continue
    
      #Lets append a new subsection to the last section
      dag.appendSubSection(str(ifo)+' @ GPS '+str(trig.gpsTime[ifo]))

      # SETUP DATAFIND JOBS FOR HOFT QCANS
      dHoftNode[ifo] = qscanDataFindNode(dataJob,'futrig','q-hoft-datafind',cp,trig.gpsTime[ifo],ifo,opts,dag,prev_dNode,'hoft_datafind')
      if dHoftNode[ifo].validNode: prev_dNode = dHoftNode[ifo]

      # SETUP DATAFIND JOBS FOR REGULAR QSCANS
      dNode = qscanDataFindNode(dataJob,'futrig','q-datafind',cp,trig.gpsTime[ifo],ifo,opts,dag,prev_dNode,'datafind')
      if dNode.validNode: prev_dNode = dNode
    
      # SETUP FOREGROUND QSCAN JOBS
      qFgNode = qscanNode(qscanFgJob,trig.gpsTime[ifo],cp,dHoftNode[ifo].outputFileName,ifo,'foreground-hoft-qscan',opts,dHoftNode[ifo],dag,'hoft_datafind','hoft_qscan')
      qFgNode = qscanNode(qscanFgJob,trig.gpsTime[ifo],cp,dNode.outputFileName,ifo,'foreground-qscan',opts,dNode,dag,'datafind','qscan')
      qFgNode = qscanNode(qscanFgJob,trig.gpsTime[ifo],cp,dNode.outputFileName,ifo,'foreground-seismic-qscan',opts,dNode,dag,'datafind','seis_qscan')

      # SETUP INSPIRAL JOBS
      inspiralNode = followUpInspNode(inspJob,inspiral_process_params[ifo], ifo, trig, cp, opts, dag)
      headInspiralNode = followUpInspNode(headInspJob,inspiral_process_params[ifo], ifo, trig, cp, opts, dag, 'head', bank)


      # SETUP PLOT JOBS
      plotNode = plotSNRCHISQNode(plotJob,ifo,inspiralNode.output_file_name,trig,page,dag,inspiralNode,opts)

      # SETUP FRAME CHECK JOBS
      frcheckNode = FrCheckNode(frcheckJob,inspiral_process_params[ifo], ifo, trig, cp, opts, dag)

      # MCMC JOBS
      MCMCNode = mcmcNode(MCMCJob,inspiral_process_params[ifo], ifo, trig, cp, opts, dag)

      # PLOT MCMC JOBS
      PLOTMCMCNode = plotmcmcNode(PLOTMCMCJob,ifo,trig,cp,opts,dag,MCMCNode)

      # SETUP STATUS SUMMARY PLOT JOBS
      statusNode = IFOstatus_checkNode(statusJob, ifo, trig, cp, opts, dag)


    # SETUP H1H2 SPECIFIC JOBS
    # first make sure that the coincident trigger is found in at least one Hanford ifo and that both Hanford ifos are in the analysed times
    lho_flag = trig.ifolist_in_coinc.count('H1') + trig.ifolist_in_coinc.count('H2')
    if lho_flag == 2:
      #times = {"H1":trig.gpsTime["H1"],"H2":trig.gpsTime["H2"]} # equivalent to trig.gpsTime ?
      #Lets append a new subsection to the last section
      dag.appendSubSection('H1H2 followup' +' @ GPS '+str(trig.gpsTime['H1']))
      h1h2Qevent = h1h2QeventNode(h1h2QJob,dHoftNode,trig.gpsTime,['H1','H2'],'qevent',cp,opts,dag,'H1H2_qevent')


    # LOOP OVER IFOS NOT FOUND IN COINCIDENCE (THOUGH IN THE ANALYSED TIMES)
    for j in range(0,len(trig.ifoTag)-1,2):
      ifo = trig.ifoTag[j:j+2]
      if not trig.ifolist_in_coinc.count(ifo):

        #Lets append a new subsection to the last section
        dag.appendSubSection(str(ifo)+" (non coincident ifo)")

        # SETUP DATAFIND JOBS FOR HOFT QCANS
        dHoftNode = qscanDataFindNode(dataJob,'futrig','q-hoft-datafind',cp,trig.gpsTime[trig.ifolist_in_coinc[0]],ifo,opts,dag,prev_dNode,'hoft_datafind')
        if dHoftNode.validNode: prev_dNode = dHoftNode

        # SETUP DATAFIND JOBS FOR REGULAR QCANS
        dNode = qscanDataFindNode(dataJob,'futrig','q-datafind',cp,trig.gpsTime[trig.ifolist_in_coinc[0]],ifo,opts,dag,prev_dNode,'datafind')
        if dNode.validNode: prev_dNode = dNode

        # SETUP FOREGROUND QSCAN JOBS
        qFgNode = qscanNode(qscanFgJob,trig.gpsTime[trig.ifolist_in_coinc[0]],cp,dHoftNode.outputFileName,ifo,'foreground-hoft-qscan',opts,dHoftNode,dag,'hoft_datafind','hoft_qscan')
        qFgNode = qscanNode(qscanFgJob,trig.gpsTime[trig.ifolist_in_coinc[0]],cp,dNode.outputFileName,ifo,'foreground-qscan',opts,dNode,dag,'datafind','qscan')

        # generate the snr/chisq time series for each triggered template
        for itf in trig.ifolist_in_coinc:
          # SETUP INSPIRAL JOBS
          inspiralNode = followUpInspNode(inspJob,inspiral_process_params[ifo], itf, trig, cp, opts, dag)
          # SETUP PLOT JOBS
          plotNode = plotSNRCHISQNode(plotJob,ifo,inspiralNode.output_file_name,trig,page,dag,inspiralNode,opts,itf)



  # Loop for analyseQscan jobs (which must be children of all qscan jobs)
  for trig in followuptrigs:

    for ifo in trig.ifolist_in_coinc:

      anaQscanNode = analyseQscanNode(anaQscanJob,trig.gpsTime[ifo],ifo,'foreground-qscan','QSCAN/QSCAN.cache','QSCANLITE/QSCANLITE.cache',cp,opts,dag,'analyse_qscan')
      anaQscanNode = analyseQscanNode(anaQscanJob,trig.gpsTime[ifo],ifo,'foreground-seismic-qscan','QSCAN/QSCAN.cache','QSCANLITE/QSCANLITE.cache',cp,opts,dag,'analyse_seis_qscan')
      anaQscanNode = analyseQscanNode(anaQscanJob,trig.gpsTime[ifo],ifo,'foreground-hoft-qscan','QSCAN/QSCAN.cache','QSCANLITE/QSCANLITE.cache',cp,opts,dag,'analyse_hoft_qscan')
    
##########################################################################
dag.writeAll()
if opts.write_to_iulgroup:
  dag.publishToHydra()

sys.exit(0)
