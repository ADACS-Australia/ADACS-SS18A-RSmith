#!/usr/bin/env @PYTHONPROG@
"""
Something

$Id$

This program creates cache files for the output of inspiral hipe
"""

__author__ = 'Chad Hanna <channa@phys.lsu.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import sys, os, copy, math
import socket, time
import re, string
from optparse import *
import tempfile
import ConfigParser
import urlparse
from UserDict import UserDict
sys.path.append('@PYTHONLIBDIR@')

##############################################################################
# import the modules we need to build the pipeline
from glue import pipeline
from glue import lal
#from pylab import *
from glue import segments 
from glue import segmentsUtils
from glue.ligolw import ligolw
from glue.ligolw import table
from glue.ligolw import lsctables
from glue.ligolw import utils
from pylal import CoincInspiralUtils
from pylal.fu_utils import *
from pylal.fu_writeXMLparams import * 
from pylal import Fr      
##############################################################################
# redefine the columns of interest
##############################################################################
lsctables.SimInspiralTable.loadcolumns = ["waveform","geocent_end_time",
    "geocent_end_time_ns","h_end_time","h_end_time_ns","l_end_time",
    "l_end_time_ns","source","mass1","mass2","mchirp","eta","distance",
    "spin1x","spin1y","spin1z","spin2x","spin2y","spin2z","eff_dist_h",
    "eff_dist_l","eff_dist_g","eff_dist_t","eff_dist_v"]

lsctables.SnglInspiralTable.loadcolumns = ["ifo","end_time","end_time_ns",
    "eff_distance","mass1","mass2","mchirp","eta","snr","chisq","chisq_dof",
    "sigmasq","event_id"]

################ TRIG BANK FROM SIRE FILE CONDOR DAG JOB ######################

class trigBankFollowUpJob(pipeline.CondorDAGJob):
  """
  A followup trig bank job
  """
  def __init__(self, options, cp, tag_base='TRIGBANK_FOLLOWUP'):
    """
    """
    self.__executable = string.strip(cp.get('trig-bank','executable'))
    self.__universe = "standard"
    pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
#    self.tag_base = tag_base
#    self.options = options
    self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")

    self.set_stdout_file('logs/futrigbank-$(macrogpsstarttime)-$(cluster)-$(process).out')
    self.set_stderr_file('logs/futrigbank-$(macrogpsstarttime)-$(cluster)-$(process).err')
    self.set_sub_file('futrigbank.sub')


class trigBankFollowUpNode(pipeline.CondorDAGNode):
  """
  Runs an instance of a trig bank followup job
  """
  def __init__(self,job,time,proc,opts,xml_glob):
    """
    job = A CondorDAGJob that can run an instance of trigbank followup.
    """
    pipeline.CondorDAGNode.__init__(self,job)
    self.output_file_name = ""
    self.output_user_tag = "FOLLOWUP_" + str(time)

    for row in proc:
      self.add_var_opt(row.param[2:],row.value)
      if row.param[2:] == 'gps-start-time':
        self.add_macro("macrogpsstarttime",row.value)
        self.start_time = row.value
      if row.param[2:] == 'input-ifo':
        self.input_ifo = row.value
      if row.param[2:] == 'ifo-tag':
        self.ifo_tag = row.value
      if row.param[2:] == 'gps-end-time':
        self.end_time = row.value

    self.add_var_opt("user-tag",self.output_user_tag)
    self.add_file_arg(xml_glob)
    self.output_file_name = self.input_ifo + "-TRIGBANK_" + self.ifo_tag + \
      "_" + self.output_user_tag + "-" + self.start_time + "-" + \
      str(eval(self.end_time)-eval(self.start_time)) + ".xml"

class inspiralFollowUpJob(pipeline.CondorDAGJob):
  """
  A followup trig bank job
  """
  def __init__(self, options, cp, tag_base='INSPIRAL_FOLLOWUP'):
    """
    """
    self.__executable = string.strip(cp.get('inspiral','executable'))
    self.__universe = "standard"
    pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
    self.tag_base = tag_base
    self.options = options
    self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")

    self.set_stdout_file('logs/fuinspiral-$(macrogpsstarttime)-$(cluster)-$(process).out')
    self.set_stderr_file('logs/fuinspiral-$(macrogpsstarttime)-$(cluster)-$(process).err')
    self.set_sub_file('fuinspiral.sub')


class inspiralFollowUpNode(pipeline.CondorDAGNode):
  """
  Runs an instance of a trig bank followup job
  """
  def __init__(self,job,time,proc,opts,bankFile):
    """
    job = A CondorDAGJob that can run an instance of trigbank followup.
    """
    pipeline.CondorDAGNode.__init__(self,job)
    self.output_file_name = ""
    self.output_user_tag = "FOLLOWUP_" + str(time)
    self.add_rsq_veto = 1
    for row in proc:
      if row.param[2:] == 'bank-file':
        continue
      if row.param[2:] == 'trig-start-time':
        continue
      if row.param[2:] == 'trig-end-time':
        continue
      self.add_var_opt(row.param[2:],row.value)
      if row.param[2:] == 'gps-start-time':
        self.add_macro("macrogpsstarttime",row.value)
        self.start_time = row.value
      if row.param[2:] == 'channel-name':
        self.input_ifo = row.value[0:2]
      if row.param[2:] == 'ifo-tag':
        self.ifo_tag = row.value
      if row.param[2:] == 'gps-end-time':
        self.end_time = row.value
      if row.param[2:] == 'enable-rsq-veto':
        self.add_rsq_veto = 0
      if row.param[2:] == 'disable-rsq-veto':
        self.add_rsq_veto = 0
    self.add_var_opt("user-tag",self.output_user_tag)
    self.add_var_opt("write-snrsq","")
    self.add_var_opt("write-chisq","")
    self.add_var_opt("write-spectrum","")
    self.add_var_opt("bank-file",bankFile)
    self.add_var_opt("trig-start-time", str( eval(str(time))-1.0) )
    self.add_var_opt("trig-end-time", str( eval(str(time))+1.0) )
    if self.add_rsq_veto:
      self.add_var_opt("enable-rsq-veto","")
    self.output_file_name = self.input_ifo + "-INSPIRAL_" + self.ifo_tag + \
      "_" + self.output_user_tag + "-" + self.start_time + "-" +\
      str(eval(self.end_time)-eval(self.start_time)) + ".xml"
    
class plotSNRCHISQJob(pipeline.CondorDAGJob):
  """
  A followup trig bank job
  """
  def __init__(self, options, cp, tag_base='PLOT_FOLLOWUP'):
    """
    """
    self.__executable = string.strip(cp.get('plots','executable'))
    self.__universe = "vanilla"
    pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
    self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")
    self.set_stdout_file('logs/fuplotsnrchisq-$(macrogpsstarttime)-$(cluster)-$(process).out')
    self.set_stderr_file('logs/fuplotsnrchisq-$(macrogpsstarttime)-$(cluster)-$(process).err')
    self.set_sub_file('fuplotsnrchisq.sub')


class plotSNRCHISQNode(pipeline.CondorDAGNode):
  """
  Runs an instance of a trig bank followup job
  """
  def __init__(self,job,time,fileName,trig,page,plotFlag):
    """
    job = A CondorDAGJob that can run an instance of trigbank followup.
    """
    pipeline.CondorDAGNode.__init__(self,job)
    self.output_file_name = ""
    self.add_var_opt("frame-file",fileName.replace(".xml",".gwf"))
    self.add_var_opt("gps",time)
    self.add_var_opt("inspiral-xml-file",fileName)
    self.container = HTMLcontainer(trig,"SNR_and_CHISQ_plots")
    self.add_var_opt("output-html-file",self.container.locallink)
    self.add_var_opt("output-path","followuptrigs/"+self.container.name+"/")
    self.add_var_opt("image-file", self.container.localimage)
    self.add_var_opt("page", page)
    if plotFlag: self.initializeHTML()
#    tableFile = open(self.container.locallink,'w')
#    writeIULHeader(tableFile)
#    tableFile.write('<h3>Follow up of trigger [' +str(trig.eventID) +']</h3>\n')
    self.container.text = "click here for SNR and CHISQ plots"
#    tableFile.close()
  def getContainer(self):
    return self.container
  def initializeHTML(self):
    tableFile = open(self.container.locallink,'w')
    writeIULHeader(tableFile)
    tableFile.write('<h3>Follow up of trigger [' +str(trig.eventID) +']</h3>\n')
    tableFile.close()
##############################################################################
# qscan class for qscan jobs

class qscanJob(pipeline.CondorDAGJob):
  """
  A qscan job
  """
  def __init__(self, opts, cp, tag_base='QSCAN'):
    """
    """
    self.__executable = string.strip(cp.get('qscan','executable'))
    self.__universe = "vanilla"
    pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
    self.tag_base = tag_base
    self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")
    self.set_stdout_file('logs/qscan-$(macrogpsstarttime)-$(cluster)-$(process).out')     
    self.set_stderr_file('logs/qscan-$(macrogpsstarttime)-$(cluster)-$(process).err')
    self.set_sub_file('qscan.sub')

##############################################################################
# qscan class for qscan Node

class qscanNode(pipeline.CondorDAGNode):
  """
  Runs an instance of a qscan job
  """
  def __init__(self,job,time,cp,dir,trig,ifo,qFlag):
    """
    job = A CondorDAGJob that can run an instance of qscan.
    """
    pipeline.CondorDAGNode.__init__(self,job)
    self.add_var_arg(str(time))
    self.add_macro("macrogpsstarttime",str(time))
    if (ifo == 'H1') or (ifo == 'H2'):
      self.add_file_arg(string.strip(cp.get('qscan','Hconfig-file')))
      #self.add_file_arg(string.strip(cp.get('qscan','Hcache-file')))
      self.add_file_arg('Hq.cache')
    if (ifo == 'L1'):
      self.add_file_arg(string.strip(cp.get('qscan','Lconfig-file')))
      #self.add_file_arg(string.strip(cp.get('qscan','Lcache-file')))
      self.add_file_arg('Lq.cache')
    self.container = HTMLcontainer(trig,"QSCAN")
#   self.add_var_arg(dir+"/"+self.container.name+"/")
    self.add_var_arg(string.strip(cp.get('qscan','output')))
    if qFlag: self.initializeHTML(trig)
    tableFile = open(self.container.locallink,'a')
#   writeIULHeader(tableFile)
    tableFile.write('<a href="http://ldas-jobs.ligo.caltech.edu/~channa/' + \
                    str(time) + '/">' + str(ifo) + \
                    ' QScan of trigger [' +str(trig.eventID) +']</a><br>\n')
    self.container.text = "click here for QScans"
    tableFile.close()

  def getContainer(self):
    return self.container
  def initializeHTML(self,trig):
    tableFile = open(self.container.locallink,'w')
    writeIULHeader(tableFile)   
    tableFile.write('<h3>Follow up of trigger [' +str(trig.eventID) +']</h3>\n')

class segJob(pipeline.CondorDAGJob):
  """
  A segwizard job
  """
  def __init__(self, opts, cp, tag_base='SEG'):
    """
    """
    self.__executable = string.strip(cp.get('seg','executable'))
    self.__universe = "local"
    pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
    self.tag_base = tag_base
    self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")
    self.add_condor_cmd('getenv','True')
    self.set_stdout_file('followuptrigs/seg-$(macrogpsstarttime).out')
    self.set_stderr_file('logs/seg-$(macrogpsstarttime)-$(cluster)-$(process).err')
    self.set_sub_file('seg.sub')
  #  self.add_condor_cmd("should_transfer_files","YES")
  #  self.add_condor_cmd("when_to_transfer_output","ON_EXIT")
  #  self.add_condor_cmd("transfer_input_files",string.strip(cp.get('seg','tclshexe')) )

class segNode(pipeline.CondorDAGNode):
  """
  Runs an instance of a qscan job
  """
  def __init__(self,job,time,cp,dir,trig,ifo,sFlag):
    """
    job = A CondorDAGJob that can run an instance of qscan.
    """
    pipeline.CondorDAGNode.__init__(self,job)
    scirun = string.strip(cp.get('hipe-cache','science-run'))
    self.add_var_arg( scirun )
    if scirun == "S4" or scirun == "S3":
      self.add_var_arg('00')
    self.add_var_arg(str(ifo).lower())
    self.add_var_arg('nearest')
    self.add_var_arg( str(int(eval(str(time)))) )
    self.add_macro("macrogpsstarttime",str(time))
#   self.add_input_file( string.strip(cp.get('seg','tclshexe')) )
    self.container = HTMLcontainer(trig,"DATA_QUALITY")
#   self.add_var_arg(dir+"/"+self.container.name+"/")
    if sFlag: self.initializeHTML(trig)
    tableFile = open(self.container.locallink,'a')
#   writeIULHeader(tableFile)
    tableFile.write('<a href="http://www.lsc-group.phys.uwm.edu/iulgroup/' + \
              string.strip(cp.get("output","page")) + "/followuptrigs/" + \
              'seg-' + str(time) + '.out">' + str(ifo) + \
              ' Data Quality of trigger [' +str(trig.eventID) +']</a><br>\n' )
    self.container.text = "click here for Data Quality"
    tableFile.close()

  def getContainer(self):
    return self.container
  def initializeHTML(self,trig):
    tableFile = open(self.container.locallink,'w')
    writeIULHeader(tableFile)
    tableFile.write('<h3>Follow up of trigger [' +str(trig.eventID) +']</h3>\n')


##############################################################################
#
#  MAIN PROGRAM
#
##############################################################################

###### OPTION PARSING AND SANITY CHECKS #####################################
usage = """usage: %prog [options]
"""

parser = OptionParser( usage )

parser.add_option("-v", "--version",action="store_true",default=False,\
    help="print version information and exit")

parser.add_option("-l", "--log-path",action="store",type="string",\
    metavar=" PATH",help="directory to write condor log file")

parser.add_option("-f", "--config-file",action="store",type="string",\
    metavar=" FILE",help="ini file")

parser.add_option("-g", "--generate-cache",action="store_true",\
    default=False, help="write LAL cache")

parser.add_option("-m", "--datafind",action="store_true",\
    default=False, help="use datafind to get qscan/trends data")

parser.add_option("-r","--read-triggers",action="store_true",default=False,\
    help="Read the followup trigger XML (COIRE FILE)" )

parser.add_option("-q", "--qscan",action="store_true",default=False,\
    help="do qscans")

parser.add_option("-d", "--data-quality",action="store_true",default=False,\
    help="do data quality lookup")

parser.add_option("-t", "--trig-bank",action="store_true",default=False,\
    help="do trigbanks for single triggers")

parser.add_option("-i", "--inspiral",action="store_true",default=False,\
    help="do inspirals for single triggers - output SNR/CHISQ/PSD")

parser.add_option("-p", "--plots",action="store_true",default=False,\
    help="plot SNR/CHISQ from inspiral stage")

parser.add_option("-w", "--write-to-iulgroup",action="store_true", \
    default=False, help="publish the page to the iulgroup")

parser.add_option("-c", "--cache-path",action="store",type="string",\
    default="./.",metavar=" PATH",help="directory to find  all hipe XML files")

parser.add_option("-s", "--science-run", action="store",type="string",\
    default="S5", metavar=" RUN", help="name of science run")

command_line = sys.argv[1:]
(opts,args) = parser.parse_args()

if opts.version:
  print "$Id$"
  sys.exit(0)

if opts.datafind and not opts.read_triggers:
  print >> sys.stderr, "Must read triggers for datafind step"
  print >> sys.stderr, "Use --read-triggers XML to specify triggers to followup"
  sys.exit(1)

if not opts.cache_path:
  print >> sys.stderr, "No cache path specified."
  print >> sys.stderr, "Use --cache-path PATH to specify a location."
  sys.exit(1)

if not opts.science_run:
  print >> sys.stderr, "No science run specified."
  print >> sys.stderr, "Use --science-run RUN to specify a run."
  sys.exit(1)

## READ IN THE CONFIG (.ini) FILE
cp = ConfigParser.ConfigParser()
cp.read(opts.config_file)

## set up the local directories
setupdirs()

############# TURN THE HIPE OUTPUT INTO LAL CACHE FILES #######################
if opts.generate_cache:
  cache = getCache(opts)
  cache.getCacheAll(cp)
  cache.writeCacheAll()
  print >> sys.stderr, "\nHIPE CACHE FILES WRITTEN TO:"
  for n, t in cache.nameMaps:
    print >> sys.stderr, " * " + n + " [" + str(len(cache[t])) + "]"


##############################################################################
# create a log file that the Condor jobs will write to
basename = 'followup'
tempfile.tempdir = opts.log_path
tempfile.template = basename + '.dag.log.'
logfile = tempfile.mktemp()
fh = open( logfile, "w" )
fh.close()

##############################################################################
# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
dag.set_dag_file( basename )
subsuffix = '.sub'

##############################################################################
#Decide if we are actually reading any trigger files
page = string.strip(cp.get('output','page'))

if not opts.read_triggers and not opts.write_to_iulgroup:
  sys.exit(0)
  # We are done here
if not opts.read_triggers and opts.write_to_iulgroup:
  publishToIULGroup(page)
  # don't overwrite anything, just publish what we have
  sys.exit(0)

# Since we are continuing get useful stuff from the ini file.
xml_glob = string.strip(cp.get('triggers','xml-glob'))
numtrigs = string.strip(cp.get('triggers','num-trigs'))
statistic =  string.strip(cp.get('triggers','statistic'))
bla =  string.strip(cp.get('triggers','bitten-l-a'))
blb =  string.strip(cp.get('triggers','bitten-l-b'))


print string.strip(cp.get('hipe-cache','TMPLTBANK')) 
############# READ IN THE COIRE FILES #########################################

found, coincs = readFiles(xml_glob,getstatistic(statistic,bla,blb))
missed = None
followuptrigs = getfollowuptrigs(numtrigs,page,coincs,missed)

print "\n.......Found " + str(len(coincs)) + " trigs to follow up" 

################ DO DATA FINDING ##############################################
if opts.datafind:
  times = []
  for trigs in followuptrigs:
    time = 0
    try: time = trigs.gpsTime['H1']
    except: pass
    try: time = trigs.gpsTime['H2']
    except: pass
    try: time = trigs.gpsTime['L1']
    except: pass
    if time: times.append(time)
  times.sort()
  command = string.strip(cp.get('datafind','executable')) + \
          " --observatory=L" + " --type=" + \
          string.strip(cp.get('datafind','qscan-level')) + \
          " --gps-start-time=" + str(times[0]) + " --gps-end-time=" + \
          str(times[-1]) + " --url-type=file" + " --lal-cache > Lq.cache.tmp"

  print "....Finding Qscan LLO data ["+str(times[0])+"-"+str(times[-1])+"]\n"
  os.system(command)
  Lq = open('Lq.cache.tmp','r')
  lines = Lq.readlines()
  Lqout = open('Lq.cache','w')
  for line in lines:
    total = line.replace("file://localhost","").split("/")[0:-1]
    first = total[0].split()
    first.insert(-1,str(eval(first[-1]) + eval(first[-2])))
    total[0] = " ".join(first)
    total[0] += " "
    Lqout.write("/".join(total)+'\n')

  command = string.strip(cp.get('datafind','executable')) + \
          " --observatory=H" + " --type=" + \
          string.strip(cp.get('datafind','qscan-level')) + \
          " --gps-start-time=" + str(times[0]) + " --gps-end-time=" + \
          str(times[-1]) + " --url-type=file" + " --lal-cache > Hq.cache.tmp" 
  print "....Finding Qscan LHO data ["+str(times[0])+"-"+str(times[-1])+"]\n"
  os.system(command)
  Hq = open('Hq.cache.tmp','r')
  lines = Hq.readlines()
  Hqout = open('Hq.cache','w')
  for line in lines:
    total = line.replace("file://localhost","").split("/")[0:-1]
    first = total[0].split()
    first.insert(-1,str(eval(first[-1]) + eval(first[-2])))
    total[0] = " ".join(first)
    total[0] += " "
    Hqout.write("/".join(total)+'\n')


############ SET UP THE REQUESTED JOBS ########################################
trigJob = trigBankFollowUpJob(opts,cp)
inspJob = inspiralFollowUpJob(opts,cp)
plotJob = plotSNRCHISQJob(opts,cp)
qscanJob = qscanJob(opts,cp)
segJob = segJob(opts,cp)


trigJobCnt = 0
inspJobCnt = 0
plotJobCnt = 0
qscanJobCnt = 0
segJobCnt = 0
summaryHTMLlist = []

print "\n.......Setting up pipeline jobs"

for trig in followuptrigs:
  # Set up HTML structs for web page
  summaryHTML = summaryHTMLTable(trig)
  summaryHTML.containers.append(writeXMLparams(trig))

  # TRY GETTING TRIG BANK PROCESS PARAMS
  trig_process_params = cache.getProcessParamsFromCache( \
                     cache.filesMatchingGPS(trig.gpsTime,'TRIGBANK'), \
                     trig.gpsTime)
  try: 
    trig_process_params = cache.getProcessParamsFromCache( \
                     cache.filesMatchingGPS(trig.gpsTime,'TRIGBANK'), \
                     trig.gpsTime) 
  except: 
    print "couldn't get trigbank process params for " + str(trig.eventID)
    trig_process_params = []

  # TRY GETTING INSPIRAL PROCESS PARAMS
  try: 
    inspiral_process_params = cache.getProcessParamsFromCache( \
                     cache.filesMatchingGPS(trig.gpsTime,'INSPIRAL_'), \
                     trig.gpsTime)
  except: 
    print "couldn't get inspiral process params for " + str(trig.eventID)
    inspiral_process_params = []

  # currently this assumes you will be getting trig bank jobs...fix this!
  if opts.qscan: qFlag = 1
  else: qFlag = 0
  if opts.plots: plotsFlag = 1
  else: plotsFlag = 0
  if opts.data_quality: sFlag = 1
  else: sFlag = 0


  for ifo in trig_process_params:
    try: trig.gpsTime[ifo]
    except: continue

    # SETUP DATA QUALITY JOBS
    if trig.gpsTime[ifo]:
      try:
        sNode = segNode(segJob,trig.gpsTime[ifo],cp,trig.summarydir,trig,ifo,sFlag)
        if opts.data_quality: 
          dag.add_node(sNode)
          segJobCnt+=1
        sFlag = False
      except:
        print "couldn't add data quality  job for " + str(ifo) + "@ "+ str(trig.gpsTime[ifo])
        sNode = 0
    else: sNode = 0

    # SETUP QSCAN JOBS
    if trig.gpsTime[ifo]:
      try:
        qNode = qscanNode(qscanJob,trig.gpsTime[ifo],cp,trig.summarydir,trig,ifo,qFlag)
        if opts.qscan: 
          dag.add_node(qNode)
          qscanJobCnt+=1
        qFlag = False
        #summaryHTML.containers.append( qNode.getContainer() )
      except: 
        print "couldn't add qscan job for " + str(ifo) + "@ "+ str(trig.gpsTime[ifo])
        qNode = 0
    else: qNode = 0
    # SETUP TRIGBANK JOBS
    if trig.gpsTime[ifo]:
      try:
        trigNode = trigBankFollowUpNode(trigJob,trig.gpsTime[ifo],trig_process_params[ifo],opts,xml_glob)
        if opts.trig_bank:
          dag.add_node(trigNode)
          trigJobCnt+=1
      except:
        trigNode = 0
        print "couldn't add trigbank job for " + str(ifo) + "@ "+ str(trig.gpsTime[ifo])

    # SETUP INSPIRAL JOBS
    if trig.gpsTime[ifo] and trigNode:
      try:
        inspiralNode = inspiralFollowUpNode(inspJob,trig.gpsTime[ifo],\
           inspiral_process_params[ifo],opts,trigNode.output_file_name)
        if trigNode and opts.trig_bank: # only add a parent if it exists
          inspiralNode.add_parent(trigNode)
        if opts.inspiral:
          dag.add_node(inspiralNode)
          inspJobCnt+=1
      except: 
        inspiralNode = 0
        print "couldn't add inspiral job for " + str(ifo) + "@ "+ str(trig.gpsTime[ifo])
    else: 
      inspiralNode = 0

    # SETUP PLOT JOBS
    if trig.gpsTime[ifo] and inspiralNode:
      try:
        plotNode = plotSNRCHISQNode(plotJob,trig.gpsTime[ifo],inspiralNode.output_file_name,trig,page,plotsFlag)
        plotsFlag = 0
        if inspiralNode and opts.inspiral:
          plotNode.add_parent(inspiralNode)
        if opts.plots:
          dag.add_node(plotNode)
          plotJobCnt+=1
        #summaryHTML.containers.append( plotNode.getContainer() )
      except:
        print "couldn't add plot job for " + str(ifo) + "@ "+ str(trig.gpsTime[ifo])
  
  if sNode: summaryHTML.containers.append( sNode.getContainer() )
  if qNode: summaryHTML.containers.append( qNode.getContainer() )
  if plotNode: summaryHTML.containers.append( plotNode.getContainer() )
  summaryHTMLlist.append(summaryHTML)
    

print "\nFound " + str(trigJobCnt) + " trig bank jobs" 
print "\nFound " + str(inspJobCnt) + " inspiral Jobs" 
print "\nFound " +  str(plotJobCnt) + " plot jobs"
print "\nFound " +  str(qscanJobCnt) + " qscan jobs"
print "\nFound " +  str(segJobCnt) + " data quality jobs"
print "\n.......Writing DAG"

dag.write_sub_files()
dag.write_dag()

writeHTMLTables(summaryHTMLlist)
if opts.write_to_iulgroup:
  publishToIULGroup(page)

sys.exit(0)

