#!/usr/bin/env python2.2
"""
inspiral_pipeline.py - standalone inspiral pipeline driver script

$Id$

This script produced the necessary condor submit and dag files to run
the standalone inspiral code on LIGO data
"""

__author__ = 'Duncan Brown <duncan@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

# import standard modules and append the lalapps prefix to the python path
import sys, os
import getopt, re, string
import tempfile
import ConfigParser
sys.path.append('@PYTHONLIBDIR@')

# import the modules we need to build the pipeline
import pipeline, inspiral

def add_ifo1_file(inca,data,s_chunk,ifo1_bn):
"""
Go through the science segments in data and get the chunks that overlap with
the slid chunks. Add them to the input of inca with the correct file name.
inca = inca dag node to add the jobs to.
data = unslid data to search.
s_chunk = slid chunk to use when searching.
ifo1_bn = base name of ifo 1 inspiral jobs.
"""
  for seg in data:
    for chunk in seg:
      if (
        chunk.start() >= s_chunk.start() and chunk.start() <= s_chunk.end()
        ) or (
        chunk.end() >= s_chunk.start() and chunk.end() <= s_chunk.end()
        ):
          inca.add_var_arg(ifo1_bn + str(chunk.start()) + '-' + 
            str(chunk.dur()) + '.xml')

def usage():
  msg = """\
Usage: lalapps_inspiral_pipe [options]

  -h, --help               display this message
  -v, --version            print version information and exit
  -u, --user-tag TAG       tag the job with TAG (overrides value in ini file)

  -T, --triggered-bank     run lalapps_trigtotmplt to generate a triggered bank
  -I, --triggered-inspiral run lalapps_inspiral on the second IFO
  -C, --coincidence        run lalapps_inca on the triggers from both IFOs

  -j, --injections         add simulated inspirals from injection file

  -s, --slide-time T       slide data by T secs for background estimation
  -n, --slide-time-ns T    slide data by T nanosecs for background estimation

  -p, --playground-only    only create chunks that overlap with playground
  -P, --priority PRIO      run jobs with condor priority PRIO

  -f, --config-file FILE   use configuration file FILE
  -l, --log-path PATH      directory to write condor log file
"""
  print >> sys.stderr, msg

# pasrse the command line options to figure out what we should do
shortop = "hvu:TICjs:n:pP:f:l:"
longop = [
  "help",
  "version",
  "user-tag=",
  "triggered-bank",
  "triggered-inspiral",
  "coincidence",
  "injections",
  "slide-time=",
  "slide-time-ns=",
  "playground-only",
  "priority=",
  "config-file=",
  "log-path="
  ]

try:
  opts, args = getopt.getopt(sys.argv[1:], shortop, longop)
except getopt.GetoptError:
  usage()
  sys.exit(1)

usertag = None
config_file = None
do_trigbank = None
do_triginsp = None
do_coinc = None
do_inj = None
slide_sec = 0
slide_ns = 0
playground_only = 0
condor_prio = None
config_file = None
log_path = None

for o, a in opts:
  if o in ("-h", "--help"):
    usage()
    sys.exit(0)
  elif o in ("-v", "--version"):
    print "$Id$"
    sys.exit(0)
  elif o in ("-u", "--user-tag"):
    usertag = a
  elif o in ("-T", "--triggered-bank"):
    do_trigbank = 1
  elif o in ("-I", "--triggered-inspiral"):
    do_triginsp = 1
  elif o in ("-C", "--coincidence"):
    do_coinc = 1
  elif o in ("-j", "--injections"):
    do_inj = 1
  elif o in ("-s", "--slide-time"):
    slide_data = int(a)
  elif o in ("-n", "--slide-time-ns"):
    slide_data = int(a)
  elif o in ("-p", "--playground-only"):
    playground_only = 1
  elif o in ("-P", "--priority"):
    condor_prio = a
  elif o in ("-f", "--config-file"):
    config_file = a
  elif o in ("-l", "--log-path"):
    log_path = a
  else:
    print >> sys.stderr, "Unknown option:", o
    usage()
    sys.exit(1)

if not config_file:
  print >> sys.stderr, "No configuration file specified."
  print >> sys.stderr, "Use --config-file FILE to specify location."
  sys.exit(1)

if not log_path:
  print >> sys.stderr, "No log file path specified."
  print >> sys.stderr, "Use --log-path PATH to specify a location."
  sys.exit(1)

if not slide_sec:
  print >> sys.stderr, "Slide time (seconds) not specified."
  print >> sys.stderr, "Use --slide-time T to specify time slide length."
  sys.exit(1)
  
# try and make a directory to store the cache files and job logs
try: os.mkdir('logs')
except: pass

# create the config parser object and read in the ini file
cp = ConfigParser.ConfigParser()
cp.read(config_file)

# if a usertag has been specified, override the config file
if usertag:
  cp.set('pipeline','user-tag',usertag)
else:
  try:
    usertag = string.strip(cp.get('pipeline','user-tag'))
  except:
    usertag = None

# create a log file that the Condor jobs will write to
basename = re.sub(r'\.ini',r'',config_file)
tempfile.tempdir = log_path
if usertag:
  tempfile.template = basename + '.' + usertag + 'bkgr.dag.log.'
else:
  tempfile.template = basename + '.bkgr.dag.log.'
logfile = tempfile.mktemp()
fh = open( logfile, "w" )
fh.close()

# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
if usertag:
  dag.set_dag_file(basename + '.' + usertag + '.bkgr.dag')
else:
  dag.set_dag_file(basename + '.bkgr.dag')

# create the Condor jobs that will be used in the DAG
df_job = inspiral.DataFindJob(cp)
insp_job = inspiral.InspiralJob(cp)
trig_job = inspiral.TrigToTmpltJob(cp)
inca_job = inspiral.IncaJob(cp)

# set better submit file names than the default
if usertag:
  subsuffix = '.' + usertag + '.bkgr..sub'
else:
  subsuffix = '.bkgr.sub'
insp_job.set_sub_file( basename + '.inspiral' + subsuffix )
trig_job.set_sub_file( basename + '.trigtotmplt' + subsuffix )
inca_job.set_sub_file( basename + '.inca' + subsuffix )

# set the usertag in the jobs
if usertag:
  insp_job.add_opt('user-tag',usertag)
  trig_job.add_opt('user-tag',usertag)
  inca_job.add_opt('user-tag',usertag)

# add the injections
if do_inj:
  insp_job.add_opt('injection-file',cp.get('input','injection-file'))

# set the condor job priority
if condor_prio:
  insp_job.add_condor_cmd('priority',condor_prio)
  trig_job.add_condor_cmd('priority',condor_prio)
  inca_job.add_condor_cmd('priority',condor_prio)

# get the pad and chunk lengths from the values in the ini file
pad = int(cp.get('data', 'pad-data'))
n = int(cp.get('data', 'segment-length'))
s = int(cp.get('data', 'number-of-segments'))
r = int(cp.get('data', 'sample-rate'))
o = int(cp.get('inspiral', 'segment-overlap'))
length = ( n * s - ( s - 1 ) * o ) / r
overlap = o / r

# read science segs that are greater or equal to a chunk from the input file
data = pipeline.ScienceData()
data.read(cp.get('input','segments'),length)
data.make_chunks(length,overlap,playground_only)
data.make_chunks_from_unused(length,overlap/2,playground_only,overlap/2)

# create a set of slid science segments and chunks
s_data = pipeline.ScienceData()
s_data.read(cp.get('input','segments'),length)
s_data.slide(slide_sec)
s_data.make_chunks(length,overlap,playground_only)
s_data.make_chunks_from_unused(length,overlap/2,playground_only,overlap/2)

# get the order of the ifos to filter
ifo1 = cp.get('pipeline','ifo1')
ifo2 = cp.get('pipeline','ifo2')
ifo2_snr = cp.get('pipeline','ifo2-snr-threshold')

# set the name of the ifo to generate the triggered bank from
trig_job.add_opt('ifo-a',ifo1)

# create a list to store the inspiral jobs
insp_nodes = []

# create the base name of ifo 1 inspiral jobs
if usertag:
  ifo1_bn = ifo1 + '-INSPIRAL_' + usertag + '-'
else:
  ifo1_bn = ifo1 + '-INSPIRAL' + '-'

# iterate over the data segments that have been slid
for s_seg in s_data:

  # create the name of the cache file for the second ifo
  df2 = inspiral.DataFindNode(df_job)
  if slide_sec > 0:
    df2.set_start(seg.start() - pad - slide_sec)
    df2.set_end(seg.end() + pad)
  else:
    df2.set_start(seg.start() - pad)
    df2.set_end(seg.end() + pad + abs(slide_sec))
  df2.set_ifo(ifo2)
  
  # clear the inspiral nodes in this segment
  seg_insp_nodes = []

  for s_chunk in s_seg:

    trigbank = inspiral.TrigToTmpltNode(trig_job)
    
    # figure out which ifo1 chunks overlap with this slid chunk
    for seg in data:
      for chunk in seg:
        if (
          chunk.start() >= s_chunk.start() and chunk.start() <= s_chunk.end()
          ) or (
          chunk.end() >= s_chunk.start() and chunk.end() <= s_chunk.end()
          ):
            trigbank.add_var_arg(ifo1_bn + str(chunk.start()) + '-' + 
             str(chunk.dur()) + '.xml')

    trigbank.set_start(s_chunk.start())
    trigbank.set_end(s_chunk.end())
    if usertag:
      trigbank.set_output(ifo2 + '-TRIGBANK_' + usertag + '_' + ifo1 + '-' 
        + str(s_chunk.start()) + '-' + str(s_chunk.dur()) + '.xml')
    else:
      trigbank.set_output(ifo2 + '-TRIGBANK_' + ifo1 + '-' 
        + str(s_chunk.start()) + '-' + str(chunk.dur()) + '.xml')

    if do_trigbank:
      dag.add_node(trigbank)

    insp2 = inspiral.InspiralNode(insp_job)
    insp2.set_start(s_chunk.start())
    insp2.set_end(s_chunk.end())
    insp2.set_ifo(ifo2)
    insp2.add_var_opt('snr-threshold',ifo2_snr)
    insp2.add_var_opt('trig-start-time',s_chunk.trig_start())
    insp2.set_cache(df2.get_output())
    insp2.set_bank(trigbank.get_output())
    insp2.add_var_opt('slide-time',slide_sec)
    insp2.add_var_opt('slide-time-ns',slide_ns)

    if do_trigbank:
      insp2.add_parent(trigbank)
    if do_triginsp:
      dag.add_node(insp2)

    # add the inspiral job for this chunk to the stored list
    seg_insp_nodes.append(insp2)

  # add the inspiral job for this segment to the list
  insp_nodes.append(seg_insp_nodes)

# now find coincidences between the inspiral jobs
for i in range(len(s_data)):
  for j in range(len(s_data[i])):
    s_chunk = s_data[i][j]
    inca = inspiral.IncaNode(inca_job)
    inca.set_start(s_chunk.start())
    inca.set_end(s_chunk.end())
    inca.set_ifo_a(ifo1)
    inca.set_ifo_b(ifo2)
    
    # if there is a chunk before this one, add it to the job
    try: 
      c = s_data[i][j-1]
      inca.add_var_arg(insp_nodes[i][j-1].get_output())
      add_ifo1_file(inca,data,c,ifo1_bn)
      if do_triginsp:
        inca.add_parent(insp_nodes[i][j-1])
    except IndexError:
      pass

    # add this chunk to the job
    inca.add_var_arg(insp_nodes[i][j].get_output())
    add_ifo1_file(inca,data,s_chunk,ifo1_bn)
    if do_triginsp:
      inca.add_parent(insp_nodes[i][j])
    
    # if there is a chunk after this one, add it to the job
    try:
      c = s_data[i][j+1]
      inca.add_var_arg(insp_nodes[i][j+1].get_output())
      add_ifo1_file(inca,data,c,ifo1_bn)
      if do_triginsp:
        inca.add_parent(insp_nodes[i][j+1])
    except IndexError:
      pass
      
    if do_coinc:
      dag.add_node(inca)

# write out the DAG
dag.write_sub_files()
dag.write_dag()

# write out a log file for this script
if usertag:
  log_fh = open(basename + '.pipeline.' + usertag + '.bkgr.log', 'w')
else:
  log_fh = open(basename + '.pipeline.bkgr.log', 'w')
  
log_fh.write( "$Id$" + "\n\n" )
log_fh.write( "Invoked with arguments:\n" )
for o, a in opts:
  log_fh.write( o + ' ' + a + '\n' )
log_fh.write( "\n" )
log_fh.write( "Parsed " + str(len(s_data)) + " science segments\n" )
total_data = 0
for s_seg in s_data:
  for s_chunk in s_seg:
    total_data += len(s_chunk)
print >> log_fh, "total data =", total_data
print >> log_fh, "\n===========================================\n"
print >> log_fh, s_data
for s_seg in s_data:
  print >> log_fh, s_seg
  for chunk in seg:
    print >> log_fh, s_chunk

sys.exit(0)

