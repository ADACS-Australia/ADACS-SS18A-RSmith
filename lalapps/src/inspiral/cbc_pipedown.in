#!/usr/bin/env @PYTHONPROG@

usage = \
"""
Program to construct post-processing dag.
"""

__author__ = 'Collin Capano <cdcapano@physics.syr.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import os, sys, re
import ConfigParser
from optparse import OptionParser
import tempfile
sys.path.append('@PYTHONLIBDIR@')

##############################################################################
# import the modules we need to build the pipeline
from glue import pipeline
from glue import lal
from glue.ligolw import lsctables
import inspiral

##############################################################################
# Function Definitions

def get_veto_segments_name( veto_cat_num, cumulative = True ):
  """
  Given a category number, returns a veto segments name 
  as set by segs_from_cats.

  @veto_cat_num: integer representing the category veto
  @cumulative: If set to True, will add CUMULATIVE to the name.
  """
  if cumulative:
    return ''.join([ 'VETO_CAT', str(veto_cat_num), '_CUMULATIVE' ])
  else:
    return ''.join([ 'VETO_CAT', str(veto_cat_num) ])
    

##############################################################################
#
#  MAIN PROGRAM
#
##############################################################################

##############################################################################
# parse command-line arguments
parser = OptionParser( version = "", usage = usage )

parser.add_option( "", "--ihope-cache", action = "store", type = "string",
  default = None,
  help =
    "The ihope cache to read."
  )
parser.add_option( "", "--config-file", action = "store", type = "string",
  default = None,
  help =
    "The .ini file to use."
  )
parser.add_option( "", "--log-path", action = "store", type = "string",
  default = None,
  help =
    "Directory to write condor log file and perform SQLite operation in. " +
    "Should be a local directory."
  )
parser.add_option( "", "--gps-start-time", action = "store", type = "string",
  default = None,
  help =
    "GPS start time of the ihope run."
  )
parser.add_option( "", "--gps-end-time", action = "store", type = "string",
  default = None,
  help =
    "GPS end time of the ihope run."
  )
parser.add_option( "", "--generate-all-data-plots", action = "store_true",
  default = False,
  help =
    "Turn on if want to open the box. Otherwise, only plots of playground " +
    "data will be made. WARNING: Even if this option is off, all_data and " +
    "and exclude_play coincs will still exist in the resulting databases " +
    "(they just won't be plotted)."
  )

(options, args) = parser.parse_args()

##############################################################################
# Sanity check of input arguments
if not options.ihope_cache:
  raise ValueError, "An ihope-cache file is required."
if not options.config_file:
  raise ValueError, "A config-file is required."
if not options.log_path:
  raise ValueError, "A log-path is required."

##############################################################################
# parse the ini file and initialize
cp = ConfigParser.ConfigParser()
cp.read(options.config_file)

tmp_space = cp.get('pipeline', 'node-tmp-dir')

experiment_start = options.gps_start_time
experiment_duration = str(int(options.gps_end_time) - int(options.gps_start_time))

# if logs directory not present, create it
try:
  os.mkdir('logs/')
except:
  pass

##############################################################################
# create a log file that the Condor jobs will write to
basename = re.sub(r'\.ini', r'', options.config_file)

tempfile.tempdir = options.log_path
tempfile.template = '.'.join([ basename, 'dag.log.' ])
logfile = tempfile.mktemp()
fh = open( logfile, "w" )
fh.close()

##############################################################################
# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
dag.set_dag_file(basename)

##############################################################################
# Open the ihope cache and sort THINCA_SECOND files by user_tag

print "Parsing the ihope cache..."
ihope_cache = [line for line in file(options.ihope_cache) \
  if "THINCA_SECOND" in line or "THINCA_SLIDE_SECOND" in line \
  or " INJECTIONS" in line or "INSPIRAL_SECOND" in line]

zero_lag_cache = lal.Cache([lal.CacheEntry(entry) for entry in ihope_cache \
  if "THINCA_SECOND" in entry])
slide_cache = lal.Cache([lal.CacheEntry(entry) for entry in ihope_cache \
  if "THINCA_SLIDE_SECOND" in entry])
inj_cache = lal.Cache([lal.CacheEntry(entry) for entry in ihope_cache if \
  " INJECTIONS" in entry])
all_inspirals_cache = lal.Cache([lal.CacheEntry(entry) for entry in ihope_cache \
  if "INSPIRAL_SECOND" in entry]) 

del ihope_cache

# get the user tags from the zero_lag_cache
user_tags = set([ '_'.join([ entry.description.split('_')[ii] 
  for ii in range( 3, len(entry.description.split('_')) ) ])
  for entry in zero_lag_cache ])

##############################################################################
# Cycle over the user tags, creating a pipeline for each

for tag in user_tags:

  print "Creating jobs for %s..." % tag
  
  # determine whether or not this was an injection run by checking if there
  # is an injection file for this tag

  file_sieve = ''.join([ '*', tag.split('_CAT')[0], '*' ])
  inj_file = inj_cache.sieve( description = file_sieve, exact_match = True )
  if len(inj_file) == 0:
    simulation = False
  elif len(inj_file) == 1:
    simulation = True
    inj_file = inj_file[0].url
  else:
    raise ValueError, "More than one injection file found for %s" % tag

  # write the jobs that will be run
  t2c_job = inspiral.ThincaToCoincJob(cp)
  tosql_job = pipeline.LigolwSqliteJob(cp)
  dbsimplify_job = inspiral.DBSimplifyJob(cp)
  comp_durs_job = inspiral.ComputeDurationsJob(cp)
  cluster_job = inspiral.ClusterCoincsJob(cp)
  ucfar_job = inspiral.CFarJob(cp, ['cfar-uncombined'])
  ccfar_job = inspiral.CFarJob(cp, ['cfar-combined'])
  printlc_job = inspiral.PrintLCJob(cp)
  minifup_job = inspiral.MiniFollowupsJob(cp)
  if simulation:
    dbaddinj_job = inspiral.DBAddInjJob(cp)
  else:
    plotslides_job = inspiral.PlotSlidesJob(cp)
    plotcumhist_job = inspiral.PlotCumhistJob(cp)
    plotifar_job = inspiral.PlotIfarJob(cp)

  # set better submit file names than the default
  subsuffix = 'sub'
  t2c_job.set_sub_file( '.'.join([ basename, tag, 'thinca_to_coinc', 'sub' ]) )
  tosql_job.set_sub_file( '.'.join([ basename, tag, 'ligolw_sqlite', 'sub' ]) )
  dbsimplify_job.set_sub_file( '.'.join([ basename, tag, 'dbsimplify', 'sub' ]) )
  comp_durs_job.set_sub_file( '.'.join([ basename, tag, 'compute_durations', 'sub' ]) )
  cluster_job.set_sub_file( '.'.join([ basename, tag, 'cluster_coincs', 'sub' ]) )
  ucfar_job.set_sub_file( '.'.join([ basename, tag, 'uncombined.cfar', 'sub' ]) )
  ccfar_job.set_sub_file( '.'.join([ basename, tag, 'combined.cfar', 'sub' ]) )
  printlc_job.set_sub_file( '.'.join([ basename, tag, 'printlc', 'sub' ]) )
  minifup_job.set_sub_file( '.'.join([ basename, tag, 'minifollowups', 'sub' ]) )
  if simulation:
    dbaddinj_job.set_sub_file ( '.'.join([ basename, tag, 'dbaddinj', 'sub' ]) )
  else:
    plotslides_job.set_sub_file( '.'.join([ basename, tag, 'plotslides', 'sub' ]) )
    plotcumhist_job.set_sub_file( '.'.join([ basename, tag, 'plotcumhist', 'sub' ]) )
    plotifar_job.set_sub_file( '.'.join([ basename, tag, 'plotifar', 'sub' ]) )

  ############################################################################
  # Step 1: Setup thinca_to_coinc nodes

  print "\tsetting up thinca_to_coinc nodes..."

  # set job options
  t2c_job.set_experiment_start_time(options.gps_start_time)
  t2c_job.set_experiment_end_time(options.gps_end_time)
  if simulation:
    t2c_job.set_simulation()

  # sieve zero_lag_cache for THINCA_SECOND files with this tag
  file_sieve = '*' + tag
  thinca_cache = zero_lag_cache.sieve( description = file_sieve, exact_match = True )
  # if not an injection run, check that there are an equal number of
  # slide files
  if not simulation and \
    len(slide_cache.sieve( description = file_sieve, exact_match = True )) != \
    len(thinca_cache):
    raise ValueError, "Number of %s slide files doesn't equal number of zero-lag files." % tag

  # also sieve all_inspirals_cache
  file_sieve = tag.split('_CAT_')[0]
  inspiral_cache = all_inspirals_cache.sieve( description = file_sieve )

  # get distinct on_instruments in the thinca cache
  distinct_instrument_sets = set([ entry.observatory for entry in thinca_cache ])
  # from the distinct_instrument_sets figure out what distinct ifos there are
  distinct_ifo_set = set()
  for on_instruments in distinct_instrument_sets:
    distinct_ifo_set |= lsctables.instrument_set_from_ifos(on_instruments)
  distinct_ifos = ''.join(sorted(distinct_ifo_set))

  # get the veto file
  if 'VETO' in tag:
    # get the category veto (assuming tag has form: *CAT_N_VETO)
    cat_num = tag.split('_')[-2]
  else:
    # is category 1 veto
    cat_num = '1'
  veto_file_path = cp.get('input', 'ihope-segments-directory')
  veto_file_name = ''.join([
    distinct_ifos, '-VETOTIME_CAT_', cat_num, '-', experiment_start, '-', experiment_duration, 
    '.xml' ])
  veto_file = '/'.join([ veto_file_path, veto_file_name ])
  if not os.path.exists( veto_file ):
    raise ValueError, "Veto file %s could not be found." % veto_file
  # get the veto segments name
  veto_segs_name = get_veto_segments_name( cat_num, cumulative = True )

  t2c_output = []
  t2c_nodes = []
  this_cache = lal.Cache()
  cache_start_time = int(experiment_start)
  for on_instruments in distinct_instrument_sets:
    instrument_cache = thinca_cache.sieve( ifos = on_instruments, exact_match = True )
    instrument_cache.sort()
    for entry_num, entry in enumerate(instrument_cache):
      if not this_cache:
        # first entry; store type for cache naming
        # and add all the second_inspiral files to the cache
        cache_type = entry.description
        this_cache.extend(inspiral_cache)
      # Add the zero-lag thinca entry to this_cache 
      this_cache.append(entry)
      # if this isn't a simulation add the slide file
      if not simulation:
        slide_entry = lal.CacheEntry(re.sub('THINCA', 'THINCA_SLIDE', str(entry)))
        this_cache.append(slide_entry)
      # if n*20th entry or last remainder, write to cache
      if (entry_num + 1) % 20 == 0 or (entry_num + 1) == len(instrument_cache):
        # set duration to be end of this entry - start of first entry in cache
        cache_duration = entry.segment[1].seconds - cache_start_time
        this_cache_name = '.'.join([ 
          '-'.join([on_instruments, cache_type, str(cache_start_time), str(cache_duration)]),
          'cache' ])
        this_cache_file = open( this_cache_name, 'w' )
        this_cache.tofile(this_cache_file)
        this_cache_file.close()
        # write node
        this_t2c_node = inspiral.ThincaToCoincNode(t2c_job)
        this_t2c_node.set_instruments(lsctables.ifos_from_instrument_set( distinct_ifo_set ))
        this_t2c_node.set_input_cache(this_cache_name)
        this_t2c_node.set_veto_segments( veto_file )
        this_t2c_node.set_veto_segments_name( veto_segs_name )
        dag.add_node(this_t2c_node)
        t2c_nodes.append(this_t2c_node)
        # add output files to t2c_output
        t2c_output += this_t2c_node.get_output_from_cache()
        # set end-time of last entry to be start time of next cache
        cache_start_time = entry.segment[1].seconds
        del this_cache
        this_cache = lal.Cache()
      
  # write output cache
  output_cache = lal.Cache().from_urls(t2c_output)
  # create cache name from what's in the output cache
  cache_type = output_cache[0].description
  t2c_output_cache = '.'.join([ 
    '-'.join([ distinct_ifos, cache_type, experiment_start, experiment_duration ]), 
    'cache' ])
  fp = open( t2c_output_cache, 'w' )
  output_cache.tofile( fp )
  fp.close()

  ############################################################################
  # Step 2a: Setup a LigolwSqliteNode for putting thinca_to_coincs 
  # into a sql db

  print "\tsetting up node to put thinca_to_coinc files into a SQLite database..."
  
  # set node options
  t2c2sql_node = pipeline.LigolwSqliteNode( tosql_job )
  t2c2sql_node.set_input_cache( t2c_output_cache )
  t2c2sql_node.set_tmp_space( tmp_space )
  # database name has form: 
  # distinct_ifos-CBC_TRIGDB_RAW-USER_TAG-cache_start-cache_duration.sql
  db_type = 'CBC_TRIGDB_RAW-' + tag
  raw_result_db = '.'.join([
    '-'.join([ distinct_ifos, db_type, experiment_start, experiment_duration ]),
    'sql' ])
  # check to make sure the database doesn't already exist
  if os.path.exists( raw_result_db ):
    raise ValueError, "Raw result database %s already exists. " % raw_result_db + \
    "Either rm or mv it, then re-run. (If it exists, you may also want to " + \
    "mv the CLUSTERED database so it doesn't get overwritten by cluster_coincs.)" \
    
  
  t2c2sql_node.set_database( raw_result_db )
  
  # set parent nodes to be all the thinca_to_coinc nodes
  [t2c2sql_node.add_parent( node ) for node in t2c_nodes]
      
  dag.add_node( t2c2sql_node )
  
  ############################################################################
  # Step 2b: Setup a LigolwSqliteNode for putting the veto-segments file 
  # into the raw database
  
  # cache the veto file
  veto_cache = lal.Cache().from_urls( [veto_file] )
  veto_cache_file = re.sub('.xml', '.cache', os.path.basename(veto_file))
  fp = open( veto_cache_file, 'w' )
  veto_cache.tofile( fp )
  fp.close()
  
  # write node to add the veto file
  veto2sql_node = pipeline.LigolwSqliteNode( tosql_job )
  veto2sql_node.set_input_cache( veto_cache_file )
  veto2sql_node.set_tmp_space( tmp_space )
  veto2sql_node.set_database( raw_result_db )
  
  # set parent node
  veto2sql_node.add_parent( t2c2sql_node )
  
  dag.add_node( veto2sql_node )
  
  ############################################################################
  # Step 3: Setup a DBSimplifyNode to clean up the output of the t2c2sql_node 
  
  print "\tsetting up dbsimplify node to clean the database..."
  
  # set node options
  dbsimplify_node = inspiral.DBSimplifyNode( dbsimplify_job )
  dbsimplify_node.set_tmp_space( tmp_space )
  dbsimplify_node.set_database( raw_result_db )
  
  # set parent node
  dbsimplify_node.add_parent( veto2sql_node )
  
  dag.add_node( dbsimplify_node )
  
  ############################################################################
  # Step 3b: Compute durations in the database

  print "\tsetting up compute_durations node..."

  # set node options
  comp_durs_node = inspiral.ComputeDurationsNode( comp_durs_job)
  comp_durs_node.set_tmp_space( tmp_space )
  comp_durs_node.set_database( raw_result_db )

  # set parent node
  comp_durs_node.add_parent( dbsimplify_node )

  dag.add_node(comp_durs_node)
  
  ############################################################################
  # Step 3c: If simulation, add injection file.
  
  if simulation:
    print "\tsetting up dbaddinj node to add the injection file..."
  
    # set node options
    dbaddinj_node = inspiral.DBAddInjNode( dbaddinj_job )
    dbaddinj_node.set_tmp_space( tmp_space )
    dbaddinj_node.set_database( raw_result_db )
    dbaddinj_node.set_injection_file( inj_file )
  
    dbaddinj_node.add_parent( comp_durs_node )
  
    dag.add_node( dbaddinj_node )
  
  ############################################################################
  # Step 4: Setup a ClusterCoincsNode to cluster the output of dbsimplify_node
  
  print "\tsetting up cluster node to cluster coincs in the database..."
  
  # set node options
  cluster_node = inspiral.ClusterCoincsNode( cluster_job )
  cluster_node.set_tmp_space( tmp_space )
  cluster_node.set_input( raw_result_db )
  # output database name has form:
  # distinct_ifos-CBC_TRIGDB_CLUSTERED-USER_TAG-gps_start_time-durations.sql
  db_type = 'CBC_TRIGDB_CLUSTERED-' + tag
  result_db = '.'.join([
    '-'.join([ distinct_ifos, db_type, experiment_start, experiment_duration ]),
    'sql' ]) 
  cluster_node.set_output(result_db)
  
  # set parent node
  cluster_node.add_parent( comp_durs_node )
  if simulation:
    cluster_node.add_parent( dbaddinj_node )
  
  dag.add_node( cluster_node )
  
  ############################################################################
  # Step 5a: Setup a CfarNode to compute the uncombined false alarm rates
  
  print "\tsetting up cfar nodes:"
  print "\t\tfor uncombined false alarm rates..."
  
  # set node options: output database is same as input
  ucfar_node = inspiral.CFarNode( ucfar_job )
  ucfar_node.set_tmp_space( tmp_space )
  ucfar_node.set_input( result_db )
  ucfar_node.set_output( result_db )
  
  # set parent node
  ucfar_node.add_parent( cluster_node )
  
  dag.add_node( ucfar_node )
  
  ############################################################################
  # Step 5b: Setup a CfarNode to compute the combined false alarm rates
  
  print "\t\tfor combined false alarm rates..."
  
  # set node options: output database is same as input
  ccfar_node = inspiral.CFarNode( ccfar_job )
  ccfar_node.set_tmp_space( tmp_space )
  ccfar_node.set_input( result_db )
  ccfar_node.set_output( result_db )
  
  # set parent node
  ccfar_node.add_parent( ucfar_node )
  
  dag.add_node( ccfar_node )

  ############################################################################
  # Summary: Setup PrintLC and MiniFollowup Nodes to generate a summary of 
  # loudest events

  print "\tsetting up printlc and minifollowup nodes..."

  # set datatypes to generate files for
  if simulation:
    datatypes = ['simulation']
  elif 'PLAYGROUND' in tag:
    datatypes = ['playground', 'slide']
  else:
    datatypes = ['all_data', 'playground', 'exclude_play', 'slide']

  for datatype in datatypes:
    print "\t\tfor %s..." % datatype
    # set file naming type
    type = '_'.join([ tag, 'LOUDEST', datatype.upper(), 'EVENTS_BY', cp.get('printlc', 'ranking-stat').upper()])
    # cycle over all ifos times, creating different tables for each
    for on_instruments in distinct_instrument_sets:
      on_instruments = lsctables.ifos_from_instrument_set(lsctables.instrument_set_from_ifos(on_instruments))
      # set output and extracted xml file names
      summary_filename = '.'.join([
        '-'.join([ ''.join(on_instruments.split(',')), type + '_SUMMARY', experiment_start, experiment_duration ]),
        'xml' ])
      xml_filename =  '.'.join([
        '-'.join([ ''.join(on_instruments.split(',')), type, experiment_start, experiment_duration ]),
        'xml' ])
      # set node options
      printlc_node = inspiral.PrintLCNode( printlc_job )
      printlc_node.set_tmp_space( tmp_space )
      printlc_node.set_input( result_db )
      printlc_node.set_output( summary_filename )
      printlc_node.set_extract_to_xml( xml_filename )
      printlc_node.set_include_only_coincs( '[ALLin' + on_instruments + ']' )
      printlc_node.set_datatype( datatype )

      # set parent node
      printlc_node.add_parent( ccfar_node )

      dag.add_node( printlc_node )

      # create the minifollowups nodes
      prefix = '-'.join([ ''.join(on_instruments.split(',')), tag ])
      suffix = re.sub(prefix + '_', '', summary_filename).rstrip('.xml')

      minifup_node = inspiral.MiniFollowupsNode( minifup_job )
      minifup_node.set_cache_file( options.ihope_cache )
      minifup_node.set_cache_string( tag )
      minifup_node.set_prefix( prefix )
      minifup_node.set_suffix( suffix )
      minifup_node.set_input_xml( xml_filename )
      minifup_node.set_input_xml_summary( summary_filename )
      minifup_node.set_output_html_table( re.sub('.xml', '.html', summary_filename ) )

      minifup_node.add_parent( printlc_node )
      dag.add_node( minifup_node )
      

  ############################################################################
  # Plotting: Generate all result plots
  
  print "\tsetting up plotting jobs..."
  
  if not simulation:
    print "\t\tcreating plotslides node..."
    # set plotslides_job options
    if not options.generate_all_data_plots or 'PLAYGROUND' in tag:
      plotslides_job.set_plot_playground_only()
    # Write plotslides node
    plotslides_node = inspiral.PlotSlidesNode( plotslides_job )
    plotslides_node.set_tmp_space( tmp_space )
    plotslides_node.set_input( result_db )
    plotslides_node.set_user_tag( tag )
  
    # set parent node
    plotslides_node.add_parent( ccfar_node )
  
    dag.add_node( plotslides_node )
  
    # create plotcumhist node
    print "\t\tcreating plotcumhist node..."

    if not options.generate_all_data_plots or 'PLAYGROUND' in tag:
      plotcumhist_job.set_plot_playground_only()
    plotcumhist_node = inspiral.PlotCumhistNode( plotcumhist_node )
    plotcumhist_node.set_tmp_space( tmp_space )
    plotcumhist_node.set_input( result_db )
    plotcumhist_node.set_user_tag( tag )

    plotcumhist_node.add_parent( ccfar_node )

    dag.add_node( plotcumhist_node )

    # Write plotifar nodes for different datatypes
    print "\t\tcreating plotifar node for datatypes:"
    for datatype in ['all_data', 'playground', 'exclude_play']: 
      # only create nodes for non-playground if options.plot-playground-only not set
      if (not options.generate_all_data_plots or 'PLAYGROUND' in tag)  and datatype != 'playground':
        continue
      print "\t\t\t%s..." % datatype
      plotifar_node = inspiral.PlotIfarNode( plotifar_job )
      plotifar_node.set_tmp_space( tmp_space )
      plotifar_node.set_input( result_db )
      plotifar_node.set_datatype( datatype )
      plotifar_node.set_user_tag( tag )
      
      # set parent node
      plotifar_node.add_parent( ccfar_node )
  
      dag.add_node( plotifar_node )
  
  
##############################################################################
##############################################################################
# Final Step: Write the DAG

print "Writing DAG and sub files..."

dag.write_sub_files()
dag.write_dag()

print "Finished!"
print "Now run:\n\tcondor_submit_dag %s" % os.path.basename(dag.get_dag_file())
sys.exit(0)

