#!/usr/bin/env @PYTHONPROG@

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import os, sys, re, copy
import ConfigParser
from optparse import OptionParser
import tempfile
sys.path.append('@PYTHONLIBDIR@')

##############################################################################
# import the modules we need to build the pipeline
from glue import pipeline
from glue import lal
from glue import iterutils
from glue.ligolw import ligolw
from glue.ligolw import lsctables
from glue.ligolw import utils
from glue.ligolw.utils import process
from glue import git_version
import inspiral

__prog__ = 'cbc_pipedown'
__author__ = 'Collin Capano <cdcapano@physics.syr.edu>'

description = \
"Program to construct post-processing dag."

##############################################################################
# Function Definitions

def get_veto_cat_from_tag( tag ):
  """
  Returns the veto category number in a tag.
  Assumes tag has the form _CAT_N_VETO for N>1 and
  nothing for N=1.
  """
  if 'VETO' in tag:
    cat_num = int(tag.split('_')[-2])
  else:
    # is category 1 veto
    cat_num = 1

  return cat_num

def get_veto_segments_name( veto_cat_num, cumulative = True ):
  """
  Given a category number, returns a veto segments name 
  as set by segs_from_cats.

  @veto_cat_num: integer representing the category veto
  @cumulative: If set to True, will add CUMULATIVE to the name.
  """
  if cumulative:
    return ''.join([ 'VETO_CAT', str(veto_cat_num), '_CUMULATIVE' ])
  else:
    return ''.join([ 'VETO_CAT', str(veto_cat_num) ])
    

##############################################################################
#
#  MAIN PROGRAM
#
##############################################################################

##############################################################################
# parse command-line arguments
parser = OptionParser( 
  version = git_version.verbose_msg,
  usage   = "%prog [options]",
  description = description
  )

parser.add_option( "", "--ihope-cache", action = "store", type = "string",
  default = None,
  help =
    "The ihope cache to read."
  )
parser.add_option( "", "--config-file", action = "store", type = "string",
  default = None,
  help =
    "The .ini file to use."
  )
parser.add_option( "", "--log-path", action = "store", type = "string",
  default = None,
  help =
    "Directory to write condor log file and perform SQLite operation in. " +
    "Should be a local directory."
  )
parser.add_option( "", "--gps-start-time", action = "store", type = "string",
  default = None,
  help =
    "GPS start time of the ihope run."
  )
parser.add_option( "", "--gps-end-time", action = "store", type = "string",
  default = None,
  help =
    "GPS end time of the ihope run."
  )
parser.add_option( "", "--generate-all-data-plots", action = "store_true",
  default = False,
  help =
    "Turn on if want to open the box. Otherwise, only plots of playground " +
    "data will be made. WARNING: Even if this option is off, all_data and " +
    "and exclude_play coincs will still exist in the resulting databases " +
    "(they just won't be plotted)."
  )

(options, args) = parser.parse_args()

##############################################################################
# Sanity check of input arguments
if not options.ihope_cache:
  raise ValueError, "An ihope-cache file is required."
if not options.config_file:
  raise ValueError, "A config-file is required."
if not options.log_path:
  raise ValueError, "A log-path is required."

##############################################################################
# Create log file
logdoc = ligolw.Document()
logdoc.appendChild(ligolw.LIGO_LW())
proc_id = process.register_to_xmldoc(logdoc, __prog__, options.__dict__, version = git_version.id)

##############################################################################
# parse the ini file and initialize
cp = ConfigParser.ConfigParser()
cp.read(options.config_file)

tmp_space = cp.get('pipeline', 'node-tmp-dir')

experiment_start = options.gps_start_time
experiment_duration = str(int(options.gps_end_time) - int(options.gps_start_time))

# if logs directory not present, create it
try:
  os.mkdir('logs/')
except:
  pass

##############################################################################
# create a log file that the Condor jobs will write to
basename = re.sub(r'\.ini', r'', options.config_file)

tempfile.tempdir = options.log_path
tempfile.template = '.'.join([ basename, 'dag.log.' ])
logfile = tempfile.mktemp()
fh = open( logfile, "w" )
fh.close()

##############################################################################
# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
dag.set_dag_file(basename)

##############################################################################
# Open the ihope cache and sort THINCA_SECOND files by user_tag

print "Parsing the ihope cache..."
ihope_cache = [line for line in file(options.ihope_cache) \
  if "THINCA_SECOND" in line or "THINCA_SLIDE_SECOND" in line \
  or " INJECTIONS" in line or "INSPIRAL_SECOND" in line]

zero_lag_cache = lal.Cache([lal.CacheEntry(entry) for entry in ihope_cache \
  if "THINCA_SECOND" in entry])
slide_cache = lal.Cache([lal.CacheEntry(entry) for entry in ihope_cache \
  if "THINCA_SLIDE_SECOND" in entry])
inj_cache = lal.Cache([lal.CacheEntry(entry) for entry in ihope_cache if \
  " INJECTIONS" in entry])
all_inspirals_cache = lal.Cache([lal.CacheEntry(entry) for entry in ihope_cache \
  if "INSPIRAL_SECOND" in entry]) 

del ihope_cache

# get the user tags from the zero_lag_cache
user_tags = set([ '_'.join([ entry.description.split('_')[ii] 
  for ii in range( 3, len(entry.description.split('_')) ) ])
  for entry in zero_lag_cache ])

# create a list to store non-injection databases for later caching
non_sim_dbs = []

##############################################################################
# Create the needed jobs to be run

# thinca_to_coinc jobs
t2c_job = inspiral.ThincaToCoincJob(cp)
t2c_job.set_experiment_start_time(options.gps_start_time)
t2c_job.set_experiment_end_time(options.gps_end_time)
# because t2c simulation jobs require the --simulation option to be set, these
# must be separate
t2c_sim_job = inspiral.ThincaToCoincJob(cp)
t2c_sim_job.set_simulation()
t2c_sim_job.set_experiment_start_time(options.gps_start_time)
t2c_sim_job.set_experiment_end_time(options.gps_end_time)

# ligolw_sqlite jobs; there are 4 different types:
# 1. sql_replace_job: reads from a cache and writes to a database
#   with the --replace option set (only for adding thinca_to_coinc files )
# 2. sql_fromcache_job: reads from a cache and writes to a database without
#   the replace option
# 3. sql_frominput_job: adds a single file to a databse (for veto xml files)
# 4. sql_extract_job: extracts xmls from a database (for turning simulation
#   databases into xml files)
sql_replace_job = pipeline.LigolwSqliteJob(cp)
sql_replace_job.set_replace()
sql_fromcache_job = pipeline.LigolwSqliteJob(cp)
sql_frominput_job = pipeline.LigolwSqliteJob(cp)
sql_extract_job = pipeline.LigolwSqliteJob(cp)

# for plotting jobs with plot_playground_only option, only create jobs without
# if generate-all-data-plots set
plotslides_play_job = inspiral.PlotSlidesJob(cp)
plotslides_play_job.set_plot_playground_only()
plotcumhist_play_job = inspiral.PlotCumhistJob(cp)
plotcumhist_play_job.set_plot_playground_only()
if options.generate_all_data_plots:
  plotslides_job = inspiral.PlotSlidesJob(cp)
  plotcumhist_job = inspiral.PlotCumhistJob(cp)

# since some files won't be passed through minifollowups, need two sets of
# printsims and printmissed jobs, one with columns, one without
printsims_job = inspiral.LigolwCBCPrintJob(cp, 'printsims', ['cbc_print', 'printsims'])
printsims_nominifup_job = inspiral.LigolwCBCPrintJob(cp, 'printsims', ['cbc_print', 'printsims'])
printmissed_job = inspiral.LigolwCBCPrintJob(cp, 'printmissed', ['cbc_print', 'printmissed'])
printmissed_nominifup_job = inspiral.LigolwCBCPrintJob(cp, 'printmissed', ['cbc_print', 'printmissed'])
# time-slide minifollowup jobs require a separate time-slides argument
# and missed injection minifollowups don't use full xml files
minifup_job = inspiral.MiniFollowupsJob(cp)
minifup_ts_job = inspiral.MiniFollowupsJob(cp)
minifup_ts_job.set_time_slides()
minifup_cm_job = inspiral.MiniFollowupsJob(cp)

# the number of and options given to plotfm job is optional and is set by the ini file
# to handle this, we do a similar thing as is done in ihope with inspinj: the 'plot_found/missed'
# section in the ini file gives the names of sections to read for plotfm jobs; these sections are cycled over
# and temprarily re-named to 'plotfm' then passed to the PlotFMJob class to create the job
plotfm_jobs = []
if cp.has_section('plotfm'):
  for plotfm_tag in cp.options('plotfm'):
    # create a copy of the config parser, blank out the plotfm section, and replace
    # with a new plotfm section with the options taken from plotfm_tag section
    fmcp = copy.deepcopy(cp)
    fmcp.remove_section('plotfm')
    fmcp.add_section('plotfm')
    for fmopt, fmval in cp.items(plotfm_tag):
      fmcp.set('plotfm', fmopt, fmval)
    plotfm_jobs.append( (plotfm_tag, inspiral.PlotFMJob(fmcp)) )

# all other jobs: all the nodes share the same static options for these
dbsimplify_job = inspiral.DBSimplifyJob(cp)
dbaddinj_job = inspiral.DBAddInjJob(cp)
inspinjfind_job = inspiral.InspInjFindJob(cp)
comp_durs_job = inspiral.ComputeDurationsJob(cp)
cluster_job = inspiral.ClusterCoincsJob(cp)
ucfar_job = inspiral.CFarJob(cp, ['cfar-uncombined'])
ccfar_job = inspiral.CFarJob(cp, ['cfar-combined'])
printlc_job = inspiral.LigolwCBCPrintJob(cp, 'printlc', ['cbc_print', 'printlc'])
plotifar_job = inspiral.PlotIfarJob(cp)

# MVSC jobs
mvsc_get_doubles_job = inspiral.MvscGetDoublesJob(cp)
mvsc_train_forest_job = inspiral.MvscTrainForestJob(cp)
mvsc_use_forest_job = inspiral.MvscUseForestJob(cp)
mvsc_update_sql_job = inspiral.MvscUpdateSqlJob(cp)

# set submit file names
t2c_job.set_sub_file( '.'.join([ basename, 'thinca_to_coinc', 'sub' ]) )
t2c_sim_job.set_sub_file( '.'.join([ basename, 'simulation', 'thinca_to_coinc', 'sub' ]) )
sql_replace_job.set_sub_file( '.'.join([ basename, 'replace_from_cache', 'ligolw_sqlite', 'sub' ]) )
sql_fromcache_job.set_sub_file( '.'.join([ basename, 'add_from_cache', 'ligolw_sqlite', 'sub' ]) )
sql_frominput_job.set_sub_file( '.'.join([ basename, 'add_from_input', 'ligolw_sqlite', 'sub' ]) )
sql_extract_job.set_sub_file( '.'.join([ basename, 'extract', 'ligolw_sqlite', 'sub' ]) )
dbsimplify_job.set_sub_file( '.'.join([ basename, 'dbsimplify', 'sub' ]) )
dbaddinj_job.set_sub_file( '.'.join([ basename, 'dbaddinj', 'sub' ]) )
inspinjfind_job.set_sub_file( '.'.join([ basename, 'inspinjfind', 'sub' ]) )
comp_durs_job.set_sub_file( '.'.join([ basename, 'compute_durations', 'sub' ]) )
cluster_job.set_sub_file( '.'.join([ basename, 'cluster_coincs', 'sub' ]) )
mvsc_get_doubles_job.set_sub_file( '.'.join([ basename, 'mvsc_get_doubles', 'sub' ]) )
mvsc_train_forest_job.set_sub_file( '.'.join([ basename, 'mvsc_train_forest', 'sub' ]) )
mvsc_use_forest_job.set_sub_file( '.'.join([ basename, 'mvsc_use_forest', 'sub' ]) )
mvsc_update_sql_job.set_sub_file( '.'.join([ basename, 'mvsc_update_sql', 'sub' ]) )
ucfar_job.set_sub_file( '.'.join([ basename, 'uncombined', 'cfar', 'sub' ]) )
ccfar_job.set_sub_file( '.'.join([ basename, 'combined', 'cfar', 'sub' ]) )
printlc_job.set_sub_file( '.'.join([ basename, 'printlc', 'sub' ]) )
printsims_job.set_sub_file( '.'.join([ basename, 'printsims', 'sub' ]) )
printsims_nominifup_job.set_sub_file( '.'.join([ basename, 'no_minifollowups', 'printsims', 'sub' ]) )
printmissed_job.set_sub_file( '.'.join([ basename, 'printmissed', 'sub' ]) )
printmissed_nominifup_job.set_sub_file( '.'.join([ basename, 'no_minifollowups', 'printmissed', 'sub' ]) )
minifup_job.set_sub_file( '.'.join([ basename, 'minifollowups', 'sub' ]) )
minifup_ts_job.set_sub_file( '.'.join([ basename, 'time_slides', 'minifollowups', 'sub' ]) )
minifup_cm_job.set_sub_file( '.'.join([ basename, 'closest_missed', 'minifollowups', 'sub' ]) )
plotifar_job.set_sub_file( '.'.join([ basename, 'plotifar', 'sub' ]) )
plotslides_play_job.set_sub_file( '.'.join([ basename, 'playground_only', 'plotslides', 'sub' ]) )
plotcumhist_play_job.set_sub_file( '.'.join([ basename, 'playground_only', 'plotcumhist', 'sub' ]) )
for fmtag, fmjob in plotfm_jobs:
  fmjob.set_sub_file( '.'.join([ basename, fmtag, 'plotfm', 'sub' ]) )
if options.generate_all_data_plots:
  plotslides_job.set_sub_file( '.'.join([ basename, 'plotslides', 'sub' ]) )
  plotcumhist_job.set_sub_file( '.'.join([ basename, 'plotcumhist', 'sub' ]) )

# set memory requirments for memory intensive jobs
t2c_job.add_condor_cmd("requirements", "Memory > 1000")
t2c_sim_job.add_condor_cmd("requirements", "Memory > 1000")
sql_replace_job.add_condor_cmd("requirements", "Memory > 1000")
sql_fromcache_job.add_condor_cmd("requirements", "Memory > 1000")
sql_frominput_job.add_condor_cmd("requirements", "Memory > 1000")
sql_extract_job.add_condor_cmd("requirements", "Memory > 1000")
inspinjfind_job.add_condor_cmd("requirements", "Memory > 1000")
printlc_job.add_condor_cmd("requirements", "Memory > 1000")
printsims_job.add_condor_cmd("requirements", "Memory > 1000")
printsims_nominifup_job.add_condor_cmd("requirements", "Memory > 1000")
minifup_job.add_condor_cmd("requirements", "Memory > 1000")
minifup_ts_job.add_condor_cmd("requirements", "Memory > 1000")
minifup_cm_job.add_condor_cmd("requirements", "Memory > 1000")

##############################################################################
# Cycle over the user tags, creating databases for each 

veto_files = {}
cluster_nodes = {}
inspinjfind_parents = {}
sim_tags = []
for tag in user_tags:

  print "Creating jobs for %s..." % tag
  
  # determine whether or not this was an injection run by checking if there
  # is an injection file for this tag

  file_sieve = '_'.join([ r'INJECTIONS_[0-9]{1,}', tag.split('_CAT')[0] ]) + r'$'
  inj_file = [ entry for entry in inj_cache if re.match(file_sieve, entry.description) is not None ]
  if len(inj_file) == 0:
    simulation = False
  elif len(inj_file) == 1:
    simulation = True
    inj_file = inj_file[0].url
    sim_tags.append(tag.split('_CAT_')[0])
  else:
    raise ValueError, "More than one injection file found for %s" % tag


  ############################################################################
  # Step 1: Setup thinca_to_coinc nodes

  print "\tsetting up thinca_to_coinc nodes..."

  # set job options

  # sieve zero_lag_cache for THINCA_SECOND files with this tag
  file_sieve = '*' + tag
  thinca_cache = zero_lag_cache.sieve( description = file_sieve, exact_match = True )
  # if not an injection run, check that there are an equal number of
  # slide files
  if not simulation and \
    len(slide_cache.sieve( description = file_sieve, exact_match = True )) != \
    len(thinca_cache):
    raise ValueError, "Number of %s slide files doesn't equal number of zero-lag files." % tag

  # also sieve all_inspirals_cache
  file_sieve = tag.split('_CAT_')[0]
  inspiral_cache = all_inspirals_cache.sieve( description = file_sieve )

  # get distinct on_instruments in the thinca cache
  distinct_instrument_sets = set([ entry.observatory for entry in thinca_cache ])
  # from the distinct_instrument_sets figure out what distinct ifos there are
  distinct_ifo_set = set()
  for on_instruments in distinct_instrument_sets:
    distinct_ifo_set |= lsctables.instrument_set_from_ifos(on_instruments)
  distinct_ifos = ''.join(sorted(distinct_ifo_set))

  # get the veto file
  cat_num = str(get_veto_cat_from_tag( tag ))
  veto_file_path = cp.get('input', 'ihope-segments-directory')
  veto_file_name = ''.join([
    distinct_ifos, '-VETOTIME_CAT_', cat_num, '-', experiment_start, '-', experiment_duration, 
    '.xml' ])
  veto_file = '/'.join([ veto_file_path, veto_file_name ])
  if not os.path.exists( veto_file ):
    raise ValueError, "Veto file %s could not be found." % veto_file
  # store the veto file for additional later use
  veto_cat = '_'.join(['CAT', cat_num, 'VETO'])
  veto_files[veto_cat] = veto_file
  # get the veto segments name
  veto_segs_name = get_veto_segments_name( cat_num, cumulative = True )

  t2c_output = []
  t2c_nodes = []
  this_cache = lal.Cache()
  for on_instruments in distinct_instrument_sets:
    instrument_cache = thinca_cache.sieve( ifos = on_instruments, exact_match = True )
    instrument_cache.sort()
    cache_start_time = int(experiment_start)
    for entry_num, entry in enumerate(instrument_cache):
      if not this_cache:
        # first entry; store type for cache naming
        # and add all the second_inspiral files to the cache
        cache_type = entry.description
        this_cache.extend(inspiral_cache)
      # Add the zero-lag thinca entry to this_cache 
      this_cache.append(entry)
      # if this isn't a simulation add the slide file
      if not simulation:
        slide_entry = lal.CacheEntry(re.sub('THINCA', 'THINCA_SLIDE', str(entry)))
        this_cache.append(slide_entry)
      # if n*20th entry or last remainder, write to cache
      if (entry_num + 1) % 20 == 0 or (entry_num + 1) == len(instrument_cache):
        # set duration to be end of this entry - start of first entry in cache
        cache_duration = entry.segment[1].seconds - cache_start_time
        this_cache_name = '.'.join([ 
          '-'.join([on_instruments, cache_type, str(cache_start_time), str(cache_duration)]),
          'cache' ])
        this_cache_file = open( this_cache_name, 'w' )
        this_cache.tofile(this_cache_file)
        this_cache_file.close()
        # write node
        if simulation:
          this_t2c_node = inspiral.ThincaToCoincNode(t2c_sim_job)
        else:
          this_t2c_node = inspiral.ThincaToCoincNode(t2c_job)
        this_t2c_node.set_category('thinca_to_coinc')
        this_t2c_node.set_instruments(lsctables.ifos_from_instrument_set( distinct_ifo_set ))
        this_t2c_node.set_input_cache(this_cache_name)
        this_t2c_node.set_veto_segments( veto_file )
        this_t2c_node.set_veto_segments_name( veto_segs_name )
        dag.add_node(this_t2c_node)
        t2c_nodes.append(this_t2c_node)
        # add output files to t2c_output
        t2c_output += this_t2c_node.get_output_from_cache()
        # set end-time of last entry to be start time of next cache
        cache_start_time = entry.segment[1].seconds
        del this_cache
        this_cache = lal.Cache()
      
  # write output cache
  output_cache = lal.Cache().from_urls(t2c_output)
  # create cache name from what's in the output cache
  cache_type = output_cache[0].description
  t2c_output_cache = '.'.join([ 
    '-'.join([ distinct_ifos, cache_type, experiment_start, experiment_duration ]), 
    'cache' ])
  fp = open( t2c_output_cache, 'w' )
  output_cache.tofile( fp )
  fp.close()

  ############################################################################
  # Step 2: Setup a LigolwSqliteNode for putting thinca_to_coincs 
  # into a sql db

  print "\tsetting up node to put thinca_to_coinc files into a SQLite database..."
  
  # set node options
  t2c2sql_node = pipeline.LigolwSqliteNode( sql_replace_job )
  t2c2sql_node.set_category('ligolw_sqlite')
  t2c2sql_node.set_input_cache( t2c_output_cache )
  t2c2sql_node.set_tmp_space( tmp_space )
  # database name has form: 
  # distinct_ifos-USER_TAG_CBC_RAW_RESULTS-cache_start-cache_duration.sqlite
  db_type = tag + '_RAW_CBC_RESULTS'
  raw_result_db = '.'.join([
    '-'.join([ distinct_ifos, db_type, experiment_start, experiment_duration ]),
    'sqlite' ])
  # check to make sure the database doesn't already exist
  if os.path.exists( raw_result_db ):
    print "WARNING: Raw result database %s already exists; " % raw_result_db + \
    "if it isn't moved, it will be overwritten when DAG is submitted."
    
  t2c2sql_node.set_database( raw_result_db )
  
  # set parent nodes to be all the thinca_to_coinc nodes
  [t2c2sql_node.add_parent( node ) for node in t2c_nodes]
      
  dag.add_node( t2c2sql_node )
  
  ############################################################################
  # Step 3: Setup a DBSimplifyNode to clean up the output of the t2c2sql_node 
  
  print "\tsetting up dbsimplify node to clean the database..."
  
  # set node options
  dbsimplify_node = inspiral.DBSimplifyNode( dbsimplify_job )
  dbsimplify_node.set_category('dbsimplify')
  dbsimplify_node.set_tmp_space( tmp_space )
  dbsimplify_node.set_database( raw_result_db )
  
  # set parent node
  dbsimplify_node.add_parent( t2c2sql_node )
  
  dag.add_node( dbsimplify_node )
 
  ############################################################################
  # Step 4: Setup a ClusterCoincsNode to cluster the output of dbsimplify_node
  
  print "\tsetting up cluster node to cluster coincs in the database..."
  
  # set node options
  cluster_node = inspiral.ClusterCoincsNode( cluster_job )
  cluster_node.set_category('cluster_coincs')
  cluster_node.set_tmp_space( tmp_space )
  cluster_node.set_input( raw_result_db )
  # output database name has form:
  # distinct_ifos-CBC_TRIGDB_CLUSTERED-USER_TAG-gps_start_time-durations.sqlite
  db_type = tag + '_CLUSTERED_CBC_RESULTS'
  result_db = '.'.join([
    '-'.join([ distinct_ifos, db_type, experiment_start, experiment_duration ]),
    'sqlite' ]) 
  cluster_node.set_output(result_db)
  
  # set parent node
  cluster_node.add_parent( dbsimplify_node )
  
  dag.add_node( cluster_node )

  # add to list of parents for veto2sql nodes
  cluster_nodes[tag] = cluster_node
  
  ############################################################################
  # Step 5: Do additional jobs for injection tags

  if simulation:
    # add dbaddinj node
    print "\tsetting up dbaddinj node to add the injection file..."
  
    # set node options
    dbaddinj_node = inspiral.DBAddInjNode( dbaddinj_job )
    dbaddinj_node.set_category('dbaddinj')
    dbaddinj_node.set_tmp_space( tmp_space )
    dbaddinj_node.set_database( result_db )
    dbaddinj_node.set_injection_file( inj_file )
  
    dbaddinj_node.add_parent( cluster_node )
  
    dag.add_node( dbaddinj_node )

    # add sqlite extract node
    print "\tsetting up ligolw_sqlite node to extract the injection database to an xml..."
    
    # set node options
    simxml_node = pipeline.LigolwSqliteNode( sql_extract_job )
    simxml_node.set_category('ligolw_sqlite')
    simxml_node.set_tmp_space( tmp_space )
    simxml_node.set_database( result_db )
    sim_xml = result_db.replace('.sqlite', '.xml')
    simxml_node.set_xml_output( sim_xml )

    simxml_node.add_parent( dbaddinj_node )
    dag.add_node( simxml_node )

    # add to list of inspinjfind parents
    if veto_cat not in inspinjfind_parents:
      inspinjfind_parents[veto_cat] = []
    inspinjfind_parents[veto_cat].append( simxml_node )

  else:
    # just cache the result_db
    non_sim_dbs.append( result_db )
    

##############################################################################
# done cycling over tags: Create inspinjfind job and node

# cache the sim xmls by veto category
print "Creating inspinjfind nodes..."

inspinjfind_nodes = {}
sim_caches = {}
sim_tags = set(sim_tags+['ALLINJ'])
for veto_cat, node_list in inspinjfind_parents.items():
  
  # create a inspinjfind node for each veto_category
  inspinjfind_node = inspiral.InspInjFindNode( inspinjfind_job )
  inspinjfind_node.set_category('inspinjfind')

  # add input files and parents
  for simxml_node in node_list:
    inspinjfind_node.add_file_arg( simxml_node.get_output() )
    inspinjfind_node.add_parent( simxml_node )
  dag.add_node( inspinjfind_node )

  inspinjfind_nodes[veto_cat] = inspinjfind_node

  # Cache the files
  sim_cache = lal.Cache().from_urls( inspinjfind_node.get_input_files() )
  if veto_cat == 'CAT_1_VETO':
    cache_type = 'ALLINJ_CLUSTERED_CBC_RESULTS'
  else:
    cache_type = '_'.join(['ALLINJ', veto_cat, 'CLUSTERED_CBC_RESULTS' ])
  instruments = set()
  for entry in sim_cache:
    instruments |= lsctables.instrument_set_from_ifos(entry.observatory)
  instruments = ''.join(sorted(instruments))
  sim_cache_name = '.'.join([ 
    '-'.join([ instruments, cache_type, experiment_start, experiment_duration ]), 
    'cache' ])
  sim_cache.tofile( open(sim_cache_name, 'w') )
  sim_caches[veto_cat] = sim_cache_name


##############################################################################
# now cycle over non-injection databases, 
# carrying out the rest of the pipeline

# cache the non_sim_dbs
result_dbs_cache = lal.Cache().from_urls( non_sim_dbs )

for result_db in result_dbs_cache:
  
  # get tag and veto_cat
  print "Creating jobs for %s database..." % result_db.description
  tag = result_db.description.replace('_CLUSTERED_CBC_RESULTS', '')
  cat_num = get_veto_cat_from_tag( tag )
  veto_cat = '_'.join([ 'CAT', str(cat_num), 'VETO' ])

  # get all possible instruments_on in this database
  instruments = lsctables.instrument_set_from_ifos(result_db.observatory)
  distinct_instrument_sets = [instruments]
  distinct_instrument_sets.extend( set(sub_combo)
    for nn in range(2, len(instruments))
    for sub_combo in iterutils.choices( list(instruments), nn ) )

  # add the injection xmls to the FULL_DATA databases
  if 'FULL_DATA' in tag and veto_cat in sim_caches:

    # create a sqlite node to add the injetion results
    sim2fulldb_node = pipeline.LigolwSqliteNode( sql_fromcache_job )
    sim2fulldb_node.set_category('ligolw_sqlite')
    sim2fulldb_node.set_input_cache( sim_caches[veto_cat] )
    sim2fulldb_node.set_database( result_db.path() )
    sim2fulldb_node.set_tmp_space( tmp_space )
    
    sim2fulldb_node.add_parent( inspinjfind_nodes[veto_cat] )
    sim2fulldb_node.add_parent( cluster_nodes[tag] )
    dag.add_node( sim2fulldb_node )

    # create a dbsimplify node to clean the database
    dbsimplify2_node = inspiral.DBSimplifyNode( dbsimplify_job )
    dbsimplify2_node.set_category('dbsimplify')
    dbsimplify2_node.set_tmp_space( tmp_space )
    dbsimplify2_node.set_database( result_db.path() )

    dbsimplify2_node.add_parent( sim2fulldb_node )
    dag.add_node( dbsimplify2_node )

  ############################################################################
  # Setup a LigolwSqliteNode for putting the veto-segments file into the
  # database 
  
  # write node to add the veto file
  veto2sql_node = pipeline.LigolwSqliteNode( sql_frominput_job )
  veto2sql_node.set_category('ligolw_sqlite')
  veto2sql_node.add_file_arg( veto_files[veto_cat] )
  veto2sql_node.set_tmp_space( tmp_space )
  veto2sql_node.set_database( result_db.path() )
  
  # set parent node
  veto2sql_node.add_parent( cluster_nodes[tag] )
  if 'FULL_DATA' in tag and veto_cat in sim_caches:
    veto2sql_node.add_parent( dbsimplify2_node )
  
  dag.add_node( veto2sql_node )
  
  ############################################################################
  # Step 3b: Compute durations in the database

  print "\tsetting up compute_durations node..."

  # set node options
  comp_durs_node = inspiral.ComputeDurationsNode( comp_durs_job)
  comp_durs_node.set_category('compute_durations')
  comp_durs_node.set_tmp_space( tmp_space )
  comp_durs_node.set_database( result_db.path() )

  # set parent node
  comp_durs_node.add_parent( veto2sql_node )

  dag.add_node(comp_durs_node)
  
  ############################################################################
  # Step 4: MVSC Calculation

  if 'FULL_DATA' in tag and veto_cat in sim_caches:
    print "\tsetting up MVSC nodes..."
    rank_nodes = []
    zl_rank_nodes = []
    info_pat_files = []
    dat_files = []

    # loop over pairs of detectors
    for comb in list(iterutils.choices(list(instruments), 2)):
      comb = ','.join(comb)

      # mvsc_get_doubles_node
      mvsc_get_doubles_node = inspiral.MvscGetDoublesNode( mvsc_get_doubles_job )
      mvsc_get_doubles_node.set_instruments( comb )
      mvsc_get_doubles_node.set_databases( [result_db.path()] )
      mvsc_get_doubles_node.set_trainingstr( tag + "_training" )
      mvsc_get_doubles_node.set_testingstr( tag + "_testing" )
      mvsc_get_doubles_node.set_zerolagstr( tag + "_zerolag" )
      mvsc_get_doubles_node.add_parent( comp_durs_node )
      mvsc_get_doubles_node.finalize()
      dag.add_node( mvsc_get_doubles_node )
      for file in mvsc_get_doubles_node.get_output_files():
        if file.endswith('_info.pat'): info_pat_files.append(file)

      # loop over round robin
      mvsc_train_forest_nodes = []
      for i in range(mvsc_get_doubles_node.number):
        file_for_this_set = mvsc_get_doubles_node.out_file_group[i]

        # mvsc_train_forest_node
        mvsc_train_forest_node = inspiral.MvscTrainForestNode( mvsc_train_forest_job )
        mvsc_train_forest_node.add_training_file( file_for_this_set[0] )
        mvsc_train_forest_node.add_parent( mvsc_get_doubles_node )
        mvsc_train_forest_node.finalize()
        dag.add_node( mvsc_train_forest_node )
        mvsc_train_forest_nodes.append( mvsc_train_forest_node )

        # mvsc_use_forest_node
        rank_node = inspiral.MvscUseForestNode( mvsc_use_forest_job )
        rank_node.set_trained_file( mvsc_train_forest_nodes[i].trainedforest )
        rank_node.set_file_to_rank( file_for_this_set[1] )
        rank_node.add_parent( mvsc_train_forest_node )
        rank_node.finalize()
        dag.add_node( rank_node )
        rank_nodes.append( rank_node )

        dat_files.extend( rank_node.get_output_files() )
      # finish loop over round robin

      # mvsc_use_forest_node for zerolag
      zl_rank_node = inspiral.MvscUseForestNode( mvsc_use_forest_job )
      zl_rank_node.set_trained_file( mvsc_train_forest_nodes[0].trainedforest )
      zl_rank_node.set_file_to_rank( mvsc_get_doubles_node.zerolag_file[0] )
      zl_rank_node.add_parent( mvsc_train_forest_nodes[0] )
      zl_rank_node.finalize()
      dag.add_node( zl_rank_node )
      zl_rank_nodes.append( zl_rank_node )

      dat_files.extend( zl_rank_node.get_output_files() )
    # finish loop over pairs of detectors

    mvsc_update_sql_node = inspiral.MvscUpdateSqlNode( mvsc_update_sql_job )
    for inputfile in [result_db.path()] + info_pat_files + dat_files:
      mvsc_update_sql_node.add_var_arg( inputfile )
    for p_node in zl_rank_nodes + rank_nodes:
      mvsc_update_sql_node.add_parent( p_node )
    dag.add_node( mvsc_update_sql_node )

  ############################################################################
  # Step 5a: Setup a CfarNode to compute the uncombined false alarm rates
  
  print "\tsetting up cfar nodes:"
  print "\t\tfor uncombined false alarm rates..."
  
  # set node options: output database is same as input
  ucfar_node = inspiral.CFarNode( ucfar_job )
  ucfar_node.set_category('cfar')
  ucfar_node.set_tmp_space( tmp_space )
  ucfar_node.set_input( result_db.path() )
  ucfar_node.set_output( result_db.path() )
  
  # set parent node
  ucfar_node.add_parent( comp_durs_node )
  if 'FULL_DATA' in tag and veto_cat in sim_caches:
    ucfar_node.add_parent( mvsc_update_sql_node )
  
  dag.add_node( ucfar_node )
  
  ############################################################################
  # Step 5b: Setup a CfarNode to compute the combined false alarm rates
  
  print "\t\tfor combined false alarm rates..."
  
  # set node options: output database is same as input
  ccfar_node = inspiral.CFarNode( ccfar_job )
  ccfar_node.set_category('cfar')
  ccfar_node.set_tmp_space( tmp_space )
  ccfar_node.set_input( result_db.path() )
  ccfar_node.set_output( result_db.path() )
  
  # set parent node
  ccfar_node.add_parent( ucfar_node )
  
  dag.add_node( ccfar_node )

  ############################################################################
  # Summary: Setup PrintLC and MiniFollowup Nodes to generate a summary of 
  # loudest non-simulation events

  print "\tsetting up printlc and minifollowup nodes..."

  # set datatypes to generate files for
  if 'PLAYGROUND' in tag:
    datatypes = ['playground', 'slide']
  else:
    datatypes = ['all_data', 'playground', 'slide']

  for datatype in datatypes:
    print "\t\tfor %s..." % datatype
    # set file naming type
    type_prefix = tag
    type = '_'.join([ type_prefix, 'LOUDEST', datatype.upper(), 'EVENTS_BY', cp.get('printlc', 'ranking-stat').upper()])
    # cycle over all ifos times, creating different tables for each
    for on_instruments in distinct_instrument_sets:
      on_instruments = lsctables.ifos_from_instrument_set(on_instruments)
      # set output and extracted xml file names
      summary_filename = '.'.join([
        '-'.join([ ''.join(on_instruments.split(',')), type + '_SUMMARY', experiment_start, experiment_duration ]),
        'xml' ])
      full_filename = '-'.join([ ''.join(on_instruments.split(',')), type, experiment_start, experiment_duration ])
      # set node options
      printlc_node = inspiral.PrintLCNode( printlc_job )
      printlc_node.set_category('printlc')
      printlc_node.set_tmp_space( tmp_space )
      printlc_node.set_input( result_db.path() )
      printlc_node.set_output( summary_filename )
      printlc_node.set_extract_to_xml( '.'.join([ full_filename, 'xml' ]) )
      printlc_node.set_extract_to_database( '.'.join([ full_filename, 'sqlite' ]) )
      printlc_node.set_include_only_coincs( '[ALLin' + on_instruments + ']' )
      printlc_node.set_datatype( datatype )

      # set parent node
      printlc_node.add_parent( ccfar_node )

      dag.add_node( printlc_node )

      # create the minifollowups nodes (not for CAT_1)
      if veto_cat != 'CAT_1_VETO':
        prefix = '-'.join([ ''.join(on_instruments.split(',')), tag ])
        suffix = re.sub(prefix + '_', '', summary_filename).rstrip('.xml')
  
        if datatype == 'slide':
          minifup_node = inspiral.MiniFollowupsNode( minifup_ts_job )
        else:
          minifup_node = inspiral.MiniFollowupsNode( minifup_job )
        minifup_node.set_category('minifollowups')
        minifup_node.set_cache_file( options.ihope_cache )
        minifup_node.set_cache_string( tag.split('_CAT_')[0] )
        minifup_node.set_prefix( prefix )
        minifup_node.set_suffix( suffix )
        minifup_node.set_input_xml( '.'.join([ full_filename, 'xml' ]) )
        minifup_node.set_input_xml_summary( summary_filename )
        minifup_node.set_output_html_table( re.sub('.xml', '.html', summary_filename ) )
        minifup_node.set_table_name( 'loudest_events' )
  
        minifup_node.add_parent( printlc_node )
        dag.add_node( minifup_node )
      
  ############################################################################
  # Injection summary: Setup printsims, minifollowup, and printmissed nodes

  if 'PLAYGROUND' not in tag:
    print "\tsetting up injection summary nodes..."

    # cycle over all the different types of injections
    for sim_tag in sim_tags:
  
      # 
      # Printsims
      #
      datatypes = ['playground']
      # TODO: if you're wondering why the list only has one entry... this originally cycled over
      # all_data and playground. As the rank column was dropped from minifollowups, and since
      # this would require wip to pick up two different files (playground for closed box, all_data for open)
      # I'm just dropping the all_data for now. If we decide to put it back later, I'll do so; otherwise,
      # if it isn't a useful feature, I'll drop the list and hardcode a datatype below at a future date.
  
      for datatype in datatypes:
        # set file naming type
        type = '_'.join([ sim_tag, veto_cat, 'QUIETEST_FOUND_COMPARED_TO', datatype.upper(), 'BY', cp.get('printsims', 'ranking-stat').upper()])
        # cycle over all ifos times, creating different tables for each
        for on_instruments in distinct_instrument_sets:
          on_instruments = lsctables.ifos_from_instrument_set(on_instruments)
          # set output-format to html for ALLINJ since won't run minifup on these types
          if sim_tag == 'ALLINJ':
            summ_file_type = 'html'
            columns = ','.join([
              'rank',
              'injected_decisive_distance',
              'injected_end_time',
              'injected_end_time_utc__Px_click_for_daily_ihope_xP_',
              'elogs',
              'injected_mass1',
              'injected_mass2',
              'sim_tag',
              'recovered_combined_far',
              'recovered_snr'])
          else:
            summ_file_type = 'xml'
          summary_filename = '.'.join([
            '-'.join([ ''.join(on_instruments.split(',')), type + '_SUMMARY', experiment_start, experiment_duration ]),
            summ_file_type ])
          full_filename = '-'.join([ ''.join(on_instruments.split(',')), type, experiment_start, experiment_duration ])
          # set node options
          if sim_tag == 'ALLINJ':
            printsims_node = inspiral.PrintSimsNode( printsims_nominifup_job )
            printsims_node.set_columns( columns )
          else:
            printsims_node = inspiral.PrintSimsNode( printsims_job )
          printsims_node.set_category('printsims')
          printsims_node.set_tmp_space( tmp_space )
          printsims_node.set_input( result_db.path() )
          printsims_node.set_output( summary_filename )
          printsims_node.set_extract_to_xml( '.'.join([ full_filename, 'xml' ]) )
          printsims_node.set_include_only_coincs( '[ALLin' + on_instruments + ']' )
          printsims_node.set_comparison_datatype( datatype )
          printsims_node.set_sim_tag( sim_tag )
          printsims_node.set_output_format( summ_file_type )
    
          # set parent node
          printsims_node.add_parent( ccfar_node )
    
          dag.add_node( printsims_node )
    
          # create the minifollowups nodes
          # this is not done for ALLINJ because there is no ALLINJ tag in the ihope_cache
          if not (veto_cat == 'CAT_1_VETO' or sim_tag == 'ALLINJ'):
            prefix = '-'.join([ ''.join(on_instruments.split(',')), '_'.join([sim_tag, veto_cat]) ])
            suffix = re.sub(prefix + '_', '', summary_filename).rstrip('.xml')
      
            minifup_node = inspiral.MiniFollowupsNode( minifup_job )
            minifup_node.set_category('minifollowups')
            minifup_node.set_cache_file( options.ihope_cache )
            minifup_node.set_cache_string( sim_tag )
            minifup_node.set_prefix( prefix )
            minifup_node.set_suffix( suffix )
            minifup_node.set_input_xml( '.'.join([ full_filename, 'xml' ]) )
            minifup_node.set_input_xml_summary( summary_filename )
            minifup_node.set_output_html_table( re.sub('.xml', '.html', summary_filename ) )
            minifup_node.set_table_name( 'selected_found_injections' )
      
            minifup_node.add_parent( printsims_node )
            dag.add_node( minifup_node )
      
      #
      #  Printmissed
      #
      type = '_'.join([ sim_tag, veto_cat, 'CLOSEST_MISSED_INJECTIONS' ])
      # cycle over all ifos times, creating different tables for each
      for on_instruments in distinct_instrument_sets:
        on_instruments = lsctables.ifos_from_instrument_set(on_instruments)
        # set output and extracted xml file names
        if sim_tag == 'ALLINJ':
          summ_file_type = 'html'
          columns = ','.join([
            'rank',
            'decisive_distance',
            'end_time',
            'end_time_utc__Px_click_for_daily_ihope_xP_',
            'elogs',
            'mchirp',
            'mass1',
            'mass2',
            'eff_dist_h',
            'eff_dist_l',
            'eff_dist_v',
            'sim_tag',
            'mini_followup'
            'mass1',
            'mass2',
            'eff_dist_h',
            'eff_dist_l',
            'eff_dist_v',
            'sim_tag'])
        else:
          summ_file_type = 'xml'
        summary_filename = '.'.join([
          '-'.join([ ''.join(on_instruments.split(',')), type + '_SUMMARY', experiment_start, experiment_duration ]),
          summ_file_type ])

        # set node options
        if sim_tag == 'ALLINJ':
          printmissed_node = inspiral.PrintMissedNode( printmissed_nominifup_job )
          printmissed_node.set_columns( columns )
        else:
          printmissed_node = inspiral.PrintMissedNode( printmissed_job )
        printmissed_node.set_category('printmissed')
        printmissed_node.set_tmp_space( tmp_space )
        printmissed_node.set_input( result_db.path() )
        printmissed_node.set_output( summary_filename )
        printmissed_node.set_include_only_coincs( '[ALLin' + on_instruments + ']' )
        printmissed_node.set_sim_tag( sim_tag )
        printmissed_node.set_output_format( summ_file_type )
  
        # set parent node
        printmissed_node.add_parent( ccfar_node )
  
        dag.add_node( printmissed_node )

        # create the minifollowups nodes
        # this is not done for ALLINJ because there is no ALLINJ tag in the ihope_cache
        if not (veto_cat == 'CAT_1_VETO' or sim_tag == 'ALLINJ'):
          prefix = '-'.join([ ''.join(on_instruments.split(',')), '_'.join([sim_tag, veto_cat]) ])
          suffix = re.sub(prefix + '_', '', summary_filename).rstrip('.xml')
    
          minifup_node = inspiral.MiniFollowupsNode( minifup_cm_job )
          minifup_node.set_category('minifollowups')
          minifup_node.set_cache_file( options.ihope_cache )
          minifup_node.set_cache_string( sim_tag )
          minifup_node.set_prefix( prefix )
          minifup_node.set_suffix( suffix )
          minifup_node.set_input_xml( summary_filename )
          minifup_node.set_output_html_table( re.sub('.xml', '.html', summary_filename ) )
          minifup_node.set_table_name( 'close_missed_injections' )
    
          minifup_node.add_parent( printmissed_node )
          dag.add_node( minifup_node )

        # create the plotfm node for each plotfm job
        for plotfm_tag, plotfm_job in plotfm_jobs:
          plotfm_node = inspiral.PlotFMNode( plotfm_job )
          plotfm_node.set_category('plotfm')
          plotfm_node.set_tmp_space( tmp_space )
          plotfm_node.add_file_arg( result_db.path() )
          plotfm_node.set_sim_tag( sim_tag )
          plotfm_node.set_user_tag( '_'.join([ plotfm_tag, sim_tag, veto_cat ]) )
            
          plotfm_node.add_parent( ccfar_node )
          dag.add_node( plotfm_node )



  ############################################################################
  # Plotting: Generate all result plots
  
  print "\tsetting up plotting jobs..."

  # Write plotslides node
  print "\t\tcreating plotslides node..."
  if not options.generate_all_data_plots or 'PLAYGROUND' in tag:
    plotslides_node = inspiral.PlotSlidesNode( plotslides_play_job )
  else:
    plotslides_node = inspiral.PlotSlidesNode( plotslides_job )

  plotslides_node.set_category('plotslides')
  plotslides_node.set_tmp_space( tmp_space )
  plotslides_node.set_input( result_db.path() )
  plotslides_node.set_user_tag( tag )

  plotslides_node.add_parent( ccfar_node )
  dag.add_node( plotslides_node )

  # create plotcumhist node
  print "\t\tcreating plotcumhist node..."
  if not options.generate_all_data_plots or 'PLAYGROUND' in tag:
    plotcumhist_node = inspiral.PlotCumhistNode( plotcumhist_play_job )
  else:
    plotcumhist_node = inspiral.PlotCumhistNode( plotcumhist_job )

  plotcumhist_node.set_tmp_space( tmp_space )
  plotcumhist_node.set_input( result_db.path() )
  plotcumhist_node.set_user_tag( tag )

  plotcumhist_node.add_parent( ccfar_node )

  dag.add_node( plotcumhist_node )

  # Write plotifar nodes for different datatypes
  print "\t\tcreating plotifar node for datatypes:"
  for datatype in ['all_data', 'playground', 'exclude_play']: 
    # only create nodes for non-playground if options.plot-playground-only not set
    if (not options.generate_all_data_plots or 'PLAYGROUND' in tag)  and datatype != 'playground':
      continue
    print "\t\t\t%s..." % datatype
    plotifar_node = inspiral.PlotIfarNode( plotifar_job )
    plotifar_node.set_category('plotifar')
    plotifar_node.set_tmp_space( tmp_space )
    plotifar_node.set_input( result_db.path() )
    plotifar_node.set_datatype( datatype )
    plotifar_node.set_user_tag( tag )
    
    # set parent node
    plotifar_node.add_parent( ccfar_node )

    dag.add_node( plotifar_node )
 
  
##############################################################################
##############################################################################
# Final Step: Write the DAG

print "Writing DAG and sub files..."

# set max-jobs: currently, only minifollowups is set
dag.add_maxjobs_category('minifollowups', 15)

dag.write_sub_files()
dag.write_dag()

# write process end time to log file and write log file
process.set_process_end_time(proc_id)
utils.write_filename(logdoc, basename+'.log.xml', xsl_file = "ligolw.xsl")

print "Finished!"
print "Now run:\n\tcondor_submit_dag %s" % os.path.basename(dag.get_dag_file())

sys.exit(0)

