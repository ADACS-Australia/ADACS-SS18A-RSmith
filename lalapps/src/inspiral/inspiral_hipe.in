#!/usr/bin/env @PYTHONPROG@
"""
inspiral_hipe.in - standalone inspiral pipeline driver script

$Id$

This script generates the condor DAG necessary to analyze LIGO and GEO data 
through the inspiral pipeline.  The DAG does datafind, tmpltbank, inspiral, 
inca and sire steps of the pipeline.  It analyzes the single, double, triple 
and quadro ifo times accordingly.  It can also be run with injections.
"""

__author__ = 'Stephen Fairhurst <sfairhur@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import sys, os, copy, math
import popen2, time
import re, string
from optparse import *
import tempfile
import ConfigParser
import urlparse
sys.path.append('@PYTHONLIBDIR@')

##############################################################################
# import the modules we need to build the pipeline
from glue import pipeline
import inspiral

##############################################################################
# some functions to make life easier later
class AnalyzedIFOData:
  """
  Contains the information for the data that needs to be filtered.
  """
  def __init__(self,chunk,node):
    self.__analysis_chunk = chunk
    self.__dag_node = node

  def set_chunk(self,chunk):
    self.__analysis_chunk = chunk

  def get_chunk(self):
    return self.__analysis_chunk

  def set_dag_node(self,node):
    self.__dag_node = node

  def get_dag_node(self):
    return self.__dag_node

##############################################################################
# function to generate the coincident segments from four input segment lists
def generate_segments(ifo1_data, ifo2_data, ifo3_data, ifo4_data):
  """
    compute the segments arising as the overlap of the four sets of single
    ifo segment lists.
    ifo1_data = data segments for ifo1
    ifo2_data = data segments for ifo2
    ifo3_data = data segments for ifo3
    ifo4_data = data segments for ifo4
  """ 

  segment_list = pipeline.ScienceData()
  segment_list = copy.deepcopy(ifo1_data)
  segment_list.intersect_4(ifo2_data, ifo3_data, ifo4_data)
   
  return segment_list

    
##############################################################################
# function to set up datafind, template bank and inspiral jobs for an ifo  
def analyze_ifo(ifo_name,ifo_data,ifo_to_do,tmplt_job,insp_job,df_job,\
  prev_df,dag,usertag=None):
  """
  Analyze the data from a single IFO.  Since the way we treat all this data is
  the same, this function is the same for all interferometers. Returns the last
  LSCdataFind job that was executed and the chunks analyzed.
  
  ifo_name = the name of the IFO
  ifo_data = the master science segs 
  ifo_to_do = the science segments we need to analyze
  tmplt_job = if not GeoBank: template bank job we should use 
  insp_job = the condor job that we should use to analyze data
  df_job = the condor job to find the data
  prev_df = the previous LSCdataFind job that was executed
  dag = the DAG to attach the nodes to
  usertag = the usertag to add to the job names
  """

  # add the non veto inspiral options
  insp_job.add_ini_opts(cp,'no-veto-inspiral')
  
  # add the ifo specific options
  insp_job.add_ini_opts(cp,ifo_name.lower() + '-inspiral')

  GeoBank = None
  if ifo_name == 'G1':
    data_opts = 'geo-data'
    try: type = cp.get('input','geo-type')
    except: type = None
    channel = cp.get('input','geo-channel')
    # to analyze GEO data we may use a fixed bank specified in ini file
    try: GeoBank = cp.get('input','geo-bank')
    except: GeoBank = None
    if GeoBank: print "For G1 we use bank ", GeoBank
  else:
    data_opts = 'ligo-data'
    try: type = cp.get('input','ligo-type')
    except: type = None
    channel = cp.get('input','ligo-channel')
   
  
  tmplt_job.add_ini_opts(cp,data_opts)
  insp_job.add_ini_opts(cp,data_opts)

  tmplt_job.set_channel(channel)
  insp_job.set_channel(channel)

  # see if we are using calibrated data
  calibrated = False
  for opt in cp.options(data_opts):
    if (opt.find('calibrated') >-1):
      calibrated = True
  if calibrated: print "we use calibrated data for ", ifo_name  
 

  chunks_analyzed = []
  # loop over the master science segments
  for seg in ifo_data:

    # loop over the master analysis chunks in the science segment
    for chunk in seg:
      done_this_chunk = 0

      # now loop over all the data that we need to filter
      for seg_to_do in ifo_to_do:

        # if the current chunk is in one of the segments we need to filter
        if inspiral.overlap_test(chunk,seg_to_do) and not done_this_chunk:

          # make sure we only filter the master chunk once
          done_this_chunk = 1

          # make sure we have done one and only one datafind for the segment
          if not opts.read_cache:
            if not seg.get_df_node():
              df = pipeline.LSCDataFindNode(df_job)
              df.set_observatory(ifo_name[0])
              df.set_start(seg.start())
              df.set_end(seg.end())
              seg.set_df_node(df)
              if type: df.set_type(type)
              if prev_df: df.add_parent(prev_df)
              if opts.datafind: dag.add_node(df)
              prev_df = df
          else:
            prev_df = None  

          # make a template bank job for the master chunk
          bank = inspiral.TmpltBankNode(tmplt_job)
          bank.set_start(chunk.start())
          bank.set_end(chunk.end())
          bank.set_ifo(ifo_name)
          if not opts.read_cache: bank.set_cache(df.get_output())
          else: bank.set_cache(cp.get('datafind',ifo_name+"-cache"))
          if not calibrated: bank.calibration()
          if opts.datafind: bank.add_parent(df)
          if (opts.template_bank and not GeoBank): dag.add_node(bank)
                 
          # make an inspiral job for the master chunk
          insp = inspiral.InspiralNode(insp_job)
          insp.set_start(chunk.start())
          insp.set_end(chunk.end())
          insp.set_trig_start(chunk.trig_start())
          insp.set_trig_end(chunk.trig_end())
          insp.set_ifo(ifo_name)
          if not opts.read_cache: insp.set_cache(df.get_output())
          else:  insp.set_cache(cp.get('datafind',ifo_name+"-cache"))
          if not calibrated: insp.calibration()
          if GeoBank: 
            insp.set_bank(GeoBank)
          else:  
            insp.set_bank(bank.get_output())
          if opts.datafind: insp.add_parent(df)
          if (opts.template_bank and not GeoBank): insp.add_parent(bank)
          if opts.inspiral: dag.add_node(insp)

          # store this chunk in the list of filtered data
          chunks_analyzed.append(AnalyzedIFOData(chunk,insp))

  return tuple([prev_df,chunks_analyzed])


##############################################################################
# function to do inca on single IFO data  
def single_coinc(ifo_analyzed,ifo_name,ifo_single,single_coinc_job,dag,
  usertag=None):
  """
  Run inca on the single coincident times from each of the IFOs. Since the way 
  we treat all this data is the same, this function is the same for all cases.
  
  ifo_analyzed = the analyzed chunks for the IFO
  ifo_name = the name of the IFO
  ifo_single = the single IFO science segments 
  single_coinc_job = the condor job to do single IFO inca
  dag = the DAG to attach the nodes to
  """

  single_coinc_analyzed = []
  for seg in ifo_single:
    sinca = inspiral.IncaNode(single_coinc_job)
    sinca.set_start(seg.start())
    sinca.set_end(seg.end())
    sinca.set_ifo_a(ifo_name)
    sinca.add_var_opt('single-ifo',' ')

    # add all master chunks that overlap with segment to input
    for ifo_done in ifo_analyzed:
      if inspiral.overlap_test(ifo_done.get_chunk(),seg):
        sinca.add_var_arg(ifo_done.get_dag_node().get_output())
        if opts.inspiral: sinca.add_parent(ifo_done.get_dag_node())
    
    if opts.coincidence: dag.add_node(sinca)
    single_coinc_analyzed.append(AnalyzedIFOData(seg,sinca))

  return single_coinc_analyzed
  
##############################################################################
# function to do thinca on coincident IFO data  
def thinca_coinc(ifos,single_data_analyzed,coinc_segs,coinc_job,
    dag,do_coinc,do_insp,usertag=None,ifotag=None,slides=None):
  """
  Run thinca on the coincident times from each of the sets of IFOs. 
  Since the way we treat all this data is the same, this function is the same 
  for all. 
  
  ifos = the ifos we are to analyze
  single_data_analyzed = dictionary of single ifo data analyzed
  coinc_segs = the double coincident IFO science segments 
  coinc_job = the condor job to do two IFO thinca
  dag = the DAG to attach the nodes to
  do_coinc = whether we should add these jobs to the dag
  do_insp  = whether previous inspiral jobs are in the dag
  usertag = the usertag to add to the output file name
  ifotag = the ifotag to add to the output file name
  slides = number of time slides req'd, used as a boolean
  """

  # add time slide arguments if doing a slide
  if slides:
    coinc_job.add_ini_opts(cp,'thinca-slide')
    
  coinc_analyzed = []
  for seg in coinc_segs:
    thinca = inspiral.ThincaNode(coinc_job)
    thinca.set_start(seg.start())
    thinca.set_end(seg.end())
    if ifotag:
      thinca.set_ifo_tag(ifotag)
    if slides:
      thinca.set_num_slides(slides)
    
    # scroll through ifos, adding the appropriate ones
    for ifo in ifo_list:
      if ifo in ifos:
        thinca.set_ifo(ifo)

        # add all IFO A master chunks that overlap with segment to input
        for done in single_data_analyzed[ifo]:
          if inspiral.overlap_test(done.get_chunk(),seg):
            thinca.add_var_arg(done.get_dag_node().get_output())
            if do_insp: thinca.add_parent(done.get_dag_node())

    if do_coinc: dag.add_node(thinca)
    coinc_analyzed.append(AnalyzedIFOData(seg,thinca))

  return coinc_analyzed


##############################################################################
# function to set up trigbank and inspiral jobs for an ifo  
def trig_inspiral(ifo_data,ifo_name,coinc_data,coinc_ifos,trig_job,\
                  insp_veto_job,dag,usertag=None):

  """
  Perform the trigbank and second inspiral steps for the single ifo data.  
  Since the way we treat all this data is the same, this function is the same 
  for all interferometers. Returns the chunks analyzed.

  ifo_data = the master science segs for the IFO
  ifo_name = the name of the ifo
  coinc_data = the coincident science segments and thinca jobs
  coinc_ifos  = the list of ifos in the coincident times
  trig_job = the triggered bank job we should use 
  insp_veto_job = the condor job that we should use to analyze data
  dag = the DAG to attach the nodes to
  usertag = the usertag to add to the job names
  """

  # add the veto option
  insp_veto_job.add_ini_opts(cp,'veto-inspiral')
  # add ifo specific options
  insp_veto_job.add_ini_opts(cp,ifo_name.lower() + '-inspiral')
  
  if ifo == 'G1':
    data_opts = 'geo-data'
    channel = cp.get('input','geo-channel')
  else:
    data_opts = 'ligo-data'
    channel = cp.get('input','ligo-channel')
 
  insp_veto_job.add_ini_opts(cp,data_opts)
  insp_veto_job.set_channel(channel)
  
     
  calibrated = False
  for opt in cp.options(data_opts):
    if (opt.find('calibrated') >-1):
      calibrated = True
  

  trig_chunks_analyzed = []
  # loop over the master science segments
  for seg in ifo_data:

    # loop over the master analysis chunks in the science segment
    for chunk in seg:
      done_this_chunk = 0

      # now loop over all the data that we need to filter
      for coinc_done in coinc_data:

        # if the current chunk is in one of the segments we need to filter
        if (inspiral.overlap_test(chunk,coinc_done.get_chunk()) ):

          if not done_this_chunk:
            # make sure we only filter the master chunk once
            done_this_chunk = 1

            # make a trigbank job for the master chunk
            trigbank = inspiral.TrigToTmpltNode(trig_job)
            trigbank.set_ifo_tag(coinc_ifos)
            trigbank.set_output_ifo(ifo_name)
            trigbank.set_input_ifo(ifo_name)
            trigbank.set_start(chunk.start())
            trigbank.set_end(chunk.end())

            if opts.trigbank: dag.add_node(trigbank)

            # make an inspiral job for the master chunk
            insp = inspiral.InspiralNode(insp_veto_job)
            insp.set_start(chunk.start())
            insp.set_end(chunk.end())
            insp.set_trig_start(chunk.trig_start())
            insp.set_trig_end(chunk.trig_end())
            insp.set_ifo(ifo_name)
            insp.set_ifo_tag(coinc_ifos)
            if not opts.read_cache: 
              insp.set_cache(seg.get_df_node().get_output())
            else: insp.set_cache(cp.get('datafind',ifo_name+'-cache')) 
            if not calibrated: insp.calibration()
            insp.set_bank(trigbank.get_trig_out())
            if opts.datafind: insp.add_parent(seg.get_df_node())
            if opts.trigbank: insp.add_parent(trigbank)
            if opts.inspiral_veto: dag.add_node(insp)

            # store this chunk in the list of filtered data
            trig_chunks_analyzed.append(AnalyzedIFOData(chunk,insp))

            # jobs already set up, just add the var_arg and parent to trigbank
          trigbank.add_var_arg(coinc_done.get_dag_node().get_output())
          if opts.coincidence: trigbank.add_parent(coinc_done.get_dag_node())

  return trig_chunks_analyzed

##############################################################################
# function to sweep up data using sire  
def sire_segs(segments,output,file_name,sire_job,dag,do_input,usertag=None,
  cluster=None,ifo_cut=None):

  """
  Do a sire to sweep up all the triggers of a specific kind
  
  segments  = list of inca segments to use
  output    = which output of inca/inspiral to use
  file_name = output file name
  sire_job  = sire job to use for the analysis
  dag       = name of the dag
  do_input  = whether the files to be sired were produced by this dag
  usertag   = the usertag to add to the output file name
  cluster   = whether we should add the cluster arguments
  ifo_cut   = keep triggers from only this ifo (if None, keep all)
  """
  
  sire = inspiral.SireNode(sire_job)

  if ifo_cut:
    sire.add_var_opt('ifo-cut',ifo_cut)

  # set the sire file name
  fname = file_name
  if usertag:
    fname += '_' + usertag
  fname += '.input'
 
 
  # add all the input files
  f = open(fname, 'w')
  for seg in segments:
    if do_input: sire.add_parent(seg.get_dag_node())
    if output == 'output':
      f.write('%s\n' % seg.get_dag_node().get_output())
    if output == 'output_a':
      f.write('%s\n' % seg.get_dag_node().get_output_a())
    if output == 'output_b':
      f.write('%s\n' % seg.get_dag_node().get_output_b())
  f.close()
 
  # add the input file name to sire
  sire.add_var_opt('input',fname)
  
  # set the output files names for sire
  if inj_coinc:
    sire.set_inj_outputs(file_name,inj_coinc,usertag)
    
  else:
    sire.set_outputs(file_name,usertag)

  if do_input: dag.add_node(sire) 

##############################################################################
# function to set up the cohbank and cohinspiral jobs for a network of ifos

def coherent_analysis(ifos,single_ifo_segments,coinc_nodes,cb_job,
                      trig_coh_jobs,insp_coh_jobs,cohinsp_job,dag):
  """
  Do a coherent analysis

  ifos                = ordered set of ifo identifiers
  single_ifo_segments = dictionary of data segments for the singl ifos
  coinc_nodes         = The coincident nodes for this ifo combination
  cb_job              = coherent bank job
  trig_coh_jobs       = dictionary of trigbank jobs
  insp_coh_jobs       = dictionary of inspiral jobs to produce c-data 
  cohinsp_job         = the coherent_inspiral job
  dag                 = name of the dag
  """

  # set the options for the various jobs
  # XXX make this automatic in inspiral.py XXX
  cb_job.add_ini_opts(cp,'cohbank')

  # set up inspirals:
  data_opts = {}
  for ifo in ifo_list:
    if ifo in ifos:
      insp_coh_jobs[ifo].add_ini_opts(cp,'no-veto-inspiral')
      insp_coh_jobs[ifo].add_ini_opts(cp,ifo.lower() + '-inspiral')
      if ifo == 'G1':
        data_opts[ifo] = 'geo-data'
        chan = cp.get('input','geo-channel')
      else:
        data_opts[ifo] = 'ligo-data'
        chan = cp.get('input','ligo-channel')
      insp_coh_jobs[ifo].add_ini_opts(cp,data_opts[ifo])
      insp_coh_jobs[ifo].set_channel(chan)

  # XXX make this automatic XXX
  cohinsp_job.add_ini_opts(cp,'chia')
  
  ifotag = ifos + 'C'

  for coinc_done in coinc_nodes:
    # set up cohbank
    cohbnk = inspiral.CohBankNode( cb_job )
    cohbnk.set_bank(coinc_done.get_dag_node().get_output())
    if opts.second_coinc: cohbnk.add_parent(coinc_done.get_dag_node())
    if opts.coherent_bank: dag.add_node(cohbnk)
    
    # set up cohinsp
    cohinsp = inspiral.ChiaNode(cohinsp_job)
    cohinsp.set_bank(cohbnk.get_output())
    cohinsp.set_start(coinc_done.get_chunk().start())
    cohinsp.set_end(coinc_done.get_chunk().end())
    cohinsp.add_var_opt('ifo-tag',ifotag)
    
    # set up trigbank and inspiral
    for ifo in ifo_list:
      if ifo in ifos:
        for seg in single_ifo_segments[ifo]:
          for chunk in seg:
            if inspiral.overlap_test(coinc_done.get_chunk(),chunk):

              trigbank = inspiral.TrigToTmpltNode(trig_coh_jobs[ifo])
              trigbank.set_ifo_tag(ifotag)
              trigbank.set_output_ifo(ifo)
              trigbank.set_input_ifo(ifo)
              trigbank.set_start(chunk.start())
              trigbank.set_end(chunk.end())
              trigbank.add_var_opt('parameter-test','no_test')
              trigbank.add_var_opt('data-type','all_data')
              trigbank.add_var_opt('debug-level','33')
              trigbank.add_var_arg(cohbnk.get_output())
              if opts.coherent_bank: trigbank.add_parent(cohbnk)
              if opts.coherent_bank: dag.add_node(trigbank)

              insp = inspiral.InspiralNode(insp_coh_jobs[ifo])
              insp.set_start(chunk.start())
              insp.set_end(chunk.end())
              insp.set_ifo(ifo)
              insp.set_ifo_tag(ifotag)
              insp.add_var_opt('write-cdata','')
              calibrated = False
              for opt in cp.options(data_opts[ifo]):
                if (opt.find('calibrated') >-1):
                  calibrated = True
              if not calibrated: insp.calibration()
              if not opts.read_cache: 
                insp.set_cache(seg.get_df_node().get_output())
              else: insp.set_cache(cp.get('datafind',ifo + "-cache"))
              insp.set_bank(trigbank.get_trig_out())
              if opts.coherent_bank: insp.add_parent(trigbank)
              if opts.coherent_bank: dag.add_node(insp)
              if opts.coherent_inspiral: cohinsp.add_parent(insp)

    if opts.coherent_inspiral: dag.add_node(cohinsp)
     
  return 0

##############################################################################
#
#  MAIN PROGRAM
#
##############################################################################
usage = """usage: %prog [options] 
"""

parser = OptionParser( usage )

parser.add_option("-v", "--version",action="store_true",default=False,\
    help="print version information and exit")
    
parser.add_option("-u", "--user-tag",action="store",type="string",\
    default=None,metavar=" USERTAG",\
    help="tag the jobs with USERTAG (overrides value in ini file)")

parser.add_option("-g", "--g1-data",action="store_true",default=False,\
    help="analyze g1 data")
parser.add_option("-a", "--h1-data",action="store_true",default=False,\
    help="analyze h1 data")
parser.add_option("-b", "--h2-data",action="store_true",default=False,\
    help="analyze h2 data")
parser.add_option("-l", "--l1-data",action="store_true",default=False,\
    help="analyze l1 data")

parser.add_option("-S", "--one-ifo",action="store_true",default=False,\
    help="analyze single ifo data (not usable for GEO)")
parser.add_option("-D", "--two-ifo",action="store_true",default=False,\
    help="analyze two interferometer data")
parser.add_option("-T", "--three-ifo",action="store_true",default=False,\
    help="analyze three interferometer data")
parser.add_option("-Q", "--four-ifo",action="store_true",default=False,\
    help="analyze four intereferometer data")
  
parser.add_option("-A", "--analyze-all",action="store_true",default=False,\
    help="analyze all ifos and all data (over-rides above)")

parser.add_option("-d", "--datafind",action="store_true",default=False,\
    help="run LSCdataFind to create frame cache files")
parser.add_option("-t", "--template-bank",action="store_true",default=False,\
    help="run lalapps_tmpltbank to generate template banks")
parser.add_option("-i", "--inspiral" ,action="store_true",default=False,\
    help="run lalapps_inspiral to generate triggers")
parser.add_option("-c", "--coincidence",action="store_true",default=False,\
    help="run lalapps_thinca to test for coincidence")
parser.add_option("-B", "--trigbank",action="store_true",default=False,\
    help="run lalapps_trigbank for banks of coinc triggers")
parser.add_option("-V", "--inspiral-veto",action="store_true",default=False,\
    help="run lalapps_inspiral with vetos")
parser.add_option("-C", "--second-coinc" ,action="store_true",default=False,\
    help="run lalapps_thinca on the inspiral veto triggers")
parser.add_option("-j", "--coherent-bank",action="store_true",default=False,\
    help="run lalapps_coherentbank to make coherent bank")
parser.add_option("-k", "--coherent-inspiral",action="store_true",
    default=False,help="run lalapps_coherent_inspiral for coherent analysis")
parser.add_option("-s", "--sire",action="store_true",default=False,\
    help="do sires to sweep up triggers")

parser.add_option("-R", "--read-cache",action="store_true",default=False,\
    help="read cache file from ini-file (if LSCDataFind is broken)")

parser.add_option("-P", "--priority",action="store",type="int",\
    metavar=" PRIO",help="run jobs with condor priority PRIO")

  
parser.add_option("-f", "--config-file",action="store",type="string",\
    metavar=" FILE",help="use configuration file FILE")

parser.add_option("-p", "--log-path",action="store",type="string",\
    metavar=" PATH",help="directory to write condor log file")

parser.add_option("-o", "--output-segs",action="store_true",default=False,\
    help="output the segment lists of analyzed data")

parser.add_option("-x","--dax", action="store_true", default=False,\
    help="create a dax instead of a dag")

command_line = sys.argv[1:]
(opts,args) = parser.parse_args()

#################################
# if --version flagged
if opts.version:
  print "$Id$"
  sys.exit(0)

#################################
# Sanity check of input arguments
if not opts.config_file:
  print >> sys.stderr, "No configuration file specified."
  print >> sys.stderr, "Use --config-file FILE to specify location."
  sys.exit(1)

if not opts.log_path:
  print >> sys.stderr, "No log file path specified."
  print >> sys.stderr, "Use --log-path PATH to specify a location."
  sys.exit(1)

  
if not opts.g1_data and not opts.h1_data and not opts.h2_data and \
    not opts.l1_data and not opts.analyze_all:
  print >> sys.stderr, "No ifos specified.  Please specify at least one of"
  print >> sys.stderr, "--g1-data, --h1-data, --h2-data, --l1-data"
  print >> sys.stderr, "or use --analyze-all to analyze all ifos all data"
  sys.exit(1)

if not opts.one_ifo and not opts.two_ifo and not opts.three_ifo and \
    not opts.four_ifo and not opts.analyze_all:
  print >> sys.stderr, "No number of ifos given. Please specify at least one of"
  print >> sys.stderr, "--one-ifo, --two-ifo, --three-ifo, --four-ifo"
  print >> sys.stderr, "or use --analyze-all to analyze all ifos all data"
  sys.exit(1)
  
if not opts.datafind and not opts.template_bank and \
     not opts.inspiral and not opts.coincidence and \
     not opts.trigbank and not opts.inspiral_veto and \
     not opts.second_coinc and not opts.coherent_bank and \
     not opts.coherent_inspiral:
  print >> sys.stderr, "No steps of the pipeline specified."
  print >> sys.stderr, "Please specify at least one of"
  print >> sys.stderr, "--datafind, --template-bank, --inspiral,"
  print >> sys.stderr, "--coincidence, --trigbank, --inspiral-veto,"
  print >> sys.stderr, "--second-coinc, --coherent-bank or --coherent-inspiral"
  sys.exit(1)
   
ifo_list = ['H1','H2','L1','G1']

ifotag = None
inj_coinc = 0
do_clust = True
#################################
# store the values
do = {}
do['G1'] = opts.g1_data
do['H1'] = opts.h1_data
do['H2'] = opts.h2_data
do['L1'] = opts.l1_data

#################################
# analyze everything if --analyze-all set
if opts.analyze_all:
  for ifo in ifo_list:
    do[ifo] = True
  opts.one_ifo   = True
  opts.two_ifo   = True
  opts.three_ifo = True
  opts.four_ifo  = True

##############################################################################
# determine all possible coincident sets of ifos and those to analyze:
analyze = []
ifo_coincs = []

# one ifo
for ifo1 in ifo_list:
  if opts.one_ifo and do[ifo1]:
      analyze.append(ifo1)

# two ifo
for ifo1 in ifo_list:
  for ifo2 in ifo_list:
    if ifo1 < ifo2:
      ifo_coincs.append(ifo1 + ifo2)
      if opts.two_ifo and do[ifo1] and do[ifo2]:
        analyze.append(ifo1 + ifo2)

# three ifo
for ifo1 in ifo_list:
  for ifo2 in ifo_list:
    for ifo3 in ifo_list:
      if ifo1 < ifo2 and ifo2 < ifo3:
        ifo_coincs.append(ifo1 + ifo2 + ifo3)
        if opts.three_ifo and do[ifo1] and do[ifo2] and do[ifo3]:
          analyze.append(ifo1 + ifo2 + ifo3)

# four ifo
for ifo1 in ifo_list:
  for ifo2 in ifo_list:
    for ifo3 in ifo_list:
      for ifo4 in ifo_list:
        if ifo1 < ifo2 and ifo2 < ifo3 and ifo3 < ifo4:
          ifo_coincs.append(ifo1 + ifo2 + ifo3 + ifo4)
          if opts.four_ifo and do[ifo1] and do[ifo2] and do[ifo3] and do[ifo4]:
            analyze.append(ifo1 + ifo2 + ifo3 + ifo4)

ifo_combinations = copy.deepcopy(ifo_list)
ifo_combinations.extend(ifo_coincs)


##############################################################################
# try to make a directory to store the cache files and job logs
try: os.mkdir('cache')
except: pass
try: os.mkdir('logs')
except: pass

##############################################################################
# create the config parser object and read in the ini file
cp = ConfigParser.ConfigParser()
cp.read(opts.config_file)

##############################################################################
# if a usertag has been specified, override the config file
if opts.user_tag:
  usertag = opts.usertag
  cp.set('pipeline','user-tag',usertag)
else:
  try:
    usertag = string.strip(cp.get('pipeline','user-tag'))
  except:
    usertag = None
  
##############################################################################
# create a log file that the Condor jobs will write to
basename = re.sub(r'\.ini',r'',opts.config_file)
tempfile.tempdir = opts.log_path
if usertag:
  tempfile.template = basename + '.' + usertag + '.dag.log.'
else:
  tempfile.template = basename + '.dag.log.'
logfile = tempfile.mktemp()
fh = open( logfile, "w" )
fh.close()

##############################################################################
# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile, opts.dax)
if usertag:
  dag.set_dag_file(basename + '.' + usertag )
else:
  dag.set_dag_file(basename )

# set better submit file names than the default
if usertag:
  subsuffix = '.' + usertag + '.sub'
else:
  subsuffix = '.sub'

##############################################################################
# create the Condor jobs that will be used in the DAG

# datafind:
df_job = pipeline.LSCDataFindJob('cache','logs',cp,opts.dax)
df_job.set_sub_file( basename + '.datafind'+ subsuffix )

# tmpltbank:
tmplt_jobs = {}

for ifo in ifo_list:
  tmplt_jobs[ifo] = inspiral.TmpltBankJob(cp)
  tmplt_jobs[ifo].set_sub_file( basename + '.tmpltbank' + subsuffix )

# inspiral:
insp_jobs = {}

for ifo in ifo_list:
  insp_jobs[ifo] = inspiral.InspiralJob(cp)
  insp_jobs[ifo].set_sub_file( basename + '.inspiral_' + ifo + subsuffix )

# single_inca:
single_inca_job = inspiral.IncaJob(cp)
single_inca_job.set_sub_file( basename + '.s_inca' + subsuffix )

# thinca:
thinca_jobs = {}

for ifos in ifo_coincs:
  thinca_jobs[ifos] = inspiral.ThincaJob(cp)
  thinca_jobs[ifos].set_sub_file(basename + '.thinca_' + ifos + 
      subsuffix )

# thinca_slide:
thinca_slide_jobs = {}

for ifos in ifo_coincs:
  thinca_slide_jobs[ifos] = inspiral.ThincaJob(cp)
  thinca_slide_jobs[ifos].set_sub_file(basename + '.thinca_slides_' + 
      ifos + subsuffix )

# trigbank:
trig_job = inspiral.TrigToTmpltJob(cp)
trig_job.set_sub_file( basename + '.trigbank' + subsuffix )

# inspiral veto:
insp_veto_jobs = {}

for ifo in ifo_list:
  insp_veto_jobs[ifo] = inspiral.InspiralJob(cp)
  insp_veto_jobs[ifo].set_sub_file( basename + '.inspiral_veto' + ifo +
    subsuffix )

# thinca:
thinca2_jobs = {}

for ifos in ifo_coincs:
  thinca2_jobs[ifos] = inspiral.ThincaJob(cp)
  thinca2_jobs[ifos].set_sub_file(basename + '.thinca2_' + ifos + 
      subsuffix )

# thinca_slide:
thinca2_slide_jobs = {}

for ifos in ifo_coincs:
  thinca2_slide_jobs[ifos] = inspiral.ThincaJob(cp)
  thinca2_slide_jobs[ifos].set_sub_file(basename + '.thinca2_slides_' + 
      ifos + subsuffix )

# coherent bank:
cb_job = inspiral.CohBankJob(cp)
cb_job.set_sub_file( basename + '.cohbank' + subsuffix )

# trigbank coherent:
trig_coh_jobs = {}

for ifo in ifo_list:
  trig_coh_jobs[ifo] = inspiral.TrigToTmpltJob(cp)
  trig_coh_jobs[ifo].set_sub_file( basename + '.trigbank_coherent_' + ifo + 
      subsuffix )

# inspiral (coherent):
insp_coh_jobs = {}

for ifo in ifo_list:
  insp_coh_jobs[ifo] = inspiral.InspiralJob(cp)
  insp_coh_jobs[ifo].set_sub_file( basename + '.insp_coh_' + ifo +
    subsuffix )

# coherent_inspiral:
cohinsp_job = inspiral.ChiaJob(cp)
cohinsp_job.set_sub_file( basename + '.cohinspiral' + subsuffix )

# sire:
sire_job = inspiral.SireJob(cp)
sire_job.set_sub_file( basename + '.sire' + subsuffix )

sire_clust_job = inspiral.SireJob(cp)
sire_clust_job.set_sub_file( basename + '.sire_clust' + subsuffix )


all_jobs = [single_inca_job, trig_job, cb_job, sire_job, 
  sire_clust_job]
all_jobs.extend(tmplt_jobs.values())
all_jobs.extend(insp_jobs.values())
all_jobs.extend(thinca_jobs.values())
all_jobs.extend(thinca_slide_jobs.values())
all_jobs.extend(insp_veto_jobs.values())
all_jobs.extend(thinca2_jobs.values())
all_jobs.extend(thinca2_slide_jobs.values())
all_jobs.extend(trig_coh_jobs.values())
all_jobs.extend(insp_coh_jobs.values())

##############################################################################
# set the usertag in the jobs
if usertag:
  for job in all_jobs:
    job.add_opt('user-tag',usertag)

all_jobs.append(df_job)
##############################################################################
# set the condor job priority
if opts.priority:
  for job in all_jobs:
    job.add_condor_cmd('priority',opts.priority)

##############################################################################
# read in the injection-file from the ini file 
# and add to inspiral and sire jobs
try:
  inj_file = string.strip(cp.get('input','injection-file'))
except:
  inj_file = None
  
if inj_file:
  inj_jobs = [sire_job, sire_clust_job]
  inj_jobs.extend(insp_jobs.values())
  inj_jobs.extend(insp_veto_jobs.values())
  inj_jobs.extend(insp_coh_jobs.values())

  for job in inj_jobs:
    job.add_opt('injection-file',inj_file)


##############################################################################
# read in the number of time-slides from the ini file 
try:
  num_slides = int(cp.get('input','num-slides'))
except:
  num_slides = None


#############################################################################
# read in playground data mask from ini file 
# set the playground_only option and add to inca and sire jobs
try:
  play_data_mask = string.strip(cp.get('pipeline','playground-data-mask'))
except:
  play_data_mask = None

play_jobs = [trig_job]
play_jobs.extend(thinca_jobs.values())
play_jobs.extend(thinca_slide_jobs.values())
play_jobs.extend(thinca2_jobs.values())
play_jobs.extend(thinca2_slide_jobs.values())

if play_data_mask == 'playground_only':
  playground_only = 2
  
  single_inca_job.add_opt('playground-only',' ')
  sire_job.add_opt('playground-only',' ')
  sire_clust_job.add_opt('playground-only',' ')

  for job in play_jobs:
    job.add_opt('data-type','playground_only')

elif play_data_mask == 'exclude_playground':
  playground_only = 0
  
  single_inca_job.add_opt('no-playground',' ')
  sire_job.add_opt('exclude-playground',' ')
  sire_clust_job.add_opt('exclude-playground',' ')
  
  for job in play_jobs:
    job.add_opt('data-type','exclude_play')


elif play_data_mask == 'all_data':
  playground_only = 0

  single_inca_job.add_opt('all-data',' ')
  sire_job.add_opt('all-data',' ')
  sire_clust_job.add_opt('all-data',' ')
  
  for job in play_jobs:
    job.add_opt('data-type','all_data')

else:
  print "Invalid playground data mask " + play_data_mask + " specified"
  sys.exit(1)

##############################################################################
if inj_file:
  # add the injection coincidence to the sire jobs
  sire_job.add_ini_opts(cp,'sire-inj')
  sire_clust_job.add_ini_opts(cp,'sire-inj')
 
 
############################################################################## 
# add the clustering info to sire clust 
sire_clust_job.add_ini_opts(cp,'sire-cluster')

##############################################################################
# get the pad and chunk lengths from the values in the ini file
pad = int(cp.get('data', 'pad-data'))
n = int(cp.get('data', 'segment-length'))
s = int(cp.get('data', 'number-of-segments'))
r = int(cp.get('data', 'sample-rate'))
o = int(cp.get('inspiral', 'segment-overlap'))
length = ( n * s - ( s - 1 ) * o ) / r
overlap = o / r


##############################################################################
#  The meat of the DAG generation comes below
#
#
#  The various data sets we compute are:
# 
#  data[ifo] : the science segments and master chunks
#
#  data_out[ifo] : the analyzable data 
#
#  not_data_out[ifo] : non analyzable data
#
#  analyzed_data[ifos] : the 1,2,3,4 ifo coincident data 
#
#  data_to_do[ifo] : the data to analyze for each ifo
#       (depends upon which of single,double,triple, quadruple data we analyze) 
#
#  And the lists of jobs are:
#
#  chunks_analyzed[ifo] : list of chunks analyzed for each ifo
#
#  single_coinc_nodes[ifo] : the single coincident inca jobs
#
#  coinc_nodes[ifos] : the double, triple, quadruple coincident thinca nodes

#  coinc_slide_nodes[ifos] : the double, triple, quadruple coincident 
#                            thinca slide nodes
#
#  analyzed_coinc_data[ifos][ifo] : the single ifo analyzed data (post coinc)
#                                   for each coincident combination, we have
#                                   data from each of the active ifos
#
#  coinc2_nodes[ifos] : the double, triple, quadruple coincident thinca nodes
#                       from the second coincidence step
#
#
#  coinc2_slide_nodes[ifos] : the double, triple, quadruple coincident 
#                             thinca slide nodes from the second coincidence 
#                             step
##############################################################################



##############################################################################
#   Step 1: read science segs that are greater or equal to a chunk 
#   from the input file

print "reading in single ifo science segments and creating master chunks...",
sys.stdout.flush()

segments = {}
data = {}

for ifo in ifo_list:
  try:
    segments[ifo] = cp.get('input', ifo +'-segments')
  except:
    segments[ifo] = None
  
  data[ifo] = pipeline.ScienceData() 
  if segments[ifo]:
    data[ifo].read(segments[ifo],length + 2 * pad) 
    data[ifo].make_chunks(length,overlap,playground_only,0,overlap/2,pad)
    data[ifo].make_chunks_from_unused(length,overlap/2,playground_only,
        0,0,overlap/2,pad)

print "done"

##############################################################################
#   Step 2: determine analyzable times

data_out = {}
not_data_out = {}
for ifo in ifo_list:
  data_out[ifo] = copy.deepcopy(data[ifo])

  # remove start and end of science segments which aren't analyzed for triggers
  for seg in data_out[ifo]:
    seg.set_start(seg.start() + overlap/2 + pad)
    seg.set_end(seg.end() - overlap/2 - pad)

  if playground_only:
    data_out[ifo].play()

  not_data_out[ifo] = copy.deepcopy(data_out[ifo])
  not_data_out[ifo].coalesce()
  not_data_out[ifo].invert()

# determine the data we can analyze for various detector combinations
analyzed_data = {}

# determine the coincident data, if it is to be analyzed
for ifos in ifo_combinations:
  analyzed_data[ifos] = pipeline.ScienceData()
  if ifos in analyze:
    selected_data = []
    for ifo in ifo_list:
      if ifo in ifos:
        selected_data.append(data_out[ifo])
      else:
        selected_data.append(not_data_out[ifo])
    analyzed_data[ifos] = generate_segments(selected_data[0], 
        selected_data[1], selected_data[2], selected_data[3])

  
##############################################################################
# Step 3: Compute the Science Segments to analyze

data_to_do = {}

for ifo in ifo_list:
  data_to_do[ifo] = copy.deepcopy(analyzed_data[ifo])
  for ifos in analyze:
    if ifo in ifos:
      data_to_do[ifo].union(analyzed_data[ifos])
  data_to_do[ifo].coalesce() 

##############################################################################
# Step 4: Determine which of the master chunks needs to be filtered

chunks_analyzed = {}

prev_df = None

for ifo in ifo_list:
  print "setting up jobs to filter " + ifo + " data...",
  sys.stdout.flush()

  (prev_df,chunks_analyzed[ifo]) = analyze_ifo(ifo,data[ifo],data_to_do[ifo],  
      tmplt_jobs[ifo],insp_jobs[ifo],df_job,prev_df,dag,usertag)
  
  # Step 4S: Run sire on the single ifo triggers
  if opts.sire and len(chunks_analyzed[ifo]):   
    sire_segs(chunks_analyzed[ifo],'output',ifo + '-INSPIRAL',sire_clust_job,
        dag,opts.inspiral,usertag,do_clust,ifo)

  print "done" 


##############################################################################
# Step 5: Run inca in single ifo mode on the single ifo triggers.

print "setting up jobs to inca single IFO data...",
sys.stdout.flush()

single_coinc_nodes = {}
for ifo in ifo_list:
  single_coinc_nodes[ifo] = []
  single_coinc_nodes[ifo] = single_coinc(chunks_analyzed[ifo],ifo,
     analyzed_data[ifo],single_inca_job,dag,usertag)
  
  # Step 5S: Run sire on the single ifo triggers
  if opts.sire and do[ifo] and len(analyzed_data[ifo]):
    sire_segs(single_coinc_nodes[ifo],'output_a',ifo + '-SIRE_SINGLE_IFO',
        sire_clust_job,dag,opts.coincidence,usertag,do_clust,ifo)

print "done"
  
 
##############################################################################
# Step 6: Run thinca on each of the disjoint sets of coincident data

coinc_nodes = {}
coinc_slide_nodes = {}

for ifos in ifo_coincs:
  print "setting up thinca jobs on " + ifos + " data...",
  sys.stdout.flush()

  coinc_nodes[ifos] = thinca_coinc(ifos,chunks_analyzed,analyzed_data[ifos],
      thinca_jobs[ifos],dag,opts.coincidence,opts.inspiral,usertag)

  # Step 6 slide: Time slides 
  coinc_slide_nodes[ifos] = []
  if num_slides:
    coinc_slide_nodes[ifos] = thinca_coinc(ifos,chunks_analyzed,
        analyzed_data[ifos],thinca_slide_jobs[ifos],dag,opts.coincidence,
        opts.inspiral,usertag,ifotag,num_slides)

  
  # Step 6 sire: Run sire triggers
  # Do sire with all coincidenct triggers, and clustered sires for each ifo
  if opts.sire and len(analyzed_data[ifos]):
    sire_segs(coinc_nodes[ifos],'output',ifos + '-SIRE',sire_job,dag,
        opts.coincidence,usertag)
    for ifo in ifo_list: 
      if ifo in ifos:
        sire_segs(coinc_nodes[ifos],'output',ifo + '-SIRE_' + ifos,sire_job,
            dag,opts.coincidence,usertag,ifo)
  
  # Concatenate the zerolag and slide nodes
  coinc_nodes[ifos] = coinc_nodes[ifos] + coinc_slide_nodes[ifos]
  
  print "done"


##############################################################################
# Step 7: Run trigbank and inspiral on each instrument in two ifo times

analyzed_coinc_data = {}

for ifos in ifo_coincs:
  analyzed_coinc_data[ifos] = {}
  
  for ifo in ifo_list:
    if ifo in ifos:
      print "setting up jobs to filter " + ifo + \
        " data with coinc trigs from " + ifos + " times ...",
      sys.stdout.flush()

      analyzed_coinc_data[ifos][ifo] = trig_inspiral(data[ifo],ifo,
          coinc_nodes[ifos],ifos,trig_job,insp_veto_jobs[ifo],dag,usertag)

      print "done"


##############################################################################
# Step 8: Run thinca2 on each of the disjoint sets of coincident data

coinc2_nodes = {}
coinc2_slide_nodes = {}

for ifos in ifo_coincs:
  print "setting up second thinca jobs on " + ifos + " data...",
  sys.stdout.flush()

  coinc2_nodes[ifos] = thinca_coinc(ifos,analyzed_coinc_data[ifos],
      analyzed_data[ifos],thinca2_jobs[ifos],dag,opts.second_coinc,
      opts.inspiral_veto,usertag,ifos)

  # Step 8slide: Time slides on the data
  if num_slides:
    coinc2_slide_nodes[ifos] = thinca_coinc(ifos,
        analyzed_coinc_data[ifos],analyzed_data[ifos],thinca2_slide_jobs[ifos],
        dag,opts.second_coinc,opts.inspiral_veto,usertag,ifos,num_slides)
  
  # Step 8 sire: Run sire triggers
  # Do sire with all coincidenct triggers, and clustered sires for each ifo
  if opts.sire and len(analyzed_data[ifos]):
    sire_segs(coinc_nodes[ifos],'output',ifos + '-SIRE_VETO_',sire_job,dag,
    opts.second_coinc,usertag)
    
    for ifo in ifo_list: 
      if ifo in ifos:
        sire_segs(coinc_nodes[ifos],'output',ifo + '-SIRE_VETO_' + ifos,
            sire_job,dag,opts.second_coinc,usertag,ifo)
    
  print "done"


##############################################################################
# Step 9: Do the coherent analyses:

for ifos in ifo_coincs:
  print "setting up coherent jobs on " + ifos + " data...",
  sys.stdout.flush()
  
  coherent_analysis(ifos,data,coinc2_nodes[ifos],cb_job,
      trig_coh_jobs,insp_coh_jobs,cohinsp_job,dag)

  print "done"

##############################################################################
# Step 10: Write out the DAG, help message and log file
dag.write_sub_files()
dag.write_dag()

##############################################################################  
# write a message telling the user that the DAG has been written
if opts.dax:
  
  print "\nCreated an abstract DAX file", dag.get_dag_file()
  print "which can be transformed into a concrete DAG with gencdag."
  print "\nSee the documentation on http://www.lsc-group.phys.uwm.edu/lscdatagrid/griphynligo/pegasus_lsc.html"

else:
  print "\nCreated a DAG file which can be submitted by executing"
  print "\n   condor_submit_dag", dag.get_dag_file()
  print """\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
  If you are running LSCdataFind jobs, do not forget to initialize your grid 
  proxy certificate on the condor submit machine by running the commands
  
     unset X509_USER_PROXY
           grid-proxy-init -hours 72

  Enter your pass phrase when promted. The proxy will be valid for 72 hours. 
  If you expect the LSCdataFind jobs to take longer to complete, increase the
  time specified in the -hours option to grid-proxy-init. You can check that 
  the grid proxy has been sucessfully created by executing the command:
  
    grid-cert-info -all -file /tmp/x509up_u`id -u`
  
  This will also give the expiry time of the proxy. You should also make sure
  that the environment variable LSC_DATAFIND_SERVER is set the hostname and
  optional port of server to query. For example on the UWM medusa cluster this
  you should use
  
    export LSC_DATAFIND_SERVER=dataserver.phys.uwm.edu
  
  Contact the administrator of your cluster to find the hostname and port of the
  LSCdataFind server.
  """

##############################################################################
# write out a log file for this script
if usertag:
  log_fh = open(basename + '.pipeline.' + usertag + '.log', 'w')
else:
  log_fh = open(basename + '.pipeline.log', 'w')
  
log_fh.write( "$Id$" + "\n" )
log_fh.write( "$Name$" + "\n\n" )
log_fh.write( "Invoked with arguments:" )
for arg in command_line:
  if arg[0] == '-':
    log_fh.write( "\n" )
  log_fh.write( arg + ' ')

log_fh.write( "\n" )
log_fh.write( "Config file has CVS strings:\n" )
log_fh.write( cp.get('pipeline','version') + "\n" )
log_fh.write( cp.get('pipeline','cvs-tag') + "\n\n" )

print >> log_fh, "\n===========================================\n"
print >> log_fh, "Science Segments and master chunks:\n"

for ifo in ifo_list:
  print >> log_fh, ifo + "Data\n"
  for seg in data[ifo]:
    print >> log_fh, " ", seg
    for chunk in seg:
      print >> log_fh, "   ", chunk


for ifo in ifo_list:
  print >> log_fh, "\n===========================================\n"
  log_fh.write( 
    "Filtering " + str(len(chunks_analyzed[ifo])) + " " + ifo + \
    " master chunks\n" )
  total_time = 0
  for ifo_done in chunks_analyzed[ifo]:
    print >> log_fh, ifo_done.get_chunk()
    total_time += len(ifo_done.get_chunk())
  print >> log_fh, "\n total time", total_time, "seconds"

for ifo in ifo_list:
  print >> log_fh, "\n===========================================\n"
  log_fh.write( "Writing " + str(len(analyzed_data[ifo])) + " " + ifo + \
    " single IFO science segments\n" )
  total_time = 0
  for seg in analyzed_data[ifo]:
    print >> log_fh, seg
    total_time += seg.dur()
  print >> log_fh, "\n total time", total_time, "seconds"

  if opts.output_segs and len(analyzed_data[ifo]):
    if playground_only:
      f = open(ifo + '_play_segs_analyzed.txt', 'w')
    else:  
      f = open(ifo + '_segs_analyzed.txt', 'w')
    for seg in analyzed_data[ifo]:
      f.write('%4d %10d %10d %6d\n' % (seg.id(), seg.start(), seg.end(), 
        seg.dur()))
    f.close()


for ifos in ifo_coincs:  
  print >> log_fh, "\n===========================================\n"
  log_fh.write( "Writing " + str(len(analyzed_data[ifos])) + " " + ifos + \
    " coincident segments\n" )
  total_time = 0
  for seg in analyzed_data[ifos]:
    print >> log_fh, seg
    total_time += seg.dur()
  print >> log_fh, "\n total time", total_time, "seconds"

  if opts.output_segs and len(analyzed_data[ifos]):
    if playground_only:
      f = open(ifos + '_play_segs_analyzed.txt', 'w')
    else:  
      f = open(ifos + '_segs_analyzed.txt', 'w')
    for seg in analyzed_data[ifos]:
      f.write('%4d %10d %10d %6d\n' % (seg.id(), seg.start(), seg.end(), 
        seg.dur()))
    f.close()

sys.exit(0)

