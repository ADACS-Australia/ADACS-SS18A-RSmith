#!/usr/bin/env @PYTHONPROG@
"""
inspiral_hipe.in - standalone inspiral pipeline driver script

$Id$

This script generates the condor DAG necessary to analyze LIGO data through 
the inspiral pipeline.  The DAG does datafind, tmpltbank, inspiral, inca and 
sire steps of the pipeline.  It analyzes the single, double and triple ifo 
times accordingly.  It can also be run with injections.
"""

__author__ = 'Stephen Fairhurst <sfairhur@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import sys, os, copy, math
import popen2, time
import getopt, re, string
import tempfile
import ConfigParser
import urlparse
sys.path.append('@PYTHONLIBDIR@')

##############################################################################
# import the modules we need to build the pipeline
from glue import pipeline
import inspiral

##############################################################################
# some functions to make life easier later
class AnalyzedIFOData:
  """
  Contains the information for the data that needs to be filtered.
  """
  def __init__(self,chunk,node):
    self.__analysis_chunk = chunk
    self.__dag_node = node

  def set_chunk(self,chunk):
    self.__analysis_chunk = chunk

  def get_chunk(self):
    return self.__analysis_chunk

  def set_dag_node(self,node):
    self.__dag_node = node

  def get_dag_node(self):
    return self.__dag_node

    
##############################################################################
# function to set up datafind, template bank and inspiral jobs for an ifo  
def analyze_ifo(ifo_data,ifo_name,ifo_to_do,tmplt_job,insp_job,df_job,snr,
  chisq,pad,prev_df,dag,usertag=None):
  """
  Analyze the data from a single IFO.  Since the way we treat all this data is
  the same, this function is the same for all interferometers. Returns the last
  LSCdataFind job that was executed and the chunks analyzed.
  
  ifo_data = the master science segs for the IFO
  ifo_to_do = the science segments we need to analyze
  ifo_name = the name of the IFO
  tmplt_job = the template bank job we should use 
  insp_job = the condor job that we should use to analyze data
  df_job = the condor job to find the data
  snr = the signal-to-noise threshold for this IFO
  chisq = the chi squared threshold for this IFO
  pad = data start/end padding
  prev_df = the previous LSCdataFind job that was executed
  dag = the DAG to attach the nodes to
  usertag = the usertag to add to the job names
  """
  chunks_analyzed = []
  # loop over the master science segments
  for seg in ifo_data:

    # make sure that we do not do a datafind more than once per science segment
    done_df_for_seg = None

    # loop over the master analysis chunks in the science segment
    for chunk in seg:
      done_this_chunk = 0

      # now loop over all the data that we need to filter
      for seg_to_do in ifo_to_do:

        # if the current chunk is in one of the segments we need to filter
        if inspiral.overlap_test(chunk,seg_to_do) and not done_this_chunk:

          # make sure we only filter the master chunk once
          done_this_chunk = 1

          # make sure we have done one and only one datafind for the segment
          if not done_df_for_seg:
            df = pipeline.LSCDataFindNode(df_job)
            df.set_observatory(ifo_name[0])
            df.set_start(seg.start() - pad)
            df.set_end(seg.end() + pad)
            if prev_df: df.add_parent(prev_df)
            if do_datafind: dag.add_node(df)
            prev_df = df
            done_df_for_seg = 1

          # make a template bank job for the master chunk
          bank = inspiral.TmpltBankNode(tmplt_job)
          bank.set_start(chunk.start())
          bank.set_end(chunk.end())
          bank.set_ifo(ifo_name)
          bank.set_cache(df.get_output())
          bank.calibration()
          if do_datafind: bank.add_parent(df)
          if do_tmpltbank: dag.add_node(bank)

          # make an inspiral job for the master chunk
          insp = inspiral.InspiralNode(insp_job)
          insp.set_start(chunk.start())
          insp.set_end(chunk.end())
          insp.add_var_opt('snr-threshold',snr)
          insp.add_var_opt('chisq-threshold',chisq)
          insp.add_var_opt('trig-start-time',chunk.trig_start())
          insp.add_var_opt('trig-end-time',chunk.trig_end())
          insp.set_ifo(ifo_name)
          insp.set_cache(df.get_output())
          insp.calibration()
          insp.set_bank(bank.get_output())
          if do_datafind: insp.add_parent(df)
          if do_tmpltbank: insp.add_parent(bank)
          if do_inspiral: dag.add_node(insp)

          # store this chunk in the list of filtered data
          chunks_analyzed.append(AnalyzedIFOData(chunk,insp))

  return tuple([prev_df,chunks_analyzed])

##############################################################################
# function to do inca on single IFO data  
def single_coinc(ifo_analyzed,ifo_name,ifo_single,single_coinc_job,dag,
  usertag=None):
  """
  Run inca on the single coincident times from each of the IFOs. Since the way 
  we treat all this data is the same, this function is the same for all cases.
  
  ifo_analyzed = the analyzed chunks for the IFO
  ifo_name = the name of the IFO
  ifo_single = the single IFO science segments 
  single_coinc_job = the condor job to do single IFO inca
  dag = the DAG to attach the nodes to
  """

  single_coinc_analyzed = []
  for seg in ifo_single:
    sinca = inspiral.IncaNode(single_coinc_job)
    sinca.set_start(seg.start())
    sinca.set_end(seg.end())
    sinca.set_ifo_a(ifo_name)
    sinca.add_var_opt('single-ifo',' ')

    # add all master chunks that overlap with segment to input
    for ifo_done in ifo_analyzed:
      if inspiral.overlap_test(ifo_done.get_chunk(),seg):
        sinca.add_var_arg(ifo_done.get_dag_node().get_output())
        if do_inspiral: sinca.add_parent(ifo_done.get_dag_node())
    
    if do_coincidence: dag.add_node(sinca)
    single_coinc_analyzed.append(AnalyzedIFOData(seg,sinca))

  return single_coinc_analyzed
  
##############################################################################
# function to do thinca on double IFO data  
def double_coinc(ifo_a_analyzed,ifo_a_name,ifo_b_analyzed,ifo_b_name, 
  ifo_double,double_coinc_job,dt,dag,usertag=None):
  """
  Run thinca on the double coincident times from each of the sets of IFOs. 
  Since the way we treat all this data is the same, this function is the same 
  for all three cases. 
  
  ifo_a_analyzed = the analyzed chunks for the first IFO
  ifo_a_name = the name of the first IFO
  ifo_b_analyzed = the analyzed chunks for the second IFO
  ifo_b_name = the name of the second IFO
  ifo_double = the double coincident IFO science segments 
  double_coinc_job = the condor job to do two IFO thinca
  dt = the time coincidence window
  dag = the DAG to attach the nodes to
  """

  double_coinc_analyzed = []
  for seg in ifo_double:
    thinca = inspiral.ThincaNode(double_coinc_job)
    thinca.set_start(seg.start())
    thinca.set_end(seg.end())
    for ifo in [ifo_a_name, ifo_b_name]:
      thinca.set_ifo(ifo)

    # add all IFO A master chunks that overlap with segment to input
    for done in ifo_a_analyzed:
      if inspiral.overlap_test(done.get_chunk(),seg):
        thinca.add_var_arg(done.get_dag_node().get_output())
        if do_inspiral: thinca.add_parent(done.get_dag_node())
    # add all IFO B master chunks that overlap with segment to input
    for done in ifo_b_analyzed:
      if inspiral.overlap_test(done.get_chunk(),seg):
        thinca.add_var_arg(done.get_dag_node().get_output())
        if do_inspiral: thinca.add_parent(done.get_dag_node())
    if do_coincidence: dag.add_node(thinca)
    double_coinc_analyzed.append(AnalyzedIFOData(seg,thinca))

  return double_coinc_analyzed

##############################################################################
# function to do inca on triple IFO data 
def triple_coinc(ifo_a_analyzed,ifo_a_name,ifo_b_analyzed,ifo_b_name,
  ifo_c_analyzed,ifo_c_name,ifo_triple,triple_coinc_job,dt,dag,
  usertag=None):
  """
  Run thinca on the double coincident times from each of the sets of IFOs. 
  Since the way we treat all this data is the same, this function is the same 
  for all three cases. 
  
  ifo_a_analyzed = the analyzed chunks for the first IFO
  ifo_a_name = the name of the first IFO
  ifo_b_analyzed = the analyzed chunks for the second IFO
  ifo_b_name = the name of the second IFO
  ifo_c_analyzed = the analyzed chunks for the third IFO
  ifo_c_name = the name of the third IFO
  ifo_triple = the triple coincident IFO science segments 
  triple_coinc_job = the condor job to do three IFO thinca
  dt = the time coincidence window
  dag = the DAG to attach the nodes to
  """

  triple_coinc_analyzed = []
  for seg in ifo_triple:
    thinca = inspiral.ThincaNode(triple_coinc_job)
    thinca.set_start(seg.start())
    thinca.set_end(seg.end())
    for ifo in [ifo_a_name, ifo_b_name, ifo_c_name]:
      thinca.set_ifo(ifo)

    # add all IFO A master chunks that overlap with segment to input
    for done in ifo_a_analyzed:
      if inspiral.overlap_test(done.get_chunk(),seg):
        thinca.add_var_arg(done.get_dag_node().get_output())
        if do_inspiral: thinca.add_parent(done.get_dag_node())
    # add all IFO B master chunks that overlap with segment to input
    for done in ifo_b_analyzed:
      if inspiral.overlap_test(done.get_chunk(),seg):
        thinca.add_var_arg(done.get_dag_node().get_output())
        if do_inspiral: thinca.add_parent(done.get_dag_node())
    # add all IFO C master chunks that overlap with segment to input
    for done in ifo_c_analyzed:
      if inspiral.overlap_test(done.get_chunk(),seg):
        thinca.add_var_arg(done.get_dag_node().get_output())
        if do_inspiral: thinca.add_parent(done.get_dag_node())
    
    if do_coincidence: dag.add_node(thinca)
    triple_coinc_analyzed.append(AnalyzedIFOData(seg,thinca))

  return triple_coinc_analyzed

##############################################################################
# function to do sweep up data using sire  
def sire_segs(segments,output,file_name,sire_job,sire_clust_job,dag,
  usertag=None):
  """
  Do a sire to sweep up all the triggers of a specific kind
  
  segments  = list of inca segments to use
  output    = which output of inca/inspiral to use
  file_name = output file name
  sire_job  = sire job to use for the analysis
  sire_clust_job = sire job with the clustering arguments already added
  dag       = name of the dag
  """
  
  sire = inspiral.SireNode(sire_job)
  sire_clust = inspiral.SireNode(sire_clust_job) 
  
  # set the sire file name
  fname = file_name
  if usertag:
    fname += '_' + usertag
  fname += '.input'
  
  # add all the input files
  f = open(fname, 'w')
  for seg in segments:
    sire.add_parent(seg.get_dag_node())
    sire_clust.add_parent(seg.get_dag_node())
    if output == 'output':
      f.write('%s\n' % seg.get_dag_node().get_output())
    if output == 'output_a':
      f.write('%s\n' % seg.get_dag_node().get_output_a())
    if output == 'output_b':
      f.write('%s\n' % seg.get_dag_node().get_output_b())
  f.close()
 
  # add the input file name to sire
  sire.add_var_opt('input',fname)
  sire_clust.add_var_opt('input',fname)
  
  # set the output files names for sire
  if inj_coinc:
    sire.set_inj_outputs(file_name,inj_coinc,usertag)
    sire_clust.set_inj_outputs(file_name,inj_coinc,usertag,cluster)
    
  else:
    sire.set_outputs(file_name,usertag)
    sire_clust.set_outputs(file_name,usertag,cluster_time)

  dag.add_node(sire) 
  dag.add_node(sire_clust) 

##############################################################################
# help message


def usage():
  msg = """\
Usage: lalapps_inspiral_hipe [options]

  -h, --help              display this message
  -v, --version           print version information and exit
  -u, --user-tag TAG      tag the job with TAG (overrides value in ini file)
 
  -a, --h1-data           analyze h1 data
  -b, --h2-data           analyze h2 data
  -l, --l1-data           analyze l1 data

  -S, --one-ifo           analyze single ifo data
  -D, --two-ifo           analyze two interferometer data
  -T, --three-ifo         analyze three interferometer data
  
  -A, --analyze-all       analyze all ifos and all data (over-rides above)

  -d, --datafind          run LSCdataFind to create frame cache files
  -t, --template-bank     run lalapps_tmpltbank to generate template banks
  -i, --inspiral          run lalapps_inspiral to generate triggers
  -c, --coincidence       run lalapps_inca to test for coincidence
  -s, --sire              do sires to sweep up triggers
  
  -P, --priority PRIO     run jobs with condor priority PRIO

  -f, --config-file FILE  use configuration file FILE
  -l, --log-path PATH     directory to write condor log file
  -o, --output-segs       output the segment lists of analyzed data
"""
  print >> sys.stderr, msg

##############################################################################
# parse the command line options to figure out what we should do
shortop = "hvu:ablSDTAdticsP:f:l:o"
longop = [
  "help",
  "version",
  "user-tag=",
  "h1-data",
  "h2-data",
  "l1-data",
  "one-ifo",
  "two-ifo",
  "three-ifo",
  "analyze-all",
  "datafind",
  "template-bank",
  "inspiral",
  "coincidence",
  "sire",
  "priority=",
  "config-file=",
  "log-path=",
  "output-segs",
  ]

try:
  opts, args = getopt.getopt(sys.argv[1:], shortop, longop)
except getopt.GetoptError:
  usage()
  sys.exit(1)

do_h1 = None
do_h2 = None
do_l1 = None

do_one_ifo = None
do_two_ifo = None
do_three_ifo = None

do_datafind = None
do_tmpltbank = None
do_inspiral = None
do_coincidence = None
do_sire = None

usertag = None
inj_file = None
inj_coinc = 0
output_segs = 0
condor_prio = None
config_file = None
log_path = None

for o, a in opts:
  if o in ("-v", "--version"):
    print "$Id$"
    sys.exit(0)
  elif o in ("-h", "--help"):
    usage()
    sys.exit(0)
  elif o in ("-a", "--h1-data"):
    do_h1 = 1
  elif o in ("-b", "--h2-data"):
    do_h2 = 1
  elif o in ("-l", "--l1-data"):
    do_l1 = 1
  elif o in ("-S", "--one-ifo"):
    do_one_ifo = 1
  elif o in ("-D", "--two-ifo"):
    do_two_ifo = 1
  elif o in ("-T", "--three-ifo"):
    do_three_ifo = 1
  elif o in ("-A", "--analyze-all"):
    do_h1 = 1
    do_h2 = 1
    do_l1 = 1
    do_one_ifo = 1
    do_two_ifo = 1
    do_three_ifo = 1
  elif o in ("-d", "--datafind"):
    do_datafind = 1
  elif o in ("-t", "--template-bank"):
    do_tmpltbank = 1
  elif o in ("-i", "--inspiral"):
    do_inspiral = 1
  elif o in ("-c", "--coincidence"):
    do_coincidence = 1
  elif o in ("-s", "--sire"):
    do_sire = 1
  elif o in ("-u", "--user-tag"):
    usertag = a
  elif o in ("-o", "--output-segs"):
    output_segs = 1
  elif o in ("-P", "--priority"):
    condor_prio = a
  elif o in ("-f", "--config-file"):
    config_file = a
  elif o in ("-l", "--log-path"):
    log_path = a
  else:
    print >> sys.stderr, "Unknown option:", o
    usage()
    sys.exit(1)

if not config_file:
  print >> sys.stderr, "No configuration file specified."
  print >> sys.stderr, "Use --config-file FILE to specify location."
  sys.exit(1)

if not log_path:
  print >> sys.stderr, "No log file path specified."
  print >> sys.stderr, "Use --log-path PATH to specify a location."
  sys.exit(1)

##############################################################################
# try to make a directory to store the cache files and job logs
try: os.mkdir('cache')
except: pass
try: os.mkdir('logs')
except: pass

##############################################################################
# create the config parser object and read in the ini file
cp = ConfigParser.ConfigParser()
cp.read(config_file)

##############################################################################
# if a usertag has been specified, override the config file
if usertag:
  cp.set('pipeline','user-tag',usertag)
else:
  try:
    usertag = string.strip(cp.get('pipeline','user-tag'))
  except:
    usertag = None
  
##############################################################################
# create a log file that the Condor jobs will write to
basename = re.sub(r'\.ini',r'',config_file)
tempfile.tempdir = log_path
if usertag:
  tempfile.template = basename + '.' + usertag + '.dag.log.'
else:
  tempfile.template = basename + '.dag.log.'
logfile = tempfile.mktemp()
fh = open( logfile, "w" )
fh.close()

##############################################################################
# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
if usertag:
  dag.set_dag_file(basename + '.' + usertag + '.dag')
else:
  dag.set_dag_file(basename + '.dag')

##############################################################################
# create the Condor jobs that will be used in the DAG
df_job = pipeline.LSCDataFindJob('cache','logs',cp)
tmplt_job = inspiral.TmpltBankJob(cp)
insp_job = inspiral.InspiralJob(cp)
single_inca_job = inspiral.IncaJob(cp)
thinca_h1h2_job = inspiral.ThincaJob(cp)
thinca_h1l1_job = inspiral.ThincaJob(cp)
thinca_h2l1_job = inspiral.ThincaJob(cp)
thinca_h1h2l1_job = inspiral.ThincaJob(cp)
sire_job = inspiral.SireJob(cp)
sire_clust_job = inspiral.SireJob(cp)

##############################################################################
# set better submit file names than the default
if usertag:
  subsuffix = '.' + usertag + '.sub'
else:
  subsuffix = '.sub'
df_job.set_sub_file( basename + '.datafind'+ subsuffix )
tmplt_job.set_sub_file( basename + '.tmpltbank' + subsuffix )
insp_job.set_sub_file( basename + '.inspiral' + subsuffix )
single_inca_job.set_sub_file( basename + '.s_inca' + subsuffix )
thinca_h1h2_job.set_sub_file( basename + '.thinca_h1h2' + subsuffix )
thinca_h1l1_job.set_sub_file( basename + '.thinca_h1l1' + subsuffix )
thinca_h2l1_job.set_sub_file( basename + '.thinca_h2l1' + subsuffix )
thinca_h1h2l1_job.set_sub_file( basename + '.thinca_h1h2l1' + subsuffix )
sire_job.set_sub_file( basename + '.sire' + subsuffix )
sire_clust_job.set_sub_file( basename + '.sire_clust' + subsuffix )

##############################################################################
# set the usertag in the jobs
if usertag:
  tmplt_job.add_opt('user-tag',usertag)
  insp_job.add_opt('user-tag',usertag)
  single_inca_job.add_opt('user-tag',usertag)
  thinca_h1h2_job.add_opt('user-tag',usertag)
  thinca_h1l1_job.add_opt('user-tag',usertag)
  thinca_h2l1_job.add_opt('user-tag',usertag)
  thinca_h1h2l1_job.add_opt('user-tag',usertag)

  sire_job.add_opt('user-tag',usertag)
  sire_clust_job.add_opt('user-tag',usertag)

##############################################################################
# set the condor job priority
if condor_prio:
  df_job.add_condor_cmd('priority',condor_prio)
  tmplt_job.add_condor_cmd('priority',condor_prio)
  insp_job.add_condor_cmd('priority',condor_prio)
  single_inca_job.add_condor_cmd('priority',condor_prio)
  thinca_h1h2_job.add_condor_cmd('priority',condor_prio)
  thinca_h1l1_job.add_condor_cmd('priority',condor_prio)
  thinca_h2l1job.add_condor_cmd('priority',condor_prio)
  thinca_h1h2l1_job.add_condor_cmd('priority',condor_prio)
  sire_job.add_condor_cmd('priority',condor_prio)
  sire_clust_job.add_condor_cmd('priority',condor_prio)

##############################################################################
# read in the injection-file from the ini file 
# and add to inspiral and sire jobs
try:
  inj_file = string.strip(cp.get('input','injection-file'))
except:
  inj_file = None
  
if inj_file:
  insp_job.add_opt('injection-file',inj_file)
  sire_job.add_opt('injection-file',inj_file)
  sire_clust_job.add_opt('injection-file',inj_file)

#############################################################################
# read in playground data mask from ini file 
# set the playground_only option and add to inca and sire jobs
try:
  play_data_mask = string.strip(cp.get('pipeline','playground-data-mask'))
except:
  play_data_mask = None

if play_data_mask == 'playground_only':
  playground_only = 2
  single_inca_job.add_opt('playground-only',' ')
  thinca_h1h2_job.add_opt('data-type','playground_only')
  thinca_h1l1_job.add_opt('data-type','playground_only')
  thinca_h2l1_job.add_opt('data-type','playground_only')
  thinca_h1h2l1_job.add_opt('data-type','playground_only')
  sire_job.add_opt('playground-only',' ')
  sire_clust_job.add_opt('playground-only',' ')

elif play_data_mask == 'exclude_playground':
  playground_only = 0
  single_inca_job.add_opt('no-playground',' ')
  thinca_h1h2_job.add_opt('data-type','exclude_play')
  thinca_h1l1_job.add_opt('data-type','exclude_play')
  thinca_h2l1_job.add_opt('data-type','exclude_play')
  thinca_h1h2l1_job.add_opt('data-type','exclude_play')
  sire_job.add_opt('exclude-playground',' ')
  sire_clust_job.add_opt('exclude-playground',' ')

elif play_data_mask == 'all_data':
  playground_only = 0
  single_inca_job.add_opt('no-playground',' ')
  thinca_h1h2_job.add_opt('data-type','all_data')
  thinca_h1l1_job.add_opt('data-type','all_data')
  thinca_h2l1_job.add_opt('data-type','all_data')
  thinca_h1h2l1_job.add_opt('data-type','all_data')
  sire_job.add_opt('all-data',' ')
  sire_clust_job.add_opt('all-data',' ')

else:
  print "Invalid playground data mask " + play_data_mask + " specified"
  sys.exit(1)

##############################################################################
# get the inspiral thresholds from the values in the ini file
l1_snr = cp.get('inspiral-thresholds','l1-snr-threshold')
h1_snr = cp.get('inspiral-thresholds','h1-snr-threshold')
h2_snr = cp.get('inspiral-thresholds','h2-snr-threshold')

l1_chisq = cp.get('inspiral-thresholds','l1-chisq-threshold')
h1_chisq = cp.get('inspiral-thresholds','h1-chisq-threshold')
h2_chisq = cp.get('inspiral-thresholds','h2-chisq-threshold')

##############################################################################
# add the sire parameters to the sire jobs
cluster_time = cp.get('sire-parameters','cluster-time')
sire_clust_job.add_opt('cluster-time',cluster_time)
sire_clust_job.add_opt('cluster-algorithm',
  cp.get('sire-parameters','cluster-algorithm'))

if inj_file:
  # add the injection coincidence to the sire jobs
  inj_coinc = cp.get('sire-parameters','injection-coincidence')
  sire_job.add_opt('injection-coincidence',inj_coinc)
  sire_clust_job.add_opt('injection-coincidence',inj_coinc)
 

##############################################################################
# get the pad and chunk lengths from the values in the ini file
pad = int(cp.get('data', 'pad-data'))
n = int(cp.get('data', 'segment-length'))
s = int(cp.get('data', 'number-of-segments'))
r = int(cp.get('data', 'sample-rate'))
o = int(cp.get('inspiral', 'segment-overlap'))
length = ( n * s - ( s - 1 ) * o ) / r
overlap = o / r


##############################################################################
#  The meat of the DAG generation comes below
#
#
#  The various data sets we compute are:
# 
#  h1_data, h2_data, l1_data : the science segments and master chunks
#
#  h1_data_out, h2_data_out, l1_data_out : the analyzable data 
#
#  not_h1_data_out, not_h2_data_out, not_l1_data_out : non analyzable data
#
#  h1_h2_l1_triple_data : the triple coincident data
#
#  h1_h2_double_data, h1_l1_double_data, h2_l1_double_data : double coinc data
#
#  h1_single_data, h2_single_data, l1_single_data : the single IFO data
#
#  h1_data_to_do, h2_data_to_do, l1_data_to_do : the data to analyze
#       (depends upon which of single,double,triple data is chosen to analyze) 
#
#
#  And the lists of jobs are:
#
#  h1_chunks_analyzed, h2_chunks_analyzed, l1_chunks_analyzed : list of chunks
#
#  h1_single_inca_nodes, h2_single_inca_nodes, l1_single_inca_nodes :
#                     the single coincident inca jobs
#
#  h1_h2_double_inca_nodes, h1_l1_double_inca_nodes, h2_l1_double_inca_nodes :
#                     the double coincident inca nodes
#
##############################################################################



##############################################################################
#   Step 1: read science segs that are greater or equal to a chunk 
#   from the input file

print "reading in single ifo science segments and creating master chunks...",
sys.stdout.flush()

# H1
try:
  h1_segments = cp.get('input', 'h1-segments')
except:
  h1_segments = None
  
h1_data = pipeline.ScienceData() 
if h1_segments:
  h1_data.read(h1_segments,length) 
  h1_data.make_chunks(length,overlap,playground_only,0,overlap/2)
  h1_data.make_chunks_from_unused(length,overlap/2,playground_only,
      0,0,overlap/2)

# H2 
try:
  h2_segments = cp.get('input', 'h2-segments')
except:
  h2_segments = None

h2_data = pipeline.ScienceData()
if h2_segments:
  h2_data.read(h2_segments,length) 
  h2_data.make_chunks(length,overlap,playground_only,0,overlap/2)
  h2_data.make_chunks_from_unused(length,overlap/2,playground_only,
      0,0,overlap/2)

# L1  
try:
  l1_segments = cp.get('input', 'l1-segments')
except:
  l1_segments = None
  
l1_data = pipeline.ScienceData()  
if l1_segments:
  l1_data.read(cp.get('input', 'l1-segments'),length) 
  l1_data.make_chunks(length,overlap,playground_only,0,overlap/2)
  l1_data.make_chunks_from_unused(length,overlap/2,playground_only,
      0,0,overlap/2)

print "done"

##############################################################################
#   Step 2: determine analyzable times

h1_data_out = copy.deepcopy(h1_data)
h2_data_out = copy.deepcopy(h2_data)
l1_data_out = copy.deepcopy(l1_data)

# remove start and end of science segments which can't be analyzed for triggers
for sci_data in [h1_data_out,h2_data_out,l1_data_out]:
  for seg in sci_data:
    seg.set_start(seg.start()+overlap/2)
    seg.set_end(seg.end()-overlap/2)

if playground_only:
  h1_data_out.play()
  h2_data_out.play()
  l1_data_out.play()
  
not_h1_data_out = copy.deepcopy(h1_data_out)
not_h1_data_out.coalesce()
not_h1_data_out.invert()

not_h2_data_out = copy.deepcopy(h2_data_out)
not_h2_data_out.coalesce()
not_h2_data_out.invert()

not_l1_data_out = copy.deepcopy(l1_data_out)
not_l1_data_out.coalesce()
not_l1_data_out.invert()
  
# determine the triple data, if it is to be analyzed
h1_h2_l1_triple_data = pipeline.ScienceData() 

if do_three_ifo and do_h1 and do_h2 and do_l1:  
  h1_h2_l1_triple_data = copy.deepcopy(h1_data_out)
  h1_h2_l1_triple_data.intersect_3(h2_data_out,l1_data_out)

# determine the double data, if it is to be analyzed
h1_h2_double_data = pipeline.ScienceData()
h1_l1_double_data = pipeline.ScienceData()
h2_l1_double_data = pipeline.ScienceData()

if do_two_ifo:
  if do_h1 and do_h2:
    h1_h2_double_data = copy.deepcopy(h1_data_out)
    h1_h2_double_data.intersect_3(h2_data_out, not_l1_data_out)
  
  if do_h1 and do_l1:
    h1_l1_double_data = copy.deepcopy(h1_data_out)
    h1_l1_double_data.intersect_3(l1_data_out, not_h2_data_out)
  
  if do_h2 and do_l1:
    h2_l1_double_data = copy.deepcopy(h2_data_out)
    h2_l1_double_data.intersect_3(l1_data_out, not_h1_data_out)
  
# determine the single data, if it is to be analyzed
h1_single_data = pipeline.ScienceData()
h2_single_data = pipeline.ScienceData()
l1_single_data = pipeline.ScienceData()
 
if do_one_ifo:
  if do_h1:
    h1_single_data = copy.deepcopy(h1_data_out)
    h1_single_data.intersect_3(not_h2_data_out, not_l1_data_out)
  
  if do_h2:
    h2_single_data = copy.deepcopy(h2_data_out)
    h2_single_data.intersect_3(not_h1_data_out, not_l1_data_out)
  
  if do_l1:
    l1_single_data = copy.deepcopy(l1_data_out)
    l1_single_data.intersect_3(not_h1_data_out, not_h2_data_out)
  
##############################################################################
# Step 3: Compute the Science Segments to analyze

h1_data_to_do = copy.deepcopy(h1_single_data)
h1_data_to_do.union(h1_h2_double_data)
h1_data_to_do.union(h1_l1_double_data)
h1_data_to_do.union(h1_h2_l1_triple_data)
h1_data_to_do.coalesce()
  
h2_data_to_do = copy.deepcopy(h2_single_data)
h2_data_to_do.union(h1_h2_double_data)
h2_data_to_do.union(h2_l1_double_data)
h2_data_to_do.union(h1_h2_l1_triple_data)
h2_data_to_do.coalesce()
  
l1_data_to_do = copy.deepcopy(l1_single_data)
l1_data_to_do.union(h1_l1_double_data)
l1_data_to_do.union(h2_l1_double_data)
l1_data_to_do.union(h1_h2_l1_triple_data)
l1_data_to_do.coalesce()

##############################################################################
# Step 4: Determine which of the master chunks needs to be filtered

# H1
h1_chunks_analyzed = []
prev_df = None

print "setting up jobs to filter H1 data...",
sys.stdout.flush()
(prev_df,h1_chunks_analyzed) = analyze_ifo(h1_data,'H1',h1_data_to_do,  
  tmplt_job,insp_job,df_job,h1_snr,h1_chisq,pad,prev_df,dag,usertag=None)
print "done"

# H2
h2_chunks_analyzed = []

print "setting up jobs to filter H2 data...",
sys.stdout.flush()
(prev_df,h2_chunks_analyzed) = analyze_ifo(h2_data,'H2',h2_data_to_do,
  tmplt_job,insp_job,df_job,h2_snr,h2_chisq,pad,prev_df,dag,usertag=None)
print "done"

# L1
l1_chunks_analyzed = []

print "setting up jobs to filter L1 data...",
sys.stdout.flush()
(prev_df,l1_chunks_analyzed) = analyze_ifo(l1_data,'L1',l1_data_to_do,
  tmplt_job,insp_job,df_job,l1_snr,l1_chisq,pad,prev_df,dag,usertag=None)
print "done"
  

##############################################################################
# Step 5: Run inca in single ifo mode on the single ifo triggers.

print "setting up jobs to inca single IFO data...",
sys.stdout.flush()

# H1
h1_single_coinc_nodes = []
h1_single_coinc_nodes = single_coinc(h1_chunks_analyzed,'H1',h1_single_data,
  single_inca_job,dag,usertag)

# H2
h2_single_coinc_nodes = []
h2_single_coinc_nodes = single_coinc(h2_chunks_analyzed,'H2',h2_single_data,
  single_inca_job,dag,usertag)

# L1
l1_single_coinc_nodes = []
l1_single_coinc_nodes = single_coinc(l1_chunks_analyzed,'L1',l1_single_data,
  single_inca_job,dag, usertag)

print "done"
  
##############################################################################
# Step 6: Run inca on each of the disjoint sets of double coincidence data

print "setting up thinca jobs on double IFO data...",

# H1-H2
h1_h2_double_coinc_nodes = []
h1_h2_double_coinc_nodes = double_coinc(h1_chunks_analyzed,'H1',
  h2_chunks_analyzed,'H2',h1_h2_double_data,thinca_h1h2_job,hh_dt,dag,usertag)
  
# H1-L1
h1_l1_double_coinc_nodes = []
h1_l1_double_coinc_nodes = double_coinc(h1_chunks_analyzed,'H1',
  l1_chunks_analyzed,'L1',h1_l1_double_data,thinca_h1l1_job,hh_dt,dag,usertag)

# H2-L1
h2_l1_double_coinc_nodes = []
h2_l1_double_coinc_nodes = double_coinc(h2_chunks_analyzed,'H2',
  l1_chunks_analyzed,'L1',h2_l1_double_data,thinca_h2l1_job,hl_dt,dag,usertag)

print "done"

############################################################################## 
# Step 7: Run inca on each of the disjoint sets of triple coincidence data
# since we do not have a triple coincidence tester, we leave this blank

print "setting up thinca jobs on three IFO data...",

# H1-H2-L1
h1_h2_l1_triple_coinc_nodes = []
h1_h2_l1_triple_coinc_nodes = triple_coinc(h1_chunks_analyzed,'H1',
  h2_chunks_analyzed,'H2',l1_chunks_analyzed,'L1',h1_h2_l1_triple_data,
  thinca_h1h2l1_job,hl_dt,dag,usertag)

print "done"

############################################################################## 
# Step 9: Run sire on each of the types of single/double/triple data
print "setting up sire jobs ...",
sys.stdout.flush()

if do_sire:
  if do_one_ifo:
    # a clustered and unclustered sire for each of the single IFO configs
    if do_h1 and len(h1_single_data):
      sire_segs(h1_single_coinc_nodes,'output_a','H1-SIRE_SINGLE_IFO',sire_job,
        sire_clust_job,dag,usertag)
    if do_h2 and len(h2_single_data):
      sire_segs(h2_single_coinc_nodes,'output_a','H2-SIRE_SINGLE_IFO',sire_job,
        sire_clust_job,dag,usertag)
    if do_l1 and len(l1_single_data):
      sire_segs(l1_single_coinc_nodes,'output_a','L1-SIRE_SINGLE_IFO',sire_job,
        sire_clust_job,dag,usertag)
   

 
  if do_two_ifo:  
    # a clustered and unclustered sire for each of the double IFO configs
    if do_h1 and do_h2 and len(h1_h2_double_data):
      sire_segs(h1_h2_double_coinc_nodes,'output','H1H2-SIRE',sire_job,
        sire_clust_job,dag,usertag)
      
    if do_h1 and do_l1 and len(h1_l1_double_data):
      sire_segs(h1_l1_double_coinc_nodes,'output','H1L1-SIRE',sire_job,
        sire_clust_job,dag,usertag)
     
    if do_h2 and do_l1 and len(h2_l1_double_data):
      sire_segs(h2_l1_double_coinc_nodes,'output','H2L1-SIRE',sire_job,
        sire_clust_job,dag,usertag)
 
print "done"
  
##############################################################################
# Step 10: Write out the DAG, help message and log file
dag.write_sub_files()
dag.write_dag()

##############################################################################  
# write a message telling the user that the DAG has been written
print "\nCreated a DAG file which can be submitted by executing"
print "\n   condor_submit_dag", dag.get_dag_file()
print """\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
If you are running LSCdataFind jobs, do not forget to initialize your grid 
proxy certificate on the condor submit machine by running the commands

  unset X509_USER_PROXY
  grid-proxy-init -hours 72

Enter your pass phrase when promted. The proxy will be valid for 72 hours. 
If you expect the LSCdataFind jobs to take longer to complete, increase the
time specified in the -hours option to grid-proxy-init. You can check that 
the grid proxy has been sucessfully created by executing the command:

  grid-cert-info -all -file /tmp/x509up_u`id -u`

This will also give the expiry time of the proxy. You should also make sure
that the environment variable LSC_DATAFIND_SERVER is set the hostname and
optional port of server to query. For example on the UWM medusa cluster this
you should use

  export LSC_DATAFIND_SERVER=dataserver.phys.uwm.edu

Contact the administrator of your cluster to find the hostname and port of the
LSCdataFind server.
"""

##############################################################################
# write out a log file for this script
if usertag:
  log_fh = open(basename + '.pipeline.' + usertag + '.log', 'w')
else:
  log_fh = open(basename + '.pipeline.log', 'w')
  
log_fh.write( "$Id$" + "\n" )
log_fh.write( "$Name$" + "\n\n" )
log_fh.write( "Invoked with arguments:\n" )
for o, a in opts:
  log_fh.write( o + ' ' + a + '\n' )

log_fh.write( "Config file has CVS strings:\n" )
log_fh.write( cp.get('pipeline','version') + "\n" )
log_fh.write( cp.get('pipeline','cvs-tag') + "\n\n" )

print >> log_fh, "\n===========================================\n"
print >> log_fh, "Science Segments and master chunks:\n"
for sci_data in [h1_data, h2_data, l1_data]:
  print >> log_fh, sci_data
  for seg in sci_data:
    print >> log_fh, " ", seg
    for chunk in seg:
      print >> log_fh, "   ", chunk


print >> log_fh, "\n===========================================\n"
log_fh.write( 
  "Filtering " + str(len(h1_chunks_analyzed)) + " H1 master chunks\n" )
total_time = 0
for h1_done in h1_chunks_analyzed:
  print >> log_fh, h1_done.get_chunk()
  total_time += len(h1_done.get_chunk())
print >> log_fh, "\n total time", total_time, "seconds"


print >> log_fh, "\n===========================================\n"
log_fh.write( 
  "Filtering " + str(len(h2_chunks_analyzed)) + " H2 master chunks\n" )
total_time = 0
for h2_done in h2_chunks_analyzed:
  print >> log_fh, h2_done.get_chunk()
  total_time += len(h2_done.get_chunk())
print >> log_fh, "\n total time", total_time, "seconds"


print >> log_fh, "\n===========================================\n"
log_fh.write( 
  "Filtering " + str(len(l1_chunks_analyzed)) + " L1 master chunks\n" )
total_time = 0
for l1_done in l1_chunks_analyzed:
  print >> log_fh, l1_done.get_chunk()
  total_time += len(l1_done.get_chunk())
print >> log_fh, "\n total time", total_time, "seconds"


print >> log_fh, "\n===========================================\n"
log_fh.write( "Writing " + str(len(h1_single_data)) + 
  " H1 single IFO science segments\n" )
total_time = 0
for seg in h1_single_data:
  print >> log_fh, seg
  total_time += seg.dur()
print >> log_fh, "\n total time", total_time, "seconds"

if output_segs and len(h1_single_data):
  if playground_only:
    f = open('h1_play_segs_analyzed.txt', 'w')
  else:  
    f = open('h1_segs_analyzed.txt', 'w')
  for seg in h1_single_data:
    f.write('%4d %10d %10d %6d\n' % (seg.id(), seg.start(), seg.end(), 
      seg.dur()))
  f.close()


print >> log_fh, "\n===========================================\n"
log_fh.write( "Writing " + str(len(h2_single_data)) + 
  " H2 single IFO science segments\n" )
total_time = 0
for seg in h2_single_data:
  print >> log_fh, seg
  total_time += seg.dur()
print >> log_fh, "\n total time", total_time, "seconds"

if output_segs and len(h2_single_data):
  if playground_only:
    f = open('h2_play_segs_analyzed.txt', 'w')
  else:  
    f = open('h2_segs_analyzed.txt', 'w')
  for seg in h2_single_data:
    f.write('%4d %10d %10d %6d\n' % (seg.id(), seg.start(), seg.end(), 
      seg.dur()))
  f.close()


print >> log_fh, "\n===========================================\n"
log_fh.write( "Writing " + str(len(l1_single_data)) + 
  " L1 single IFO science segments\n" )
total_time = 0
for seg in l1_single_data:
  print >> log_fh, seg
  total_time += seg.dur()
print >> log_fh, "\n total time", total_time, "seconds"

if output_segs and len(l1_single_data):
  if playground_only:
    f = open('l1_play_segs_analyzed.txt', 'w')
  else:  
    f = open('l1_segs_analyzed.txt', 'w')
  for seg in l1_single_data:
    f.write('%4d %10d %10d %6d\n' % (seg.id(), seg.start(), seg.end(), 
      seg.dur()))
  f.close()

  
print >> log_fh, "\n===========================================\n"
log_fh.write( "Writing " + str(len(h1_h2_double_data)) + 
  " H1/H2 double coincident segments\n" )
total_time = 0
for seg in h1_h2_double_data:
  print >> log_fh, seg
  total_time += seg.dur()
print >> log_fh, "\n total time", total_time, "seconds"

if output_segs and len(h1_h2_double_data):
  if playground_only:
    f = open('h1_h2_play_segs_analyzed.txt', 'w')
  else:  
    f = open('h1_h2_segs_analyzed.txt', 'w')
  for seg in h1_h2_double_data:
    f.write('%4d %10d %10d %6d\n' % (seg.id(), seg.start(), seg.end(), 
      seg.dur()))
  f.close()

print >> log_fh, "\n===========================================\n"
log_fh.write( "Writing " + str(len(h1_l1_double_data)) + 
  " H1/L1 double coincident segments\n" )
total_time = 0
for seg in h1_l1_double_data:
  print >> log_fh, seg
  total_time += seg.dur()
print >> log_fh, "\n total time", total_time, "seconds"

if output_segs and len(h1_l1_double_data):
  if playground_only:
    f = open('h1_l1_play_segs_analyzed.txt', 'w')
  else:  
    f = open('h1_l1_segs_analyzed.txt', 'w')
  for seg in h1_l1_double_data:
    f.write('%4d %10d %10d %6d\n' % (seg.id(), seg.start(), seg.end(), 
      seg.dur()))
  f.close()
  
print >> log_fh, "\n===========================================\n"
log_fh.write( "Writing " + str(len(h2_l1_double_data)) + 
  " H2/L1 double coincident segments\n" )
total_time = 0
for seg in h2_l1_double_data:
  print >> log_fh, seg
  total_time += seg.dur()
print >> log_fh, "\n total time", total_time, "seconds"

if output_segs and len(h2_l1_double_data):
  if playground_only:
    f = open('h2_l1_play_segs_analyzed.txt', 'w')
  else:  
    f = open('h2_l1_segs_analyzed.txt', 'w')
  for seg in h2_l1_double_data:
    f.write('%4d %10d %10d %6d\n' % (seg.id(), seg.start(), seg.end(), 
      seg.dur()))
  f.close()

  
print >> log_fh, "\n===========================================\n"
log_fh.write( "Writing " + str(len(h1_h2_l1_triple_data)) + 
  " H1/H2/L1 triple coincident segments\n" )
total_time = 0
for seg in h1_h2_l1_triple_data:
  print >> log_fh, seg
  total_time += seg.dur()
print >> log_fh, "\n total time", total_time, "seconds"

if output_segs and len(h1_h2_l1_triple_data):
  if playground_only:
    f = open('h1_h2_l1_play_segs_analyzed.txt', 'w')
  else:  
    f = open('h1_h2_l1_segs_analyzed.txt', 'w')
  for seg in h1_h2_l1_triple_data:
    f.write('%4d %10d %10d %6d\n' % (seg.id(), seg.start(), seg.end(), 
      seg.dur()))
  f.close()

  
sys.exit(0)

