#!/usr/bin/env @PYTHONPROG@
"""
inspiral_online_pipe.py - online inspiral pipeline driver script

$Id$

This script produced the necessary condor submit and dag files to run
a prototype online analysis in E11
"""

__author__ = 'Duncan Brown <duncan@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

# import standard modules and append the lalapps prefix to the python path
import sys, os
import popen2, time
import getopt, re, string
import socket
import ConfigParser
import urlparse
sys.path.append('@PYTHONLIBDIR@')

# import the modules we need to build the pipeline
from glue import pipeline
from glue import segments
import inspiral

def usage():
  msg = """\
Usage: lalapps_inspiral_online_pipe [options]

  -h, --help                     display this message
  -v, --version                  print version information and exit

  -s, --gps-start-time SEC       start time of segment to be analyzed
  -e, --gps-end-time SEC         end time of segment to be analyzed
  -a, --analysis-start-time SEC  only output triggers after GPS time
  -b, --analysis-end-time SEC    only output triggers before GPS time SEC

  -f, --dag-file-name NAME       output dag file name
  -t, --aux-data-path PATH       path to auxillary data

  -c, --config-file FILE         use configuration file FILE
  -l, --log-path PATH            directory to write condor log file
"""
  print >> sys.stderr, msg

# pasrse the command line options to figure out what we should do
shortop = "hvs:e:a:b:f:t:c:l:"
longop = [
  "help",
  "version",
  "gps-start-time=",
  "gps-end-time=",
  "analysis-start-time=",
  "analysis-end-time=",
  "dag-file-name=",
  "aux-data-path=",
  "config-file=",
  "log-path="
  ]

try:
  opts, args = getopt.getopt(sys.argv[1:], shortop, longop)
except getopt.GetoptError:
  usage()
  sys.exit(1)

gps_start_time = None
gps_end_time = None
analysis_start_time = None
analysis_end_time = None
dag_file_name = None
aux_data_path = None
config_file = None
log_path = None

for o, a in opts:
  if o in ("-v", "--version"):
    print "$Id$"
    sys.exit(0)
  elif o in ("-h", "--help"):
    usage()
    sys.exit(0)
  elif o in ("-s", "--gps-start-time"):
    gps_start_time = int(a)
  elif o in ("-e", "--gps-end-time"):
    gps_end_time = int(a)
  elif o in ("-a", "--analysis-start-time"):
    analysis_start_time = int(a)
  elif o in ("-b", "--analysis-end-time"):
    analysis_end_time = int(a)
  elif o in ("-f", "--dag-file-name"):
    dag_file_name = a
  elif o in ("-t", "--aux-data-path"):
    aux_data_path = a
  elif o in ("-c", "--config-file"):
    config_file = a
  elif o in ("-l", "--log-path"):
    log_path = a
  else:
    print >> sys.stderr, "Unknown option:", o
    usage()
    sys.exit(1)

if not gps_start_time:
  print >> sys.stderr, "No GPS start time specified."
  print >> sys.stderr, "Use --gps-start-time SEC to specify start time."
  sys.exit(1)

if not gps_end_time:
  print >> sys.stderr, "No GPS end time specified."
  print >> sys.stderr, "Use --gps-end-time SEC to specify end time."
  sys.exit(1)

if not dag_file_name:
  print >> sys.stderr, "No DAG file name specified."
  print >> sys.stderr, "Use --dag-file-name NAME to specify output DAG file."
  sys.exit(1)

if not aux_data_path:
  print >> sys.stderr, "No auxiliary data path specified."
  print >> sys.stderr, "Use --aux-data-path PATH to specify directory."
  sys.exit(1)

if not config_file:
  print >> sys.stderr, "No configuration file specified."
  print >> sys.stderr, "Use --config-file FILE to specify location."
  sys.exit(1)

if not log_path:
  print >> sys.stderr, "No log file path specified."
  print >> sys.stderr, "Use --log-path PATH to specify a location."
  sys.exit(1)

try: os.mkdir('cache')
except: pass
try: os.mkdir('logs')
except: pass

# create the config parser object and read in the ini file
cp = ConfigParser.ConfigParser()
cp.read(config_file)

# get the usertag from the config file
try:
  usertag = string.strip(cp.get('pipeline','user-tag'))
  inspstr = 'INSPIRAL_' + usertag
except:
  usertag = None
  inspstr = 'INSPIRAL'

# create a log file that the Condor jobs will write to
basename = re.sub(r'\.dag',r'',dag_file_name)
logfile = os.path.join(log_path, basename + '.log')

# create a DAG generation log file
log_fh = open(basename + '.pipeline.log', 'w')

# create the DAG writing the log to the specified file
dag = pipeline.CondorDAG(logfile)
dag.set_dag_file(dag_file_name,1)

# create the Condor jobs that will be used in the DAG
df_job = pipeline.LSCDataFindJob('cache','logs',cp)
tmplt_job = inspiral.TmpltBankJob(cp)
split_job = inspiral.SplitBankJob(cp)
insp_job = inspiral.InspiralJob(cp)
lwadd_job = pipeline.LigolwAddJob('logs',cp)
ldbdc_job = pipeline.LDBDCJob('logs',cp)

# make the lwadd job a priority so results get reported quickly
lwadd_job.add_condor_cmd('priority', '20')
lwadd_job.add_opt('add-lfn-table',None)

# set the usertag in the template bank
if usertag:
  tmplt_job.add_opt('user-tag',usertag)

# set better submit file names than the default
subsuffix = '.sub'
df_job.set_sub_file( basename + '.datafind' + subsuffix )
tmplt_job.set_sub_file( basename + '.tmpltbank' + subsuffix )
split_job.set_sub_file( basename + '.splitbank' + subsuffix )
insp_job.set_sub_file( basename + '.inspiral' + subsuffix )
lwadd_job.set_sub_file( basename + '.ligolwadd' + subsuffix )
ldbdc_job.set_sub_file( basename + '.ldbdc' + subsuffix )

# get the pad and chunk lengths from the values in the ini file
pad = int(cp.get('data', 'pad-data'))
n = int(cp.get('data', 'segment-length'))
s = int(cp.get('data', 'number-of-segments'))
r = int(cp.get('data', 'sample-rate'))
o = int(cp.get('inspiral', 'segment-overlap'))
length = ( n * s - ( s - 1 ) * o ) / r
overlap = o / r
job_analysis_time = length - overlap

# get the order of the ifos to filter
ifo = cp.get('pipeline','ifo')
run = cp.get('pipeline','run')

# create a shell script to copy the output data to the desired locations
output_location = cp.get('publish','outputdir')
dmt_location = cp.get('publish','dmtdir')
output_script = open('output.sh','w')
output_script.write("""#!/bin/bash
if [ ${1} -ne 0 ] ; then
  exit 1
else
  OUTPUTPATH=""" + output_location + """/${2}
  if [ ! -d ${OUTPUTPATH} ]; then
    mkdir -p ${OUTPUTPATH}
  fi
  cp ${3} ${OUTPUTPATH}
  ln -sf ${OUTPUTPATH}/${3} """ + dmt_location + """/${3}
fi
""")
output_script.close()
os.chmod('output.sh',0755)

# find the data between the start time and the end time
df = pipeline.LSCDataFindNode(df_job)
df.set_start(gps_start_time)
df.set_end(gps_end_time)
df.set_observatory(ifo[0])
dag.add_node(df)

# modify the start and end time by pad seconds
log_fh.write("gps_start_time = %d\n" % gps_start_time)
log_fh.write("gps_end_time = %d\n" % gps_end_time)
gps_start_time += pad
gps_end_time -= pad
log_fh.write("gps_start_time + pad = %d\n" % gps_start_time)
log_fh.write("gps_end_time - pad = %d\n" % gps_end_time)

# decide if we need to segment the data
total_analysis_time = analysis_end_time - analysis_start_time
jobsegs = segments.segmentlist()
if total_analysis_time > job_analysis_time:
  t = gps_start_time
  while t + length < gps_end_time:
    jobsegs.append( segments.segment(t, t + length) )
    t += length - overlap
  trig_start_time = None
  trig_end_time = None
  ret_start_time = jobsegs[0][0] + (overlap / 2)
  ret_end_time = jobsegs[-1][1] - (overlap / 2)
else:
  jobsegs.append( segments.segment(gps_start_time, gps_end_time) )
  ret_start_time = trig_start_time = analysis_start_time
  ret_end_time = trig_end_time = analysis_end_time
  
# write out a log file for this script
log_fh.write("analysis_start_time = %d\n" % analysis_start_time)
log_fh.write("analysis_end_time = %d\n" % analysis_end_time)
log_fh.write("total_analysis_time = %d\n" % total_analysis_time)
log_fh.write("job_analysis_time = %d\n" % job_analysis_time)
log_fh.write(str(jobsegs) + '\n')
if trig_start_time:
  log_fh.write("trig_start_time = %d\n" % trig_start_time)
if trig_end_time:
  log_fh.write("trig_end_time = %d\n" % trig_end_time)
log_fh.write("ret_start_time = %d\n" % ret_start_time)
log_fh.write("ret_end_time = %d\n" % ret_end_time)
log_fh.write("total analyzed time = %d\n" % (ret_end_time - ret_start_time))

prev_ldbdc = None

for seg in jobsegs:
  # create the template bank job
  bank = inspiral.TmpltBankNode(tmplt_job)
  bank.set_start(seg[0])
  bank.set_end(seg[1])
  bank.set_ifo(ifo)
  bank.set_cache(df.get_output())
  bank.add_parent(df)
  dag.add_node(bank)
    
  # split the template bank up into smaller banks
  split = inspiral.SplitBankNode(split_job)
  split.set_bank(bank.get_output())
  split.set_num_banks(cp.get('pipeline','numbanks'))
  split.add_parent(bank)
  dag.add_node(split)

  # create the inspiral jobs to do the analysis
  i = 0
  sub_insp = []
  for subbank in split.get_output():
    insp = inspiral.InspiralNode(insp_job)
    insp.set_start(seg[0])
    insp.set_end(seg[1])
    if trig_start_time:
      insp.add_var_opt('trig-start-time',trig_start_time)
    if trig_end_time:
      insp.add_var_opt('trig-end-time',trig_end_time)
    insp.set_ifo(ifo)
    insp.set_cache(df.get_output())
    insp.set_bank(subbank)
    if usertag:
      insp.set_user_tag(usertag + "_%2.2d" % i)
    else:
      insp.set_user_tag("%2.2d" % i)
    insp.add_parent(split)
    dag.add_node(insp)
    sub_insp.append(insp)
    i = i + 1

  # create a cache file name to hold the input to ligolw_add
  if trig_start_time:
    out_start = trig_start_time
  else:
    out_start = seg[0] + overlap / 2
  if trig_end_time:
    out_end = trig_end_time
  else:
    out_end = seg[1] - overlap / 2
  out_basename = ifo + '-' + inspstr + '-' + str(out_start) + '-'
  out_basename += str(out_end - out_start)
  insp_cache_file_name = out_basename + '.cache'
  insp_cache_file = open(insp_cache_file_name, 'w')
  path = 'file://localhost' + os.getcwd()

  # use ligolw_add to join everything back together
  lwadd = pipeline.LigolwAddNode(lwadd_job)
  lwadd.add_var_opt('input-cache',insp_cache_file_name)
  lwadd.set_output(out_basename + '.xml')
  lwadd.add_var_opt('lfn-start-time',out_start)
  lwadd.add_var_opt('lfn-end-time',out_end)
  for insp in sub_insp:
    file = insp.get_output()
    [instrument, type, start, duration, extension] = re.split("[.-]", file)
    print >> insp_cache_file, instrument[0], type, start, duration, \
      os.path.join(path , file)
    lwadd.add_parent(insp)
  output_file = lwadd.get_output()
  [instrument, type, start, duration, extension] = re.split("[.-]", output_file)
  lwadd.add_var_opt('lfn-comment',type)
  start_str = str(start)
  lwadd.set_post_script("output.sh $RETURN %s-%s-%s %s" % 
    (instrument, type, start_str[0:4], output_file))
  dag.add_node(lwadd)

  # add a ldbdc node to publish the data
  ldbdc = pipeline.LDBDCNode( ldbdc_job )
  ldbdc.set_server( cp.get('publish','ldbdserver') )
  ldbdc.set_insert( lwadd.get_output() )
  ldbdc.set_retry(3)
  ldbdc.add_parent(lwadd)
  if prev_ldbdc:
    ldbdc.add_parent(prev_ldbdc)
    prev_ldbdc = ldbdc
  dag.add_node(ldbdc)

# write the dag
dag.write_sub_files()
dag.write_dag()

log_fh.close()
  
# return the analysis start and end time to onasys
print >> sys.stdout, "%d %d" % (ret_start_time, ret_end_time)
sys.exit(0)
