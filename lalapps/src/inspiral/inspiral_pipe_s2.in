#!/usr/bin/env @PYTHONPROG@
"""
inspiral_pipe_s2.in - S2 inspiral analysis pipeline script

$Id$

This script generates the necessary condor DAG files to analyze the
data from the second LIGO science run through the S2 inspiral pipeline.

For S2 the first IFO is L1, the second IFO is H1 and the third IFO is H2.

All data from L1 that is coincident with either H1 or H2 is analyzed 
for triggers. There are three groups of output data. L1H1 double 
coincident data, L1H2 double coincident data and L1H1H2 triple coincident
data. Note that L1H1 and L1H2 are exclusive and the double coincident 
data is exclusive with the triple coincident data.
"""

__author__ = 'Duncan Brown <duncan@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import sys, os, copy
import getopt, re, string
import tempfile
import ConfigParser
sys.path.append('@PYTHONLIBDIR@')

# import the modules we need to build the pipeline
from glue import pipeline
import inspiral

##############################################################################
# some functions to make life easier later
class AnalyzedIFOData:
  """
  Contains the information for the data that needs to be filtered.
  """
  def __init__(self,chunk,node):
    self.__analysis_chunk = chunk
    self.__dag_node = node

  def set_chunk(self,chunk):
    self.__analysis_chunk = chunk

  def get_chunk(self):
    return self.__analysis_chunk

  def set_dag_node(self,node):
    self.__dag_node = node

  def get_dag_node(self):
    return self.__dag_node

def analyze_lho_ifo(l1_lho_data,l1_chunks_analyzed,ifo_data,ifo_name,
  insp_job,trig_job,df_job,snr,chisq,pad,max_slide,prev_df,dag,usertag=None):
  """
  Analyze the data from a Hanford instrument. Since the way we treat H1 and
  H2 is symmetric, this function is the same for both instruments. Returns the
  last LALdataFind job that was executed and the chunks analyzed.
  l1_lho_data = the overlap of L1 data with the Hanford IFOs
  l1_chunks_analyzed = the L1 master chunks that have been analyzed
  lho_data = the master science segments the Hanford IFO
  ifo_name = the name of the Hanford IFO
  insp_job = the condor job that we should use to analyze Hanford data
  trig_job = the trigtomplt job used for the LHO inspiral search
  df_job = the condor job to find the data
  snr = the signal-to-noise threshold for this IFO
  chisq = the chi squared threshold for this IFO
  pad = data start/end padding
  max_slide = the maximum length of a time slide
  prev_df = the previous LALdataFind job that was executed
  dag = the DAG to attach the nodes to
  """
  chunks_analyzed = []
  # loop over the master science segments
  for seg in ifo_data:
    # make sure that we do not do a data find more than once per science segment
    done_df_for_seg = None
    # loop over the master analysis chunks in the science segment
    for chunk in seg:
      done_this_chunk = 0
      # now loop over all the L1 data that we need to filter
      for seg_to_do in l1_lho_data:
        # if the current chunk is in one of the segments we need to filter
        if ( inspiral.overlap_test(chunk,seg_to_do,max_slide) ) and not (
          done_this_chunk ):
          # make sure we only filter the master chunk once
          done_this_chunk = 1
          # make sure we have done one and only one datafind for the segment
          if not done_df_for_seg:
            df = pipeline.LSCDataFindNode(df_job)
            df.set_observatory(ifo_name[0])
            df.set_start(seg.start() - pad)
            df.set_end(seg.end() + pad)
            if prev_df: df.add_parent(prev_df)
            if not slide_time or check_bkgr_dag: dag.add_node(df)
            prev_df = df
            done_df_for_seg = 1
          # make a trigbank for the master LHO chunk: to do this, we need
          # the master L1 chunks that overlap with this LHO chunk as input
          trigbank = inspiral.TrigToTmpltNode(trig_job)
          trigbank.make_trigbank(chunk,max_slide,'L1',ifo_name,usertag)
          for l1_done in l1_chunks_analyzed:
            if inspiral.overlap_test(chunk,l1_done.get_chunk(),max_slide):
              trigbank.add_file_arg(l1_done.get_dag_node().get_output())
              trigbank.add_parent(l1_done.get_dag_node())
          if not slide_time or check_bkgr_dag: dag.add_node(trigbank)
          # analyze the LHO master chunk with this triggered template bank
          insp = inspiral.InspiralNode(insp_job)
          insp.set_start(chunk.start())
          insp.set_end(chunk.end())
          insp.set_ifo(ifo_name)
          insp.calibration()
          insp.set_ifo_tag('L1')
          insp.add_var_opt('snr-threshold',snr)
          insp.add_var_opt('chisq-threshold',chisq)
          insp.add_var_opt('trig-start-time',chunk.trig_start())
          insp.add_var_opt('trig-end-time',chunk.trig_end())
          insp.set_cache(df.get_output())
          insp.set_bank(trigbank.get_output())
          insp.add_parent(df)
          insp.add_parent(trigbank)
          if not slide_time or check_bkgr_dag: dag.add_node(insp)
          # store this chunk in the list of filtered L1 data
          chunks_analyzed.append(AnalyzedIFOData(chunk,insp))
  
  return tuple([prev_df,chunks_analyzed])

##############################################################################
# help message
def usage():
  msg = """\
Usage: lalapps_inspiral_pipe [options]

  -h, --help               display this message
  -v, --version            print version information and exit
  -u, --user-tag TAG       tag the job with TAG (overrides value in ini file)
  -c, --check-bkgr-dag     don't split dependencies for time slides

  -j, --injections FILE    add simulated inspirals from sim_inspiral in FILE

  -s, --max-slide-time SEC set the largest time side for background to SEC
  -x, --slide-time SEC     run a time slide with slide time SEC seconds
  -y, --slide-time-ns NS   run a time slide with slide time NS nanoseconds

  -P, --priority PRIO      run jobs with condor priority PRIO

  -p, --playground-only    only analyze jobs in the playground
  
  -l, --log-path PATH      directory to write condor log file
  -f, --config-file FILE   use configuration file FILE

  -X, --dax                write abstract DAX file
"""
  print >> sys.stderr, msg

##############################################################################
# pasrse the command line options to figure out what we should do
shortop = "hvu:cj:pP:s:x:y:l:f:X"
longop = [
  "help",
  "version",
  "user-tag=",
  "check-bkgr-dag",
  "injections=",
  "playground-only",
  "max-slide-time=",
  "slide-time=",
  "slide-time-ns=",
  "priority=",
  "log-path=",
  "config-file=",
  "dax"
  ]

try:
  opts, args = getopt.getopt(sys.argv[1:], shortop, longop)
except getopt.GetoptError:
  usage()
  sys.exit(1)

usertag = None
inj_file = None
max_slide = 0
slide_time = 0
slide_time_ns = 0
playground_only = 0
condor_prio = None
log_path = None
config_file = None
check_bkgr_dag = None
dax = False

for o, a in opts:
  if o in ("-h", "--help"):
    usage()
    sys.exit(0)
  elif o in ("-v", "--version"):
    print "S2 Inspiral Pipeline DAG generation script"
    print "Duncan Brown <duncan@gravity.phys.uwm.edu>"
    print "CVS Version:",\
      "$Id$"
    print "CVS Tag: $Name$"
    sys.exit(0)
  elif o in ("-u", "--user-tag"):
    usertag = a
  elif o in ("-c", "--check-bkgr-dag"):
    check_bkgr_dag = 1
  elif o in ("-j", "--injections"):
    inj_file = a
  elif o in ("-p", "--playground-only"):
    playground_only = 2
  elif o in ("-s", "--max-slide-time"):
    max_slide = int(a)
  elif o in ("-x", "--slide-time"):
    slide_time = int(a)
  elif o in ("-y", "--slide-time-ns"):
    slide_time_ns = int(a)
  elif o in ("-P", "--priority"):
    condor_prio = a
  elif o in ("-l", "--log-path"):
    log_path = a
  elif o in ("-f", "--config-file"):
    config_file = a
  elif o in ("-X", "--dax"):
    dax = True
  else:
    print >> sys.stderr, "Unknown option:", o
    usage()
    sys.exit(1)

if not log_path:
  print >> sys.stderr, "No log file path specified."
  print >> sys.stderr, "Use --log-path PATH to specify a location."
  sys.exit(1)

if not config_file:
  print >> sys.stderr, "No configuration file specified."
  print >> sys.stderr, "Use --config-file FILE to specify location."
  sys.exit(1)

if slide_time_ns and not slide_time:
  print >> sys.stderr, \
    "--slide-time-ns reqires a non-zero value for --slide-time"
  sys.exit(1)

# if slide time is specified, turn off 
if slide_time:
  if abs(slide_time) >= max_slide:
    print >> sys.stderr, "Slide time must be less than", max_slide
    sys.exit(1)
  print "Slide time of", slide_time, "sec", slide_time_ns, "ns given"
  print "Turning off parts of pipeline not needed for background estimation"

##############################################################################
# try and make a directory to store the cache files and job logs
try: os.mkdir('cache')
except: pass
try: os.mkdir('logs')
except: pass

##############################################################################
# create the config parser object and read in the ini file
cp = ConfigParser.ConfigParser()
cp.read(config_file)

##############################################################################
# if a usertag has been specified, override the config file
if usertag:
  cp.set('pipeline','user-tag',usertag)
else:
  try:
    usertag = string.strip(cp.get('pipeline','user-tag'))
  except:
    usertag = None

##############################################################################
# create a log file that the Condor jobs will write to
basename = re.sub(r'\.ini',r'',config_file)
tempfile.tempdir = log_path
if usertag:
  tempfile.template = basename + '.' + usertag + '.dag.log.'
else:
  tempfile.template = basename + '.dag.log.'
logfile = tempfile.mktemp()
fh = open( logfile, "w" )
fh.close()

##############################################################################
# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile,dax)
if usertag:
  dag.set_dag_file(basename + '.' + usertag )
else:
  dag.set_dag_file(basename)

##############################################################################
# create the Condor jobs that will be used in the DAG
df_job = pipeline.LSCDataFindJob('cache','logs',cp,dax)
tmplt_job = inspiral.TmpltBankJob(cp,dax)
insp_job = inspiral.InspiralJob(cp,dax)
l_insp_job = inspiral.InspiralJob(cp,dax)
trig_job = inspiral.TrigToTmpltJob(cp)
inca_job = inspiral.IncaJob(cp)
lho_inca_job = inspiral.IncaJob(cp)
l1h1h2_inca_job = inspiral.IncaJob(cp)

# set better submit file names than the default
if usertag:
  subsuffix = '.' + usertag + '.sub'
else:
  subsuffix = '.sub'
df_job.set_sub_file( basename + '.datafind'+ subsuffix )
tmplt_job.set_sub_file( basename + '.tmpltbank' + subsuffix )
insp_job.set_sub_file( basename + '.inspiral' + subsuffix )
l_insp_job.set_sub_file( basename + '.l1_inspiral' + subsuffix )
trig_job.set_sub_file( basename + '.trigtotmplt' + subsuffix )
inca_job.set_sub_file( basename + '.inca' + subsuffix )
lho_inca_job.set_sub_file( basename + '.lho_inca' + subsuffix )
l1h1h2_inca_job.set_sub_file( basename + '.l1h1h2_inca' + subsuffix )

# set the usertag in the jobs
if usertag:
  tmplt_job.add_opt('user-tag',usertag)
  insp_job.add_opt('user-tag',usertag)
  l_insp_job.add_opt('user-tag',usertag)
  trig_job.add_opt('user-tag',usertag)
  inca_job.add_opt('user-tag',usertag)
  lho_inca_job.add_opt('user-tag',usertag)
  l1h1h2_inca_job.add_opt('user-tag',usertag)

# add the injections
if inj_file:
  insp_job.add_opt('injection-file',inj_file)
  l_insp_job.add_opt('injection-file',inj_file)

# set the condor job priority
if condor_prio:
  df_job.add_condor_cmd('priority',condor_prio)
  tmplt_job.add_condor_cmd('priority',condor_prio)
  insp_job.add_condor_cmd('priority',condor_prio)
  l_insp_job.add_condor_cmd('priority',condor_prio)
  trig_job.add_condor_cmd('priority',condor_prio)
  inca_job.add_condor_cmd('priority',condor_prio)
  lho_inca_job.add_condor_cmd('priority',condor_prio)
  l1h1h2_inca_job.add_condor_cmd('priority',condor_prio)

# add the slide argument to inca if we are doing a background estimation
if slide_time:
  inca_job.add_opt('slide-time',str(slide_time))
  lho_inca_job.add_opt('slide-time',str(slide_time))
  if slide_time_ns:
    inca_job.add_opt('slide-time-ns',str(slide_time_ns))
    lho_inca_job.add_opt('slide-time-ns',str(slide_time_ns))

##############################################################################
# get the thresholds, pad and chunk lengths from the values in the ini file
l1_snr = cp.get('pipeline','l1-snr-threshold')
h1_snr = cp.get('pipeline','h1-snr-threshold')
h2_snr = cp.get('pipeline','h2-snr-threshold')

l1_chisq = cp.get('pipeline','l1-chisq-threshold')
h1_chisq = cp.get('pipeline','h1-chisq-threshold')
h2_chisq = cp.get('pipeline','h2-chisq-threshold')

pad = int(cp.get('data', 'pad-data'))
n = int(cp.get('data', 'segment-length'))
s = int(cp.get('data', 'number-of-segments'))
r = int(cp.get('data', 'sample-rate'))
o = int(cp.get('inspiral', 'segment-overlap'))
length = ( n * s - ( s - 1 ) * o ) / r
overlap = o / r

##############################################################################
# Step 1: read science segs that are greater or equal to the length of data
# in a chunk (defined by the variable "length" above)
print "reading in single ifo science segments...",
sys.stdout.flush()
l1_data = pipeline.ScienceData()
h1_data = pipeline.ScienceData()
h2_data = pipeline.ScienceData()

l1_data.read(cp.get('input','l1-segments'),length)
h1_data.read(cp.get('input','h1-segments'),length)
h2_data.read(cp.get('input','h2-segments'),length)
print "done"

##############################################################################
# Step 2: Create analysis chunks from the single IFO science segments.  The
# instances l1_data, h1_data and h2_data will contain all data that we can
# analyze (since all the science segments are over length seconds.) 
print "making master chunks...",
sys.stdout.flush()
l1_data.make_chunks(length,overlap,playground_only,0,overlap/2)
h1_data.make_chunks(length,overlap,playground_only,0,overlap/2)
h2_data.make_chunks(length,overlap,playground_only,0,overlap/2)

l1_data.make_chunks_from_unused(length,overlap/2,playground_only,0,0,overlap/2)
h1_data.make_chunks_from_unused(length,overlap/2,playground_only,0,0,overlap/2)
h2_data.make_chunks_from_unused(length,overlap/2,playground_only,0,0,overlap/2)
print "done"

##############################################################################
# Step 3: find all L1 data that overlaps with one of the hanford instruments
print "determining what L1 data needs to be filtered...",
sys.stdout.flush()
l1_lho_data = copy.deepcopy(l1_data)
all_lho_data = copy.deepcopy(h1_data)
all_lho_data.union(h2_data)
l1_lho_data.intersection(all_lho_data)
if playground_only:
  l1_lho_data.play()
l1_lho_data.coalesce()
print "done"

##############################################################################
# Step 4: Compute the ScienceSegments for the L1/H2 double coincident data
print "computing L1/H2 double coincident data...",
sys.stdout.flush()

l1_data_out = copy.deepcopy(l1_data)
h1_data_out = copy.deepcopy(h1_data)
h2_data_out = copy.deepcopy(h2_data)
for sci_data in [l1_data_out,h1_data_out,h2_data_out]:
  for seg in sci_data:
    seg.set_start(seg.start()+overlap/2)
    seg.set_end(seg.end()-overlap/2)

not_h1_data = copy.deepcopy(h1_data_out)
not_h1_data.invert()
not_h2_data = copy.deepcopy(h2_data_out)
not_h2_data.invert()

l1_h2_data = copy.deepcopy(l1_data_out)
l1_h2_data.intersection(h2_data_out)
l1_h2_data.intersection(not_h1_data)
if playground_only:
  l1_h2_data.play()
l1_h2_data.coalesce()
print "done"

##############################################################################
# Step 5: Determine which of the L1 master chunks needs to be filtered
l1_chunks_analyzed = []
prev_df = None

print "setting up jobs to filter L1 data...",
sys.stdout.flush()
# loop over L1 master science segments
for seg in l1_data:
  # make sure that we do not do a data find more than once per science segment
  done_df_for_seg = None
  # loop over the master analysis chunks in the science segment
  for chunk in seg:
    # now loop over all the L1 data that we need to filter
    done_this_chunk = 0
    for seg_to_do in l1_lho_data:
      #if the current chunk is in one of the segments we need to filter
      if ( inspiral.overlap_test(chunk,seg_to_do,max_slide) ) and not (
        done_this_chunk ):
        # make sure we only filter the master chunk once
        done_this_chunk = 1
        # make sure we have done one and only one datafind for the segment
        if not done_df_for_seg:
          df = pipeline.LSCDataFindNode(df_job)
          df.set_observatory('L')
          df.set_start(seg.start() - pad)
          df.set_end(seg.end() + pad)
          if prev_df: df.add_parent(prev_df)
          if not slide_time or check_bkgr_dag: dag.add_node(df)
          prev_df = df
          done_df_for_seg = 1
        # make a template bank job for the master L1 chunk
        bank = inspiral.TmpltBankNode(tmplt_job)
        bank.set_start(chunk.start())
        bank.set_end(chunk.end())
        bank.set_ifo('L1')
        bank.calibration()
        bank.set_cache(df.get_output())
        bank.add_parent(df)
        if not slide_time or check_bkgr_dag: dag.add_node(bank)
        # make an inspiral job for the master L1 chunk
        insp = inspiral.InspiralNode(l_insp_job)
        insp.set_start(chunk.start())
        insp.set_end(chunk.end())
        insp.add_var_opt('snr-threshold',l1_snr)
        insp.add_var_opt('chisq-threshold',l1_chisq)
        insp.add_var_opt('trig-start-time',chunk.trig_start())
        insp.add_var_opt('trig-end-time',chunk.trig_end())
        insp.set_ifo('L1')
        insp.calibration()
        insp.set_cache(df.get_output())
        insp.set_bank(bank.get_output())
        insp.add_parent(bank)
        if not slide_time or check_bkgr_dag: dag.add_node(insp)
        # store this chunk in the list of filtered L1 data
        l1_chunks_analyzed.append(AnalyzedIFOData(chunk,insp))
print "done"

##############################################################################
# Step 6: Determine which of the H1 master chunks needs to be filtered
h1_chunks_analyzed = []

print "setting up jobs to filter H1 data...",
sys.stdout.flush()
(prev_df,h1_chunks_analyzed) = analyze_lho_ifo(
  l1_lho_data,l1_chunks_analyzed,h1_data,
  'H1',insp_job,trig_job,df_job,h1_snr,h1_chisq,pad,max_slide,prev_df,
  dag,usertag)
print "done"

##############################################################################
# Step 7: Determine which of the H2 only master chunks needs to be filtered
l1_h2_chunks_analyzed = []

print "setting up jobs to filter H2 double coincident data...",
sys.stdout.flush()
(prev_df,l1_h2_chunks_analyzed) = analyze_lho_ifo(
  l1_h2_data,l1_chunks_analyzed,h2_data,
  'H2',insp_job,trig_job,df_job,h2_snr,h2_chisq,pad,max_slide,prev_df,
  dag,usertag)
print "done"

##############################################################################
## Any thing below here needs to be redone for each time slide ###############
##############################################################################

##############################################################################
# Step 8: Run inca on each of the disjoint sets of double coincidence data
if slide_time:
  print "re-computing L1/H2 double coincident data with slide...",
  sys.stdout.flush()

  if not slide_time_ns:
    # the simple integer timeslide case
    seg_start_increment = seg_end_increment = slide_time
  else:
    # be more careful with non-integer time slides
    seg_start_increment = slide_time + 1
    seg_end_increment = slide_time

  for seg in h1_data_out:
    seg.set_start(seg.start() + seg_start_increment)
    seg.set_end(seg.end() + seg_end_increment)
  for seg in h2_data_out:
    seg.set_start(seg.start() + seg_start_increment)
    seg.set_end(seg.end() + seg_end_increment)

  not_h1_data = copy.deepcopy(h1_data_out)
  not_h1_data.invert()
  not_h2_data = copy.deepcopy(h2_data_out)
  not_h2_data.invert()

  l1_h2_data = copy.deepcopy(l1_data_out)
  l1_h2_data.intersection(h2_data_out)
  l1_h2_data.intersection(not_h1_data)
  l1_h2_data.coalesce()
  print "done"
  sys.stdout.flush()

print "setting up jobs to get L1/H1 double coincident triggers...",
sys.stdout.flush()

l1_h1_data = copy.deepcopy(l1_data_out)
l1_h1_data.intersection(h1_data_out)
l1_h1_data.intersection(not_h2_data)
if playground_only:
  l1_h1_data.play()
l1_h1_data.coalesce()

for seg in l1_h1_data:
  inca = inspiral.IncaNode(inca_job)
  inca.set_start(seg.start())
  inca.set_end(seg.end())
  inca.set_ifo_a('L1')
  inca.set_ifo_b('H1')
  inca.set_ifo_tag('L1H1')
  inca.add_var_opt('dt',cp.get('pipeline','lh-dt'))
  inca.add_var_opt('kappa',cp.get('pipeline','lh-kappa'))
  inca.add_var_opt('epsilon',cp.get('pipeline','lh-epsilon'))
  # add all L1 master chunks that overlap with segment to input
  for l1_done in l1_chunks_analyzed:
    if inspiral.overlap_test(l1_done.get_chunk(),seg):
      inca.add_file_arg(l1_done.get_dag_node().get_output())
      if not slide_time or check_bkgr_dag: 
        inca.add_parent(l1_done.get_dag_node())
  # add all H1 master chunks that overlap with segment to input
  for h1_done in h1_chunks_analyzed:
    if inspiral.overlap_test(h1_done.get_chunk(),seg,abs(slide_time)):
      inca.add_file_arg(h1_done.get_dag_node().get_output())
      if not slide_time or check_bkgr_dag:
        inca.add_parent(h1_done.get_dag_node())
  inca.get_output_a()
  inca.get_output_b()
  dag.add_node(inca)
print "done"

print "setting up jobs to get L1/H2 double coincident triggers...",
sys.stdout.flush()
for seg in l1_h2_data:
  inca = inspiral.IncaNode(inca_job)
  inca.set_start(seg.start())
  inca.set_end(seg.end())
  inca.set_ifo_a('L1')
  inca.set_ifo_b('H2')
  inca.set_ifo_tag('L1H2')
  inca.add_var_opt('dt',cp.get('pipeline','lh-dt'))
  inca.add_var_opt('kappa',cp.get('pipeline','lh-kappa'))
  inca.add_var_opt('epsilon',cp.get('pipeline','lh-epsilon'))
  # add all L1 master chunks that overlap with segment to input
  for l1_done in l1_chunks_analyzed:
    if inspiral.overlap_test(l1_done.get_chunk(),seg):
      inca.add_file_arg(l1_done.get_dag_node().get_output())
      if not slide_time or check_bkgr_dag: 
        inca.add_parent(l1_done.get_dag_node())
  # add all H2 master chunks that overlap with segment to input
  for h2_done in l1_h2_chunks_analyzed:
    if inspiral.overlap_test(h2_done.get_chunk(),seg,abs(slide_time)):
      inca.add_file_arg(h2_done.get_dag_node().get_output())
      if not slide_time or check_bkgr_dag: 
        inca.add_parent(h2_done.get_dag_node())
  inca.get_output_a()
  inca.get_output_b()
  dag.add_node(inca)
print "done"

##############################################################################
# Step 9: Run incas on the triple coincident data
print "setting up jobs to get L1/H1 triple coincident triggers...",
sys.stdout.flush()

l1_h1_h2_data = copy.deepcopy(l1_data_out)
l1_h1_h2_data.intersection(h1_data_out)
l1_h1_h2_data.intersection(h2_data_out)
if playground_only:
  l1_h1_h2_data.play()
l1_h1_h2_data.coalesce()

l1_h1_inca_nodes = []

for seg in l1_h1_h2_data:
  # do the first inca to get the triggers that are coincident between L1 and H1
  inca1 = inspiral.IncaNode(inca_job)
  inca1.set_start(seg.start())
  inca1.set_end(seg.end())
  inca1.set_ifo_a('L1')
  inca1.set_ifo_b('H1')
  inca1.set_ifo_tag('L1H1T')
  inca1.add_var_opt('dt',cp.get('pipeline','lh-dt'))
  inca1.add_var_opt('kappa',cp.get('pipeline','lh-kappa'))
  inca1.add_var_opt('epsilon',cp.get('pipeline','lh-epsilon'))
  # add all L1 master segments that overlap with segment to input
  for l1_done in l1_chunks_analyzed:
    if inspiral.overlap_test(l1_done.get_chunk(),seg):
      inca1.add_file_arg(l1_done.get_dag_node().get_output())
      if not slide_time or check_bkgr_dag: 
        inca1.add_parent(l1_done.get_dag_node())
  # add all H1 master segments that overlap with segment to input
  for h1_done in h1_chunks_analyzed:
    if inspiral.overlap_test(h1_done.get_chunk(),seg,abs(slide_time)):
      inca1.add_file_arg(h1_done.get_dag_node().get_output())
      if not slide_time or check_bkgr_dag: 
        inca1.add_parent(h1_done.get_dag_node())
  # add the nodes to the dag
  inca1.get_output_a()
  inca1.get_output_b()
  dag.add_node(inca1)
  l1_h1_inca_nodes.append(AnalyzedIFOData(seg,inca1))
print "done"

##############################################################################
# Step 10: Generate triggered banks for filter the tripe coinc H2 data
print "setting up jobs to analyze H2 triple coincident data...",
sys.stdout.flush()

l1_h1_h2_chunks_analyzed = []
# loop over the master science segments
for seg in h2_data:
  # make sure that we do not do a data find more than once per science segment
  done_df_for_seg = None
  # loop over the master analysis chunks in the science segment
  for chunk in seg:
    done_this_chunk = 0
    # now loop over all the triple coincident data that we need to filter
    for seg_to_do in l1_h1_h2_data:
      # if the current chunk is in one of the segments we need to filter
      if inspiral.overlap_test(chunk,seg_to_do) and not done_this_chunk:
        # make sure we only filter the master chunk once
        done_this_chunk = 1
        # make sure we have done one and only one datafind for the segment
        if not done_df_for_seg:
          df = pipeline.LSCDataFindNode(df_job)
          df.set_observatory('H')
          df.set_start(seg.start() - pad)
          df.set_end(seg.end() + pad)
          if prev_df: df.add_parent(prev_df)
          if not slide_time or check_bkgr_dag: dag.add_node(df)
          prev_df = df
          done_df_for_seg = 1
        # make a trigbank for the master H2 chunk: to do this, we need
        # the inca job from L1/H1 that overlaps with this H2 chunk as input
        trigbank = inspiral.TrigToTmpltNode(trig_job)
        trigbank.make_trigbank(chunk,max_slide,'H1','H2',usertag,'L1H1')
        for l1_h1_done in l1_h1_inca_nodes:
          if inspiral.overlap_test(chunk,l1_h1_done.get_chunk(),
            abs(slide_time)):
            trigbank.add_file_arg(l1_h1_done.get_dag_node().get_output_b())
            if not slide_time or check_bkgr_dag:
              trigbank.add_parent(l1_h1_done.get_dag_node())
        # analyze the H2 master chunk with this triggered template bank
        insp = inspiral.InspiralNode(insp_job)
        insp.set_start(chunk.start())
        insp.set_end(chunk.end())
        insp.set_ifo('H2')
        insp.calibration()
        insp.set_ifo_tag('L1H1')
        insp.add_var_opt('snr-threshold',h2_snr)
        insp.add_var_opt('chisq-threshold',h2_chisq)
        insp.add_var_opt('trig-start-time',chunk.trig_start())
        insp.add_var_opt('trig-end-time',chunk.trig_end())
        insp.set_cache(df.get_output())
        insp.set_bank(trigbank.get_output())
        if not slide_time or check_bkgr_dag: insp.add_parent(df)
        dag.add_node(trigbank)
        insp.add_parent(trigbank)
        dag.add_node(insp)
        # store this chunk in the list of filtered L1 data
        l1_h1_h2_chunks_analyzed.append(AnalyzedIFOData(chunk,insp))
print "done"

##############################################################################
# Step 11: Run inca to get triple coincident triggers
print "setting up jobs to get L1/H1/H2 triple coincident triggers...",
sys.stdout.flush()

# make sure we use the range cut for analyzing the LHO conincidences
lho_inca_job.add_opt('ifo-b-range-cut','')

for seg in l1_h1_h2_data:
  # do the second inca to get the H1 triggers that are in H2 or can't be seen
  inca2 = inspiral.IncaNode(lho_inca_job)
  inca2.set_start(seg.start())
  inca2.set_end(seg.end())
  inca2.set_ifo_a('H1')
  inca2.set_ifo_b('H2')
  inca2.set_ifo_tag('L1H1H2')
  inca2.add_var_opt('dt',cp.get('pipeline','hh-dt'))
  inca2.add_var_opt('kappa',cp.get('pipeline','hh-kappa'))
  inca2.add_var_opt('epsilon',cp.get('pipeline','hh-epsilon'))
  inca2.add_var_opt('ifo-b-snr-threshold',h2_snr)
  # do the third inca to clobber and L1 triggers that we can
  inca3 = inspiral.IncaNode(l1h1h2_inca_job)
  inca3.set_start(seg.start())
  inca3.set_end(seg.end())
  inca3.set_ifo_a('L1')
  inca3.set_ifo_b('H1')
  inca3.set_ifo_tag('L1H1H2')
  inca3.add_var_opt('dt',cp.get('pipeline','lh-dt'))
  inca3.add_var_opt('kappa',cp.get('pipeline','lh-kappa'))
  inca3.add_var_opt('epsilon',cp.get('pipeline','lh-epsilon'))
  # add the triggers and parent relation from the L1/H1 inca
  for l1_h1_done in l1_h1_inca_nodes:
    if ( l1_h1_done.get_chunk().start() == seg.start() and 
      l1_h1_done.get_chunk().end() == seg.end() ):
      inca2.add_parent(l1_h1_done.get_dag_node())
      inca2.add_file_arg(l1_h1_done.get_dag_node().get_output_b())
      inca3.add_parent(l1_h1_done.get_dag_node())
      inca3.add_file_arg(l1_h1_done.get_dag_node().get_output_a())
  # add all H2 master segments that overlap with segment to input
  for h2_done in l1_h1_h2_chunks_analyzed:
    if inspiral.overlap_test(h2_done.get_chunk(),seg):
      inca2.add_file_arg(h2_done.get_dag_node().get_output())
      inca2.add_parent(h2_done.get_dag_node())
  inca3.add_parent(inca2)
  inca3.add_file_arg(inca2.get_output_a())
  # add the nodes to the dag
  dag.add_node(inca2)
  inca3.get_output_a()
  inca3.get_output_b()
  dag.add_node(inca3)
print "done"
sys.stdout.flush()

##############################################################################
# Write out the DAG, help message and log file
dag.write_sub_files()
dag.write_dag()

# write a message telling the user that the DAG has been written
if dax: 	 
  print """\nCreated a DAX file which can be submitted to the Grid using 	 
Pegasus. See the page: 	 
  	 
  http://www.lsc-group.phys.uwm.edu/lscdatagrid/griphynligo/vds_howto.html 	 
  	 
for instructions.
"""
else:
  print "\nCreated a DAG file which can be submitted by executing"
  print "\n   condor_submit_dag", dag.get_dag_file()
  print """\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
If you are running LALdataFind jobs, do not forget to initialize your grid 
proxy certificate on the condor submit machine by running the commands

  unset X509_USER_PROXY
  grid-proxy-init -hours 72

Enter your pass phrase when promted. The proxy will be valid for 72 hours. 
If you expect the LALdataFind jobs to take longer to complete, increase the
time specified in the -hours option to grid-proxy-init. You can check that 
the grid proxy has been sucessfully created by executing the command:

  grid-cert-info -all -file /tmp/x509up_u`id -u`

This will also give the expiry time of the proxy.
"""

# write out a log file for this script
if usertag:
  log_fh = open(basename + '.pipeline.' + usertag + '.log', 'w')
else:
  log_fh = open(basename + '.pipeline.log', 'w')
  
log_fh.write( "$Id$" + "\n" )
log_fh.write( "$Name$" + "\n\n" )
log_fh.write( "Invoked with arguments:\n" )
for o, a in opts:
  log_fh.write( o + ' ' + a + '\n' )

log_fh.write( "Config file has CVS strings:\n" )
log_fh.write( cp.get('pipeline','version') + "\n" )
log_fh.write( cp.get('pipeline','cvs-tag') + "\n\n" )

print >> log_fh, "\n===========================================\n"
print >> log_fh, "Science Segments and master chunks:\n"
for sci_data in [l1_data, h1_data, h2_data]:
  print >> log_fh, sci_data
  for seg in sci_data:
    print >> log_fh, " ", seg
    for chunk in seg:
      print >> log_fh, "   ", chunk

print >> log_fh, "\n===========================================\n"
log_fh.write( 
  "Filtering " + str(len(l1_chunks_analyzed)) + " L1 master chunks\n" )
total_time = 0
for l1_done in l1_chunks_analyzed:
  print >> log_fh, l1_done.get_chunk()
  total_time += len(l1_done.get_chunk())
print >> log_fh, "\n total time", total_time, "seconds"

print >> log_fh, "\n===========================================\n"
log_fh.write( 
  "Filtering " + str(len(h1_chunks_analyzed)) + " H1 master chunks\n" )
total_time = 0
for h1_done in h1_chunks_analyzed:
  print >> log_fh, h1_done.get_chunk()
  total_time += len(h1_done.get_chunk())
print >> log_fh, "\n total time", total_time, "seconds"

print >> log_fh, "\n===========================================\n"
log_fh.write( 
  "Filtering " + str(len(l1_h2_chunks_analyzed)) + \
  " H2 double coinc master chunks\n" )
total_time = 0
for h2_done in l1_h2_chunks_analyzed:
  print >> log_fh, h2_done.get_chunk()
  total_time += len(h2_done.get_chunk())
print >> log_fh, "\n total time", total_time, "seconds"

print >> log_fh, "\n===========================================\n"
log_fh.write( 
  "Filtering " + str(len(l1_h1_h2_chunks_analyzed)) + \
  " H2 triple coinc master chunks\n" )
total_time = 0
for h2_done in l1_h1_h2_chunks_analyzed:
  print >> log_fh, h2_done.get_chunk()
  total_time += len(h2_done.get_chunk())
print >> log_fh, "\n total time", total_time, "seconds"

print >> log_fh, "\n===========================================\n"
log_fh.write( "Writing " + str(len(l1_h1_data)) + 
  " L1/H1 double coincident segments\n" )
total_time = 0
for seg in l1_h1_data:
  print >> log_fh, seg
  total_time += seg.dur()
print >> log_fh, "\n total time", total_time, "seconds"

print >> log_fh, "\n===========================================\n"
log_fh.write(   "Writing " + str(len(l1_h2_data)) + 
  " L1/H2 double coincident segments\n" )
total_time = 0
for seg in l1_h2_data:
  print >> log_fh, seg
  total_time += seg.dur()
print >> log_fh, "\n total time", total_time, "seconds"

print >> log_fh, "\n===========================================\n"
log_fh.write( "Writing " + str(len(l1_h1_h2_data)) + 
  " L1/H1/H2 triple coincident segments\n" )
total_time = 0
for seg in l1_h1_h2_data:
  print >> log_fh, seg
  total_time += seg.dur()
print >> log_fh, "\n total time", total_time, "seconds"
sys.exit(0)
