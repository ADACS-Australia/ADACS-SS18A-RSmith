#!/usr/bin/env @PYTHONPROG@
"""
inspiral_pipe_s2_tama.in - S2 inspiral analysis pipeline script

$Id$

This script generates the necessary condor DAG files to analyze the
data from the second LIGO science run through the S2 LIGO-TAMA inspiral 
pipeline.

All times not analyzed in the S2 analysis will be used in the LIGO-TAMA
analysis.  This includes all L1, H1 and H2 single IFO times as well as 
H1-H2 double times.  For the double times, we will search H1 and then trigger
onto H2.
"""

__author__ = 'Steve Fairhurst <sfairhur@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import sys, os, copy, math
import getopt, re, string
import tempfile
import ConfigParser
sys.path.append('@PYTHONLIBDIR@')

# import the modules we need to build the pipeline
from glue import pipeline
import inspiral

##############################################################################
# some functions to make life easier later
class AnalyzedIFOData:
  """
  Contains the information for the data that needs to be filtered.
  """
  def __init__(self,chunk,node):
    self.__analysis_chunk = chunk
    self.__dag_node = node

  def set_chunk(self,chunk):
    self.__analysis_chunk = chunk

  def get_chunk(self):
    return self.__analysis_chunk

  def set_dag_node(self,node):
    self.__dag_node = node

  def get_dag_node(self):
    return self.__dag_node

class AnalyzedTAMAData:
  """
  Contains the information for the data that needs to be filtered.
  """
  def __init__(self,seg,node):
    self.__analysis_segment = seg
    self.__dag_node = node

  def set_seg(self,seg):
    self.__analysis_segment = seg

  def get_seg(self):
    return self.__analysis_segment

  def set_dag_node(self,node):
    self.__dag_node = node

  def get_dag_node(self):
    return self.__dag_node

def compare_start( node_1, node_2):
  return cmp( node_1.get_chunk().start(), node_2.get_chunk().start() )

def analyze_ifo(ifo_data,ifo_to_do,ifo_name,tmplt_job,insp_job,df_job,snr,
  chisq,pad,prev_df,dag,usertag=None):
  """
  Analyze the data from L1 single, H1 (single and double with H2) and H2
  single. Since the way we treat all this data is the same, this function is
  the same for all three cases. Returns the last LSCdataFind job that was
  executed and the chunks analyzed.
  ifo_data = the master science segs for the IFO
  ifo_to_do = the science segments we need to analyze
  ifo_name = the name of the IFO
  tmplt_job = the template bank job we should use 
  insp_job = the condor job that we should use to analyze data
  df_job = the condor job to find the data
  snr = the signal-to-noise threshold for this IFO
  chisq = the chi squared threshold for this IFO
  pad = data start/end padding
  max_slide = the maximum length of a time slide
  prev_df = the previous LSCdataFind job that was executed
  dag = the DAG to attach the nodes to
  """
  chunks_analyzed = []
  # loop over the master science segments
  for seg in ifo_data:
    # make sure that we do not do a data find more than once per science segment
    done_df_for_seg = None
    # loop over the master analysis chunks in the science segment
    for chunk in seg:
      done_this_chunk = 0
      # now loop over all the data that we need to filter
      for seg_to_do in ifo_to_do:
        # if the current chunk is in one of the segments we need to filter
        if inspiral.overlap_test(chunk,seg_to_do) and not done_this_chunk:
          # make sure we only filter the master chunk once
          done_this_chunk = 1
          # make sure we have done one and only one datafind for the segment
          if not done_df_for_seg:
            df = pipeline.LSCDataFindNode(df_job)
            df.set_observatory(ifo_name[0])
            df.set_start(seg.start() - pad)
            df.set_end(seg.end() + pad)
            if prev_df: df.add_parent(prev_df)
            if ligo_analysis or datafind_only : dag.add_node(df)
            prev_df = df
            done_df_for_seg = 1
          # make a template bank job for the master chunk
          bank = inspiral.TmpltBankNode(tmplt_job)
          bank.set_start(chunk.start())
          bank.set_end(chunk.end())
          bank.set_ifo(ifo_name)
          bank.set_cache(df.get_output())
          bank.calibration()
          bank.add_parent(df)
          if ligo_analysis: dag.add_node(bank)
          # make an inspiral job for the master chunk
          insp = inspiral.InspiralNode(insp_job)
          insp.set_start(chunk.start())
          insp.set_end(chunk.end())
          insp.add_var_opt('snr-threshold',snr)
          insp.add_var_opt('chisq-threshold',chisq)
          insp.add_var_opt('trig-start-time',chunk.trig_start())
          insp.add_var_opt('trig-end-time',chunk.trig_end())
          insp.set_ifo(ifo_name)
          insp.set_cache(df.get_output())
          insp.calibration()
          insp.set_bank(bank.get_output())
          insp.add_parent(bank)
          if ligo_analysis: dag.add_node(insp)
          # store this chunk in the list of filtered data
          chunks_analyzed.append(AnalyzedIFOData(chunk,insp))

  return tuple([prev_df,chunks_analyzed])


def single_inca(ifo_analyzed,ifo_single,ifo_name,single_inca_job,dag,
  usertag=None):
  """
  Run inca on the single coincident times from each of the IFOs. 
  Since the way we treat all this data is the same, this function is the same 
  for all three cases. 
  ifo_analyzed = the analyzed chunks for the IFO
  ifo_single = the single IFO science segments 
  ifo_name = the name of the IFO
  single_inca_job = the condor job to do single IFO inca
  dag = the DAG to attach the nodes to
  """

  single_inca_analyzed = []
  for seg in ifo_single:
    sinca = inspiral.IncaNode(single_inca_job)
    sinca.set_start(seg.start())
    sinca.set_end(seg.end())
    sinca.set_ifo_a(ifo_name)
    sinca.add_var_opt('single-ifo',' ')

    # add all master chunks that overlap with segment to input
    for ifo_done in ifo_analyzed:
      if inspiral.overlap_test(ifo_done.get_chunk(),seg):
        sinca.add_var_arg(ifo_done.get_dag_node().get_output())
        if ligo_analysis: sinca.add_parent(ifo_done.get_dag_node())
    if ligo_analysis: dag.add_node(sinca)
    single_inca_analyzed.append(AnalyzedIFOData(seg,sinca))

  return single_inca_analyzed


def lt_coinc(segments,ligo_chunks,tama_segs,ifo_name,lt_inca_job,dt,
  dag,slide_time = None, usertag=None):
  """
  Perform coincidence test between single IFO LIGO data and TAMA data.
  Since the way we treat all this data is the same, this function is the same 
  for all three LIGO ifos. 
  segments = segment list of single IFO + TAMA times
  ligo_chunks = the master chunks for the LIGO IFO
  tama_segs = the TAMA segments
  ifo_name = the name of the LIGO IFO
  inca_job = the condor job that we should use to test coincidence
  dt = max time difference between triggers
  slide_time = the amount of time by which to slide TAMA triggers
  dag = the DAG to attach the nodes to
  """

  lt_inca_analyzed = []
  for seg in segments:
    lt_inca = inspiral.IncaNode(lt_inca_job)
    lt_inca.set_start(seg.start())
    lt_inca.set_end(seg.end())
    lt_inca.set_ifo_a(ifo_name)
    lt_inca.set_ifo_b('T1')
    lt_inca.set_ifo_tag(ifo_name + 'T1')
    lt_inca.add_var_opt('dt',dt)
    lt_inca.add_var_opt('kappa',cp.get('pipeline','ligo-t-kappa'))
    lt_inca.add_var_opt('epsilon',cp.get('pipeline','ligo-t-epsilon'))
    lt_inca.add_var_opt('parameter-test',
    cp.get('pipeline','ligo-t-param-test'))
    
    # add all LIGO master chunks that overlap with segment to input
    for ligo_done in ligo_chunks:
      if inspiral.overlap_test(ligo_done.get_chunk(),seg):
        lt_inca.add_var_arg(ligo_done.get_dag_node().get_output())
        if ligo_analysis: lt_inca.add_parent(ligo_done.get_dag_node())
    # add all TAMA segments that overlap with segment to input
    for t_done in tama_segs:
      if inspiral.overlap_test(t_done.get_seg(),seg,abs(slide_time)):
        lt_inca.add_var_arg(t_done.get_dag_node().get_output())
        if not slide_time: lt_inca.add_parent(t_done.get_dag_node())
    dag.add_node(lt_inca)
    lt_inca_analyzed.append(AnalyzedIFOData(seg,lt_inca))

  return lt_inca_analyzed


def sire_segs(segments,output,file_name,sire_job,dag,playground_only=None,
  usertag=None,tama_output=None,slide_time=None,inj_file=None,cluster=None):
  """
  Perform coincidence test between single IFO LIGO data and TAMA data.
  Since the way we treat all this data is the same, this function is the same 
  for all three LIGO ifos. 
  segments = segment list and associated dag nodes
  output = one of output, output_a or output_b
  file_name = the name of the output file
  sire_job = the condor job that we should to do sire
  dag = the DAG to attach the nodes to
  playground_only = flag playground only analysis
  usertag = the user tag for the job
  tama_output = whether to output data in TAMA ascii format
  slide_time = the time slide
  inj_file = the name of the xml file containing the injections
  cluster = flag clustering of the triggers coincident with injection
  """
  sire = inspiral.SireNode(sire_job)
  fname = file_name
  if usertag:
    fname += usertag
  fname += '.input'
  f = open(fname, 'w')
  for seg in segments:
    sire.add_parent(seg.get_dag_node())
    if output == 'output':
      f.write('%s\n' % seg.get_dag_node().get_output())
    if output == 'output_a':
      f.write('%s\n' % seg.get_dag_node().get_output_a())
    if output == 'output_b':
      f.write('%s\n' % seg.get_dag_node().get_output_b())
  f.close()
  sire.add_var_opt('input',fname)
  if inj_file:
    coinc_time = int(cp.get('pipeline','inj-coinc'))
    sire.set_inj_outputs(file_name,inj_coinc,usertag,cluster,slide_time,
      tama_output)
    sire.add_var_opt('injection-file',inj_file)
    sire.add_var_opt('injection-coincidence',coinc_time)
    if cluster:
      sire.add_var_opt('cluster-algorithm',cp.get('pipeline','cluster-alg'))
      sire.add_var_opt('cluster-time',2*coinc_time)
    
  else:
    sire.set_outputs(file_name,usertag,cluster,slide_time,tama_output)

  dag.add_node(sire) 

 
##############################################################################
# help message
def usage():
  msg = """\
Usage: lalapps_inspiral_pipe [options]

  -h, --help               display this message
  -v, --version            print version information and exit
  -u, --user-tag TAG       tag the job with TAG (overrides value in ini file)

  -s, --output-segments    output the analyzed segments and exit
  -o, --tama-overlap       analyze only times in coincidence with TAMA
  -d, --datafind-only      perform only the datafinds
  -L, --ligo-analysis      perform the ligo analysis
  -t, --tama-coincidence   test for coincidence between LIGO and TAMA triggers
  -O, --tama-output        output data in text format for TAMA exchange
  -j, --injections FILE    add simulated inspirals from sim_inspiral in FILE

  -S, --time-slide SEC     slide TAMA data forward by SEC seconds
  -N, --time-slide-ns NS   slide TAMA data forward by NS nano seconds
  -Q, --quick-slide SEC    slide TAMA data forward by SEC seconds, 
                           but only do inca on the full LIGO/TAMA trigger lists
                           (not segment by segment)
                           NOTE: Assumes no H1-H2 coincident triggers!
  
  -P, --priority PRIO      run jobs with condor priority PRIO

  -p, --playground-only    only analyze jobs in the playground

  -l, --log-path PATH      directory to write condor log file
  -f, --config-file FILE   use configuration file FILE
"""
  print >> sys.stderr, msg

##############################################################################
# pasrse the command line options to figure out what we should do
shortop = "hvu:cj:pP:l:f:sodLS:N:Q:tO"
longop = [
  "help",
  "version",
  "user-tag=",
  "injections=",
  "playground-only",
  "priority=",
  "log-path=",
  "config-file=",
  "output-segments",
  "tama-overlap",
  "datafind-only",
  "ligo-analysis",
  "time-slide=",
  "time-slide-ns=",
  "quick-slide=",
  "tama-coincidence",
  "tama-output"
  ]

try:
  opts, args = getopt.getopt(sys.argv[1:], shortop, longop)
except getopt.GetoptError:
  usage()
  sys.exit(1)

usertag = None
inj_file = None
condor_prio = None
log_path = None
config_file = None
output_segs = None
tama_overlap = None
playground_only = 0
datafind_only = 0
ligo_analysis = 0
tama_coincidence = 0
tama_trig_file = None
tama_output = 0
slide_time = 0
slide_time_ns = 0
quick_slide = 0

for o, a in opts:
  if o in ("-h", "--help"):
    usage()
    sys.exit(0)
  elif o in ("-v", "--version"):
    print "S2 LIGO TAMA Inspiral Pipeline DAG generation script"
    print "Steve Fairhurst <sfairhur@gravity.phys.uwm.edu>"
    print "CVS Version:",\
      "$Id$"
    print "CVS Tag: $Name$"
    sys.exit(0)
  elif o in ("-u", "--user-tag"):
    usertag = a
  elif o in ("-s", "--output-segments"):
    output_segs = 1
  elif o in ("-o", "--tama-overlap"):
    tama_overlap = 1
  elif o in ("-o", "--ligo-analysis"):
    ligo_analysis = 1
  elif o in ("-d", "--datafind-only"):
    datafind_only = 1
  elif o in ("-o", "--tama-coincidence"):
    tama_trig_file = a
    tama_coincidence = 1
  elif o in ("-O", "--tama-output"):
    tama_output = 1
  elif o in ("-j", "--injections"):
    inj_file = a
  elif o in ("-p", "--playground-only"):
    playground_only = 2
  elif o in ("-S", "--time-slide"):
    slide_time =  int(a) 
  elif o in ("-N", "--time-slide-ns"):
    slide_time_ns = int(a)
  elif o in ("-Q", "--quick-slide"):
    quick_slide = int(a)
  elif o in ("-P", "--priority"):
    condor_prio = a
  elif o in ("-l", "--log-path"):
    log_path = a
  elif o in ("-f", "--config-file"):
    config_file = a
  else:
    print >> sys.stderr, "Unknown option:", o
    usage()
    sys.exit(1)

if not log_path:
  print >> sys.stderr, "No log file path specified."
  print >> sys.stderr, "Use --log-path PATH to specify a location."
  sys.exit(1)

if not config_file:
  print >> sys.stderr, "No configuration file specified."
  print >> sys.stderr, "Use --config-file FILE to specify location."
  sys.exit(1)


##############################################################################
# try to make a directory to store the cache files and job logs
try: os.mkdir('cache')
except: pass
try: os.mkdir('logs')
except: pass

##############################################################################
# create the config parser object and read in the ini file
cp = ConfigParser.ConfigParser()
cp.read(config_file)

##############################################################################
# if a usertag has been specified, override the config file
if usertag:
  cp.set('pipeline','user-tag',usertag)
else:
  try:
    usertag = string.strip(cp.get('pipeline','user-tag'))
  except:
    usertag = None

##############################################################################
# create a log file that the Condor jobs will write to
basename = re.sub(r'\.ini',r'',config_file)
tempfile.tempdir = log_path
if usertag:
  tempfile.template = basename + '.' + usertag + '.dag.log.'
else:
  tempfile.template = basename + '.dag.log.'
logfile = tempfile.mktemp()
fh = open( logfile, "w" )
fh.close()

##############################################################################
# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
if usertag:
  dag.set_dag_file(basename + '.' + usertag + '.dag')
else:
  dag.set_dag_file(basename + '.dag')

##############################################################################
# create the Condor jobs that will be used in the DAG
df_job = pipeline.LSCDataFindJob('cache','logs',cp)
tmplt_job = inspiral.TmpltBankJob(cp)
insp_job = inspiral.InspiralJob(cp)
trig_job = inspiral.TrigToTmpltJob(cp)
trig_insp_job = inspiral.InspiralJob(cp)
inca_job = inspiral.IncaJob(cp)
s_inca_job = inspiral.IncaJob(cp)
tama2lw_job = inspiral.Tama2LigoLwJob(cp)
lt_inca_job = inspiral.IncaJob(cp)
sire_job = inspiral.SireJob(cp)
sire_inj_job = inspiral.SireJob(cp)
sire_cl_inj_job = inspiral.SireJob(cp)

#############################################################################
# set better submit file names than the default
if usertag:
  subsuffix = '.' + usertag + '.sub'
else:
  subsuffix = '.sub'
df_job.set_sub_file( basename + '.datafind'+ subsuffix )
tmplt_job.set_sub_file( basename + '.tmpltbank' + subsuffix )
insp_job.set_sub_file( basename + '.inspiral' + subsuffix )
trig_job.set_sub_file( basename + '.trigtotmplt' + subsuffix )
trig_insp_job.set_sub_file( basename + '.trig_inspiral' + subsuffix )
inca_job.set_sub_file( basename + '.inca' + subsuffix )
s_inca_job.set_sub_file( basename + '.s_inca' + subsuffix )
tama2lw_job.set_sub_file( basename + '.tama2lw' + subsuffix )
lt_inca_job.set_sub_file( basename + '.lt_inca' + subsuffix )
sire_job.set_sub_file( basename + '.sire' + subsuffix )
sire_inj_job.set_sub_file( basename + '.sire_inj' + subsuffix )
sire_cl_inj_job.set_sub_file( basename + '.sire_inj_clust' + subsuffix )

#############################################################################
# set the usertag in the jobs
if usertag:
  tmplt_job.add_opt('user-tag',usertag)
  insp_job.add_opt('user-tag',usertag)
  trig_job.add_opt('user-tag',usertag)
  trig_insp_job.add_opt('user-tag',usertag)
  inca_job.add_opt('user-tag',usertag)
  s_inca_job.add_opt('user-tag',usertag)
  tama2lw_job.add_opt('user-tag',usertag)
  lt_inca_job.add_opt('user-tag',usertag)
  sire_job.add_opt('user-tag',usertag)
  sire_inj_job.add_opt('user-tag',usertag)
  sire_cl_inj_job.add_opt('user-tag',usertag)

#############################################################################
# add the injections
if inj_file:
  insp_job.add_opt('injection-file',inj_file)
  trig_insp_job.add_opt('injection-file',inj_file)

#############################################################################
# add the playground information to the inca and sire jobs
if playground_only:
  inca_job.add_opt('playground-only',' ')
  s_inca_job.add_opt('playground-only',' ')
  lt_inca_job.add_opt('playground-only',' ')
  sire_job.add_opt('playground-only',' ')
  sire_inj_job.add_opt('playground-only',' ')
  sire_cl_inj_job.add_opt('playground-only',' ')
elif slide_time:
  inca_job.add_opt('all-data',' ')
  s_inca_job.add_opt('all-data',' ')
  lt_inca_job.add_opt('all_data',' ')
  sire_job.add_opt('all-data',' ')
  sire_inj_job.add_opt('all-data',' ')
  sire_cl_inj_job.add_opt('all-data',' ')
else:
  inca_job.add_opt('no-playground',' ')
  s_inca_job.add_opt('no-playground',' ')
  lt_inca_job.add_opt('no-playground',' ')
  sire_job.add_opt('exclude-playground',' ')
  sire_inj_job.add_opt('exclude-playground',' ')
  sire_cl_inj_job.add_opt('exclude-playground',' ')

#############################################################################
# add the slide argument to inca if we are doing a background estimation
if slide_time or quick_slide:
  lt_inca_job.add_opt('slide-time',str(slide_time))
  if slide_time_ns:
    lt_inca_job.add_opt('slide-time-ns',str(slide_time))

#############################################################################  
# set the condor job priority
if condor_prio:
  df_job.add_condor_cmd('priority',condor_prio)
  tmplt_job.add_condor_cmd('priority',condor_prio)
  insp_job.add_condor_cmd('priority',condor_prio)
  trig_job.add_condor_cmd('priority',condor_prio)
  trig_insp_job.add_condor_cmd('priority',condor_prio)
  inca_job.add_condor_cmd('priority',condor_prio)
  s_inca_job.add_condor_cmd('priority',condor_prio)
  tama2lw_job.add_condor_cmd('priority',condor_prio)
  lt_inca_job.add_condor_cmd('priority',condor_prio)
  sire_job.add_condor_cmd('priority',condor_prio)
  sire_inj_job.add_condor_cmd('priority',condor_prio)
  sire_cl_inj_job.add_condor_cmd('priority',condor_prio)

##############################################################################
# get the thresholds, pad and chunk lengths from the values in the ini file
l1_snr = cp.get('pipeline','l1-snr-threshold')
h1_snr = cp.get('pipeline','h1-snr-threshold')
h2_snr = cp.get('pipeline','h2-snr-threshold')

l1_chisq = cp.get('pipeline','l1-chisq-threshold')
h1_chisq = cp.get('pipeline','h1-chisq-threshold')
h2_chisq = cp.get('pipeline','h2-chisq-threshold')

pad = int(cp.get('data', 'pad-data'))
n = int(cp.get('data', 'segment-length'))
s = int(cp.get('data', 'number-of-segments'))
r = int(cp.get('data', 'sample-rate'))
o = int(cp.get('inspiral', 'segment-overlap'))
length = ( n * s - ( s - 1 ) * o ) / r
overlap = o / r



##############################################################################
# Step 1: read science segs that are greater or equal to the length of data
# in a chunk (defined by the variable "length" above)
print "reading in single ifo science segments...",
sys.stdout.flush()
l1_data = pipeline.ScienceData()
h1_data = pipeline.ScienceData()
h2_data = pipeline.ScienceData()


l1_data.read(cp.get('input','l1-segments'),length)
h1_data.read(cp.get('input','h1-segments'),length)
h2_data.read(cp.get('input','h2-segments'),length)
if tama_overlap or tama_coincidence or slide_time or quick_slide:
  # read in the tama file in the format provided and store as science data
  t_data = pipeline.ScienceData()
  t_data.tama_read(cp.get('input','tama-segments'))
  if playground_only: t_data.play()
print "done"

##############################################################################
# Step 2: Create analysis chunks from the single IFO science segments.  The
# instances l1_data, h1_data and h2_data will contain all data that we can
# analyze (since all the science segments are over length seconds.) 
print "making master chunks...",
sys.stdout.flush()
l1_data.make_chunks(length,overlap,playground_only,0,overlap/2)
h1_data.make_chunks(length,overlap,playground_only,0,overlap/2)
h2_data.make_chunks(length,overlap,playground_only,0,overlap/2)

l1_data.make_chunks_from_unused(length,overlap/2,playground_only,0,0,overlap/2)
h1_data.make_chunks_from_unused(length,overlap/2,playground_only,0,0,overlap/2)
h2_data.make_chunks_from_unused(length,overlap/2,playground_only,0,0,overlap/2)
print "done"

##############################################################################
# Step 3: find all data that is analyzable.
print "determining analyzable data",
sys.stdout.flush()

l1_data_out = copy.deepcopy(l1_data)
h1_data_out = copy.deepcopy(h1_data)
h2_data_out = copy.deepcopy(h2_data)
for sci_data in [l1_data_out,h1_data_out,h2_data_out]:
  for seg in sci_data:
    seg.set_start(seg.start()+overlap/2)
    seg.set_end(seg.end()-overlap/2)

not_l1_data_out = copy.deepcopy(l1_data_out)
not_l1_data_out.invert()

not_h1_data_out = copy.deepcopy(h1_data_out)
not_h1_data_out.invert()

not_h2_data_out = copy.deepcopy(h2_data_out)
not_h2_data_out.invert()

print "done"

##############################################################################
# Step 4: find all single ifo L1 data that we can analyze
print "computing L1 single ifo data...",
sys.stdout.flush()
l1_single_data = copy.deepcopy(l1_data_out)
l1_single_data.intersection(not_h1_data_out)
l1_single_data.intersection(not_h2_data_out)
if tama_overlap:
  l1_single_data.intersection(t_data)
if playground_only:
  l1_single_data.play()
l1_single_data.coalesce()
print "done"


##############################################################################
# Step 5: Compute the ScienceSegments for the H1/H2 double coincident data
print "computing H1/H2 double coincident data...",
sys.stdout.flush()
h1_h2_data = copy.deepcopy(h1_data_out)
h1_h2_data.intersection(h2_data_out)
h1_h2_data.intersection(not_l1_data_out)
if tama_overlap:
  h1_h2_data.intersection(t_data)
if playground_only:
  h1_h2_data.play()
h1_h2_data.coalesce()
print "done"


##############################################################################
# Step 6: Compute the Science Segments for the single ifo LHO data
print "computing single ifo H1 and H2 data...",
sys.stdout.flush()

h1_data_to_do = copy.deepcopy(h1_data_out)
h1_data_to_do.intersection(not_l1_data_out)
if tama_overlap:
  h1_data_to_do.intersection(t_data)
if playground_only:
  h1_data_to_do.play()

h1_data_to_do.coalesce()

h1_single_data = copy.deepcopy(h1_data_out)
h1_single_data.intersection(not_l1_data_out)
h1_single_data.intersection(not_h2_data_out)
if tama_overlap:
  h1_single_data.intersection(t_data)
if playground_only:
  h1_single_data.play()
h1_single_data.coalesce()

h2_single_data = copy.deepcopy(h2_data_out)
h2_single_data.intersection(not_l1_data_out)
h2_single_data.intersection(not_h1_data_out)
if tama_overlap:
  h2_single_data.intersection(t_data)
if playground_only:
  h2_single_data.play()
h2_single_data.coalesce()
print "done"

##############################################################################
# Step 7:determine the one site LIGO times:
print "computing all single site LIGO data...",
sys.stdout.flush()
ligo_data = copy.deepcopy(h2_single_data)
ligo_data.union(h1_single_data)
ligo_data.union(h1_h2_data)
ligo_data.union(l1_single_data)
ligo_data.coalesce()
print "done"

##############################################################################
# Step 8: Determine which of the L1 master chunks needs to be filtered
l1_chunks_analyzed = []
prev_df = None

print "setting up jobs to filter L1 data...",
sys.stdout.flush()
(prev_df,l1_chunks_analyzed) = analyze_ifo(l1_data,l1_single_data,'L1',
  tmplt_job,insp_job,df_job,l1_snr,l1_chisq,pad,prev_df,dag,usertag=None)
print "done"

##############################################################################
# Step 9: Determine which of the H1 master chunks needs to be filtered
h1_chunks_analyzed = []

print "setting up jobs to filter H1 data...",
sys.stdout.flush()
(prev_df,h1_chunks_analyzed) = analyze_ifo(h1_data,h1_data_to_do,'H1',
  tmplt_job,insp_job,df_job,h1_snr,h1_chisq,pad,prev_df,dag,usertag=None)
print "done"

##############################################################################
# Step 10: Determine which of the H2 master chunks needs to be filtered
h2_single_chunks_analyzed = []

print "determining which H2 double coincident chunks need filtering...",
sys.stdout.flush()
(prev_df,h2_single_chunks_analyzed) = analyze_ifo(h2_data,h2_single_data,'H2',
  tmplt_job,insp_job,df_job,h2_snr,h2_chisq,pad,prev_df,dag,usertag=None)
print "done"

##############################################################################
# Step 11: Run inca in single ifo mode on the single ifo triggers.

print "setting up jobs to inca single IFO data...",

l1_single_inca_nodes = []
l1_single_inca_nodes = single_inca(l1_chunks_analyzed,l1_single_data,'L1',
  s_inca_job,dag, usertag)

h1_single_inca_nodes = []
h1_single_inca_nodes = single_inca(h1_chunks_analyzed,h1_single_data,'H1',
  s_inca_job,dag,usertag)

h2_single_inca_nodes = []
h2_single_inca_nodes = single_inca(h2_single_chunks_analyzed,h2_single_data,
  'H2',s_inca_job,dag,usertag)
print "done"

##############################################################################
# Step 12: Determine which of the H2 master chunks needs to be triggered
h1_h2_chunks_analyzed = []

print "setting up jobs to filter H2 double coincident data...",
sys.stdout.flush()
# loop over the master science segments
for seg in h2_data:
  # make sure that we do not do a data find more than once per science segment
  done_df_for_seg = None
  # loop over the master analysis chunks in the science segment
  for chunk in seg:
    done_this_chunk = 0
    # now loop over all the H1_H2 data that we need to filter
    for seg_to_do in h1_h2_data:
      # if the current chunk is in one of the segments we need to filter
      if inspiral.overlap_test(chunk,seg_to_do) and not done_this_chunk:
        # make sure we only filter the master chunk once
        done_this_chunk = 1
        # make sure we have done one and only one datafind for the segment
        if not done_df_for_seg:
          df = pipeline.LSCDataFindNode(df_job)
          df.set_observatory('H')
          df.set_start(seg.start() - pad)
          df.set_end(seg.end() + pad)
          if prev_df: df.add_parent(prev_df)
          if ligo_analysis or datafind_only: dag.add_node(df)
          prev_df = df
          done_df_for_seg = 1
        # make a trigbank for the master H2 chunk: to do this, we need
        # the master H1 chunks that overlap with this H2 chunk as input
        trigbank = inspiral.TrigToTmpltNode(trig_job)
        trigbank.make_trigbank(chunk,0,'H1','H2',usertag)
        for h1_done in h1_chunks_analyzed:
          if inspiral.overlap_test(chunk,h1_done.get_chunk()):
            trigbank.add_var_arg(h1_done.get_dag_node().get_output())
            trigbank.add_parent(h1_done.get_dag_node())
        if ligo_analysis: dag.add_node(trigbank)
        # analyze the H2 master chunk with this triggered template bank
        trig_insp = inspiral.InspiralNode(trig_insp_job)
        trig_insp.set_start(chunk.start())
        trig_insp.set_end(chunk.end())
        trig_insp.set_ifo('H2')
        trig_insp.set_ifo_tag('H1')
        trig_insp.add_var_opt('snr-threshold',h2_snr)
        trig_insp.add_var_opt('chisq-threshold',h2_chisq)
        trig_insp.add_var_opt('trig-start-time',chunk.trig_start())
        trig_insp.add_var_opt('trig-end-time',chunk.trig_end())
        trig_insp.set_cache(df.get_output())
        trig_insp.set_bank(trigbank.get_output())
        trig_insp.calibration()
        trig_insp.add_parent(df)
        trig_insp.add_parent(trigbank)
        if ligo_analysis: dag.add_node(trig_insp)
        # store this chunk in the list of filtered L1 data
        h1_h2_chunks_analyzed.append(AnalyzedIFOData(chunk,trig_insp))
print "done"

##############################################################################
# Step 13: Run inca on each of the disjoint sets of double coincidence data

print "setting up jobs to get H1/H2 double coincident triggers...",
sys.stdout.flush()

# make sure we use the range cut for analyzing the LHO conincidences
inca_job.add_opt('ifo-b-range-cut',None)

h1_h2_inca_nodes = []
for seg in h1_h2_data:
  inca = inspiral.IncaNode(inca_job)
  inca.set_start(seg.start())
  inca.set_end(seg.end())
  inca.set_ifo_a('H1')
  inca.set_ifo_b('H2')
  inca.set_ifo_tag('H1H2')
  inca.add_var_opt('dt',cp.get('pipeline','hh-dt'))
  inca.add_var_opt('kappa',cp.get('pipeline','hh-kappa'))
  inca.add_var_opt('epsilon',cp.get('pipeline','hh-epsilon'))
  inca.add_var_opt('ifo-b-snr-threshold',h2_snr)
  inca.add_var_opt('parameter-test',cp.get('pipeline','hh-param-test'))

  # add all H1 master chunks that overlap with segment to input
  for h1_done in h1_chunks_analyzed:
    if inspiral.overlap_test(h1_done.get_chunk(),seg):
      inca.add_var_arg(h1_done.get_dag_node().get_output())
      inca.add_parent(h1_done.get_dag_node())
  # add all H2 master chunks that overlap with segment to input
  for h2_done in h1_h2_chunks_analyzed:
    if inspiral.overlap_test(h2_done.get_chunk(),seg):
      inca.add_var_arg(h2_done.get_dag_node().get_output())
      inca.add_parent(h2_done.get_dag_node())
  if ligo_analysis: dag.add_node(inca)
  h1_h2_inca_nodes.append(AnalyzedIFOData(seg,inca))
print "done"

##############################################################################
# Step 14: Run tama2ligolw on the tama triggers creating two lists:
# T1-INSPIRAL: containing all those triggers which occur in times being used 
#              in the LIGO-TAMA analysis
# TAMA-INSPIRAL: containing all TAMA triggers.

if tama_coincidence or slide_time:
  print "Converting triggers to LIGO Lw xml format...",
  sys.stdout.flush()
  
# write out tama triggers in LIGO-TAMA times as T1-INSPIRAL- ...
if tama_coincidence:
  ligo_tama_data = copy.deepcopy(ligo_data)
  ligo_tama_data.intersection(t_data)
  tama_segs_analyzed = []
  for seg in ligo_tama_data:
    tama2lw = inspiral.Tama2LigoLwNode(tama2lw_job)
    tama2lw.set_start(seg.start())
    tama2lw.set_end(seg.end())
    infile = cp.get('input','tama-triggers')
    tama2lw.set_input(infile)
    outfile = 'T1-INSPIRAL'
    if usertag: outfile += '_' + usertag 
    outfile += '-' + str(seg.start()) + '-' + str(seg.end() - seg.start()) + \
      '.xml'
    tama2lw.set_output(outfile)
    dag.add_node(tama2lw)
    tama_segs_analyzed.append(AnalyzedTAMAData(seg,tama2lw))


# write out all tama triggers as TAMA-INSPIRAL- ...
if tama_coincidence or slide_time or quick_slide:

  tama_data_analyzed = []
  for seg in t_data:
    tama2lw = inspiral.Tama2LigoLwNode(tama2lw_job)
    tama2lw.set_start(seg.start())
    tama2lw.set_end(seg.end())
    infile = cp.get('input','tama-triggers')
    tama2lw.set_input(infile)
    outfile = 'TAMA-INSPIRAL'
    if usertag: outfile += '_' + usertag 
    outfile += '-' + str(seg.start()) + '-' + str(seg.end() - seg.start()) + \
      '.xml'
    tama2lw.set_output(outfile)
    if tama_coincidence: dag.add_node(tama2lw)
    tama_data_analyzed.append(AnalyzedTAMAData(seg,tama2lw))

  tama_analyzed = []
  if slide_time or quick_slide: tama_analyzed = tama_data_analyzed
  else: tama_analyzed = tama_segs_analyzed

  print "done"


##############################################################################
## Any thing below here needs to be redone for each time slide ###############
##############################################################################

##############################################################################
# Step 15: Test for coincidence between the single IFO ligo and tama triggers
if slide_time or quick_slide:
  print "re-computing TAMA and LIGO/TAMA data with slide...",
  sys.stdout.flush()

  if not slide_time_ns:
    # the simple integer timeslide case
    seg_start_increment = seg_end_increment = slide_time
  else:
    # be more careful with non-integer time slides
    seg_start_increment = slide_time + 1
    seg_end_increment = slide_time

  if quick_slide:
    # slide start and end by quick_slide
    seg_start_increment = seg_end_increment = quick_slide
    
  for seg in t_data:
    seg.set_start(seg.start() + seg_start_increment)
    seg.set_end(seg.end() + seg_end_increment)

  ligo_tama_data = copy.deepcopy(ligo_data)
  ligo_tama_data.intersection(t_data)

  print "done"
  sys.stdout.flush()


if tama_coincidence or slide_time:
  print "setting up jobs to get (single ifo LIGO)-TAMA coincident triggers...",
  sys.stdout.flush()

  l1t1_data = copy.deepcopy(l1_single_data)
  l1t1_data.intersection(t_data)
  h1t1_data = copy.deepcopy(h1_single_data)
  h1t1_data.intersection(t_data)
  h2t1_data = copy.deepcopy(h2_single_data)
  h2t1_data.intersection(t_data)
  
  lt_dt = cp.get('pipeline','lt-dt')
  l1t1_inca_nodes = []
  l1t1_inca_nodes = lt_coinc(l1t1_data,l1_chunks_analyzed,tama_analyzed,
    'L1',lt_inca_job,lt_dt,dag,slide_time,usertag)

  ht_dt = cp.get('pipeline','ht-dt')
  h1t1_inca_nodes = []
  h1t1_inca_nodes = lt_coinc(h1t1_data,h1_chunks_analyzed,tama_analyzed,
    'H1',lt_inca_job,ht_dt,dag,slide_time,usertag)

  h2t1_inca_nodes = []
  h2t1_inca_nodes = lt_coinc(h2t1_data,h2_single_chunks_analyzed,
    tama_analyzed,'H2',lt_inca_job,ht_dt,dag,slide_time,usertag)

  print "done"

##############################################################################
# Step 16: Test for coincidence between the H1-H2 and tama triggers

if tama_coincidence or slide_time:
  print "setting up jobs to get (H1-H2 LIGO)-TAMA coincident triggers...",
  sys.stdout.flush()

  
  h1h2t1_data = copy.deepcopy(h1_h2_data)
  h1h2t1_data.intersection(t_data)

  h1h2t1_inca2_nodes = []
  h1h2t1_inca3_nodes = []

  for seg in h1h2t1_data:
    # do inca to get coincidence between H1 triggers and TAMA triggers
    lt_inca2 = inspiral.IncaNode(lt_inca_job)
    lt_inca2.set_start(seg.start())
    lt_inca2.set_end(seg.end())
    lt_inca2.set_ifo_a('H1')
    lt_inca2.set_ifo_b('T1')
    lt_inca2.set_ifo_tag('H1H2T1')
    lt_inca2.add_var_opt('dt',cp.get('pipeline','ht-dt'))
    lt_inca2.add_var_opt('kappa',cp.get('pipeline','ligo-t-kappa'))
    lt_inca2.add_var_opt('epsilon',cp.get('pipeline','ligo-t-epsilon'))
    lt_inca2.add_var_opt('parameter-test', 
    cp.get('pipeline','ligo-t-param-test'))
    # do the third inca between H1 and H2
    inca3 = inspiral.IncaNode(inca_job)
    inca3.set_start(seg.start())
    inca3.set_end(seg.end())
    inca3.set_ifo_a('H1')
    inca3.set_ifo_b('H2')
    inca3.set_ifo_tag('H1H2T1')
    inca3.add_var_opt('dt',cp.get('pipeline','hh-dt'))
    inca3.add_var_opt('kappa',cp.get('pipeline','hh-kappa'))
    inca3.add_var_opt('epsilon',cp.get('pipeline','hh-epsilon'))
    inca3.add_var_opt('parameter-test', 
    cp.get('pipeline','hh-param-test'))

    inca3.add_var_opt('ifo-b-snr-threshold',h2_snr)
    # add the triggers and parent relation from the H1/H2 inca
    for h1_h2_done in h1_h2_inca_nodes:
      if inspiral.overlap_test(h1_h2_done.get_chunk(),seg):
        if ligo_analysis: lt_inca2.add_parent(h1_h2_done.get_dag_node())
        lt_inca2.add_var_arg(h1_h2_done.get_dag_node().get_output_a())
        if ligo_analysis: inca3.add_parent(h1_h2_done.get_dag_node())
        inca3.add_var_arg(h1_h2_done.get_dag_node().get_output_b())
    # add all T1 master segments that overlap with segment to input
    for t_done in tama_analyzed:
      if inspiral.overlap_test(t_done.get_seg(),seg,abs(slide_time)):
        lt_inca2.add_var_arg(t_done.get_dag_node().get_output())
        if tama_coincidence: lt_inca2.add_parent(t_done.get_dag_node())
    inca3.add_parent(lt_inca2)
    inca3.add_var_arg(lt_inca2.get_output_a())
    # add the nodes to the dag
    dag.add_node(lt_inca2)
    dag.add_node(inca3)
    h1h2t1_inca2_nodes.append(AnalyzedIFOData(seg,lt_inca2))
    h1h2t1_inca3_nodes.append(AnalyzedIFOData(seg,inca3))

print "done"
sys.stdout.flush()

##############################################################################
# Step 17: Do any sires which are necessary

print "setting up sire jobs ...",
sys.stdout.flush()

# create a sorted list of all the ligo inca nodes 
# for H1-H2 double time, we just use H1 triggers.
ligo_inca_nodes = []
for inca_nodes in [l1_single_inca_nodes,h1_single_inca_nodes,
  h2_single_inca_nodes,h1_h2_inca_nodes]:
  for inca_node in inca_nodes:
    ligo_inca_nodes.append(inca_node)
ligo_inca_nodes.sort(compare_start)

# create a sorted list of all the ligo-tama inca nodes 
# for H1-H2 double time, we just use H1 triggers.
if tama_coincidence or slide_time:
  ligo_tama_inca_nodes = []
  for inca_nodes in [l1t1_inca_nodes,h1t1_inca_nodes,h2t1_inca_nodes,
    h1h2t1_inca2_nodes]:
    for inca_node in inca_nodes:
      ligo_tama_inca_nodes.append(inca_node)
  ligo_tama_inca_nodes.sort(compare_start)


if ligo_analysis:
  # a sire for each of the single/double IFO configurations
  sire_segs(l1_single_inca_nodes,'output_a','L1-INCA_SINGLE_IFO',sire_job,dag,
    playground_only,usertag,tama_output)
  sire_segs(h1_single_inca_nodes,'output_a','H1-INCA_SINGLE_IFO',sire_job,dag,
    playground_only,usertag,tama_output)
  sire_segs(h2_single_inca_nodes,'output_a','H2-INCA_SINGLE_IFO',sire_job,dag,
    playground_only,usertag,tama_output) 
  sire_segs(h1_h2_inca_nodes,'output_a','H1-INCA_H1H2',sire_job,dag,
    playground_only,usertag,tama_output)
  sire_segs(h1_h2_inca_nodes,'output_b','H2-INCA_H1H2',sire_job,dag,
    playground_only,usertag,tama_output)
  # one last sire that contains all the LIGO triggers.
  sire_segs(ligo_inca_nodes,'output_a','LIGO-TRIGGERS',sire_job,dag,
    playground_only,usertag,tama_output)

if tama_coincidence:
  # a sire that contains all the TAMA triggers.
  sire_segs(tama_data_analyzed,'output','TAMA-INSPIRAL',sire_job,dag,
    playground_only,usertag,tama_output)
  # a sire that contains all the TAMA triggers in LIGO-TAMA times
  sire_segs(tama_segs_analyzed,'output','T1-INSPIRAL',sire_job,dag,
    playground_only,usertag,tama_output)
if tama_coincidence or slide_time:
  # a sire for all the coincidence configurations
  sire_segs(l1t1_inca_nodes,'output_a','L1-INCA_L1T1',sire_job,dag,
    playground_only,usertag,tama_output,slide_time)
  sire_segs(l1t1_inca_nodes,'output_b','T1-INCA_L1T1',sire_job,dag,
    playground_only,usertag,tama_output,slide_time)
  sire_segs(h1t1_inca_nodes,'output_a','H1-INCA_H1T1',sire_job,dag,
    playground_only,usertag,tama_output,slide_time)
  sire_segs(h1t1_inca_nodes,'output_b','T1-INCA_H1T1',sire_job,dag,
    playground_only,usertag,tama_output,slide_time)
  sire_segs(h2t1_inca_nodes,'output_a','H2-INCA_H2T1',sire_job,dag,
    playground_only,usertag,tama_output,slide_time)
  sire_segs(h2t1_inca_nodes,'output_b','T1-INCA_H2T1',sire_job,dag,
    playground_only,usertag,tama_output,slide_time)
  sire_segs(h1h2t1_inca2_nodes,'output_b','T1-INCA_H1H2T1',sire_job,dag,
    playground_only,usertag,tama_output,slide_time)
  sire_segs(h1h2t1_inca3_nodes,'output_a','H1-INCA_H1H2T1',sire_job,dag,
    playground_only,usertag,tama_output,slide_time)
  sire_segs(h1h2t1_inca3_nodes,'output_b','H2-INCA_H1H2T1',sire_job,dag,
    playground_only,usertag,tama_output,slide_time)
    # one last sire that contains all the coincident LIGO triggers.
  sire_segs(ligo_tama_inca_nodes,'output_a','LIGO-TAMA-COINCIDENT-TRIGGERS',
    sire_job,dag,playground_only,usertag,tama_output,slide_time)
  # and all the coincident TAMA triggers 
  sire_segs(ligo_tama_inca_nodes,'output_b','TAMA-LIGO-COINCIDENT-TRIGGERS',
    sire_job,dag,playground_only,usertag,tama_output,slide_time)

if inj_file:
  if ligo_analysis:
    # sires to find all triggers coincident with injections:
    # slide_time should be set to zero, since irrelevant for LIGO only analysis
    sire_segs(l1_single_inca_nodes,'output_a','L1-INCA_SINGLE_IFO',
      sire_inj_job,dag,playground_only,usertag,tama_output,0,inj_file)
    sire_segs(h1_single_inca_nodes,'output_a','H1-INCA_SINGLE_IFO',
      sire_inj_job,dag,playground_only,usertag,tama_output,0,inj_file)
    sire_segs(h2_single_inca_nodes,'output_a','H2-INCA_SINGLE_IFO',
      sire_inj_job,dag,playground_only,usertag,tama_output,0,inj_file) 
    sire_segs(h1_h2_inca_nodes,'output_a','H1-INCA_H1H2',
      sire_inj_job,dag,playground_only,usertag,tama_output,0,inj_file)
    sire_segs(h1_h2_inca_nodes,'output_b','H2-INCA_H1H2',
      sire_inj_job,dag,playground_only,usertag,tama_output,0,inj_file)
    
   # clustered sires to find loudest triggers coincident with injections:
    cluster = 1
    sire_segs(l1_single_inca_nodes,'output_a','L1-INCA_SINGLE_IFO',
      sire_cl_inj_job,dag,playground_only,usertag,tama_output,0,
      inj_file,cluster)
    sire_segs(h1_single_inca_nodes,'output_a','H1-INCA_SINGLE_IFO',
      sire_cl_inj_job,dag,playground_only,usertag,tama_output,0,
      inj_file,cluster)
    sire_segs(h2_single_inca_nodes,'output_a','H2-INCA_SINGLE_IFO',
      sire_cl_inj_job,dag,playground_only,usertag,tama_output,0,
      inj_file,cluster) 
    sire_segs(h1_h2_inca_nodes,'output_a','H1-INCA_H1H2',
      sire_cl_inj_job,dag,playground_only,usertag,tama_output,0,
      inj_file,cluster)
    sire_segs(h1_h2_inca_nodes,'output_b','H2-INCA_H1H2',
      sire_cl_inj_job,dag,playground_only,usertag,tama_output,0,
      inj_file,cluster)
  
  if tama_coincidence:
    # sire to find all TAMA triggers coincident with injections.
    sire_segs(tama_data_analyzed,'output','TAMA-INSPIRAL',
      sire_inj_job,dag,playground_only,usertag,tama_output,slide_time,inj_file)
    # a sire that contains all the TAMA triggers coincident with injections
    # in LIGO-TAMA times
    sire_segs(tama_segs_analyzed,'output','T1-INSPIRAL',
      sire_inj_job,dag,playground_only,usertag,tama_output,slide_time,inj_file)

    # sires to find all LT coincident triggers coincident with injections.
    sire_segs(l1t1_inca_nodes,'output_a','L1-INCA_L1T1',sire_inj_job,dag,
      playground_only,usertag,tama_output,slide_time,inj_file)
    sire_segs(l1t1_inca_nodes,'output_b','T1-INCA_L1T1',sire_inj_job,dag,
      playground_only,usertag,tama_output,slide_time,inj_file)
    sire_segs(h1t1_inca_nodes,'output_a','H1-INCA_H1T1',sire_inj_job,dag,
      playground_only,usertag,tama_output,slide_time,inj_file)
    sire_segs(h1t1_inca_nodes,'output_b','T1-INCA_H1T1',sire_inj_job,dag,
      playground_only,usertag,tama_output,slide_time,inj_file)
    sire_segs(h2t1_inca_nodes,'output_a','H2-INCA_H2T1',sire_inj_job,dag,
      playground_only,usertag,tama_output,slide_time,inj_file)
    sire_segs(h2t1_inca_nodes,'output_b','T1-INCA_H2T1',sire_inj_job,dag,
      playground_only,usertag,tama_output,slide_time,inj_file)
    sire_segs(h1h2t1_inca2_nodes,'output_b','T1-INCA_H1H2T1',sire_inj_job,dag,
      playground_only,usertag,tama_output,slide_time,inj_file)
    sire_segs(h1h2t1_inca3_nodes,'output_a','H1-INCA_H1H2T1',sire_inj_job,dag,
      playground_only,usertag,tama_output,slide_time,inj_file)
    sire_segs(h1h2t1_inca3_nodes,'output_b','H2-INCA_H1H2T1',sire_inj_job,dag,
      playground_only,usertag,tama_output,slide_time,inj_file)
    # one last sire that contains all the TAMA triggers.
    sire_segs(ligo_tama_inca_nodes,'output_b','TAMA-LIGO-COINCIDENT-TRIGGERS',
      sire_inj_job,dag,playground_only,usertag,tama_output,slide_time,inj_file)

    # clustered sires to find loudest triggers coincident with injections:
    cluster = 1
    # TAMA triggers coincident with injections.
    sire_segs(tama_data_analyzed,'output','TAMA-INSPIRAL',sire_cl_inj_job,dag,
      playground_only,usertag,tama_output,slide_time,inj_file,cluster)
    # a sire that contains all the TAMA triggers coincident with injections
    # in LIGO-TAMA times
    sire_segs(tama_segs_analyzed,'output','T1-INSPIRAL',sire_cl_inj_job,dag,
      playground_only,usertag,tama_output,slide_time,inj_file,cluster)

    # sires to find all LT coincident triggers coincident with injections.
    sire_segs(l1t1_inca_nodes,'output_a','L1-INCA_L1T1',sire_cl_inj_job,dag,
      playground_only,usertag,tama_output,slide_time,inj_file,cluster)
    sire_segs(l1t1_inca_nodes,'output_b','T1-INCA_L1T1',sire_cl_inj_job,dag,
      playground_only,usertag,tama_output,slide_time,inj_file,cluster)
    sire_segs(h1t1_inca_nodes,'output_a','H1-INCA_H1T1',sire_cl_inj_job,dag,
      playground_only,usertag,tama_output,slide_time,inj_file,cluster)
    sire_segs(h1t1_inca_nodes,'output_b','T1-INCA_H1T1',sire_cl_inj_job,dag,
      playground_only,usertag,tama_output,slide_time,inj_file,cluster)
    sire_segs(h2t1_inca_nodes,'output_a','H2-INCA_H2T1',sire_cl_inj_job,dag,
      playground_only,usertag,tama_output,slide_time,inj_file,cluster)
    sire_segs(h2t1_inca_nodes,'output_b','T1-INCA_H2T1',sire_cl_inj_job,dag,
      playground_only,usertag,tama_output,slide_time,inj_file,cluster)
    sire_segs(h1h2t1_inca2_nodes,'output_b','T1-INCA_H1H2T1',sire_cl_inj_job,
      dag, playground_only,usertag,tama_output,slide_time,inj_file,cluster)
    sire_segs(h1h2t1_inca3_nodes,'output_a','H1-INCA_H1H2T1',sire_cl_inj_job,
      dag, playground_only,usertag,tama_output,slide_time,inj_file,cluster)
    sire_segs(h1h2t1_inca3_nodes,'output_b','H2-INCA_H1H2T1',sire_cl_inj_job,
      dag, playground_only,usertag,tama_output,slide_time,inj_file,cluster)
    # one last sire that contains all the TAMA triggers.
    sire_segs(ligo_tama_inca_nodes,'output_b','TAMA-LIGO-COINCIDENT-TRIGGERS',
      sire_cl_inj_job,dag,playground_only,usertag,tama_output,slide_time,
      inj_file,cluster)

print "done"

##############################################################################
# Step 18: Do the quick_slide
if quick_slide:
  
  # we are only going to do coincidence between the previously obtained
  # L1-INCA_SINGLE_IFO, H1-INCA_SINGLE_IFO, H2-INCA_SINGLE_IFO and
  # H1-INCA_H1H2 files with the TAMA triggers in TAMA-INSPIRAL
  #
  # Note: this assumes that the analysis was run previously and that ALL
  # LIGO single site data was analyzed (not just LT coinc data)
  
  # hardwire the start and end times of S2: 
  s2start = 729273613
  s2end = 734367613


  # setup the 2 sire filed
  sire_ligo = inspiral.SireNode(sire_job)
  sire_tama = inspiral.SireNode(sire_job)
  
  fname_ligo = 'LIGO-TAMA-COINCIDENT-TRIGGERS' + quick_slide + '.input'
  fname_tama = 'TAMA-LIGO-COINCIDENT-TRIGGERS' + quick_slide + '.input'
  
  fl = open(fname_ligo, 'w')
  ft = open(fname_tama, 'w')
  
  # do inca to get coincidence between H1 triggers and TAMA triggers
  for ifo in ['H1', 'H2', 'L1', 'H1H2']:
    lt_inca = inspiral.IncaNode(lt_inca_job)
    lt_inca.set_start(s2start)
    lt_inca.set_end(s2end)
    lt_inca.add_var_opt('user-tag', quick_slide)
    if ifo == 'H1H2':
      lt_inca.set_ifo_a('H1')
    else:
      lt_inca.set_ifo_a(ifo)
    lt_inca.set_ifo_b('T1')
    lt_inca.set_ifo_tag(ifo + 'T1')
    if ifo == 'L1':
      lt_inca.add_var_opt('dt',cp.get('pipeline','lt-dt'))
    else:
      lt_inca.add_var_opt('dt',cp.get('pipeline','ht-dt'))
    lt_inca.add_var_opt('kappa',cp.get('pipeline','ligo-t-kappa'))
    lt_inca.add_var_opt('epsilon',cp.get('pipeline','ligo-t-epsilon'))
    lt_inca.add_var_opt('parameter-test', 
      cp.get('pipeline','ligo-t-param-test'))
    if ifo == 'H1H2':
      lt_inca.add_var_arg('H1-INCA_H1H2.xml')
    else:
      lt_inca.add_var_arg(ifo + '-INCA_SINGLE_IFO.xml')
    lt_inca.add_var_arg('TAMA-INSPIRAL.xml')
    dag.add_node(lt_inca) 

    sire_ligo.add_parent( lt_inca )
    sire_tama.add_parent( lt_inca )

    fl.write('%s\n' % lt_inca.get_dag_node().get_output_a())
    ft.write('%s\n' % lt_inca.get_dag_node().get_output_b())
    
  fl.close()
  ft.close()

  # do two sires to get the LIGO triggers and the TAMA triggers
  sire_ligo.add_var_opt('input',fname)
  sire_tama.add_var_opt('input',fname)
  
  sire_ligo.set_outputs(fname_ligo,usertag,cluster,slide_time,tama_output)
  sire_tama.set_outputs(fname_tama,usertag,cluster,slide_time,tama_output)

  dag.add_node(sire_ligo) 
  dag.add_node(sire_tama) 


  

##############################################################################
# Write out the DAG, help message and log file
dag.write_sub_files()
dag.write_dag()

print "\nCreated a DAG file which can be submitted by executing"

print "\n   condor_submit_dag", dag.get_dag_file()
print """\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
If you are running LSCdataFind jobs, do not forget to initialize your grid 
proxy certificate on the condor submit machine by running the commands

  unset X509_USER_PROXY
  grid-proxy-init -hours 72

Enter your pass phrase when promted. The proxy will be valid for 72 hours. 
If you expect the LSCdataFind jobs to take longer to complete, increase the
time specified in the -hours option to grid-proxy-init. You can check that 
the grid proxy has been sucessfully created by executing the command:

  grid-cert-info -all -file /tmp/x509up_u`id -u`

This will also give the expiry time of the proxy. You should also make sure
that the environment variable LSC_DATAFIND_SERVER is set the hostname and
optional port of server to query. For example on the UWM medusa cluster this
you should use

  export LSC_DATAFIND_SERVER=dataserver.phys.uwm.edu

Contact the administrator of your cluster to find the hostname and port of the
LSCdataFind server.

"""

# write out a log file for this script
if usertag:
  log_fh = open(basename + '.pipeline.' + usertag + '.log', 'w')
else:
  log_fh = open(basename + '.pipeline.log', 'w')
  
log_fh.write( "$Id$" + "\n" )
log_fh.write( "$Name$" + "\n\n" )
log_fh.write( "Invoked with arguments:\n" )
for o, a in opts:
  log_fh.write( o + ' ' + a + '\n' )

log_fh.write( "Config file has CVS strings:\n" )
log_fh.write( cp.get('pipeline','version') + "\n" )
log_fh.write( cp.get('pipeline','cvs-tag') + "\n\n" )

print >> log_fh, "\n===========================================\n"
print >> log_fh, "Science Segments and master chunks:\n"
for sci_data in [l1_data, h1_data, h2_data]:
  print >> log_fh, sci_data
  for seg in sci_data:
    print >> log_fh, " ", seg
    for chunk in seg:
      print >> log_fh, "   ", chunk

print >> log_fh, "\n===========================================\n"
log_fh.write( 
  "Filtering " + str(len(l1_chunks_analyzed)) + " L1 master chunks\n" )
total_time = 0
for l1_done in l1_chunks_analyzed:
  print >> log_fh, l1_done.get_chunk()
  total_time += len(l1_done.get_chunk())
print >> log_fh, "\n total time", total_time, "seconds"

print >> log_fh, "\n===========================================\n"
log_fh.write( 
  "Filtering " + str(len(h1_chunks_analyzed)) + " H1 master chunks\n" )
total_time = 0
for h1_done in h1_chunks_analyzed:
  print >> log_fh, h1_done.get_chunk()
  total_time += len(h1_done.get_chunk())
print >> log_fh, "\n total time", total_time, "seconds"

print >> log_fh, "\n===========================================\n"
log_fh.write( 
  "Filtering " + str(len(h2_single_chunks_analyzed)) + " H2 master chunks\n" )
total_time = 0
for h2_done in h2_single_chunks_analyzed:
  print >> log_fh, h2_done.get_chunk()
  total_time += len(h2_done.get_chunk())
print >> log_fh, "\n total time", total_time, "seconds"

print >> log_fh, "\n===========================================\n"
log_fh.write( 
  "Filtering " + str(len(h1_h2_chunks_analyzed)) + \
  " H2 double coinc master chunks\n" )
total_time = 0
for h2_done in h1_h2_chunks_analyzed:
  print >> log_fh, h2_done.get_chunk()
  total_time += len(h2_done.get_chunk())
print >> log_fh, "\n total time", total_time, "seconds"

print >> log_fh, "\n===========================================\n"
log_fh.write( "Writing " + str(len(l1_single_data)) + 
  " L1 science segments\n" )
total_time = 0
for seg in l1_single_data:
  print >> log_fh, seg
  total_time += seg.dur()
print >> log_fh, "\n total time", total_time, "seconds"

if output_segs:
  if playground_only:
    f = open('l1_play_segs_analyzed.txt', 'w')
  else:  
    f = open('l1_segs_analyzed.txt', 'w')
  for seg in l1_single_data:
    f.write('%4d %10d %10d %6d\n' % (seg.id(), seg.start(), seg.end(), 
      seg.dur()))
  f.close()

print >> log_fh, "\n===========================================\n"
log_fh.write( "Writing " + str(len(h1_single_data)) + 
  " H1 science segments\n" )
total_time = 0
for seg in h1_single_data:
  print >> log_fh, seg
  total_time += seg.dur()
print >> log_fh, "\n total time", total_time, "seconds"

if output_segs:
  if playground_only:
    f = open('h1_play_segs_analyzed.txt', 'w')
  else:  
    f = open('h1_segs_analyzed.txt', 'w')
  for seg in h1_single_data:
    f.write('%4d %10d %10d %6d\n' % (seg.id(), seg.start(), seg.end(), 
      seg.dur()))
  f.close()


print >> log_fh, "\n===========================================\n"
log_fh.write( "Writing " + str(len(h2_single_data)) + 
  " H2 science segments\n" )
total_time = 0
for seg in h2_single_data:
  print >> log_fh, seg
  total_time += seg.dur()
print >> log_fh, "\n total time", total_time, "seconds"

if output_segs:
  if playground_only:
    f = open('h2_play_segs_analyzed.txt', 'w')
  else:  
    f = open('h2_segs_analyzed.txt', 'w')
  for seg in h2_single_data:
    f.write('%4d %10d %10d %6d\n' % (seg.id(), seg.start(), seg.end(), 
      seg.dur()))
  f.close()

print >> log_fh, "\n===========================================\n"
log_fh.write( "Writing " + str(len(h1_h2_data)) + 
  " H1/H2 double coincident segments\n" )
total_time = 0
for seg in h1_h2_data:
  print >> log_fh, seg
  total_time += seg.dur()
print >> log_fh, "\n total time", total_time, "seconds"

if output_segs:
  if playground_only:
    f = open('h1_h2_play_segs_analyzed.txt', 'w')
  else:  
    f = open('h1_h2_segs_analyzed.txt', 'w')
  for seg in h1_h2_data:
    f.write('%4d %10d %10d %6d\n' % (seg.id(), seg.start(), seg.end(), 
      seg.dur()))
  f.close()

print >> log_fh, "\n===========================================\n"
log_fh.write( "Writing " + str(len(ligo_data)) + 
  " LIGO single site segments\n" )
total_time = 0
for seg in ligo_data:
  print >> log_fh, seg
  total_time += seg.dur()
print >> log_fh, "\n total time", total_time, "seconds"

if output_segs:
  if playground_only:
    f = open('ligo_play_segs_analyzed.txt', 'w')
  else:  
    f = open('ligo_segs_analyzed.txt', 'w')
  for seg in ligo_data:
    f.write('%4d %10d %10d %6d\n' % (seg.id(), seg.start(), seg.end(), 
      seg.dur()))
  f.close()

if tama_overlap or tama_coincidence or slide_time or quick_slide:
  print >> log_fh, "\n===========================================\n"
  if slide_time or quick_slide: 
    print >> log_fh, "Slid Tama Segments:\n"
  else:
    print >> log_fh, "Tama Locked Segments:\n"
  total_time = 0
  for tama_seg in t_data:
    print >> log_fh, tama_seg
    total_time += tama_seg.end() - tama_seg.start()
  print >> log_fh, "\n total time", total_time, "seconds"


if tama_coincidence or slide_time or quick_slide:
  print >> log_fh, "\n===========================================\n"
  log_fh.write( 
    "Getting triggers from " + str(len(tama_analyzed)) + \
    " TAMA segments\n" )
  total_time = 0
  for t_done in tama_analyzed:
    print >> log_fh, t_done.get_seg()
    seg = t_done.get_seg()
    total_time += seg.dur() 
  print >> log_fh, "\n total time", total_time, "seconds"
  
   
if tama_overlap or tama_coincidence or slide_time:
  if output_segs:
    if slide_time:
      f = open('tama_slide_segs.txt', 'w')
    elif quick_slide:
      f = open('tama_quick_slide_segs' + quick_slide + '.txt', 'w')
    elif playground_only:
      f = open('tama_play_segs.txt', 'w')
    else:
      f = open('tama_segs.txt', 'w')
    for seg in t_data:
      f.write('%4d %10d %10d %6d\n' % (seg.id(), seg.start(), seg.end(), 
        seg.dur()))
    f.close()

if tama_coincidence or slide_time or quick_slide:  
  print >> log_fh, "\n===========================================\n"
  log_fh.write( "Writing " + str(len(ligo_tama_data)) + 
    " LIGO-TAMA coincident segments\n" )
  total_time = 0
  for seg in ligo_tama_data:
    print >> log_fh, seg
    total_time += seg.dur()
  print >> log_fh, "\n total time", total_time, "seconds"

  if output_segs:
    if slide_time:
      f = open('ligo_tama_slide_segs.txt', 'w')
    elif quick_slide:
      f = open('ligo_tama_quick_slide_segs' + quick_slide + '.txt', 'w')
    elif playground_only:
      f = open('ligo_tama_play_segs.txt', 'w')
    else:
      f = open('ligo_tama_segs.txt', 'w')
    for seg in ligo_tama_data:
      f.write('%4d %10d %10d %6d\n' % (seg.id(), seg.start(), seg.end(), 
        seg.dur()))
    f.close()

sys.exit(0)
