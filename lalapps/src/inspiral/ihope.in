#!/usr/bin/env @PYTHONPROG@
"""
ihope.in - weekly automate pipeline driver script

$Id$

This script generates the condor DAG necessary to analyze a given amount
of LIGO and GEO data.  It takes in the start and end times, generates the
segment lists, runs the zero-lag, time shift and injection analyses, generates
summary information and plots and follows up the loudest events.
"""
__author__ = 'Stephen Fairhurst <sfairhur@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import os, sys, copy
import ConfigParser
import optparse
import tempfile
sys.path.append('@PYTHONLIBDIR@')

##############################################################################
# import the modules we need to build the pipeline
from glue import segments
from glue import segmentsUtils
from glue import pipeline

##############################################################################
# Functions used in setting up the dag:

def make_external_call(command, show_stdout=False, show_command=False):
  """
  Run a program on the shell and print informative messages on failure.
  """
  if show_command: print command
    
  stdin, out, err = os.popen3(command)
  pid, status = os.wait()

  if status != 0:
      print >>sys.stderr, "External call failed."
      print >>sys.stderr, "  status: %d" % status
      print >>sys.stderr, "  stdout: %s" % out.read()
      print >>sys.stderr, "  stderr: %s" % err.read()
      print >>sys.stderr, "  command: %s" % command
      sys.exit(status)
  if show_stdout:
      print out.read()
  stdin.close()
  out.close()
  err.close()

def mkdir( newdir ):
  """
  Create a directory, don't complain if it exists
  
  newdir = name of directory to be created
  """
  if os.path.isdir(newdir): pass
  elif os.path.isfile(newdir):
    raise OSError("a file with the same name as the desired " \
                  "dir, '%s', already exists." % newdir)
  else: os.mkdir(newdir)
 
def link_executables(directory, config):
  """
  link executables to given directory
  """
  for (job, executable) in config.items("condor"):
    if job != "universe":
      if executable[0] != "/": 
        executable = "../../" + executable
      config.set("condor", job, executable)

##############################################################################
# Function to set up the segments for the analysis
def generate_segments(ifo):
  """
  generate the segments for the specified ifo
  """
  ligoIfos = ["H1","H2","L1"]

  if ifo in ligoIfos: type = cp.get("input","ligo-type")
  elif ifo == "G1": type =   cp.get("input","geo-type")
 
  ifo_type = ifo + "_" + type
  dataFindFile = ifo_type + "-" + str(opts.gps_start_time) + "-" + \
      str(opts.gps_end_time - opts.gps_start_time) + ".txt"
  segFindFile = ifo + "_SCI_SEGS-" + str(opts.gps_start_time) + "-" + \
      str(opts.gps_end_time - opts.gps_start_time) + ".txt"
  segFile = ifo + "_selectedsegs.txt"
  missedFile = ifo + "_missedsegs.txt"

  # if not generating segments, all we need is the name of the segment file
  if not opts.generate_segments: return segFile

  # run segFind to determine science segments
  print "Running LSCsegFind to determine science segments for " + ifo
  segFindCall = "LSCsegFind --interferometer=" + ifo + \
      " --type=" + cp.get("segments","analyze") + \
      " --gps-start-time=" + str(opts.gps_start_time) + \
      " --gps-end-time=" + str(opts.gps_end_time) + " > " + \
      segFindFile
  make_external_call(segFindCall)
  sfSegs = segmentsUtils.fromsegwizard(file(segFindFile)).coalesce()

  # Determine the available data (if requested)
  if opts.use_available_data:
    # run dataFind
    print "Running LSCdataFind to determine available data from " + type + \
        " frames for " + ifo
    dataFindCall = "LSCdataFind --observatory=" + ifo[0] + \
        " --type=" + ifo_type + \
        " --gps-start-time=" + str(opts.gps_start_time) + \
        " --gps-end-time=" + str(opts.gps_end_time) + " --show-times > " + \
        dataFindFile
    make_external_call(dataFindCall)
    dfSegs = segmentsUtils.fromsegwizard(file(dataFindFile)).coalesce()

  # generate the analyzed segments
  if opts.use_available_data: analyzedSegs = sfSegs.__and__(dfSegs)
  else: analyzedSegs = sfSegs

  segmentsUtils.tosegwizard(file(segFile,"w"), analyzedSegs)
  print "Writing " + ifo + " segments of total time " + \
      str(analyzedSegs.__abs__()) + "s to file: " + \
      segFile

  if opts.use_available_data:
    missedSegs = sfSegs.__and__(dfSegs.__invert__()) 
    segmentsUtils.tosegwizard(file(missedFile,"w"), missedSegs)
    print "Writing " + ifo + " segments which cannot be analyzed to file " + \
        missedFile
    print "Not analyzing %d s, representing %.2f percent of time" %  \
       (missedSegs.__abs__(), 
       100. * missedSegs.__abs__() / analyzedSegs.__abs__() )
    print

  return segFile


##############################################################################
# Function to set up lalapps_inspiral_hipe
def hipe_setup(hipeDir, config, injFile=None, dfOnly = False):
  """
  run lalapps_inspiral_hipe and add job to dag
  hipeDir = directory in which to run inspiral hipe
  config  = config file 
  injFile = injection file to use when running
  dfOnly  = only run the datafind step of the pipeline
  """

  # make the directory for running hipe
  mkdir(hipeDir)

  # create the hipe config parser, keep only relevant info
  hipecp = copy.deepcopy(config)
  hipeSections = ['condor', 'pipeline', 'input', 'calibration', 'datafind',\
      'ligo-data', 'geo-data', 'data', 'tmpltbank', 'tmpltbank-1', \
      'tmpltbank-2', 'no-veto-inspiral', 'veto-inspiral', 'inspiral', \
      'h1-inspiral', 'h2-inspiral', 'l1-inspiral', 'g1-inspiral', 'inspinj', \
      'thinca', 'thinca-1', 'thinca-2', 'thinca-slide', 'trigtotmplt', 'sire', \
      'sire-inj', 'coire', 'coire-inj', 'cohbank', 'coh-trig', 'chia']

  for seg in hipecp.sections():
    if not seg in hipeSections: hipecp.remove_section(seg)

  hipecp.remove_option("condor","hipe")
  hipecp.remove_option("condor","follow")

  if injFile:
    # add the options to the ini file
    if injFile[0] != "/": injFile = "../../" + injFile
    hipecp.set("input", "injection-file", injFile )
    hipecp.set("input", "num-slides", "")

  else: 
    # add the options to the ini file
    hipecp.set("input","num-slides", config.get("input","num-slides") )
    hipecp.set("input", "injection-file", "" )

  # link the executables 
  link_executables(hipeDir, hipecp)

  # return to the directory, write ini file and run hipe
  os.chdir(hipeDir)
  iniFile = "inspiral_hipe_" + hipeDir + ".ini"
  hipecp.write(file(iniFile,"w"))
  
  print "Running hipe in directory " + hipeDir 
  if injFile: 
    print "Injection file: " + hipecp.get("input", "injection-file") 
  else: print "No injections, " + str(hipecp.get("input","num-slides")) + \
      " time slides"
  print

  # work out the hipe call:
  hipeCommand = config.get("condor","hipe")
  hipeCommand += " --log-path " + opts.log_path
  hipeCommand += " --config-file " + iniFile
  for item in config.items("ifo-details"):
      hipeCommand += " --" + item[0] + " " + item[1]

  if dfOnly: hipeCommand += " --datafind"
  else:
    for item in config.items("hipe-arguments"):
      if item[0] != "datafind": hipeCommand += " --" + item[0] + " " + item[1]

  # run lalapps_inspiral_hipe
  make_external_call(hipeCommand) 

  # link datafind
  if not dfOnly:
    os.rmdir("cache")
    os.symlink("../datafind/cache", "cache")

  # add job to dag
  hipeJob = pipeline.CondorDAGManJob(iniFile.rstrip("ini") + "dag")
  hipeNode = pipeline.CondorDAGNode(hipeJob)

  # add postscript to deal with rescue dag
  fix_rescue(hipeNode)

  # return to the original directory
  os.chdir("..")

  return hipeNode

##############################################################################
# Function to set up lalapps_followup_pipe
def followup_setup(followDir, config, hipeDir):
  """
  run lalapps_followup_pipe and add job to dag
  followDir = directory to output the followup
  config    = config file 
  """

  # make the directory for followup pipe
  mkdir(followDir)

  # create the followup config parser, keep only relevant info
  followcp = copy.deepcopy(config)
  followSections = ['condor', 'hipe-cache', 'triggers', 'q-datafind', 'qscan', \
      'q-hoft-datafind', 'qscan-hoft', 'plots', 'output', 'seg']

  for seg in followcp.sections():
    if not seg in followSections: followcp.remove_section(seg)

  followcp.remove_option("condor","hipe")
  followcp.remove_option("condor","follow")


  # XXX this should be replaced by getting the information from the hipe cache
  # set the cache paths
  followcp.add_section("hipe-cache")
  followcp.set("hipe-cache", "hipe-cache-path", "hipe_cache")

  for path in ["tmpltbank-path", "trigbank-path", "first-inspiral-path", \
      "second-inspiral-path", "first-coinc-path", "second-coinc-path"]: 
    followcp.set("hipe-cache", path, "../" + hipeDir)

  # set the xml-glob
  followcp.set("triggers", "xml-glob", "../" + hipeDir + "/*COIRE_*")
  # to here XXX

  # correct paths to qscan config files
  for (opt, arg) in followcp.items("qscan") + followcp.items("qscan-hoft"):
    if "config-file" in opt:
      if arg[0] != "/": 
        arg = "../../" + arg
      followcp.set("condor", opt, arg)

  # link the executables 
  link_executables(followDir, config)

  # return to the directory, write ini file and run hipe
  os.chdir(followDir)
  iniFile = "followup_pipe_" + followDir + ".ini"
  followcp.write(file(iniFile,"w"))
  
  # link datafind output from original hipe
  os.symlink("../datafind/cache", "hipe_cache")

  print "Running followup pipe in directory " + followDir 

  # work out the followup_pipe call:
  followupCommand = config.get("condor","follow")
  followupCommand += " --log-path " + opts.log_path
  followupCommand += " --config-file " + iniFile

  for item in config.items("followup-arguments"):
    followupCommand += " --" + item[0] + " " + item[1]

  # run lalapps_followup_pipe
  make_external_call(followupCommand) 

  # add job to dag
  followupJob = pipeline.CondorDAGManJob(iniFile.rstrip("ini") + "dag")
  followupNode = pipeline.CondorDAGNode(followupJob)

  # add postscript to deal with rescue dag
  fix_rescue(followupNode)

  # return to the original directory
  os.chdir("..")

  return followupNode

##############################################################################
# Function to fix the rescue of inner dags
def fix_rescue(dagNode):
  """
  add a postscript to deal with the rescue dag correctly
 
  dagNode = the node for the subdag
  """
  dagNode.set_post_script( "rescue.sh")
  dagNode.add_post_script_arg( "$RETURN" ) 
  dagNode.add_post_script_arg( 
      dagNode.job().get_sub_file().rstrip(".condor.sub") )

def write_rescue():
  # Write the rescue post-script
  # XXX FIXME: This is a hack, required until condor is fixed XXX
  f = open("rescue.sh", "w")
  f.write("""#! /bin/bash
  if [ ! -n "${2}" ]
  then
    echo "Usage: `basename $0` DAGreturn DAGfile"
    exit
  fi
  
  if (( ${1}>0 ))
  then
    DIR=${2%/*}
    DAG=${2#*/}
    mv ${2} ${2}.orig
    `sed "s/DIR ${DIR}//g" ${DAG}.rescue > ${2}`
    exit ${1}
  fi""")
  f.close()
  os.chmod("rescue.sh", 0744)


##############################################################################
#
#  MAIN PROGRAM
#
##############################################################################
usage = """usage: %prog [options] 
"""

parser = optparse.OptionParser( usage )

# arguments
parser.add_option("-f", "--config-file",action="store",type="string",\
    metavar=" FILE", help="use configuration file FILE")

parser.add_option("-p", "--log-path",action="store",type="string",\
    metavar=" PATH", help="directory to write condor log file")

parser.add_option("-s", "--gps-start-time",action="store",type="int",\
    metavar=" GPS_START", help="begin analysis at GPS_START")

parser.add_option("-e", "--gps-end-time",action="store",type="int",\
    metavar=" GPS_END", help="end analysis at GPS_END")

parser.add_option("-S", "--generate-segments",action="store_true",\
    default=False, help="generate segments for analysis")

parser.add_option("-D", "--use-available-data",action="store_true",\
    default=False, help="analyze only the data available on the cluster")

parser.add_option("-B", "--run-datafind",action="store_true",\
    default=False, help="run the inspiral analysis")

parser.add_option("-A", "--run-analysis",action="store_true",\
    default=False, help="run the inspiral analysis")

parser.add_option("-I", "--run-injections",action="store_true",\
    default=False, help="run the inspiral analysis with injections")

parser.add_option("-F", "--run-followup",action="store_true",\
    default=False, help="run the inspiral followup")

(opts,args) = parser.parse_args()

##############################################################################
# Sanity check of input arguments
if not opts.config_file:
  print >> sys.stderr, "No configuration file specified."
  print >> sys.stderr, "Use --config-file FILE to specify location."
  sys.exit(1)

if not opts.log_path:
  print >> sys.stderr, "No log file path specified."
  print >> sys.stderr, "Use --log-path PATH to specify a location."
  sys.exit(1)

if not opts.gps_start_time:
  print >> sys.stderr, "No GPS start time specified for the analysis"
  print >> sys.stderr, "Use --gps-start-time GPS_START to specify a location."
  sys.exit(1)

if not opts.gps_end_time:
  print >> sys.stderr, "No GPS end time specified for the analysis"
  print >> sys.stderr, "Use --gps-end-time GPS_END to specify a location."
  sys.exit(1)

if opts.gps_end_time < opts.gps_start_time:
  print >> sys.stderr, "The GPS end time must be after the GPS start time"
  sys.exit(1)

##############################################################################
# parse the ini file:
cp = ConfigParser.ConfigParser()
cp.read(opts.config_file)

# set up the analysis directory
analysisDirectory = str(opts.gps_start_time) + "-" + str(opts.gps_end_time)

mkdir(analysisDirectory)
os.chdir(analysisDirectory)


##############################################################################
# Set up the IFOs and get the appropriate segments

ifos = []
for option in ["g1-data","h1-data","h2-data","l1-data"]:
  if cp.has_option("ifo-details",option): ifos.append(option[0:2].upper() )

if cp.has_option("ifo-details","analyze-all"): 
  ifos = ["G1", "H1", "H2", "L1"]

print "Setting up an analysis for " + str(ifos) + " from " + \
    str(opts.gps_start_time) + " to "  + str(opts.gps_end_time)
print

# Run LSCsegFind and LSCdataFind to determine the segments to analyze
for ifo in ifos:
  segFile = generate_segments(ifo)
  cp.set("input",ifo.lower() + "-segments","../" + segFile)

##############################################################################
# create a log file that the Condor jobs will write to
basename = opts.config_file.rstrip(".ini")
logname = basename + '.dag.log.'
tempfile.tempdir = opts.log_path
logfile = tempfile.mktemp(prefix=logname)
fh = open( logfile, "w" )
fh.close()

##############################################################################
# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
dag.set_dag_file(basename)

# write the file to handle the rescue dags
write_rescue()

##############################################################################
# Run lalapps_inspiral_hipe for datafind
print "Running inspiral hipe for datafind run"
hipeDfNode = hipe_setup("datafind", cp, dfOnly = True)
if opts.run_datafind: dag.add_node(hipeDfNode)

##############################################################################
# Set up the directories for each run and run lalapps_inspiral_hipe
print "Running inspiral hipe for injection runs"

hipeInjNodes = []
for (injDir, injFile) in cp.items("injections"):
  hipeInjNode = hipe_setup(injDir, cp, injFile)
  if opts.run_injections: 
    dag.add_node(hipeInjNode)
    if opts.run_datafind:
      hipeInjNode.add_parent(hipeDfNode)

  hipeInjNodes.append( hipeInjNode )

print "Running inspiral hipe for analysis run"

hipeAnalysisNode = hipe_setup("analysis", cp)
if opts.run_analysis: 
  dag.add_node(hipeAnalysisNode)
  if opts.run_datafind:
    hipeAnalysisNode.add_parent(hipeDfNode)

##############################################################################
# Set up the directories for followup and run lalapps_followup_pipe
followupNode = followup_setup("followup", cp, "analysis" )
if opts.run_followup:
  dag.add_node(followupNode)
  if opts.run_analysis:
    followupNode.add_parent(hipeAnalysisNode)


##############################################################################
# Write the dag and sub files
dag.write_sub_files()
dag.write_dag()

print 
print "  Created a DAG file which can be submitted by executing"
print "\n    cd " + analysisDirectory
print "    condor_submit_dag " + dag.get_dag_file()
print """\n  from a condor submit machine
  Before submitting the dag, you must execute

    export _CONDOR_DAGMAN_LOG_ON_NFS_IS_ERROR=FALSE

  If you are running LSCdataFind jobs, do not forget to initialize your grid
  proxy certificate on the condor submit machine by running the commands
  
    unset X509_USER_PROXY
    grid-proxy-init -hours 72
  
  Enter your pass phrase when promted. The proxy will be valid for 72 hours.
  If you expect the LSCdataFind jobs to take longer to complete, increase the
  time specified in the -hours option to grid-proxy-init. You can check that
  the grid proxy has been sucessfully created by executing the command:
  
    grid-cert-info -all -file /tmp/x509up_u`id -u`
  
  This will also give the expiry time of the proxy."""
