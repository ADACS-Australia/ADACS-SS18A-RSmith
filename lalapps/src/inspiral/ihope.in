#!/usr/bin/env @PYTHONPROG@
"""
ihope.in - weekly automate pipeline driver script

$Id$

This script generates the condor DAG necessary to analyze a given amount
of LIGO and GEO data.  It takes in the start and end times, generates the
segment lists, runs the zero-lag, time shift and injection analyses, generates
summary information and plots and follows up the loudest events.
"""
__author__ = 'Stephen Fairhurst <sfairhur@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import os, sys, copy, shutil
import ConfigParser
import optparse
import tempfile
import urllib
sys.path.append('@PYTHONLIBDIR@')

##############################################################################
# import the modules we need to build the pipeline
from glue import segments
from glue import segmentsUtils
from glue import pipeline
import inspiralutils 


##############################################################################
#
#  MAIN PROGRAM
#
##############################################################################
usage = """usage: %prog [options] 

lalapps_ihope is designed to run the inspiral analysis end to end.  It
performs several distinct steps.  The default is for the full search to
be run, various steps can be skipped by specifying --skip options.  The
required arguments are:

--config-file 
--log path
--gps-start-time
--gps-end-time

The options to run the analysis are specified in the config-file
(normally ihope.ini) file.

1) Generate the segments for which the analysis should be done.  This
can be skipped using --skip-generate-segments.  The option
--use-available-data will restrict the search to those times for which
there is data on the cluster the analysis is being run.

2) Generate the veto segments.  Can be skipped with
--skip-generate-veto-segments.

3) Find the data on the cluster.  Can be skipped with --skip-datafind.

4) Generate the template banks.  Can be skipped with --skip-tmpltbank.

5) The search itself.  Can be skipped with --skip-search.

6) The application of the data quality vetoes.  Can be skipped with
--skip-data-quality.

7) Make plots of the output.  Can be skipped with --skip-plots.

8) Followup the loudest events (currently not working).  Can be skipped
with --skip-followup.

For options 5-8, you can run 3 types of analysis:
a) playground and playground time slides. Skipped with --skip-playground
b) full data and full data time slides.  Skipped with --skip-full-data
c) software injections. Skipped with --skip-injections
"""

parser = optparse.OptionParser( usage=usage, \
      version= "%prog CVS\n" +
      "$Id$\n" +
      "$Name$\n")

# arguments
parser.add_option("-f", "--config-file",action="store",type="string",\
    metavar=" FILE", help="use configuration file FILE")

parser.add_option("-p", "--log-path",action="store",type="string",\
    metavar=" PATH", \
    help="directory to write condor log file, should be a local directory")

parser.add_option("-l", "--node-local-dir",action="store",type="string",\
    metavar=" NODEPATH", \
    help='''directory to write the temporary sql databases. This must be a
directory that is local to all nodes.''')

parser.add_option("-s", "--gps-start-time",action="store",type="int",\
    metavar=" GPS_START", help="begin analysis at GPS_START")

parser.add_option("-e", "--gps-end-time",action="store",type="int",\
    metavar=" GPS_END", help="end analysis at GPS_END")

parser.add_option("-D", "--use-available-data",action="store_true",\
    default=False, help="analyze only the data available on the cluster")

parser.add_option("-R", "--reverse-analysis",action="store_true",\
    default=False, help="do the entrire analysis using reverse chirp method")

parser.add_option("-S", "--skip-generate-segments",action="store_false",\
    default=True, dest="generate_segments", \
    help="skip generation segments for analysis")

parser.add_option("-V", "--skip-generate-veto-segments",action="store_false",\
    default=True, dest="generate_veto_segments", \
    help="skip generation segments for analysis")

parser.add_option("-B", "--skip-datafind",action="store_false",\
    default=True, dest="run_datafind", help="skip the datafind step")

parser.add_option("-T", "--skip-tmpltbank",action="store_false",\
    default=True, dest="run_tmpltbank", help="skip the template bank generation")

parser.add_option("-F", "--skip-full-data",action="store_false",\
    default=True, dest="run_full_data", help="skip the full data + slides")

parser.add_option("-P", "--skip-playground",action="store_false",\
    default=True, dest="run_playground", help="skip the playground analysis")

parser.add_option("-I", "--skip-injections",action="store_false",\
    default=True, dest="run_injections", \
    help="skip the inspiral analysis with software injections")

parser.add_option("-A", "--skip-search",action="store_false",\
    default=True, dest="run_search",
    help="skip the search of the data (i.e. don't run inspiral hipe)")

parser.add_option("-Q", "--skip-data-quality",action="store_false",\
    default=True, dest="run_data_quality", \
    help="skip generation dq veto segments and use in analysis ")

parser.add_option("-Z", "--skip-plots",action="store_false",\
    default=True, dest="run_plots",  help="skip the plotting step")

parser.add_option("-U", "--skip-followup",action="store_false",\
    default=True, dest="run_followup",  help="skip the inspiral followup")

parser.add_option("-H", "--skip-pipedown",action="store_false",\
    default=True, dest="run_pipedown",  help="skip running pipedown")

parser.add_option("-x", "--dax",action="store_true",\
    default=False, help="Make a dax workflow")

command_line = sys.argv[1:]
(opts,args) = parser.parse_args()

##############################################################################
# Sanity check of input arguments
if not opts.config_file:
  print >> sys.stderr, "No configuration file specified."
  print >> sys.stderr, "Use --config-file FILE to specify location."
  sys.exit(1)

if not opts.log_path:
  print >> sys.stderr, "No log file path specified."
  print >> sys.stderr, "Use --log-path PATH to specify a location."
  sys.exit(1)

if opts.run_pipedown:
  if not opts.node_local_dir:
    print >> sys.stderr, "No local dir specified. If running with pipedown"
    print >> sys.stderr, "use --node-local-dir to specify a local directory"
    print >> sys.stderr, "Use --help for more information"
    sys.exit(1)

if not opts.gps_start_time:
  print >> sys.stderr, "No GPS start time specified for the analysis"
  print >> sys.stderr, "Use --gps-start-time GPS_START to specify a location."
  sys.exit(1)

if not opts.gps_end_time:
  print >> sys.stderr, "No GPS end time specified for the analysis"
  print >> sys.stderr, "Use --gps-end-time GPS_END to specify a location."
  sys.exit(1)

if opts.gps_end_time < opts.gps_start_time:
  print >> sys.stderr, "The GPS end time must be after the GPS start time"
  sys.exit(1)

opts.complete_cache = (opts.run_data_quality or opts.run_plots or opts.run_pipedown or opts.run_search)

##############################################################################
# set up the analysis directory
analysisDirectory = str(opts.gps_start_time) + "-" + str(opts.gps_end_time)
inspiralutils.mkdir(analysisDirectory)

# copy the ini file into the directory
shutil.copy( opts.config_file, analysisDirectory )
opts.config_file = opts.config_file.split("/")[-1]

os.chdir(analysisDirectory)

# parse the ini file:
cp = ConfigParser.ConfigParser()
cp.read(opts.config_file)

# set gps start and end times in the cp object.
cp.set("input", "gps-start-time", str(opts.gps_start_time) )
cp.set("input", "gps-end-time", str(opts.gps_end_time) )

# Check for --disable-dag-categories

opts.do_dag_categories = not cp.has_option(
              "hipe-arguments","disable-dag-categories")

# Add local dir to pipeline

if opts.run_pipedown:
  cp.set("pipeline","node-tmp-dir",opts.node_local_dir)

##############################################################################
# create a directory called executables and copy them over
inspiralutils.mkdir("executables")
inspiralutils.mkdir("executables32")
os.chdir("../")
for (job, executable) in cp.items("condor"):
  if job != "universe":
    if job[-2:] != "32":
      shutil.copy( executable, analysisDirectory + "/executables" )
      executable = "../executables/" + executable.split("/")[-1]
      cp.set("condor", job, executable)
    else:
      shutil.copy( executable, analysisDirectory + "/executables32" )
      executable = "../executables32/" + executable.split("/")[-1]
      cp.set("condor", job, executable)


os.chdir(analysisDirectory)

##############################################################################
# create a log file that the Condor jobs will write to
basename = opts.config_file.rstrip(".ini")
logname = basename + '.dag.log.'
tempfile.tempdir = opts.log_path
logfile = tempfile.mktemp(prefix=logname)
fh = open( logfile, "w" )
fh.close()

##############################################################################
# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
dag.set_dag_file(basename)

##############################################################################
# Set up the IFOs and get the appropriate segments
ifos = [] 

for option in ["g1-data","h1-data","h2-data","l1-data","v1-data"]:
  if cp.has_option("ifo-details",option): ifos.append(option[0:2].upper() )

if cp.has_option("ifo-details","analyze-all"): 
  print >> sys.stderr, \
      "The inspiral pipeline does not yet support coincidence between"
  print >> sys.stderr, "all five IFOs. Do not use the analyze-all option."
  sys.exit(1)


print "Setting up an analysis for " + str(ifos) + " from " + \
    str(opts.gps_start_time) + " to "  + str(opts.gps_end_time)
print
sys.stdout.flush()

##############################################################################
# Determine the segments to analyze

# first, copy the segment files and make sure that the ini file reflects the
# new path, relative to the DAG directories.

inspiralutils.mkdir("segments")
os.chdir("..")
for vetoes, infile in cp.items("segments"):
  inspiralutils.copyCategoryFiles(cp,vetoes,"segments",infile,\
                analysisDirectory)

os.chdir(analysisDirectory)
os.chdir("segments")

# Determine which veto categories to filter
veto_categories = [int(cat) for cat in cp.get('segments','veto-categories').split(',')] 
veto_categories.sort()

# Download the veto definer xml file
vetoDefFile = inspiralutils.downloadVetoDefFile(cp, opts.generate_segments)

# Generate veto xml files for each ifo and for each category
inspiralutils.generate_veto_cat_files(cp, vetoDefFile, \
    opts.generate_veto_segments)

# Generate a combined veto file for each ifo for use by pipedown
gps_start = int(opts.gps_start_time)
gps_end = int(opts.gps_end_time)
duration = gps_end - gps_start

for category in [1] + veto_categories:
  ifos_string = ''
  veto_cat_files = []
  veto_cat_filename = str(gps_start) + '-' + str(duration) + '.xml'
  for ifo in ifos:
    veto_cat_files.append('-'.join([ifo, 'VETOTIME_CAT' + str(category), veto_cat_filename]))
    ifos_string += ifo
  concat_veto_call = ' '.join([ cp.get("condor", "ligolw_add"),
    "--output", '-'.join([ifos_string, 'VETOTIME_CAT_' + str(category), veto_cat_filename])] + veto_cat_files )
  inspiralutils.make_external_call(concat_veto_call)

segFile = {}
dqVetoes = {}

for ifo in ifos:
  segFile[ifo], dqVetoes[ifo] = inspiralutils.findSegmentsToAnalyze(
      cp, ifo, veto_categories,
      opts.generate_segments, opts.use_available_data,
      opts.generate_veto_segments)
  # set correct name as we're in the segments directory:
  segFile[ifo] = "../segments/" + segFile[ifo]
  cp.set("input", ifo.lower() + "-segments", segFile[ifo])
  for key in dqVetoes[ifo].keys():
    dqVetoes[ifo][key] = "../segments/" + dqVetoes[ifo][key]

os.chdir("..")

##############################################################################
# Run lalapps_inspiral_hipe for datafind and template bank generation
if opts.run_datafind or opts.run_tmpltbank:
  print "Running inspiral hipe for datafind/template bank run"
  hipeDfNode = inspiralutils.hipe_setup("datafind", cp, ifos, \
      opts.log_path, dataFind = opts.run_datafind, tmpltBank = opts.run_tmpltbank)
  dag.add_node(hipeDfNode)

##############################################################################
# Set up the directories for each run and run lalapps_inspiral_hipe

cachelist = []
cachefilename = basename + '.cache'

if opts.complete_cache:
  # We need to get the names of the template bank files that are generated in
  # the datafind step, so we can link them into all the search directories
  datafind_cache_filename = "datafind/" + inspiralutils.hipe_cache(ifos, \
      "DATAFIND", opts.gps_start_time, opts.gps_end_time)
  tmpltbank_cache = inspiralutils.tmpltbank_cache(datafind_cache_filename)

  if opts.run_playground:
    playDir = "playground"
    if opts.run_search:
      print "Running inspiral hipe for playground run"
      hipePlayNode = inspiralutils.hipe_setup(playDir, cp, ifos, \
           opts.log_path, playOnly = True, dax=opts.dax, \
           tmpltbankCache = tmpltbank_cache)
      for cacheFile in hipePlayNode.get_output_files():
        cachelist.append(playDir + "/" + cacheFile)
      dag.add_node(hipePlayNode)
      if opts.run_datafind or opts.run_tmpltbank:
        hipePlayNode.add_parent(hipeDfNode)
    else:
      if cp.get("pipeline","user-tag"):
        usertag = ecp.get("pipeline", "user-tag") + "_" + playDir.upper()
      else:
        usertag = playDir.upper()
      cacheFile = inspiralutils.hipe_cache( ifos,usertag, \
          cp.getint("input", "gps-start-time"), \
          cp.getint("input", "gps-end-time")) 
      if os.path.isfile(playDir + "/" + cacheFile):
        cachelist.append(playDir + "/" + cacheFile)
      else:
        print>>sys.stderr, "WARNING: Cache file " + playDir + "/" + cacheFile
        print>>sys.stderr, "does not exist! This might cause later failures."


  if opts.run_full_data:
    fullDir = "full_data"
    if opts.run_search:
      print "Running inspiral hipe for analysis run"
      hipeAnalysisNode = inspiralutils.hipe_setup(fullDir, cp, ifos, \
          opts.log_path,dax=opts.dax, \
          tmpltbankCache = tmpltbank_cache)
      for cacheFile in hipeAnalysisNode.get_output_files():
        cachelist.append(fullDir + "/" + cacheFile)
      dag.add_node(hipeAnalysisNode)
      if opts.run_datafind or opts.run_tmpltbank:
        hipeAnalysisNode.add_parent(hipeDfNode)
    else:
      if cp.get("pipeline","user-tag"):
        usertag = cp.get("pipeline", "user-tag") + "_" + fullDir.upper()
      else:
        usertag = fullDir.upper()
      cacheFile = inspiralutils.hipe_cache( ifos,usertag, \
          cp.getint("input", "gps-start-time"), \
          cp.getint("input", "gps-end-time")) 
      if os.path.isfile(fullDir + "/" + cacheFile):
        cachelist.append(fullDir + "/" + cacheFile)
      else:
        print>>sys.stderr, "WARNING: Cache file " + fullDir + "/" + cacheFile
        print>>sys.stderr, "does not exist! This might cause later failures."

  
  if opts.run_injections:
    hipeInjNode = {} 
    print "Running inspiral hipe for injection runs"
    for (injDir, injSeed) in cp.items("injections"):
      if opts.run_search:
        hipeInjNode[injDir] = inspiralutils.hipe_setup(injDir, cp, ifos, \
             opts.log_path, injSeed, dax=opts.dax, \
             tmpltbankCache = tmpltbank_cache)
        for cacheFile in hipeInjNode[injDir].get_output_files():
          cachelist.append(injDir + "/" + cacheFile)
        dag.add_node(hipeInjNode[injDir])
        if opts.run_datafind or opts.run_tmpltbank:
          hipeInjNode[injDir].add_parent(hipeDfNode)
      else:
        if cp.get("pipeline","user-tag"):
          usertag = cp.get("pipeline", "user-tag") + "_" + injDir.upper()
        else:
          usertag = injDir.upper()
        cacheFile = inspiralutils.hipe_cache( ifos,usertag, \
            cp.getint("input", "gps-start-time"), \
            cp.getint("input", "gps-end-time")) 
        if os.path.isfile(injDir + "/" + cacheFile):
          cachelist.append(injDir + "/" + cacheFile)
        else:
          print>>sys.stderr, "WARNING: Cache file " + injDir + "/" + cacheFile
          print>>sys.stderr, "does not exist! This might cause later failures."


################################################################################
# Setting up directories for reverse analysis

if opts.reverse_analysis:

  if opts.run_play_analysis:
    cp.set ("inspiral","reverse-chirp-bank","")
    print "Running inspiral hipe for reverse playground run"
    hipePlayNode = inspiralutils.hipe_setup("playground_reverse", cp, ifos, \
         opts.log_path, playOnly = True, dax = opts.dax, \
         tmpltbankCache = tmpltbank_cache)
    for cacheFile in hipePlayNode.get_output_files():
      cachelist.append("playground_reverse/" + cacheFile)
    dag.add_node(hipePlayNode)
    if opts.run_datafind or opts.run_tmpltbank:
      hipePlayNode.add_parent(hipeDfNode)

  if opts.run_full_data:
    cp.set ("inspiral","reverse-chirp-bank","")
    print "Running inspiral hipe for reverse analysis run"
    hipeAnalysisNode = inspiralutils.hipe_setup("full_data_reverse", cp, ifos, \
        opts.log_path, dax=opts.dax, \
        tmpltbankCache = tmpltbank_cache)
    for cacheFile in hipeAnalysisNode.get_output_files():
      cachelist.append("full_data_reverse/" + cacheFile)
    dag.add_node(hipeAnalysisNode)
    if opts.run_datafind or opts.run_tmpltbank:
      hipeAnalysisNode.add_parent(hipeDfNode)

  if opts.run_injections:
    hipeInjNode = {}
    cp.set ("inspiral","reverse-chirp-bank","")
    print "Running inspiral hipe for reverse injection runs"
    for (injDir, injSeed) in cp.items("injections_reverse"):
      hipeInjNode[injDir] = inspiralutils.hipe_setup(injDir, cp, ifos, \
           opts.log_path, injSeed, dax=opts.dax, \
           tmpltbankCache = tmpltbank_cache)
      for cacheFile in hipeInjNode[injDir].get_output_files():
        cachelist.append(injDir + "/" + cacheFile)
      dag.add_node(hipeInjNode[injDir])
      if opts.run_datafind or opts.run_tmpltbank:
        hipeInjNode[injDir].add_parent(hipeDfNode)

##############################################################################
# Run the data quality vetoes
if opts.complete_cache:
  hipePlayVetoNode = {}
  hipeAnalysisVetoNode = {}
  hipeInjVetoNode = {}

  for category in veto_categories:
    hipeInjVetoNode[category] = {}
    if opts.run_data_quality:
      print "Setting up the category " + str(category) + " veto dags"

    if opts.run_playground:
      playDir = "playground"
      if opts.run_data_quality:
        print "Running inspiral hipe for play with vetoes"
        hipePlayVetoNode[category] = inspiralutils.hipe_setup(playDir, cp, \
            ifos, opts.log_path, playOnly = True, vetoCat = category, \
            vetoFiles = dqVetoes, dax=opts.dax )
        for cacheFile in hipePlayVetoNode[category].get_output_files():
          cachelist.append(playDir + "/" + cacheFile)
        dag.add_node(hipePlayVetoNode[category])
        if category==2 and opts.run_search: 
          hipePlayVetoNode[category].add_parent(hipePlayNode)
        elif category>2: 
          hipePlayVetoNode[category].add_parent(hipePlayVetoNode[category-1])
      elif not opts.run_search:
        if cp.get("pipeline","user-tag"):
          usertag = cp.get("pipeline", "user-tag") + "_" + playDir.upper()
        else:
          usertag = playDir.upper() 
        usertag += "_CAT_" + str(category) + "_VETO"
        cacheFile = inspiralutils.hipe_cache( ifos,usertag, \
            cp.getint("input", "gps-start-time"), \
            cp.getint("input", "gps-end-time"))  
        if os.path.isfile(playDir + "/" + cacheFile):
          cachelist.append(playDir + "/" + cacheFile)
        else:
          print>>sys.stderr, "WARNING: Cache file " + playDir + "/" + cacheFile
          print>>sys.stderr, "does not exist! This might cause later failures."



    if opts.run_full_data:
      fullDir = "full_data"
      if opts.run_data_quality:
        print "Running inspiral hipe for full data with vetoes"
        hipeAnalysisVetoNode[category] = inspiralutils.hipe_setup(fullDir, \
            cp, ifos, opts.log_path, vetoCat = category, vetoFiles = dqVetoes, dax = opts.dax)
        for cacheFile in hipeAnalysisVetoNode[category].get_output_files():
          cachelist.append(fullDir + "/" + cacheFile)
        dag.add_node(hipeAnalysisVetoNode[category])
        if category==2 and opts.run_search: 
          hipeAnalysisVetoNode[category].add_parent(hipeAnalysisNode)
        elif category>2: 
          hipeAnalysisVetoNode[category].add_parent( \
              hipeAnalysisVetoNode[category-1])
      elif not opts.run_search:
        if cp.get("pipeline","user-tag"):
          usertag = cp.get("pipeline", "user-tag") + "_" + fullDir.upper()
        else:
          usertag = fullDir.upper()
        usertag += "_CAT_" + str(category) + "_VETO"
        cacheFile = inspiralutils.hipe_cache( ifos,usertag, \
            cp.getint("input", "gps-start-time"), \
            cp.getint("input", "gps-end-time")) 
        if os.path.isfile(fullDir + "/" + cacheFile):
          cachelist.append(fullDir + "/" + cacheFile)
        else:
          print>>sys.stderr, "WARNING: Cache file " + fullDir + "/" + cacheFile
          print>>sys.stderr, "does not exist! This might cause later failures."


			      
    if opts.run_injections:
      if opts.run_data_quality:
        print "Running inspiral hipe for injections with vetoes"
      for (injDir, injSeed) in cp.items("injections"):
        if opts.run_data_quality:
          hipeInjVetoNode[category][injDir] = inspiralutils.hipe_setup(injDir, \
              cp, ifos, opts.log_path, \
              injSeed= injSeed, vetoCat = category, vetoFiles = dqVetoes, \
              dax=opts.dax )
          for cacheFile in hipeInjVetoNode[category][injDir].get_output_files():
            cachelist.append(injDir + "/" + cacheFile)
          dag.add_node(hipeInjVetoNode[category][injDir])
          if category==2 and opts.run_search: 
            hipeInjVetoNode[category][injDir].add_parent(hipeInjNode[injDir])
          elif category>2: 
            hipeInjVetoNode[category][injDir].add_parent(\
                hipeInjVetoNode[category-1][injDir])
        elif not opts.run_search:
          if cp.get("pipeline","user-tag"):
            usertag = cp.get("pipeline", "user-tag") + "_" + injDir.upper()
          else:
            usertag = injDir.upper()
          usertag += "_CAT_" + str(category) + "_VETO"
          cacheFile = inspiralutils.hipe_cache( ifos,usertag, \
              cp.getint("input", "gps-start-time"), \
              cp.getint("input", "gps-end-time")) 
          if os.path.isfile(injDir + "/" + cacheFile):
            cachelist.append(injDir + "/" + cacheFile)
          else:
            print>>sys.stderr, "WARNING: Cache file " + injDir + "/" + cacheFile
            print>>sys.stderr, "does not exist! This might cause later failures."


#############################################################################
# run the data quality vetoes for reverse chirp analysis

if opts.reverse_analysis:
  if opts.run_data_quality:
    hipePlayVetoNode = {}
    hipeAnalysisVetoNode = {}
    hipeInjVetoNode = {}

    for category in veto_categories:
      print "Setting up the category " + str(category) + " veto dags" + " for reverse chirp analysis"
      cp.set ("inspiral","reverse-chirp-bank","")
      if opts.run_play_analysis:
        print "Running inspiral hipe for playground  with vetoes for reverse analysis"

        hipePlayVetoNode[category] = inspiralutils.hipe_setup("playground_reverse", cp, \
            ifos, opts.log_path, playOnly = True, vetoCat = category, \
            vetoFiles = dqVetoes)
        for cacheFile in hipePlayVetoNode[category].get_output_files():
          cachelist.append("playground-reverse/" + cacheFile)
        dag.add_node(hipePlayVetoNode[category])
        if category==2 and opts.run_search:
          hipePlayVetoNode[category].add_parent(hipePlayNode)
        elif category>2:
          hipePlayVetoNode[category].add_parent(hipePlayVetoNode[category-1])


      if opts.run_full_data:
        print "Running inspiral hipe for full data with vetoes for reverse analysis"
        hipeAnalysisVetoNode[category] = inspiralutils.hipe_setup("full_data_reverse", \
            cp, ifos, opts.log_path, vetoCat = category, vetoFiles = dqVetoes)
        for cacheFile in hipeAnalysisVetoNode[category].get_output_files():
          cachelist.append("full_data_reverse/" + cacheFile)
        dag.add_node(hipeAnalysisVetoNode[category])
        if category==2 and opts.run_search:
          hipeAnalysisVetoNode[category].add_parent(hipeAnalysisNode)
        elif category>2:
          hipeAnalysisVetoNode[category].add_parent( \
              hipeAnalysisVetoNode[category-1])

      if opts.run_injections:
        print "Running inspiral hipe for injections with vetoes for reverse analysis"
        for (injDir, injSeed) in cp.items("injections-reverse"):
          if category == 2: hipeInjVetoNode[injDir] = {}
          hipeInjVetoNode[injDir][category] = inspiralutils.hipe_setup(injDir, \
              cp, ifos, opts.log_path, \
              injSeed= injSeed, vetoCat = category, vetoFiles = dqVetoes)
          for cacheFile in hipeInjVetoNode[injDir][category].get_output_files():
            cachelist.append(injDir + "/" + cacheFile)
          dag.add_node(hipeInjVetoNode[injDir][category])
          if category==2 and opts.run_search:
            hipeInjVetoNode[injDir][category].add_parent(hipeInjNode[injDir])
          elif category>2:
            hipeInjVetoNode[injDir][category].add_parent(\
                hipeInjVetoNode[injDir][category-1])

##############################################################################
# cat the cache files together
  

if len(cachelist):
  command = "cat " + " ".join(cachelist) + " > " + cachefilename
  os.popen(command)
  
##############################################################################
# Run lalapps_pipedown

if opts.run_pipedown:
  # Set up parents correctly
  parentNodes = []
  if opts.run_search:
    if opts.run_full_data:
      parentNodes.append(hipeAnalysisNode)
    if opts.run_playground:
      parentNodes.append(hipePlayNode)

  if opts.run_data_quality:
    if opts.run_playground:
      for category in veto_categories:
        parentNodes.append(hipePlayVetoNode[category])
    if opts.run_full_data:
      for category in veto_categories:
        parentNodes.append(hipeAnalysisVetoNode[category])

  if opts.run_injections:
    for (injDir, injSeed) in cp.items("injections"):
      if opts.run_search:
        parentNodes.append(hipeInjNode[injDir])
      if opts.run_data_quality:
        for category in veto_categories:
          parentNodes.append(hipeInjVetoNode[category][injDir])

  if len(parentNodes) == 0:
    parentNodes = None

  playgroundOnly = False

  print "Running lalapps_pipedown"
  dag = inspiralutils.pipedownSetup(dag,cp,opts.log_path,"pipedown",\
                            "../" + cachefilename,parentNodes,playgroundOnly)

##############################################################################
# Set up the directories for plotting and run lalapps_plot
if opts.run_plots:
  if opts.run_playground:

    # set up parents correctly
    if opts.run_search: 
      parentNodes = [hipePlayNode]
    else: parentNodes = None

    if opts.run_data_quality:
      parentVetoNodes = [hipePlayVetoNode]
    else: parentVetoNodes = None
    
    # make the playground slide/zerolag plots
    print "Making plots depending only on the playground"
    dag = inspiralutils.zeroSlidePlots(dag, "playground_summary_plots", cp, \
        opts.log_path, "PLAYGROUND", "PLAYGROUND", "../" + cachefilename, \
        opts.do_dag_categories, parentNodes, parentVetoNodes, \
        veto_categories, ifos)

  if opts.run_playground and opts.run_full_data:

    # set up parents correctly
    if opts.run_search: 
      parentNodes = [hipePlayNode, hipeAnalysisNode]
    else: parentNodes = None

    if opts.run_data_quality and opts.run_playground:
      parentVetoNodes = [hipePlayVetoNode, hipeAnalysisVetoNode]
    else: parentVetoNodes = None

    # make the playground slide/zerolag plots
    print "Making plots with full data slides, playground zero lag"
    dag = inspiralutils.zeroSlidePlots(dag, "full_data_slide_summary_plots", \
        cp, opts.log_path, "PLAYGROUND", "FULL_DATA", "../" + cachefilename, \
        opts.do_dag_categories, parentNodes, parentVetoNodes, \
        veto_categories, ifos)
 
  if opts.run_full_data:

    # set up parents correctly
    parentNodes = None
    parentVetoNodes = None
    if opts.run_full_data:
      if opts.run_search:
        parentNodes = [hipeAnalysisNode]
      if opts.run_data_quality:
        parentVetoNodes = [hipeAnalysisVetoNode]

    # make the full data slide/zerolag plots
    print "Making full data zero lag plots."
    dag = inspiralutils.zeroSlidePlots(dag, "full_data_summary_plots", cp, \
          opts.log_path, "FULL_DATA", "FULL_DATA", "../" + cachefilename, \
          opts.do_dag_categories, parentNodes, parentVetoNodes, veto_categories , ifos)


  # Setting up plots for software injection run.
  if opts.run_injections:
    print "Making plots depending on the injection runs"
    if opts.run_full_data:
      slideSuffix = "FULL_DATA"
    elif opts.run_playground:
      slideSuffix = "PLAYGROUND"
    else:
      slideSuffix = "NONE_AVAILABLE"
    if opts.run_playground:
      zeroLagSuffix = "PLAYGROUND"
    else:
      zeroLagSuffix = "NONE_AVAILABLE"
    for (injDir, injSeed) in cp.items("injections"):
  
      # set up parents correctly
      if opts.run_search: 
        parentNodes=[hipeInjNode[injDir]]
        if opts.run_full_data:
          parentNodes.append(hipeAnalysisNode)
        if opts.run_playground:
          parentNodes.append(hipePlayNode)
      else: parentNodes = None

      if opts.run_data_quality:
        parentVetoNodes = [hipeInjVetoNode[veto_categories[-1]][injDir]]
        if opts.run_full_data:
          parentVetoNodes.append(hipeAnalysisVetoNode[veto_categories[-1]])
        if opts.run_playground:
          parentVetoNodes.append(hipePlayVetoNode[veto_categories[-1]])
      else: parentVetoNodes = None

      dag = inspiralutils.injZeroSlidePlots(dag, injDir + "_summary_plots", \
          cp, opts.log_path, injDir.upper(), zeroLagSuffix, slideSuffix, \
          "../" + cachefilename, opts.do_dag_categories, parentNodes, 
          parentVetoNodes, veto_categories, ifos)

    print "Making plots of all injection runs together"

    # set up parents correctly
    if opts.run_search: 
      parentNodes = hipeInjNode.values()
      if opts.run_full_data:
        parentNodes.append(hipeAnalysisNode)
      if opts.run_playground:
        parentNodes.append(hipePlayNode)
    else: parentNodes = None

    if opts.run_data_quality:
      parentVetoNodes = hipeInjVetoNode[veto_categories[-1]].values()
      if opts.run_playground:
        parentVetoNodes.append(hipePlayVetoNode[veto_categories[-1]])
      if opts.run_full_data:
        parentVetoNodes.append(hipeAnalysisVetoNode[veto_categories[-1]])
    else: parentVetoNodes = None

    dag = inspiralutils.injZeroSlidePlots(dag, "allinj_summary_plots", cp, \
        opts.log_path, "*INJ", zeroLagSuffix, slideSuffix, \
        "../" + cachefilename, opts.do_dag_categories, parentNodes, 
        parentVetoNodes, veto_categories, ifos)

  if opts.do_dag_categories:
    dag.add_maxjobs_category('plotting',2)

##############################################################################
# Set up the directories for followup and run lalapps_followup_pipe
if opts.run_followup:
  followupNode = inspiralutils.followup_setup("followup", cp, opts, "full_data")
  # FIXME: the cache files need to be appended to cachelist
  dag.add_node(followupNode)
  if opts.run_search:
    followupNode.add_parent(hipeAnalysisNode)

##############################################################################
# set the number of retries for each of the sub-dags run by ihope
try:
  num_retries = int(cp.get('pipeline','retry-subdag'))
  for node in dag.get_nodes():
    node.set_retry(num_retries)
except:
  # do not retry nodes
  pass

##############################################################################
# Write the dag and sub files
dag.write_sub_files()
dag.write_dag()

print 
print "  Created a DAG file which can be submitted by executing"
print "\n    cd " + analysisDirectory
print "    condor_submit_dag " + dag.get_dag_file()
print """\n  from a condor submit machine
  Before submitting the dag, you must execute

    export _CONDOR_DAGMAN_LOG_ON_NFS_IS_ERROR=FALSE

  If you are running LSCdataFind jobs, do not forget to initialize your grid
  proxy certificate on the condor submit machine by running the commands
  
    unset X509_USER_PROXY
    grid-proxy-init -hours 72
  
  Enter your pass phrase when prompted. The proxy will be valid for 72 hours.
  If you expect the LSCdataFind jobs to take longer to complete, increase the
  time specified in the -hours option to grid-proxy-init. You can check that
  the grid proxy has been sucessfully created by executing the command:
  
    grid-cert-info -all -file /tmp/x509up_u`id -u`
  
  This will also give the expiry time of the proxy."""

##############################################################################
# write out a log file for this script
log_fh = open(basename + '.pipeline.log', 'w')
  
log_fh.write( "$Id$" \
    + "\n" )
log_fh.write( "$Name$" + "\n\n" )
log_fh.write( "Invoked with arguments:" )
for arg in command_line:
  if arg[0] == '-':
    log_fh.write( "\n" )
  log_fh.write( arg + ' ')

log_fh.write( "\n" )
log_fh.write( "Config file has CVS strings:\n" )
log_fh.write( cp.get('pipeline','version') + "\n" )
log_fh.write( cp.get('pipeline','cvs-tag') + "\n\n" )

##############################################################################
