#!/usr/bin/env @PYTHONPROG@
"""
ihope.in - weekly automate pipeline driver script

$Id$

This script generates the condor DAG necessary to analyze a given amount
of LIGO and GEO data.  It takes in the start and end times, generates the
segment lists, runs the zero-lag, time shift and injection analyses, generates
summary information and plots and follows up the loudest events.
"""
__author__ = 'Stephen Fairhurst <sfairhur@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import os, sys, copy
import ConfigParser
import optparse
import tempfile
import urllib
sys.path.append('@PYTHONLIBDIR@')

##############################################################################
# import the modules we need to build the pipeline
from glue import segments
from glue import segmentsUtils
from glue import pipeline
import inspiralutils 

dq_url_pattern = "http://ldas-cit.ligo.caltech.edu/segments/S5/%s/dq_segments.txt"

##############################################################################
#
#  MAIN PROGRAM
#
##############################################################################
usage = """usage: %prog [options] 
"""

parser = optparse.OptionParser( usage )

# arguments
parser.add_option("-f", "--config-file",action="store",type="string",\
    metavar=" FILE", help="use configuration file FILE")

parser.add_option("-p", "--log-path",action="store",type="string",\
    metavar=" PATH", help="directory to write condor log file")

parser.add_option("-s", "--gps-start-time",action="store",type="int",\
    metavar=" GPS_START", help="begin analysis at GPS_START")

parser.add_option("-e", "--gps-end-time",action="store",type="int",\
    metavar=" GPS_END", help="end analysis at GPS_END")

parser.add_option("-D", "--use-available-data",action="store_true",\
    default=False, help="analyze only the data available on the cluster")

parser.add_option("-S", "--skip-generate-segments",action="store_false",\
    default=True, dest="generate_segments", \
    help="skip generation segments for analysis")

parser.add_option("-B", "--skip-datafind",action="store_false",\
    default=True, dest="run_datafind", help="skip the datafind step")

parser.add_option("-F", "--skip-full-data",action="store_false",\
    default=True, dest="run_full_data", help="skip the full data + slides")

parser.add_option("-P", "--skip-play-analysis",action="store_false",\
    default=True, dest="run_play_analysis", help="skip the playground analysis")

parser.add_option("-I", "--skip-injections",action="store_false",\
    default=True, dest="run_injections", \
    help="skip the inspiral analysis with injections")

parser.add_option("-A", "--skip-analysis",action="store_false",\
    default=True, dest="run_analysis", help="skip the inspiral analysis")

parser.add_option("-Q", "--skip-data-quality",action="store_false",\
    default=True, dest="run_data_quality", \
    help="skip generation dq veto segments and use in analysis ")

parser.add_option("-U", "--skip-followup",action="store_false",\
    default=True, dest="run_followup",  help="skip the inspiral followup")

command_line = sys.argv[1:]
(opts,args) = parser.parse_args()

##############################################################################
# Sanity check of input arguments
if not opts.config_file:
  print >> sys.stderr, "No configuration file specified."
  print >> sys.stderr, "Use --config-file FILE to specify location."
  sys.exit(1)

if not opts.log_path:
  print >> sys.stderr, "No log file path specified."
  print >> sys.stderr, "Use --log-path PATH to specify a location."
  sys.exit(1)

if not opts.gps_start_time:
  print >> sys.stderr, "No GPS start time specified for the analysis"
  print >> sys.stderr, "Use --gps-start-time GPS_START to specify a location."
  sys.exit(1)

if not opts.gps_end_time:
  print >> sys.stderr, "No GPS end time specified for the analysis"
  print >> sys.stderr, "Use --gps-end-time GPS_END to specify a location."
  sys.exit(1)

if opts.gps_end_time < opts.gps_start_time:
  print >> sys.stderr, "The GPS end time must be after the GPS start time"
  sys.exit(1)

##############################################################################
# parse the ini file:
cp = ConfigParser.ConfigParser()
cp.read(opts.config_file)

# set up the analysis directory
analysisDirectory = str(opts.gps_start_time) + "-" + str(opts.gps_end_time)

inspiralutils.mkdir(analysisDirectory)
os.chdir(analysisDirectory)

##############################################################################
# create a log file that the Condor jobs will write to
basename = opts.config_file.rstrip(".ini")
logname = basename + '.dag.log.'
tempfile.tempdir = opts.log_path
logfile = tempfile.mktemp(prefix=logname)
fh = open( logfile, "w" )
fh.close()

##############################################################################
# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
dag.set_dag_file(basename)

# write the file to handle the rescue dags
inspiralutils.write_rescue()


##############################################################################
# Set up the IFOs and get the appropriate segments

ifos = []
for option in ["g1-data","h1-data","h2-data","l1-data","v1-data"]:
  if cp.has_option("ifo-details",option): ifos.append(option[0:2].upper() )

if cp.has_option("ifo-details","analyze-all"): 
  #ifos = ["G1", "H1", "H2", "L1", "V1"]
  print >> sys.stderr, "The inspiral pipeline does not yet support coincidence between"
  print >> sys.stderr, "all five IFOs. Do not use the analyze-all option."
  sys.exit(1)


print "Setting up an analysis for " + str(ifos) + " from " + \
    str(opts.gps_start_time) + " to "  + str(opts.gps_end_time)
print
sys.stdout.flush()

##############################################################################
# Determine the segments to analyze
segFile = {}
dqVetoes = {}

for ifo in ifos:
  inspiralutils.findSegmentsToAnalyze(cp, opts, ifo, dq_url_pattern, segFile, \
      dqVetoes) 

##############################################################################
# Run lalapps_inspiral_hipe for datafind
if opts.run_datafind:
  print "Running inspiral hipe for datafind run"
  hipeDfNode = inspiralutils.hipe_setup("datafind", cp, opts, ifos, \
      dfOnly = True)
  dag.add_node(hipeDfNode)


##############################################################################
# Set up the directories for each run and run lalapps_inspiral_hipe
if opts.run_analysis:
  if opts.run_play_analysis:
    print "Running inspiral hipe for playground run"
    hipePlayNode = inspiralutils.hipe_setup("playground", cp, opts, ifos, \
         playOnly = True)
    dag.add_node(hipePlayNode)
    if opts.run_datafind:
      hipePlayNode.add_parent(hipeDfNode)

  if opts.run_full_data:
    print "Running inspiral hipe for analysis run"
    hipeAnalysisNode = inspiralutils.hipe_setup("analysis", cp, opts, ifos)
    dag.add_node(hipeAnalysisNode)
    if opts.run_datafind:
      hipeAnalysisNode.add_parent(hipeDfNode)

  if opts.run_injections:
    hipeInjNode = {} 
    print "Running inspiral hipe for injection runs"
    for (injDir, injFile) in cp.items("injections"):
      hipeInjNode[injDir] = inspiralutils.hipe_setup(injDir, cp, opts, ifos, \
           injFile)
      dag.add_node(hipeInjNode[injDir])
      if opts.run_datafind:
        hipeInjNode[injDir].add_parent(hipeDfNode)

##############################################################################
# Run the data quality vetoes
if opts.run_data_quality:
  hipePlayVetoNode = {}
  hipeAnalysisVetoNode = {}
  hipeInjVetoNode = {}

  for category in [2,3,4]:
    print "Setting up the category " + str(category) + " veto dags"

    if opts.run_play_analysis:
      print "Running inspiral hipe for play with vetoes"
      hipePlayVetoNode[category] = inspiralutils.hipe_setup("playground", cp, \
          opts, ifos, playOnly = True, vetoCat = category, vetoFiles = dqVetoes)
      dag.add_node(hipePlayVetoNode[category])
      if category==2 and opts.run_analysis: 
        hipePlayVetoNode[category].add_parent(hipePlayNode)
      elif category>2: 
        hipePlayVetoNode[category].add_parent(hipePlayVetoNode[category-1])


    if opts.run_full_data:
      print "Running inspiral hipe for full data with vetoes"
      hipeAnalysisVetoNode[category] = inspiralutils.hipe_setup("analysis", \
          cp, opts, ifos, vetoCat = category, vetoFiles = dqVetoes)
      dag.add_node(hipeAnalysisVetoNode[category])
      if category==2 and opts.run_analysis: 
        hipeAnalysisVetoNode[category].add_parent(hipeAnalysisNode)
      elif category>2: 
        hipeAnalysisVetoNode[category].add_parent( \
            hipeAnalysisVetoNode[category-1])

    if opts.run_injections:
      print "Running inspiral hipe for injections with vetoes"
      for (injDir, injFile) in cp.items("injections"):
        if category == 2: hipeInjVetoNode[injDir] = {}
        hipeInjVetoNode[injDir][category] = inspiralutils.hipe_setup(injDir, \
            cp, opts, ifos, injFile = cp.get("injections", injDir), 
            vetoCat = category, vetoFiles = dqVetoes)
        dag.add_node(hipeInjVetoNode[injDir][category])
        if category==2 and opts.run_analysis: 
          hipeInjVetoNode[injDir][category].add_parent(hipeInjNode[injDir])
        elif category>2: 
          hipeInjVetoNode[injDir][category].add_parent(\
              hipeInjVetoNode[injDir][category-1])

##############################################################################
# Set up the directories for followup and run lalapps_followup_pipe
if opts.run_followup:
  followupNode = inspiralutils.followup_setup("followup", cp, opts, "analysis")
  dag.add_node(followupNode)
  if opts.run_analysis:
    followupNode.add_parent(hipeAnalysisNode)


##############################################################################
# Write the dag and sub files
dag.write_sub_files()
dag.write_dag()

print 
print "  Created a DAG file which can be submitted by executing"
print "\n    cd " + analysisDirectory
print "    condor_submit_dag " + dag.get_dag_file()
print """\n  from a condor submit machine
  Before submitting the dag, you must execute

    export _CONDOR_DAGMAN_LOG_ON_NFS_IS_ERROR=FALSE

  If you are running LSCdataFind jobs, do not forget to initialize your grid
  proxy certificate on the condor submit machine by running the commands
  
    unset X509_USER_PROXY
    grid-proxy-init -hours 72
  
  Enter your pass phrase when prompted. The proxy will be valid for 72 hours.
  If you expect the LSCdataFind jobs to take longer to complete, increase the
  time specified in the -hours option to grid-proxy-init. You can check that
  the grid proxy has been sucessfully created by executing the command:
  
    grid-cert-info -all -file /tmp/x509up_u`id -u`
  
  This will also give the expiry time of the proxy."""

##############################################################################
# write out a log file for this script
log_fh = open(basename + '.pipeline.log', 'w')
  
log_fh.write( "$Id$" \
    + "\n" )
log_fh.write( "$Name$" + "\n\n" )
log_fh.write( "Invoked with arguments:" )
for arg in command_line:
  if arg[0] == '-':
    log_fh.write( "\n" )
  log_fh.write( arg + ' ')

log_fh.write( "\n" )
log_fh.write( "Config file has CVS strings:\n" )
log_fh.write( cp.get('pipeline','version') + "\n" )
log_fh.write( cp.get('pipeline','cvs-tag') + "\n\n" )

##############################################################################
