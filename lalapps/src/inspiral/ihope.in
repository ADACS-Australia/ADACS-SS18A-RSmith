#!/usr/bin/env @PYTHONPROG@
"""
ihope.in - weekly automate pipeline driver script

$Id$

This script generates the condor DAG necessary to analyze a given amount
of LIGO and GEO data.  It takes in the start and end times, generates the
segment lists, runs the zero-lag, time shift and injection analyses, generates
summary information and plots and follows up the loudest events.
"""
__author__ = 'Stephen Fairhurst <sfairhur@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import os, sys, copy
import ConfigParser
import optparse
import tempfile
import urllib
sys.path.append('@PYTHONLIBDIR@')

##############################################################################
# import the modules we need to build the pipeline
from glue import segments
from glue import segmentsUtils
from glue import pipeline

dq_url_pattern = "http://ldas-cit.ligo.caltech.edu/segments/S5/%s/dq_segments.txt"

##############################################################################
# Functions used in setting up the dag:
def make_external_call(command, show_stdout=False, show_command=False):
  """
  Run a program on the shell and print informative messages on failure.
  """
  if show_command: print command

  stdin, out, err = os.popen3(command)
  pid, status = os.wait()

  if status != 0:
      print >>sys.stderr, "External call failed."
      print >>sys.stderr, "  status: %d" % status
      print >>sys.stderr, "  stdout: %s" % out.read()
      print >>sys.stderr, "  stderr: %s" % err.read()
      print >>sys.stderr, "  command: %s" % command
      sys.exit(status)
  if show_stdout:
      print out.read()
  stdin.close()
  out.close()
  err.close()

##############################################################################
def mkdir( newdir ):
  """
  Create a directory, don't complain if it exists
  
  newdir = name of directory to be created
  """
  if os.path.isdir(newdir): pass
  elif os.path.isfile(newdir):
    raise OSError("a file with the same name as the desired " \
                  "dir, '%s', already exists." % newdir)
  else: os.mkdir(newdir)
 
##############################################################################
def link_executables(directory, config):
  """
  link executables to given directory
  """
  for (job, executable) in config.items("condor"):
    if job != "universe":
      if executable[0] != "/": 
        executable = "../../" + executable
      config.set("condor", job, executable)

##############################################################################
# Function to set up the segments for the analysis
def science_segments(ifo):
  """
  generate the segments for the specified ifo
  """
  segFindFile = ifo + "-SCI_SEGS-" + str(opts.gps_start_time) + "-" + \
      str(opts.gps_end_time - opts.gps_start_time) + ".txt"

  # if not generating segments, all we need is the name of the segment file
  if not opts.generate_segments: return segFindFile

  # run segFind to determine science segments
  segFindCall = "LSCsegFind --interferometer=" + ifo + \
      " --type=\"" + cp.get("segments", "analyze") + "\""\
      " --gps-start-time=" + str(opts.gps_start_time) + \
      " --gps-end-time=" + str(opts.gps_end_time) + " > " + segFindFile 
  make_external_call(segFindCall)
  return segFindFile

##############################################################################
# Function to set up the segments for the analysis
def veto_segments(ifo, segmentList):
  """
  generate veto segments for the given ifo
  """
  # download the dq segments to generate the veto files
  fname = ifo + "-dq_segments.txt"
  print "Downloading the latest daily dump of segment database to " \
      + fname + " ...",
  segFile, info = urllib.urlretrieve(dq_url_pattern % ifo, fname)
  print "done"
  combinedVeto = {}
  for category in range(1, 4 + 1):
    dqFile = cp.get("segments", ifo.lower() + "-cat-" + str(category) + \
        "-veto-file")

    vetoFile = ifo + "-CATEGORY_" + str(category) + "_VETO_SEGS-" + \
        str(opts.gps_start_time) + "-" + \
        str(opts.gps_end_time - opts.gps_start_time) + ".txt"

    combinedFile = ifo + "-COMBINED_CAT_" + str(category) + "_VETO_SEGS-" + \
        str(opts.gps_start_time) + "-" + \
        str(opts.gps_end_time - opts.gps_start_time) + ".txt"

    dqCall = "pylal_query_dq --ifo " + ifo + " --dq-segfile " + segFile + \
        " --segfile " + segmentList + " --flagfile ../" + dqFile + \
        " --outfile " + vetoFile

    # generate the segments
    if opts.generate_segments:
      make_external_call(dqCall)
      vetoSegs = segmentsUtils.fromsegwizard(open(vetoFile)).coalesce()

      if category > 1:
        vetoSegs |= segmentsUtils.fromsegwizard(open(combinedVeto[category-1]))

      segmentsUtils.tosegwizard(file(combinedFile,"w"), vetoSegs)

    combinedVeto[category] = combinedFile

  return combinedVeto
      
##############################################################################
# Function to set up the segments for the analysis
def datafind_segments(ifo):
  ligoIfos = ["H1","H2","L1"]

  if ifo in ligoIfos: type = cp.get("input","ligo-type")
  elif ifo == "G1": type =   cp.get("input","geo-type")
 
  ifo_type = ifo + "_" + type
  dataFindFile = ifo_type + "-" + str(opts.gps_start_time) + "-" + \
      str(opts.gps_end_time - opts.gps_start_time) + ".txt"

  print "Running LSCdataFind to determine available data from " + type + \
      " frames for " + ifo
  dataFindCall = "LSCdataFind --observatory=" + ifo[0] + \
      " --type=" + ifo_type + \
      " --gps-start-time=" + str(opts.gps_start_time) + \
      " --gps-end-time=" + str(opts.gps_end_time) + " --show-times > " + \
      dataFindFile
  make_external_call(dataFindCall)
  dfSegs = segmentsUtils.fromsegwizard(file(dataFindFile)).coalesce()

  return dfSegs

##############################################################################
# Function to set up lalapps_inspiral_hipe
def hipe_setup(hipeDir, config, injFile=None, dfOnly = False):
  """
  run lalapps_inspiral_hipe and add job to dag
  hipeDir = directory in which to run inspiral hipe
  config  = config file 
  injFile = injection file to use when running
  dfOnly  = only run the datafind step of the pipeline
  """

  # make the directory for running hipe
  mkdir(hipeDir)

  # create the hipe config parser, keep only relevant info
  hipecp = copy.deepcopy(config)
  hipeSections = ['condor', 'pipeline', 'input', 'calibration', 'datafind',\
      'ligo-data', 'geo-data', 'data', 'tmpltbank', 'tmpltbank-1', \
      'tmpltbank-2', 'no-veto-inspiral', 'veto-inspiral', 'inspiral', \
      'h1-inspiral', 'h2-inspiral', 'l1-inspiral', 'g1-inspiral', \
      'thinca', 'thinca-1', 'thinca-2', 'thinca-slide', 'trigtotmplt', 'sire', \
      'sire-inj', 'coire', 'coire-inj']

  for seg in hipecp.sections():
    if not seg in hipeSections: hipecp.remove_section(seg)

  hipecp.remove_option("condor","hipe")
  hipecp.remove_option("condor","follow")

  hipecp.set("input", "gps-start-time", opts.gps_start_time)
  hipecp.set("input", "gps-end-time", opts.gps_end_time)

  if injFile:
    # add the options to the ini file
    if injFile[0] != "/": injFile = "../../" + injFile
    hipecp.set("input", "injection-file", injFile )
    hipecp.set("input", "num-slides", "")

  else: 
    # add the options to the ini file
    hipecp.set("input","num-slides", config.get("input","num-slides") )
    hipecp.set("input", "injection-file", "" )

  # link the executables 
  link_executables(hipeDir, hipecp)

  # return to the directory, write ini file and run hipe
  os.chdir(hipeDir)
  iniFile = "inspiral_hipe_" + hipeDir + ".ini"
  hipecp.write(file(iniFile,"w"))
  
  print "Running hipe in directory " + hipeDir 
  if injFile: 
    print "Injection file: " + hipecp.get("input", "injection-file") 
  else: print "No injections, " + str(hipecp.get("input","num-slides")) + \
      " time slides"
  print

  # work out the hipe call:
  hipeCommand = config.get("condor","hipe")
  hipeCommand += " --log-path " + opts.log_path
  hipeCommand += " --config-file " + iniFile
  for item in config.items("ifo-details"):
      hipeCommand += " --" + item[0] + " " + item[1]

  if dfOnly: hipeCommand += " --datafind"
  else:
    for item in config.items("hipe-arguments"):
      if item[0] != "datafind": hipeCommand += " --" + item[0] + " " + item[1]

  # run lalapps_inspiral_hipe
  make_external_call(hipeCommand) 

  # link datafind
  if not dfOnly:
    try:
      os.rmdir("cache")
      os.symlink("../datafind/cache", "cache")
    except: pass

  # add job to dag
  hipeJob = pipeline.CondorDAGManJob(hipeDir + "/" + iniFile.rstrip("ini") + \
      "dag")
  hipeNode = pipeline.CondorDAGNode(hipeJob)

  # add postscript to deal with rescue dag
  fix_rescue(hipeNode)

  # return to the original directory
  os.chdir("..")

  return hipeNode

##############################################################################
# Function to set up lalapps_inspiral_hipe
def hipe_veto(hipeDir, config, vetoCategory, vetoFiles, injFile=None):
  """
  run lalapps_inspiral_hipe and add job to dag
  hipeconfig  = config file 
  injFile = injection file to use when running
  """
  # create the hipe config parser, keep only relevant info
  vetocp = copy.deepcopy(config)
  vetoSections = ['condor', 'pipeline', 'input', 'thinca', 'thinca-2', \
      'thinca-slide', 'coire', 'coire-inj']

  vetocp.remove_option("condor","hipe")
  vetocp.remove_option("condor","follow")

  vetocp.set("input", "gps-start-time", opts.gps_start_time)
  vetocp.set("input", "gps-end-time", opts.gps_end_time)

  for seg in vetocp.sections():
    if not seg in vetoSections: vetocp.remove_section(seg)

  if injFile:
    # add the options to the ini file
    if injFile[0] != "/": injFile = "../../" + injFile
    vetocp.set("input", "injection-file", injFile )
    vetocp.set("input", "num-slides", "")

  else: 
    # add the options to the ini file
    vetocp.set("input","num-slides", config.get("input","num-slides") )
    vetocp.set("input", "injection-file", "" )

  for section in ["thinca", "coire"]: 
    vetocp.set(section, "user-tag","CAT_" + str(vetoCategory) + "_VETO")

  # return to the directory, write ini file and run hipe
  os.chdir(hipeDir)
  iniFile = "inspiral_hipe_cat" + vetoCategory + "_veto_" + hipeDir + ".ini"
  vetocp.write(file(iniFile,"w"))
  
  print "Running hipe for category " + str(category) + " vetoes"

  # work out the hipe call:
  hipeCommand = config.get("condor","hipe")
  hipeCommand += " --log-path " + opts.log_path
  hipeCommand += " --config-file " + iniFile
  for item in config.items("ifo-details"):
      hipeCommand += " --" + item[0] + " " + item[1]

  for item in config.items("hipe-arguments"):
    if item[0] in ["second-coinc", "coire-second-coinc"]:
      hipeCommand += " --" + item[0] + " " + item[1]

  # run lalapps_inspiral_hipe
  make_external_call(hipeCommand) 

  # add job to dag
  vetoJob = pipeline.CondorDAGManJob(hipeDir + "/" + iniFile.rstrip("ini") + \
      "dag")
  vetoNode = pipeline.CondorDAGNode(vetoJob)

  # add postscript to deal with rescue dag
  fix_rescue(vetoNode)

  # return to the original directory
  os.chdir("..")

  return vetoNode

##############################################################################
# Function to set up lalapps_followup_pipe
def followup_setup(followupDir, config, hipeDir):
  """
  run lalapps_followup_pipe and add job to dag
  followupDir = directory to output the followup
  config    = config file 
  """

  # make the directory for followup pipe
  mkdir(followupDir)

  # create the followup config parser, keep only relevant info
  followupcp = copy.deepcopy(config)
  followupSections = ['condor', 'hipe-cache', 'triggers', 'datafind', 
      'q-datafind', 'qscan', 'q-hoft-datafind', 'qscan-hoft', 
      'plots', 'output', 'seg']

  for seg in followupcp.sections():
    if not seg in followupSections: followupcp.remove_section(seg)

  followupcp.remove_option("condor","hipe")
  followupcp.remove_option("condor","follow")

  # XXX this should be replaced by getting the information from the hipe cache
  # set the cache paths
  followupcp.add_section("hipe-cache")
  followupcp.set("hipe-cache", "hipe-cache-path", "hipe_cache")
  followupcp.set("hipe-cache", "science-run", "S5")

  for path in ["tmpltbank-path", "trigbank-path", "first-inspiral-path", \
      "second-inspiral-path", "first-coinc-path", "second-coinc-path"]: 
    followupcp.set("hipe-cache", path, "../" + hipeDir)

  # set the xml-glob
  followupcp.set("triggers", "xml-glob", "../" + hipeDir + "/*COIRE*H*xml")
  # to here XXX

  # correct paths to qscan config files
  for section in ["qscan", "qscan-hoft"]:
    for (opt, arg) in followupcp.items(section):
      if "config-file" in opt and arg[0] != "/": 
        arg = "../../" + arg
        followupcp.set(section, opt, arg)

  # link the executables 
  link_executables(followupDir, followupcp)

  # return to the directory, write ini file and run hipe
  os.chdir(followupDir)
  iniFile = "followup_pipe_" + followupDir + ".ini"
  followupcp.write(file(iniFile,"w"))
  
  # link datafind output from original hipe
  try: os.symlink("../datafind/cache", "hipe_cache")
  except: pass
  print "Running followup pipe in directory " + followupDir 

  # work out the followup_pipe call:
  followupCommand = config.get("condor","follow")
  followupCommand += " --log-path " + opts.log_path
  followupCommand += " --config-file " + iniFile

  for item in config.items("followup-arguments"):
    followupCommand += " --" + item[0] + " " + item[1]

  # set up a fake followup dag -- the real one can't be generated until the
  # analysis is done
  followupDag = iniFile.rstrip("ini") + "dag"
  f = open(followupDag,"w")
  f.write("\n")
  f.close()

  # add job to dag
  followupJob = pipeline.CondorDAGManJob(followupDir + "/" + followupDag)
  followupNode = pipeline.CondorDAGNode(followupJob)

  # write the pre-script to run lalapps_followup_pipe at the appropriate time
  f = open(followupDag + ".pre","w")
  f.write("#! /bin/bash\n")
  f.write("cd followup\n")
  f.write(followupCommand)
  f.write("cd ..\n")
  f.close()
  os.chmod(followupDag + ".pre", 0744)
  followupNode.set_pre_script(followupDir + "/" + followupDag + ".pre")

  # add postscript to deal with rescue dag
  fix_rescue(followupNode)

  # return to the original directory
  os.chdir("..")

  return followupNode

##############################################################################
# Function to fix the rescue of inner dags
def fix_rescue(dagNode):
  """
  add a postscript to deal with the rescue dag correctly
 
  dagNode = the node for the subdag
  """
  dagNode.set_post_script( "rescue.sh")
  dagNode.add_post_script_arg( "$RETURN" ) 
  dagNode.add_post_script_arg( 
      dagNode.job().get_sub_file().rstrip(".condor.sub") )

def write_rescue():
  # Write the rescue post-script
  # XXX FIXME: This is a hack, required until condor is fixed XXX
  f = open("rescue.sh", "w")
  f.write("""#! /bin/bash
  if [ ! -n "${2}" ]
  then
    echo "Usage: `basename $0` DAGreturn DAGfile"
    exit
  fi
  
  if (( ${1}>0 ))
  then
    DIR=${2%/*}
    DAG=${2#*/}
    mv ${2} ${2}.orig
    `sed "s/DIR ${DIR}//g" ${DAG}.rescue > ${2}`
    exit ${1}
  fi""")
  f.close()
  os.chmod("rescue.sh", 0744)


##############################################################################
#
#  MAIN PROGRAM
#
##############################################################################
usage = """usage: %prog [options] 
"""

parser = optparse.OptionParser( usage )

# arguments
parser.add_option("-f", "--config-file",action="store",type="string",\
    metavar=" FILE", help="use configuration file FILE")

parser.add_option("-p", "--log-path",action="store",type="string",\
    metavar=" PATH", help="directory to write condor log file")

parser.add_option("-s", "--gps-start-time",action="store",type="int",\
    metavar=" GPS_START", help="begin analysis at GPS_START")

parser.add_option("-e", "--gps-end-time",action="store",type="int",\
    metavar=" GPS_END", help="end analysis at GPS_END")

parser.add_option("-S", "--generate-segments",action="store_true",\
    default=False, help="generate segments for analysis")

parser.add_option("-Q", "--data-quality",action="store_true",\
    default=False, help="generate dq veto segments and use in analysis ")

parser.add_option("-D", "--use-available-data",action="store_true",\
    default=False, help="analyze only the data available on the cluster")

parser.add_option("-B", "--run-datafind",action="store_true",\
    default=False, help="run the datafind step")

parser.add_option("-A", "--run-analysis",action="store_true",\
    default=False, help="run the inspiral analysis")

parser.add_option("-I", "--run-injections",action="store_true",\
    default=False, help="run the inspiral analysis with injections")

parser.add_option("-F", "--run-followup",action="store_true",\
    default=False, help="run the inspiral followup")

(opts,args) = parser.parse_args()

##############################################################################
# Sanity check of input arguments
if not opts.config_file:
  print >> sys.stderr, "No configuration file specified."
  print >> sys.stderr, "Use --config-file FILE to specify location."
  sys.exit(1)

if not opts.log_path:
  print >> sys.stderr, "No log file path specified."
  print >> sys.stderr, "Use --log-path PATH to specify a location."
  sys.exit(1)

if not opts.gps_start_time:
  print >> sys.stderr, "No GPS start time specified for the analysis"
  print >> sys.stderr, "Use --gps-start-time GPS_START to specify a location."
  sys.exit(1)

if not opts.gps_end_time:
  print >> sys.stderr, "No GPS end time specified for the analysis"
  print >> sys.stderr, "Use --gps-end-time GPS_END to specify a location."
  sys.exit(1)

if opts.gps_end_time < opts.gps_start_time:
  print >> sys.stderr, "The GPS end time must be after the GPS start time"
  sys.exit(1)

##############################################################################
# parse the ini file:
cp = ConfigParser.ConfigParser()
cp.read(opts.config_file)

# set up the analysis directory
analysisDirectory = str(opts.gps_start_time) + "-" + str(opts.gps_end_time)

mkdir(analysisDirectory)
os.chdir(analysisDirectory)

##############################################################################
# create a log file that the Condor jobs will write to
basename = opts.config_file.rstrip(".ini")
logname = basename + '.dag.log.'
tempfile.tempdir = opts.log_path
logfile = tempfile.mktemp(prefix=logname)
fh = open( logfile, "w" )
fh.close()

##############################################################################
# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
dag.set_dag_file(basename)

# write the file to handle the rescue dags
write_rescue()


##############################################################################
# Set up the IFOs and get the appropriate segments

ifos = []
for option in ["g1-data","h1-data","h2-data","l1-data"]:
  if cp.has_option("ifo-details",option): ifos.append(option[0:2].upper() )

if cp.has_option("ifo-details","analyze-all"): 
  ifos = ["G1", "H1", "H2", "L1"]

print "Setting up an analysis for " + str(ifos) + " from " + \
    str(opts.gps_start_time) + " to "  + str(opts.gps_end_time)
print
sys.stdout.flush()

##############################################################################
# Determine the segments to analyze
segFile = {}
dqVetoes = {}

for ifo in ifos:
  segFile[ifo] = ifo + "_selectedsegs.txt"
  missedFile = ifo + "_missedsegs.txt"

  print "Generating science segments for " + ifo + " ...",
  sys.stdout.flush()
  sciSegFile = science_segments(ifo)
  sciSegs = segmentsUtils.fromsegwizard(file(sciSegFile)).coalesce()
  print " done."
  sys.stdout.flush()

  if opts.data_quality:
    print "Generating veto segments for " + ifo
    sys.stdout.flush()
    dqVetoes[ifo] = veto_segments(ifo, sciSegFile) 

  if opts.generate_segments:
    sciSegs = sciSegs.__and__(dqVetoes[1].__invert__())

    if opts.use_available_data:
      dfSegs = datafind_segments(ifo)
      analyzedSegs = sciSegs.__and__(dfSegs)
      missedSegs = sciSegs.__and__(dfSegs.__invert__()) 
      segmentsUtils.tosegwizard(file(missedFile,"w"), missedSegs)
      print "Writing " + ifo + " segments which cannot be analyzed to file " + \
          missedFile
      print "Not analyzing %d s, representing %.2f percent of time" %  \
         (missedSegs.__abs__(), 
         100. * missedSegs.__abs__() / analyzedSegs.__abs__() )
      print

    else: analyzedSegs = sciSegs

    segmentsUtils.tosegwizard(file(segFile[ifo],"w"), analyzedSegs)
    print "Writing " + ifo + " segments of total time " + \
        str(analyzedSegs.__abs__()) + "s to file: " + segFile[ifo]


##############################################################################
# Run lalapps_inspiral_hipe for datafind
if opts.run_datafind:
  print "Running inspiral hipe for datafind run"
  hipeDfNode = hipe_setup("datafind", cp, dfOnly = True)
  dag.add_node(hipeDfNode)

##############################################################################
# Set up the directories for each run and run lalapps_inspiral_hipe
hipeInjNodes = []

if opts.run_injections:
  print "Running inspiral hipe for injection runs"
  for (injDir, injFile) in cp.items("injections"):
    hipeInjNode = hipe_setup(injDir, cp, injFile)
    hipeInjNodes.append( hipeInjNode )
    dag.add_node(hipeInjNode)
    if opts.run_datafind:
      hipeInjNode.add_parent(hipeDfNode)

    if opts.data_quality:
      for category in range(2, 4 + 1):
        hipeInjVetoNode = hipe_veto_setup(injDir, cp, category, dqVetoes, \
            injFile)

if opts.run_analysis:
  print "Running inspiral hipe for analysis run"
  hipeAnalysisNode = hipe_setup("analysis", cp)
  dag.add_node(hipeAnalysisNode)
  if opts.run_datafind:
    hipeAnalysisNode.add_parent(hipeDfNode)


##############################################################################
# Set up the directories for followup and run lalapps_followup_pipe
if opts.run_followup:
  followupNode = followup_setup("followup", cp, "analysis" )
  dag.add_node(followupNode)
  if opts.run_analysis:
    followupNode.add_parent(hipeAnalysisNode)


##############################################################################
# Write the dag and sub files
dag.write_sub_files()
dag.write_dag()

print 
print "  Created a DAG file which can be submitted by executing"
print "\n    cd " + analysisDirectory
print "    condor_submit_dag " + dag.get_dag_file()
print """\n  from a condor submit machine
  Before submitting the dag, you must execute

    export _CONDOR_DAGMAN_LOG_ON_NFS_IS_ERROR=FALSE

  If you are running LSCdataFind jobs, do not forget to initialize your grid
  proxy certificate on the condor submit machine by running the commands
  
    unset X509_USER_PROXY
    grid-proxy-init -hours 72
  
  Enter your pass phrase when promted. The proxy will be valid for 72 hours.
  If you expect the LSCdataFind jobs to take longer to complete, increase the
  time specified in the -hours option to grid-proxy-init. You can check that
  the grid proxy has been sucessfully created by executing the command:
  
    grid-cert-info -all -file /tmp/x509up_u`id -u`
  
  This will also give the expiry time of the proxy."""
