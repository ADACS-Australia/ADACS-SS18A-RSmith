#!/usr/bin/env @PYTHONPROG@
"""
ihope.in - weekly automate pipeline driver script

$Id$

This script generates the condor DAG necessary to analyze a given amount
of LIGO and GEO data.  It takes in the start and end times, generates the
segment lists, runs the zero-lag, time shift and injection analyses, generates
summary information and plots and follows up the loudest events.
"""
__author__ = 'Stephen Fairhurst <sfairhur@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import os, sys, copy
import ConfigParser
import optparse
import tempfile
import urllib
sys.path.append('@PYTHONLIBDIR@')

##############################################################################
# import the modules we need to build the pipeline
from glue import segments
from glue import segmentsUtils
from glue import pipeline

dq_url_pattern = "http://ldas-cit.ligo.caltech.edu/segments/S5/%s/dq_segments.txt"

##############################################################################
# Functions used in setting up the dag:
def make_external_call(command, show_stdout=False, show_command=False):
  """
  Run a program on the shell and print informative messages on failure.
  """
  if show_command: print command

  stdin, out, err = os.popen3(command)
  pid, status = os.wait()

  if status != 0:
      print >>sys.stderr, "External call failed."
      print >>sys.stderr, "  status: %d" % status
      print >>sys.stderr, "  stdout: %s" % out.read()
      print >>sys.stderr, "  stderr: %s" % err.read()
      print >>sys.stderr, "  command: %s" % command
      sys.exit(status)
  if show_stdout:
      print out.read()
  stdin.close()
  out.close()
  err.close()

##############################################################################
def mkdir( newdir ):
  """
  Create a directory, don't complain if it exists
  
  newdir = name of directory to be created
  """
  if os.path.isdir(newdir): pass
  elif os.path.isfile(newdir):
    raise OSError("a file with the same name as the desired " \
                  "dir, '%s', already exists." % newdir)
  else: os.mkdir(newdir)
 
##############################################################################
def link_executables(directory, config):
  """
  link executables to given directory
  """
  for (job, executable) in config.items("condor"):
    if job != "universe":
      if executable[0] != "/": 
        executable = "../../" + executable
      config.set("condor", job, executable)

##############################################################################
# Function to set up the segments for the analysis
def science_segments(ifo, config):
  """
  generate the segments for the specified ifo
  """
  segFindFile = ifo + "-SCIENCE_SEGMENTS-" + str(opts.gps_start_time) + "-" + \
      str(opts.gps_end_time - opts.gps_start_time) + ".txt"

  # if not generating segments, all we need is the name of the segment file
  if not opts.generate_segments: return segFindFile

  executable = config.get("condor", "segfind")
  if executable[0] != "/": executable = "../" + executable

  # run segFind to determine science segments
  segFindCall = executable + " --interferometer=" + ifo + \
      " --type=\"" + cp.get("segments", "analyze") + "\""\
      " --gps-start-time=" + str(opts.gps_start_time) + \
      " --gps-end-time=" + str(opts.gps_end_time) + " > " + segFindFile 
  make_external_call(segFindCall)
  return segFindFile

##############################################################################
# Function to set up the segments for the analysis
def veto_segments(ifo, config, segmentList, dqSegfile, categories):
  """
  generate veto segments for the given ifo
  
  ifo         = name of the ifo
  segmentList = list of science mode segments
  dqSegfile   = the file containing dq flags
  categories  = list of veto categories 
  """
  executable = config.get("condor", "query_dq")
  if executable[0] != "/": executable = "../" + executable

  vetoFiles = {}

  for category in categories:
    dqFile = config.get("segments", ifo.lower() + "-cat-" + str(category) + \
        "-veto-file")

    vetoFile = ifo + "-CATEGORY_" + str(category) + "_VETO_SEGS-" + \
        str(opts.gps_start_time) + "-" + \
        str(opts.gps_end_time - opts.gps_start_time) + ".txt"

    dqCall = executable + " --ifo " + ifo + " --dq-segfile " + dqSegFile + \
        " --segfile " + segmentList + " --flagfile ../" + dqFile + \
        " --outfile " + vetoFile

    # generate the segments
    make_external_call(dqCall)

    # if there are previous vetoes, generate combined
    try: previousSegs = \
        segmentsUtils.fromsegwizard(open(vetoFiles[category-1]))
    except: previousSegs = None

    if previousSegs:
      combinedFile = ifo + "-COMBINED_CAT_" + str(category) + "_VETO_SEGS-" + \
          str(opts.gps_start_time) + "-" + \
          str(opts.gps_end_time - opts.gps_start_time) + ".txt"

      vetoSegs = segmentsUtils.fromsegwizard(open(vetoFile)).coalesce()
      vetoSegs |= previousSegs
      segmentsUtils.tosegwizard(file(combinedFile,"w"), vetoSegs)
      vetoFiles[category] = combinedFile

    else: vetoFiles[category] = vetoFile

  return vetoFiles
      
##############################################################################
# Function to set up the segments for the analysis
def datafind_segments(ifo, config):
  ligoIfos = ["H1","H2","L1"]

  if ifo in ligoIfos: type = cp.get("input","ligo-type")
  elif ifo == "G1": type =   cp.get("input","geo-type")
 
  executable = config.get("condor", "datafind")
  if executable[0] != "/": executable = "../" + executable

  ifo_type = ifo + "_" + type
  dataFindFile = ifo_type + "-" + str(opts.gps_start_time) + "-" + \
      str(opts.gps_end_time - opts.gps_start_time) + ".txt"

  print "Running LSCdataFind to determine available data from " + type + \
      " frames for " + ifo
  dataFindCall = executable + " --observatory=" + ifo[0] + \
      " --type=" + ifo_type + \
      " --gps-start-time=" + str(opts.gps_start_time) + \
      " --gps-end-time=" + str(opts.gps_end_time) + " --show-times > " + \
      dataFindFile
  make_external_call(dataFindCall)
  dfSegs = segmentsUtils.fromsegwizard(file(dataFindFile)).coalesce()

  return dfSegs

##############################################################################
# Function to set up lalapps_inspiral_hipe
def hipe_setup(hipeDir, config, injFile=None, dfOnly = False, playOnly = False,
  vetoCat = None, vetoFiles = None):
  """
  run lalapps_inspiral_hipe and add job to dag
  hipeDir   = directory in which to run inspiral hipe
  config    = config file 
  injFile   = injection file to use when running
  dfOnly    = only run the datafind step of the pipeline
  vetoCat   = run this category of veto
  vetoFiles = dictionary of veto files
  """
  global ifos

  # make the directory for running hipe
  mkdir(hipeDir)

  # create the hipe config parser, keep only relevant info
  hipecp = copy.deepcopy(config)
  if dfOnly:
    hipeSections = ['condor', 'pipeline', 'input', 'datafind','data',\
        'ligo-data','inspiral']
  elif vetoCat:
    hipeSections = ['condor', 'pipeline', 'input', 'data', 'ligo-data', \
        'inspiral', 'thinca', 'thinca-2', 'datafind', \
        'thinca-slide', 'coire', 'coire-inj']
  else:
    hipeSections = ['condor', 'pipeline', 'input', 'calibration', 'datafind',\
        'ligo-data', 'geo-data', 'data', 'tmpltbank', 'tmpltbank-1', \
        'tmpltbank-2', 'no-veto-inspiral', 'veto-inspiral', 'inspiral', \
        'h1-inspiral', 'h2-inspiral', 'l1-inspiral', 'g1-inspiral', \
        'thinca', 'thinca-1', 'thinca-2', 'thinca-slide', 'trigtotmplt', \
        'sire', 'sire-inj', 'coire', 'coire-inj']

  for seg in hipecp.sections():
    if not seg in hipeSections: hipecp.remove_section(seg)

  hipecp.remove_option("condor","hipe")
  hipecp.remove_option("condor","follow")

  hipecp.set("input", "gps-start-time", opts.gps_start_time)
  hipecp.set("input", "gps-end-time", opts.gps_end_time)

  # set the data type
  if playOnly:
    hipecp.set("pipeline", "playground-data-mask", "playground_only")
  else:
    hipecp.set("pipeline", "playground-data-mask", "all_data")

  # deal with vetoes
  if vetoCat:
    for section in ["thinca", "coire"]: 
      hipecp.set(section, "user-tag","CAT_" + str(vetoCat) + "_VETO")
    for ifo in ifos:
      hipecp.set("thinca", ifo.lower() + "-veto-file", 
          vetoFiles[ifo][vetoCat])
      
  if injFile:
    # add the injection options to the ini file
    if injFile[0] != "/": injFile = "../../" + injFile
    hipecp.set("input", "injection-file", injFile )
    hipecp.set("input", "num-slides", "")
  else: 
    # add the time slide to the ini file
    hipecp.set("input","num-slides", config.get("input","num-slides") )
    hipecp.set("input", "injection-file", "" )

    # sanity check of numSlides
    maxLength = None
    if playOnly: maxLength = 600
    elif hipecp.has_option("input", "max-thinca-segment"):
      maxLength = hipecp.getint("input", "max-thinca-segment")

    if maxLength:
      maxSlide = max([cp.getint("thinca-slide", ifo.lower() + "-slide") 
          for ifo in ifos])
      numSlides = (maxLength/2/maxSlide - 1)
      if numSlides < hipecp.getint("input", "num-slides"):
        print "Setting number of slides to " + str(numSlides) + \
            " to avoid double wrapping"
        hipecp.set("input","num-slides", str(numSlides))

  # link the executables 
  if not vetoCat: link_executables(hipeDir, hipecp)

  # return to the directory, write ini file and run hipe
  os.chdir(hipeDir)
  iniFile = "inspiral_hipe_" 
  if vetoCat: iniFile += "cat" + str(vetoCat) + "_veto_"
  iniFile += hipeDir + ".ini"

  hipecp.write(file(iniFile,"w"))
  
  print "Running hipe in directory " + hipeDir 
  if injFile: print "Injection file: " + hipecp.get("input", "injection-file") 
  else: print "No injections, " + str(hipecp.get("input","num-slides")) + \
      " time slides"
  if vetoCat: print "Running the category " + str(vetoCat) + " vetoes"
  print

  # work out the hipe call:
  hipeCommand = config.get("condor","hipe")
  hipeCommand += " --log-path " + opts.log_path
  hipeCommand += " --config-file " + iniFile
  if playOnly: hipeCommand += " --priority 10"
  for item in config.items("ifo-details"):
      hipeCommand += " --" + item[0] + " " + item[1]

  for item in config.items("hipe-arguments"):
    if (dfOnly and item[0] == "datafind") or  \
        (vetoCat and item[0] in ["second-coinc", "coire-second-coinc"]) or \
        (not dfOnly and not vetoCat and item[0] != "datafind"):
      hipeCommand += " --" + item[0] + " " + item[1]

  # run lalapps_inspiral_hipe
  make_external_call(hipeCommand) 

  # link datafind
  if not dfOnly and not vetoCat:
    try:
      os.rmdir("cache")
      os.symlink("../datafind/cache", "cache")
    except: pass

  # make hipe job/node
  hipeJob = pipeline.CondorDAGManJob(hipeDir + "/" + iniFile.rstrip("ini") + \
      "dag")
  if vetoCat: hipeJob.add_opt("maxjobs", "5")
  hipeNode = pipeline.CondorDAGNode(hipeJob)

  # add postscript to deal with rescue dag
  fix_rescue(hipeNode)

  # return to the original directory
  os.chdir("..")

  return hipeNode

##############################################################################
# Function to set up lalapps_followup_pipe
def followup_setup(followupDir, config, hipeDir):
  """
  run lalapps_followup_pipe and add job to dag
  followupDir = directory to output the followup
  config    = config file 
  """

  # make the directory for followup pipe
  mkdir(followupDir)

  # create the followup config parser, keep only relevant info
  followupcp = copy.deepcopy(config)
  followupSections = ['condor', 'hipe-cache', 'triggers', 'datafind', 
      'q-datafind', 'qscan', 'q-hoft-datafind', 'qscan-hoft', 
      'plots', 'output', 'seg']

  for seg in followupcp.sections():
    if not seg in followupSections: followupcp.remove_section(seg)

  followupcp.remove_option("condor","hipe")
  followupcp.remove_option("condor","follow")

  # XXX this should be replaced by getting the information from the hipe cache
  # set the cache paths
  followupcp.add_section("hipe-cache")
  followupcp.set("hipe-cache", "hipe-cache-path", "hipe_cache")
  followupcp.set("hipe-cache", "science-run", "S5")

  for path in ["tmpltbank-path", "trigbank-path", "first-inspiral-path", \
      "second-inspiral-path", "first-coinc-path", "second-coinc-path"]: 
    followupcp.set("hipe-cache", path, "../" + hipeDir)

  # set the xml-glob
  followupcp.set("triggers", "xml-glob", "../" + hipeDir + "/*COIRE*H*xml")
  # to here XXX

  # correct paths to qscan config files
  for section in ["qscan", "qscan-hoft"]:
    for (opt, arg) in followupcp.items(section):
      if "config-file" in opt and arg[0] != "/": 
        arg = "../../" + arg
        followupcp.set(section, opt, arg)

  # link the executables 
  link_executables(followupDir, followupcp)

  # return to the directory, write ini file and run hipe
  os.chdir(followupDir)
  iniFile = "followup_pipe_" + followupDir + ".ini"
  followupcp.write(file(iniFile,"w"))
  
  # link datafind output from original hipe
  try: os.symlink("../datafind/cache", "hipe_cache")
  except: pass
  print "Running followup pipe in directory " + followupDir 

  # work out the followup_pipe call:
  followupCommand = config.get("condor","follow")
  followupCommand += " --log-path " + opts.log_path
  followupCommand += " --config-file " + iniFile

  for item in config.items("followup-arguments"):
    followupCommand += " --" + item[0] + " " + item[1]

  # set up a fake followup dag -- the real one can't be generated until the
  # analysis is done
  followupDag = iniFile.rstrip("ini") + "dag"
  f = open(followupDag,"w")
  f.write("\n")
  f.close()

  # add job to dag
  followupJob = pipeline.CondorDAGManJob(followupDir + "/" + followupDag)
  followupNode = pipeline.CondorDAGNode(followupJob)

  # write the pre-script to run lalapps_followup_pipe at the appropriate time
  f = open(followupDag + ".pre","w")
  f.write("#! /bin/bash\n")
  f.write("cd followup\n")
  f.write(followupCommand)
  f.write("cd ..\n")
  f.close()
  os.chmod(followupDag + ".pre", 0744)
  followupNode.set_pre_script(followupDir + "/" + followupDag + ".pre")

  # add postscript to deal with rescue dag
  fix_rescue(followupNode)

  # return to the original directory
  os.chdir("..")

  return followupNode

##############################################################################
# Function to fix the rescue of inner dags
def fix_rescue(dagNode):
  """
  add a postscript to deal with the rescue dag correctly
 
  dagNode = the node for the subdag
  """
  dagNode.set_post_script( "rescue.sh")
  dagNode.add_post_script_arg( "$RETURN" ) 
  dagNode.add_post_script_arg( 
      dagNode.job().get_sub_file().rstrip(".condor.sub") )

def write_rescue():
  # Write the rescue post-script
  # XXX FIXME: This is a hack, required until condor is fixed XXX
  f = open("rescue.sh", "w")
  f.write("""#! /bin/bash
  if [ ! -n "${2}" ]
  then
    echo "Usage: `basename $0` DAGreturn DAGfile"
    exit
  fi
  
  if (( ${1}>0 ))
  then
    DIR=${2%/*}
    DAG=${2#*/}
    mv ${2} ${2}.orig
    `sed "s/DIR ${DIR}//g" ${DAG}.rescue > ${2}`
    exit ${1}
  fi""")
  f.close()
  os.chmod("rescue.sh", 0744)


##############################################################################
#
#  MAIN PROGRAM
#
##############################################################################
usage = """usage: %prog [options] 
"""

parser = optparse.OptionParser( usage )

# arguments
parser.add_option("-f", "--config-file",action="store",type="string",\
    metavar=" FILE", help="use configuration file FILE")

parser.add_option("-p", "--log-path",action="store",type="string",\
    metavar=" PATH", help="directory to write condor log file")

parser.add_option("-s", "--gps-start-time",action="store",type="int",\
    metavar=" GPS_START", help="begin analysis at GPS_START")

parser.add_option("-e", "--gps-end-time",action="store",type="int",\
    metavar=" GPS_END", help="end analysis at GPS_END")

parser.add_option("-S", "--skip-generate-segments",action="store_false",\
    default=True, dest="generate_segments", \
    help="skip generation segments for analysis")

parser.add_option("-D", "--use-available-data",action="store_true",\
    default=False, help="analyze only the data available on the cluster")

parser.add_option("-B", "--skip-datafind",action="store_false",\
    default=True, dest="run_datafind", help="skip the datafind step")

parser.add_option("-A", "--skip-analysis",action="store_false",\
    default=True, dest="run_analysis", help="skip the inspiral analysis")

parser.add_option("-F", "--skip-full-data",action="store_false",\
    default=True, dest="run_full_data", help="skip the full data + slides")

parser.add_option("-P", "--skip-play-analysis",action="store_false",\
    default=True, dest="run_play_analysis", help="skip the playground analysis")

parser.add_option("-I", "--skip-injections",action="store_false",\
    default=True, dest="run_injections", \
    help="skip the inspiral analysis with injections")

parser.add_option("-Q", "--skip-data-quality",action="store_false",\
    default=True, dest="run_data_quality", \
    help="skip generation dq veto segments and use in analysis ")

parser.add_option("-U", "--skip-followup",action="store_false",\
    default=True, dest="run_followup",  help="skip the inspiral followup")

(opts,args) = parser.parse_args()

##############################################################################
# Sanity check of input arguments
if not opts.config_file:
  print >> sys.stderr, "No configuration file specified."
  print >> sys.stderr, "Use --config-file FILE to specify location."
  sys.exit(1)

if not opts.log_path:
  print >> sys.stderr, "No log file path specified."
  print >> sys.stderr, "Use --log-path PATH to specify a location."
  sys.exit(1)

if not opts.gps_start_time:
  print >> sys.stderr, "No GPS start time specified for the analysis"
  print >> sys.stderr, "Use --gps-start-time GPS_START to specify a location."
  sys.exit(1)

if not opts.gps_end_time:
  print >> sys.stderr, "No GPS end time specified for the analysis"
  print >> sys.stderr, "Use --gps-end-time GPS_END to specify a location."
  sys.exit(1)

if opts.gps_end_time < opts.gps_start_time:
  print >> sys.stderr, "The GPS end time must be after the GPS start time"
  sys.exit(1)

##############################################################################
# parse the ini file:
cp = ConfigParser.ConfigParser()
cp.read(opts.config_file)

# set up the analysis directory
analysisDirectory = str(opts.gps_start_time) + "-" + str(opts.gps_end_time)

mkdir(analysisDirectory)
os.chdir(analysisDirectory)

##############################################################################
# create a log file that the Condor jobs will write to
basename = opts.config_file.rstrip(".ini")
logname = basename + '.dag.log.'
tempfile.tempdir = opts.log_path
logfile = tempfile.mktemp(prefix=logname)
fh = open( logfile, "w" )
fh.close()

##############################################################################
# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
dag.set_dag_file(basename)

# write the file to handle the rescue dags
write_rescue()


##############################################################################
# Set up the IFOs and get the appropriate segments

ifos = []
for option in ["g1-data","h1-data","h2-data","l1-data"]:
  if cp.has_option("ifo-details",option): ifos.append(option[0:2].upper() )

if cp.has_option("ifo-details","analyze-all"): 
  ifos = ["G1", "H1", "H2", "L1"]

print "Setting up an analysis for " + str(ifos) + " from " + \
    str(opts.gps_start_time) + " to "  + str(opts.gps_end_time)
print
sys.stdout.flush()

##############################################################################
# Determine the segments to analyze
segFile = {}
dqVetoes = {}

for ifo in ifos:
  segFile[ifo] = ifo + "-SELECTED_SEGS.txt"
  missedFile = ifo + "-MISSED_SEGS.txt"
  dqSegFile = ifo + "-DQ_SEGMENTS.txt"

  cp.set("input", ifo.lower() + "-segments", "../" + segFile[ifo])

  if opts.generate_segments: 
    print "Generating science segments for " + ifo + " ...",
    sys.stdout.flush()
  sciSegFile = science_segments(ifo, cp)
  sciSegs = segmentsUtils.fromsegwizard(file(sciSegFile)).coalesce()
  if opts.generate_segments: print " done."

  # download the dq segments to generate the veto files
  if opts.generate_segments: 
    print "Downloading the latest daily dump of segment database to " \
        + dqSegFile + " ...",
    dqSegFile, info = urllib.urlretrieve(dq_url_pattern % ifo, dqSegFile)
    print "done"

    print "Generating cat 1 veto segments for " + ifo + " ...",
    sys.stdout.flush()
    vetoFiles = veto_segments(ifo, cp, sciSegFile, dqSegFile, [1]) 
    print "done"

    # remove cat 1 veto times
    vetoSegs = segmentsUtils.fromsegwizard(open(vetoFiles[1])).coalesce()
    sciSegs = sciSegs.__and__(vetoSegs.__invert__())

    if opts.use_available_data:
      dfSegs = datafind_segments(ifo, cp)
      analyzedSegs = sciSegs.__and__(dfSegs)
      missedSegs = sciSegs.__and__(dfSegs.__invert__()) 
      segmentsUtils.tosegwizard(file(missedFile,"w"), missedSegs)
      print "Writing " + ifo + " segments which cannot be analyzed to file " + \
          missedFile
      print "Not analyzing %d s, representing %.2f percent of time" %  \
         (missedSegs.__abs__(), 
         100. * missedSegs.__abs__() / analyzedSegs.__abs__() )

    else: analyzedSegs = sciSegs

    segmentsUtils.tosegwizard(file(segFile[ifo],"w"), analyzedSegs)
    print "Writing " + ifo + " segments of total time " + \
        str(analyzedSegs.__abs__()) + "s to file: " + segFile[ifo]
    print

  if opts.run_data_quality:
    print "Generating veto segments for " + ifo + "...",
    sys.stdout.flush()
    dqVetoes[ifo] = veto_segments(ifo, cp, segFile[ifo], dqSegFile, [2,3,4]) 
    print "done"

##############################################################################
# Run lalapps_inspiral_hipe for datafind
if opts.run_datafind:
  print "Running inspiral hipe for datafind run"
  hipeDfNode = hipe_setup("datafind", cp, dfOnly = True)
  dag.add_node(hipeDfNode)


##############################################################################
# Set up the directories for each run and run lalapps_inspiral_hipe
if opts.run_analysis:
  if opts.run_play_analysis:
    print "Running inspiral hipe for playground run"
    hipePlayNode = hipe_setup("playground", cp, playOnly = True)
    dag.add_node(hipePlayNode)
    if opts.run_datafind:
      hipePlayNode.add_parent(hipeDfNode)

  if opts.run_full_data:
    print "Running inspiral hipe for analysis run"
    hipeAnalysisNode = hipe_setup("analysis", cp)
    dag.add_node(hipeAnalysisNode)
    if opts.run_datafind:
      hipeAnalysisNode.add_parent(hipeDfNode)

  if opts.run_injections:
    hipeInjNode = {} 
    print "Running inspiral hipe for injection runs"
    for (injDir, injFile) in cp.items("injections"):
      hipeInjNode[injDir] = hipe_setup(injDir, cp, injFile)
      dag.add_node(hipeInjNode[injDir])
      if opts.run_datafind:
        hipeInjNode[injDir].add_parent(hipeDfNode)

##############################################################################
# Run the data quality vetoes
if opts.run_data_quality:
  hipePlayVetoNode = {}
  hipeAnalysisVetoNode = {}
  hipeInjVetoNode = {}

  for category in [2,3,4]:
    print "Setting up the category " + str(category) + " veto dags"

    if opts.run_play_analysis:
      print "Running inspiral hipe for play with vetoes"
      hipePlayVetoNode[category] = hipe_setup("playground", cp, 
          playOnly = True, vetoCat = category, vetoFiles = dqVetoes)
      dag.add_node(hipePlayVetoNode[category])
      if opts.run_analysis: 
        hipePlayVetoNode[category].add_parent(hipePlayNode)


    if opts.run_full_data:
      print "Running inspiral hipe for full data with vetoes"
      hipeAnalysisVetoNode[category] = hipe_setup("analysis", cp, 
          vetoCat = category, vetoFiles = dqVetoes)
      dag.add_node(hipeAnalysisVetoNode[category])
      if opts.run_analysis: 
        hipeAnalysisVetoNode[category].add_parent(hipeAnalysisNode)

    if opts.run_injections:
      print "Running inspiral hipe for injections with vetoes"
      for (injDir, injFile) in cp.items("injections"):
        if category == 2: hipeInjVetoNode[injDir] = {}
        hipeInjVetoNode[injDir][category] = hipe_setup(injDir, cp, 
            injFile = cp.get("injections", injDir), 
            vetoCat = category, vetoFiles = dqVetoes)
        dag.add_node(hipeInjVetoNode[injDir][category])
        if opts.run_analysis: 
          hipeInjVetoNode[injDir][category].add_parent(hipeInjNode[injDir])

##############################################################################
# Set up the directories for followup and run lalapps_followup_pipe
if opts.run_followup:
  followupNode = followup_setup("followup", cp, "analysis" )
  dag.add_node(followupNode)
  if opts.run_analysis:
    followupNode.add_parent(hipeAnalysisNode)


##############################################################################
# Write the dag and sub files
dag.write_sub_files()
dag.write_dag()

print 
print "  Created a DAG file which can be submitted by executing"
print "\n    cd " + analysisDirectory
print "    condor_submit_dag " + dag.get_dag_file()
print """\n  from a condor submit machine
  Before submitting the dag, you must execute

    export _CONDOR_DAGMAN_LOG_ON_NFS_IS_ERROR=FALSE

  If you are running LSCdataFind jobs, do not forget to initialize your grid
  proxy certificate on the condor submit machine by running the commands
  
    unset X509_USER_PROXY
    grid-proxy-init -hours 72
  
  Enter your pass phrase when promted. The proxy will be valid for 72 hours.
  If you expect the LSCdataFind jobs to take longer to complete, increase the
  time specified in the -hours option to grid-proxy-init. You can check that
  the grid proxy has been sucessfully created by executing the command:
  
    grid-cert-info -all -file /tmp/x509up_u`id -u`
  
  This will also give the expiry time of the proxy."""
