#!/usr/bin/env @PYTHONPROG@
"""
ihope.in - weekly automate pipeline driver script

$Id$

This script generates the condor DAG necessary to analyze a given amount
of LIGO and GEO data.  It takes in the start and end times, generates the
segment lists, runs the zero-lag, time shift and injection analyses, generates
summary information and plots and follows up the loudest events.
"""
__author__ = 'Stephen Fairhurst <sfairhur@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import os, sys, copy, shutil
import ConfigParser
import optparse
import tempfile
import urllib
sys.path.append('@PYTHONLIBDIR@')

##############################################################################
# import the modules we need to build the pipeline
from glue import segments
from glue import segmentsUtils
from glue import pipeline
import inspiralutils 


##############################################################################
#
#  MAIN PROGRAM
#
##############################################################################
usage = """usage: %prog [options] 

lalapps_ihope is designed to run the inspiral analysis end to end.  It
performs several distinct steps.  The default is for the full search to
be run, various steps can be skipped by specifying --skip options.  The
required arguments are:

--config-file 
--log path
--gps-start-time
--gps-end-time

The options to run the analysis are specified in the config-file
(normally ihope.ini) file.

1) Generate the segments for which the analysis should be done.  This
can be skipped using --skip-generate-segments.  The option
--use-available-data will restrict the search to those times for which
there is data on the cluster the analysis is being run.

2) Generate the veto segments.  Can be skipped with
--skip-generate-veto-segments.

3) Find the data on the cluster.  Can be skipped with --skip-datafind.

4) The search itself.  Can be skipped with --skip-search.

5) The application of the data quality vetoes.  Can be skipped with
--skip-data-quality.

6) Make plots of the output.  Can be skipped with --skip-plots.

7) Followup the loudest events (currently not working).  Can be skipped
with --skip-followup.

For options 4-7, you can run 3 types of analysis:
a) playground and playground time slides. Skipped with --skip-playground
b) full data and full data time slides.  Skipped with --skip-full-data
c) software injections. Skipped with --skip-injections
"""

parser = optparse.OptionParser( usage )

# arguments
parser.add_option("-f", "--config-file",action="store",type="string",\
    metavar=" FILE", help="use configuration file FILE")

parser.add_option("-p", "--log-path",action="store",type="string",\
    metavar=" PATH", \
    help="directory to write condor log file, should be a local directory")

parser.add_option("-s", "--gps-start-time",action="store",type="int",\
    metavar=" GPS_START", help="begin analysis at GPS_START")

parser.add_option("-e", "--gps-end-time",action="store",type="int",\
    metavar=" GPS_END", help="end analysis at GPS_END")

parser.add_option("-D", "--use-available-data",action="store_true",\
    default=False, help="analyze only the data available on the cluster")

parser.add_option("-S", "--skip-generate-segments",action="store_false",\
    default=True, dest="generate_segments", \
    help="skip generation segments for analysis")

parser.add_option("-V", "--skip-generate-veto-segments",action="store_false",\
    default=True, dest="generate_veto_segments", \
    help="skip generation segments for analysis")

parser.add_option("-B", "--skip-datafind",action="store_false",\
    default=True, dest="run_datafind", help="skip the datafind step")

parser.add_option("-F", "--skip-full-data",action="store_false",\
    default=True, dest="run_full_data", help="skip the full data + slides")

parser.add_option("-P", "--skip-play-analysis",action="store_false",\
    default=True, dest="run_play_analysis", help="skip the playground analysis")

parser.add_option("-I", "--skip-injections",action="store_false",\
    default=True, dest="run_injections", \
    help="skip the inspiral analysis with injections")

parser.add_option("-A", "--skip-search",action="store_false",\
    default=True, dest="run_search",
    help="skip the search of the data (i.e. don't run inspiral hipe)")

parser.add_option("-Q", "--skip-data-quality",action="store_false",\
    default=True, dest="run_data_quality", \
    help="skip generation dq veto segments and use in analysis ")

parser.add_option("-Z", "--skip-plots",action="store_false",\
    default=True, dest="run_plots",  help="skip the plotting step")

parser.add_option("-U", "--skip-followup",action="store_false",\
    default=True, dest="run_followup",  help="skip the inspiral followup")

command_line = sys.argv[1:]
(opts,args) = parser.parse_args()

##############################################################################
# Sanity check of input arguments
if not opts.config_file:
  print >> sys.stderr, "No configuration file specified."
  print >> sys.stderr, "Use --config-file FILE to specify location."
  sys.exit(1)

if not opts.log_path:
  print >> sys.stderr, "No log file path specified."
  print >> sys.stderr, "Use --log-path PATH to specify a location."
  sys.exit(1)

if not opts.gps_start_time:
  print >> sys.stderr, "No GPS start time specified for the analysis"
  print >> sys.stderr, "Use --gps-start-time GPS_START to specify a location."
  sys.exit(1)

if not opts.gps_end_time:
  print >> sys.stderr, "No GPS end time specified for the analysis"
  print >> sys.stderr, "Use --gps-end-time GPS_END to specify a location."
  sys.exit(1)

if opts.gps_end_time < opts.gps_start_time:
  print >> sys.stderr, "The GPS end time must be after the GPS start time"
  sys.exit(1)

##############################################################################
# set up the analysis directory
analysisDirectory = str(opts.gps_start_time) + "-" + str(opts.gps_end_time)
inspiralutils.mkdir(analysisDirectory)

# copy the ini file into the directory
shutil.copy( opts.config_file, analysisDirectory )
opts.config_file = opts.config_file.split("/")[-1]

os.chdir(analysisDirectory)

# parse the ini file:
cp = ConfigParser.ConfigParser()
cp.read(opts.config_file)

# set gps start and end times in the cp object.
cp.set("input", "gps-start-time", str(opts.gps_start_time) )
cp.set("input", "gps-end-time", str(opts.gps_end_time) )

##############################################################################
# create a directory called executables and copy them over
inspiralutils.mkdir("executables")
os.chdir("../")
for (job, executable) in cp.items("condor"):
  if job != "universe":
    shutil.copy( executable, analysisDirectory + "/executables" )
    executable = "../executables/" + executable.split("/")[-1]
    cp.set("condor", job, executable)

os.chdir(analysisDirectory)

##############################################################################
# create a log file that the Condor jobs will write to
basename = opts.config_file.rstrip(".ini")
logname = basename + '.dag.log.'
tempfile.tempdir = opts.log_path
logfile = tempfile.mktemp(prefix=logname)
fh = open( logfile, "w" )
fh.close()

##############################################################################
# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
dag.set_dag_file(basename)

# write the file to handle the rescue dags
inspiralutils.write_rescue()

##############################################################################
# Set up the IFOs and get the appropriate segments
ifos = [] 

for option in ["g1-data","h1-data","h2-data","l1-data","v1-data"]:
  if cp.has_option("ifo-details",option): ifos.append(option[0:2].upper() )

if cp.has_option("ifo-details","analyze-all"): 
  print >> sys.stderr, \
      "The inspiral pipeline does not yet support coincidence between"
  print >> sys.stderr, "all five IFOs. Do not use the analyze-all option."
  sys.exit(1)


print "Setting up an analysis for " + str(ifos) + " from " + \
    str(opts.gps_start_time) + " to "  + str(opts.gps_end_time)
print
sys.stdout.flush()

##############################################################################
# Determine the segments to analyze
inspiralutils.mkdir("segments")
os.chdir("..")
for (vetoes, file) in cp.items("segments"):
  if "veto-file" in vetoes:
    shutil.copy( file, analysisDirectory + "/segments" )
    file = "../segments/" + file.split("/")[-1]
    cp.set("segments", vetoes, file)
os.chdir(analysisDirectory)
os.chdir("segments")

segFile = {}
dqVetoes = {}

for ifo in ifos:
  segFile[ifo], dqVetoes[ifo] = inspiralutils.findSegmentsToAnalyze(cp, ifo, \
      opts.generate_segments, opts.use_available_data, \
      opts.generate_veto_segments)
  # set correct name as we're in the segments directory:
  segFile[ifo] = "../segments/" + segFile[ifo]
  cp.set("input", ifo.lower() + "-segments", segFile[ifo])
  for key in dqVetoes[ifo].keys():
    dqVetoes[ifo][key] = "../segments/" + dqVetoes[ifo][key]


os.chdir("..")

##############################################################################
# Run lalapps_inspiral_hipe for datafind
if opts.run_datafind:
  print "Running inspiral hipe for datafind run"
  hipeDfNode = inspiralutils.hipe_setup("datafind", cp, ifos, \
      opts.log_path, dfOnly = True)
  dag.add_node(hipeDfNode)


##############################################################################
# Set up the directories for each run and run lalapps_inspiral_hipe

cachelist = []
cachefilename = basename + '.cache'

if opts.run_search:

  if opts.run_play_analysis:
    print "Running inspiral hipe for playground run"
    hipePlayNode = inspiralutils.hipe_setup("playground", cp, ifos, \
         opts.log_path, playOnly = True)
    for cacheFile in hipePlayNode.get_output_files():
      cachelist.append("playground/" + cacheFile)
    dag.add_node(hipePlayNode)
    if opts.run_datafind:
      hipePlayNode.add_parent(hipeDfNode)

  if opts.run_full_data:
    print "Running inspiral hipe for analysis run"
    hipeAnalysisNode = inspiralutils.hipe_setup("full_data", cp, ifos, \
        opts.log_path)
    for cacheFile in hipeAnalysisNode.get_output_files():
      cachelist.append("full_data/" + cacheFile)
    dag.add_node(hipeAnalysisNode)
    if opts.run_datafind:
      hipeAnalysisNode.add_parent(hipeDfNode)

  if opts.run_injections:
    hipeInjNode = {} 
    print "Running inspiral hipe for injection runs"
    for (injDir, injSeed) in cp.items("injections"):
      hipeInjNode[injDir] = inspiralutils.hipe_setup(injDir, cp, ifos, \
           opts.log_path, injSeed)
      for cacheFile in hipeInjNode[injDir].get_output_files():
        cachelist.append(injDir + "/" + cacheFile)
      dag.add_node(hipeInjNode[injDir])
      if opts.run_datafind:
        hipeInjNode[injDir].add_parent(hipeDfNode)

##############################################################################
# Run the data quality vetoes
if opts.run_data_quality:
  hipePlayVetoNode = {}
  hipeAnalysisVetoNode = {}
  hipeInjVetoNode = {}

  for category in [2,3,4]:
    print "Setting up the category " + str(category) + " veto dags"

    if opts.run_play_analysis:
      print "Running inspiral hipe for play with vetoes"
      hipePlayVetoNode[category] = inspiralutils.hipe_setup("playground", cp, \
          ifos, opts.log_path, playOnly = True, vetoCat = category, \
          vetoFiles = dqVetoes)
      for cacheFile in hipePlayVetoNode[category].get_output_files():
        cachelist.append("playground/" + cacheFile)
      dag.add_node(hipePlayVetoNode[category])
      if category==2 and opts.run_search: 
        hipePlayVetoNode[category].add_parent(hipePlayNode)
      elif category>2: 
        hipePlayVetoNode[category].add_parent(hipePlayVetoNode[category-1])


    if opts.run_full_data:
      print "Running inspiral hipe for full data with vetoes"
      hipeAnalysisVetoNode[category] = inspiralutils.hipe_setup("full_data", \
          cp, ifos, opts.log_path, vetoCat = category, vetoFiles = dqVetoes)
      for cacheFile in hipeAnalysisVetoNode[category].get_output_files():
        cachelist.append("full_data/" + cacheFile)
      dag.add_node(hipeAnalysisVetoNode[category])
      if category==2 and opts.run_search: 
        hipeAnalysisVetoNode[category].add_parent(hipeAnalysisNode)
      elif category>2: 
        hipeAnalysisVetoNode[category].add_parent( \
            hipeAnalysisVetoNode[category-1])

    if opts.run_injections:
      print "Running inspiral hipe for injections with vetoes"
      for (injDir, injSeed) in cp.items("injections"):
        if category == 2: hipeInjVetoNode[injDir] = {}
        hipeInjVetoNode[injDir][category] = inspiralutils.hipe_setup(injDir, \
            cp, ifos, opts.log_path, \
            injSeed= injSeed, vetoCat = category, vetoFiles = dqVetoes)
        for cacheFile in hipeInjVetoNode[injDir][category].get_output_files():
          cachelist.append(injDir + "/" + cacheFile)
        dag.add_node(hipeInjVetoNode[injDir][category])
        if category==2 and opts.run_search: 
          hipeInjVetoNode[injDir][category].add_parent(hipeInjNode[injDir])
        elif category>2: 
          hipeInjVetoNode[injDir][category].add_parent(\
              hipeInjVetoNode[injDir][category-1])

##############################################################################
# Set up the directories for plotting and run lalapps_plot
if opts.run_plots:
  if opts.run_play_analysis:
    print "Making plots depending only on the playground"
    # single ifo
    plotcp = copy.deepcopy(cp)
    plotcp.add_section("plot-arguments")
    plotcp.set("plot-arguments","plotinspiralrange","")
    plotcp.set("plot-arguments","plotnumtemplates","")
    plotcp.set("plot-arguments","plotinspiral","")
    plotcp.set("plot-arguments","write-script","")
    playPlotNode = inspiralutils.plot_setup("playground_summary_plots", \
        plotcp, opts.log_path, "both", "", "PLAYGROUND", "PLAYGROUND", \
        "PLAYGROUND", "../" + cachefilename, tag="SNGL" )
    dag.add_node(playPlotNode)
    if opts.run_search and opts.run_play_analysis:
      playPlotNode.add_parent(hipePlayNode)

    # coincs
    plotcp = copy.deepcopy(cp)
    plotcp.add_section("plot-arguments")
    plotcp.set("plot-arguments","plotthinca","")
    plotcp.set("plot-arguments","write-script","")
    playPlotCoincNode = inspiralutils.plot_setup("playground_summary_plots", 
        plotcp, opts.log_path, "first", "", "PLAYGROUND", "PLAYGROUND", \
        "PLAYGROUND", "../" + cachefilename, tag="COINC" )
    dag.add_node(playPlotCoincNode)
    if opts.run_search and opts.run_play_analysis:
      playPlotCoincNode.add_parent(hipePlayNode)

    # coincs second (require DQ)
    plotcp = copy.deepcopy(cp)
    plotcp.add_section("plot-arguments")
    plotcp.set("plot-arguments","plotthinca","")
    plotcp.set("plot-arguments","write-script","")
    playPlotVetoNode = inspiralutils.plot_setup("playground_summary_plots", 
        plotcp, opts.log_path, "second", "", "PLAYGROUND_CAT_2_VETO", \
        "PLAYGROUND_CAT_2_VETO", "PLAYGROUND_CAT_2_VETO", \
        "../" + cachefilename, tag="COINC_CAT_2" )
    dag.add_node(playPlotVetoNode)
    if opts.run_data_quality and opts.run_play_analysis:
      playPlotVetoNode.add_parent(hipePlayVetoNode[2])


##############################################################################
# Set up the directories for followup and run lalapps_followup_pipe
if opts.run_followup:
  followupNode = inspiralutils.followup_setup("followup", cp, opts, "full_data")
  # FIXME: the cache files need to be appended to cachelist
  dag.add_node(followupNode)
  if opts.run_search:
    followupNode.add_parent(hipeAnalysisNode)


##############################################################################
# Write the dag and sub files
dag.write_sub_files()
dag.write_dag()

print 
print "  Created a DAG file which can be submitted by executing"
print "\n    cd " + analysisDirectory
print "    condor_submit_dag " + dag.get_dag_file()
print """\n  from a condor submit machine
  Before submitting the dag, you must execute

    export _CONDOR_DAGMAN_LOG_ON_NFS_IS_ERROR=FALSE

  If you are running LSCdataFind jobs, do not forget to initialize your grid
  proxy certificate on the condor submit machine by running the commands
  
    unset X509_USER_PROXY
    grid-proxy-init -hours 72
  
  Enter your pass phrase when prompted. The proxy will be valid for 72 hours.
  If you expect the LSCdataFind jobs to take longer to complete, increase the
  time specified in the -hours option to grid-proxy-init. You can check that
  the grid proxy has been sucessfully created by executing the command:
  
    grid-cert-info -all -file /tmp/x509up_u`id -u`
  
  This will also give the expiry time of the proxy."""

##############################################################################
# cat the cache files together


if len(cachelist):
  command = "cat " + " ".join(cachelist) + " > " + cachefilename
  os.popen(command)

##############################################################################
# write out a log file for this script
log_fh = open(basename + '.pipeline.log', 'w')
  
log_fh.write( "$Id$" \
    + "\n" )
log_fh.write( "$Name$" + "\n\n" )
log_fh.write( "Invoked with arguments:" )
for arg in command_line:
  if arg[0] == '-':
    log_fh.write( "\n" )
  log_fh.write( arg + ' ')

log_fh.write( "\n" )
log_fh.write( "Config file has CVS strings:\n" )
log_fh.write( cp.get('pipeline','version') + "\n" )
log_fh.write( cp.get('pipeline','cvs-tag') + "\n\n" )

##############################################################################
