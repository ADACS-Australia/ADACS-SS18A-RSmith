#!/usr/bin/python
#
# $Id$
#
# Copyright (C) 2005  Kipp C. Cannon
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#

"""
Pipeline generation script for the excess power online analysis.
"""

import ConfigParser
import math
from optparse import OptionParser
import os
import stat
import sys
import tempfile
import time

from glue import lal
from glue import pipeline
from glue import segments
from glue import segmentsUtils
import power

__author__ = "Kipp Cannon <kipp@gravity.phys.uwm.edu>"
__date__ = "$Date$"[11:-2]
__version__ = "$Revision$"[7:-2]


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#

parser = OptionParser(version="%prog CVS $Id$")
parser.add_option("-s", metavar = "GPSSECONDS", dest = "data_start", help = "set data segment start time")
parser.add_option("-e", metavar = "GPSSECONDS", dest = "data_end", help = "set data segment end time")
parser.add_option("-a", metavar = "GPSSECONDS", dest = "trig_start", help = "set analysis segment start time")
parser.add_option("-b", metavar = "GPSSECONDS", dest = "trig_end", help = "set analysis segment end time")
parser.add_option("-f", metavar = "FILENAME", dest = "dag_name", help = "set output .dag file name")
parser.add_option("-t", metavar = "PATH", dest = "aux_dir", help = "set auxiliary data directory")
parser.add_option("--condor-log-dir", metavar = "PATH", help = "set directory for Condor log")
parser.add_option("--config-file", metavar = "FILENAME", default = "online_power.ini", help = "set .ini config file name")
parser.add_option("--instrument", metavar = "INSTRUMENT", help = "set instrument name (default = value of instrument variable in [pipeline] section of .ini file)")
parser.add_option("--publish-dest", metavar = "PATH", help = "set directory for output triggers")
parser.add_option("--dmtmon-dest", metavar = "PATH", help = "set directory for DMT monitor output")
parser.add_option("--gsiscp-dest", metavar = "PATH", help = "set destination for gsiscp")
parser.add_option("--user-tag", metavar = "TAG", help = "set user tag on jobs that need it")
parser.add_option("--calibration-cache", metavar = "FILENAME", help = "set name of calibration cache relative to auxiliary data directory")
options, args = parser.parse_args()
del parser, args

# data segment
try:
	options.seg = segments.segment(lal.LIGOTimeGPS(options.data_start), lal.LIGOTimeGPS(options.data_end))
except:
	raise Exception, "failure parsing -s and/or -e; try --help"

# trigger segment
try:
	options.trig_seg = segments.segment(lal.LIGOTimeGPS(options.trig_start), lal.LIGOTimeGPS(options.trig_end))
except:
	raise Exception, "failure parsing -a and/or -b; try --help"
if options.trig_seg not in options.seg:
	raise Exception, "trigger segment not contained in data segment!"

# .dag name
try:
	options.dag_name = os.path.splitext(options.dag_name)[0]
except:
	raise Exception, "failure parsing -f; try --help"

# .ini file config parser
options.config_file = os.path.join(options.aux_dir, options.config_file)
try:
	config_parser = ConfigParser.SafeConfigParser()
	config_parser.read(options.config_file)
except:
	raise Exception, "failure parsing -t and/or --config-file; try --help"

# Condor log file
try:
	handle, condor_log = tempfile.mkstemp(".log", "power_", options.condor_log_dir)
	del handle
except:
	raise Exception, "failure parsing --condor-log-dir; try --help"

# instrument
try:
	if options.instrument:
		config_parser.set('pipeline', 'instrument', options.instrument)
	else:
		options.instrument = config_parser.get('pipeline', 'instrument')
except Exception, e:
	raise Exception, "must either provide --instrument, or instrument variable in [pipeline] section of .ini: %s" % str(e)

# publish location
try:
	if options.publish_dest:
		config_parser.set('publish', 'destination', options.publish_dest)
	else:
		options.publish_dest = config_parser.get('publish', 'destination')
except Exception, e:
	raise Exception, "must either provide --publish-dest, or destination variable in [publish] section of .ini: %s" % str(e)

# gsiscp location
try:
	if options.gsiscp_dest:
		config_parser.set('gsiscp', 'destination', options.gsiscp_dest)
	else:
		options.gsiscp_dest = config_parser.get('gsiscp', 'destination')
except Exception, e:
	raise Exception, "must either provide --gsiscp-dest, or destination variable in [gsiscp] section of .ini: %s" % str(e)

# user tag
try:
	if options.user_tag:
		config_parser.set('pipeline', 'user-tag', options.user_tag)
	else:
		options.user_tag = config_parser.get('pipeline', 'user-tag')
except Exception, e:
	raise Exception, "must either provide --user-tag, or user-tag variable in [pipeline] section of .ini: %s" % str(e)

# calibration cache
try:
	if options.calibration_cache:
		config_parser.set('pipeline', 'calibration-cache', options.calibration_cache)
	else:
		options.calibration_cache = config_parser.get('pipeline', 'calibration-cache')
except Exception, e:
	raise Exception, "must either provide --calibration-cache, or calibration-cache variable in [pipeline] section of .ini: %s" % str(e)
options.calibration_cache = os.path.join(options.aux_dir, options.calibration_cache)


#
# =============================================================================
#
#                              Static Parameters
#
# =============================================================================
#

cache_dir = "cache/"
out_dir = "logs/"
if options.seg.duration() < 3600:
	psds_per_job = 4
	psds_per_injection = 4
else:
	psds_per_job = 16
	psds_per_injection = 16
# do this injections beteen flow and fhigh
num_injections = 22


#
# =============================================================================
#
#                               Custom Job Types
#
# =============================================================================
#

class PublishJob(pipeline.CondorDAGJob):
	def __init__(self, out_dir):
		pipeline.CondorDAGJob.__init__(self, "vanilla", config_parser.get("condor", "publish"))
		self.set_stdout_file(os.path.join(out_dir, "publish-$(cluster)-$(process).out"))
		self.set_stderr_file(os.path.join(out_dir, "publish-$(cluster)-$(process).err"))
		self.set_sub_file("publish.sub")

	def write_sub_file(self):
		# this is a hack, but I can't be bothered...
		self.add_file_arg("$(macrodestination)")
		pipeline.CondorDAGJob.write_sub_file(self)


class PublishNode(pipeline.CondorDAGNode):
	def set_input(self, source):
		self.add_file_arg(source)

	def get_input(self):
		return self.get_input_files()

	def set_output(self, destination):
		self.__output = destination
		self.add_macro("macrodestination", destination)

	def get_output(self):
		return self.__output


class GsiScpJob(pipeline.CondorDAGJob):
	def __init__(self, out_dir):
		pipeline.CondorDAGJob.__init__(self, "scheduler", "/ldcg/stow_pkgs/ldg-3.5/ldg/globus/bin/gsiscp")
		self.set_stdout_file(os.path.join(out_dir, "gsiscp-$(cluster)-$(process).out"))
		self.set_stderr_file(os.path.join(out_dir, "gsiscp-$(cluster)-$(process).err"))
		self.set_sub_file("gsiscp.sub")
		self.add_condor_cmd("getenv", "True")

	def write_sub_file(self):
		# this is a hack, but I can't be bothered...
		self.add_file_arg("$(macrodestination)")
		pipeline.CondorDAGJob.write_sub_file(self)


class GsiScpNode(pipeline.CondorDAGNode):
	def set_output(self, destination):
		self.__output = destination
		self.add_macro("macrodestination", destination)

	def get_output(self):
		return self.__output


class Burst2MonJob(pipeline.CondorDAGJob):
	def __init__(self, out_dir, config_parser):
		pipeline.CondorDAGJob.__init__(self, "vanilla", config_parser.get("condor", "ligolw_burst2mon"))
		self.set_sub_file("ligolw_burst2mon.sub")
		self.set_stdout_file(os.path.join(out_dir, "ligolw_burst2mon-$(cluster)-$(process).out"))
		self.set_stderr_file(os.path.join(out_dir, "ligolw_burst2mon-$(cluster)-$(process).err"))
		self.add_condor_cmd("getenv", "True")
		self.add_ini_opts(config_parser, "ligolw_burst2mon")


class Burst2MonNode(pipeline.CondorDAGNode, pipeline.AnalysisNode):
	pass


class BuclusterJob(pipeline.CondorDAGJob):
	def __init__(self, out_dir, config_parser):
		pipeline.CondorDAGJob.__init__(self, "vanilla", config_parser.get("condor", "ligolw_bucluster"))
		self.set_sub_file("ligolw_bucluster.sub")
		self.set_stdout_file(os.path.join(out_dir, "ligolw_bucluster-$(cluster)-$(process).out"))
		self.set_stderr_file(os.path.join(out_dir, "ligolw_bucluster-$(cluster)-$(process).err"))
		self.add_condor_cmd("getenv", "True")
		self.add_ini_opts(config_parser, "ligolw_bucluster")


class BuclusterNode(pipeline.CondorDAGNode, pipeline.AnalysisNode):
	pass


class BinjfindJob(pipeline.CondorDAGJob):
	def __init__(self, out_dir, config_parser):
		pipeline.CondorDAGJob.__init__(self, "vanilla", config_parser.get("condor", "ligolw_binjfind"))
		self.set_sub_file("ligolw_binjfind.sub")
		self.set_stdout_file(os.path.join(out_dir, "ligolw_binjfind-$(cluster)-$(process).out"))
		self.set_stderr_file(os.path.join(out_dir, "ligolw_binjfind-$(cluster)-$(process).err"))
		self.add_condor_cmd("getenv", "True")
		self.add_ini_opts(config_parser, "ligolw_binjfind")


class BinjfindNode(pipeline.CondorDAGNode, pipeline.AnalysisNode):
	pass


class TisiJob(pipeline.CondorDAGJob):
	def __init__(self, out_dir, config_parser):
		pipeline.CondorDAGJob.__init__(self, "vanilla", config_parser.get("condor", "ligolw_tisi"))
		self.set_sub_file("ligolw_tisi.sub")
		self.set_stdout_file(os.path.join(out_dir, "ligolw_tisi-$(cluster)-$(process).out"))
		self.set_stderr_file(os.path.join(out_dir, "ligolw_tisi-$(cluster)-$(process).err"))
		self.add_condor_cmd("getenv", "True")
		self.add_ini_opts(config_parser, "ligolw_tisi")


class TisiNode(pipeline.CondorDAGNode, pipeline.AnalysisNode):
	pass


#
# =============================================================================
#
#                              Define .sub Files
#
# =============================================================================
#

# LSCdataFind
lscdatafindjob = pipeline.LSCDataFindJob(cache_dir, out_dir, config_parser)

# lalapps_binj
binjjob = power.BurstInjJob(config_parser)

# lalapps_power
powerjob = power.PowerJob(out_dir, config_parser)

# ligolw_add
lladdjob = pipeline.LigolwAddJob(out_dir, config_parser)

# ligolw_tisi
tisijob = TisiJob(out_dir, config_parser)

# ligolw_binjfind
binjfindjob = BinjfindJob(out_dir, config_parser)

# ligolw_bucluster
buclusterjob = BuclusterJob(out_dir, config_parser)

# ligolw_burst2mon
llb2mjob = Burst2MonJob(out_dir, config_parser)

# publish
publishjob = PublishJob(out_dir)

# gsiscp
gsiscpjob = GsiScpJob(out_dir)


#
# =============================================================================
#
#                                 Segmentation
#
# =============================================================================
#

def split_segment(psds):
	"""
	Split the data segment into correctly-overlaping segments.  We try
	to have the numbers of PSDs in each segment be equal to psds, but
	with a short segment at the end if needed.
	"""
	psd_length = float(powerjob.get_opts()["psd-average-points"]) / float(powerjob.get_opts()["resample-rate"])
	window_length = float(powerjob.get_opts()["window-length"]) / float(powerjob.get_opts()["resample-rate"])
	window_shift = float(powerjob.get_opts()["window-shift"]) / float(powerjob.get_opts()["resample-rate"])
	filter_corruption = float(powerjob.get_opts()["filter-corruption"]) / float(powerjob.get_opts()["resample-rate"])

	psd_overlap = window_length - window_shift

	joblength = psds * (psd_length - psd_overlap) + psd_overlap + 2 * filter_corruption
	joboverlap = 2 * filter_corruption + psd_overlap

	# can't use range() with non-integers
	segs = segments.segmentlist()
	t = options.seg[0]
	while t < options.seg[1] - joboverlap:
		segs.append(segments.segment(t, t + joblength) & options.seg)
		t += joblength - joboverlap
	return segs

jobsegs = split_segment(psds_per_job)
injsegs = split_segment(psds_per_injection)

print >>sys.stderr, "Segment split: " + str(jobsegs)
print >>sys.stderr, "Injections split: " + str(injsegs)


#
# =============================================================================
#
#                           LSCdataFind DAG Fragment
#
# =============================================================================
#

def make_datafind_fragment(dag, observatory, seg):
	datafind_pad = 512

	node = pipeline.LSCDataFindNode(lscdatafindjob)
	node.set_name("LSCdataFind")
	node.set_start(options.seg[0] - datafind_pad)
	node.set_end(options.seg[1] + 1)
	node.set_observatory(observatory)
	dag.add_node(node)

	return node


#
# =============================================================================
#
#                          lalapps_power DAG Fragment
#
# =============================================================================
#

def make_power_fragment(dag, parents, framecache, seg, tag, injargs = {}):
	suffix = "%s-%s-%s" % (tag, int(seg[0]), int(seg.duration()))

	node = power.PowerNode(powerjob)
	node.set_name("lalapps_power-%s" % suffix)
	map(node.add_parent, parents)
	node.set_cache(framecache)
	node.set_ifo(options.instrument)
	node.set_start(seg[0])
	node.set_end(seg[1])
	node.set_user_tag(tag)
	for arg, value in injargs.iteritems():
		# this is a hack, but I can't be bothered
		node.add_var_arg("--%s %s" % (arg, value))
	dag.add_node(node)

	node.set_post_script(config_parser.get("condor", "fix_sngl_burst"))
	node.add_post_script_arg(node.get_output())

	return node


#
# =============================================================================
#
#                           ligolw_add DAG Fragment
#
# =============================================================================
#

def make_lladd_fragment(dag, parents, seg, tag):
	cache_name = os.path.join(cache_dir, "lladd-%s.cache" % tag)
	cachefile = file(cache_name, "w")

	node = pipeline.LigolwAddNode(lladdjob)
	node.set_name("lladd-%s" % tag)
	for parent in parents:
		node.add_parent(parent)
		cache = lal.CacheEntry()
		cache.observatory = "ANY"
		cache.description = "EMPTY"
		cache.segment = seg
		cache.url = "file://localhost" + os.path.join(os.getcwd(), parent.get_output())
		print >>cachefile, str(cache)
	node.add_var_opt("input-cache", cache_name)
	dag.add_node(node)

	return node


#
# =============================================================================
#
#        DAG Fragment Combining Multiple lalapps_power With ligolw_add
#
# =============================================================================
#

def make_multipower_fragment(dag, powerparents, lladdparents, framecache, seglist, tag, injargs = {}):
	node = make_lladd_fragment(dag, [make_power_fragment(dag, powerparents, framecache, seg, tag, injargs) for seg in seglist] + lladdparents, seglist.extent(), "POWER_%s" % tag)
	node.set_output("%s-POWER_%s-%s-%s.xml" % (options.instrument, tag, int(seglist.extent()[0]), int(seglist.extent().duration())))
	return node


#
# =============================================================================
#
#                             Publish DAG Fragment
#
# =============================================================================
#

def make_publish_fragment(dag, tag, parent, destination):
	start = segmentsUtils.fromfilenames([parent.get_output()])[0][0]
	destination = os.path.join(destination, "%d/%d" % (start/1000000, start/100000))
	try:
		os.makedirs(destination)
	except OSError, e:
		# errno 17 == "file exists"
		if e.errno != 17:
			raise e
	node = PublishNode(publishjob)
	node.set_name("publish-%s" % tag)
	node.add_parent(parent)
	node.set_input(parent.get_output())
	node.set_output(destination)
	dag.add_node(node)

	return node


#
# =============================================================================
#
#                       DMT Monitor Output DAG Fragment
#
# =============================================================================
#

def make_burst2mon_fragment(dag, parent, seg, tag):
	cluster = BuclusterNode(buclusterjob)
	cluster.set_name("ligolw_bucluster-POWERMON")
	cluster.add_parent(parent)
	cluster.set_input(parent.get_output())
	cluster.set_output("%s-POWERMON_%s-%s-%s.xml" % (options.instrument, tag, int(seg[0]), int(seg.duration())))
	cluster.add_macro("macrocomment", tag)
	dag.add_node(cluster)

	node = Burst2MonNode(llb2mjob)
	node.set_name("ligolw_burst2mon")
	node.add_parent(cluster)
	node.set_input(cluster.get_output())
	node.set_output(os.path.join(options.dmtmon_dest, cluster.get_output()))
	node.add_macro("macrocomment", tag)
	dag.add_node(node)

	return node


#
# =============================================================================
#
#                             GSISCP DAG Fragment
#
# =============================================================================
#

def make_gsiscp_fragment(dag, parents, destination):
	node = GsiScpNode(gsiscpjob)
	node.set_name("gsiscp")
	for parent in parents:
		node.add_parent(parent)
		node.add_file_arg(parent.get_output())
	node.set_output(destination)
	dag.add_node(node)

	return node


#
# =============================================================================
#
#                          lalapps_binj DAG Fragment
#
# =============================================================================
#

def make_binj_fragment(dag, tag, seg, offset, flow, fhigh, fratio):
	# one injection every time-step / pi seconds
	period = num_injections * float(binjjob.get_opts()["time-step"]) / math.pi
	start = seg[0] - seg[0] % period + period * offset

	node = power.BurstInjNode(binjjob)
	node.set_start(start)
	node.set_end(seg[1])
	node.set_name("lalapps_binj-%d-%d" % (int(node.get_start()), int(flow)))
	node.set_user_tag(tag)
	node.add_macro("macroflow", flow)
	node.add_macro("macrofhigh", fhigh)
	node.add_macro("macrofratio", fratio)
	node.add_macro("macroseed", int(time.time() + node.get_start()))
	dag.add_node(node)

	return node


#
# =============================================================================
#
#         DAG Fragment Combining Multiple lalapps_binj With ligolw_add
#
# =============================================================================
#

def make_multibinj_fragment(dag, tag, seg):
	flow = float(powerjob.get_opts()["low-freq-cutoff"])
	fhigh = flow + float(powerjob.get_opts()["bandwidth"])

	# determine frequency ratio from number of injections across band
	# (take a bit off to allow fhigh to be picked up despite round-off
	# errors)
	fratio = 0.9999999 * (fhigh / flow) ** (1.0/(num_injections - 1.0))

	binjnodes = [make_binj_fragment(dag, tag, seg, 0.0, flow, fhigh, fratio)]

	node = make_lladd_fragment(dag, binjnodes, seg, tag)
	node.set_output("HL-%s-%d-%d.xml" % (tag, int(seg[0]), int(seg.duration())))

	return node


#
# =============================================================================
#
#                           ligolw_tisi DAG Fragment
#
# =============================================================================
#

def make_tisi_fragment(dag, tag):
	node = TisiNode(tisijob)
	node.set_name("ligolw_tisi")
	node.set_output("TISI_%s.xml" % tag)
	node.add_macro("macrocomment", tag)
	dag.add_node(node)

	return node


#
# =============================================================================
#
#                         ligolw_binjfind DAG Fragment
#
# =============================================================================
#

def make_binjfind_fragment(dag, parent, tag):
	cluster = BuclusterNode(buclusterjob)
	cluster.set_name("ligolw_bucluster-INJECTIONS")
	cluster.add_parent(parent)
	cluster.set_input(parent.get_output())
	cluster.set_output(parent.get_output())
	cluster.add_macro("macrocomment", tag)
	dag.add_node(cluster)

	binjfind = BinjfindNode(binjfindjob)
	binjfind.set_name("ligolw_binjfind")
	binjfind.add_parent(cluster)
	binjfind.set_input(cluster.get_output())
	binjfind.set_output(cluster.get_output())
	binjfind.add_macro("macrocomment", tag)
	dag.add_node(binjfind)

	return binjfind


#
# =============================================================================
#
#                               DAG Construction
#
# =============================================================================
#

os.mkdir(cache_dir)
os.mkdir(out_dir)

dag = pipeline.CondorDAG(condor_log)
dag.set_dag_file(options.dag_name)

lscdatafindnode = make_datafind_fragment(dag, options.instrument[0], options.seg)

darmpowerfrag = make_multipower_fragment(dag, [lscdatafindnode], [], lscdatafindnode.get_output(), jobsegs, options.user_tag)

make_publish_fragment(dag, "POWER", darmpowerfrag, options.publish_dest)
if options.dmtmon_dest:
	make_burst2mon_fragment(dag, darmpowerfrag, jobsegs.extent(), options.user_tag)

binjfrag = make_multibinj_fragment(dag, "INJECTIONS_%s" % options.user_tag, injsegs.extent())
tisifrag = make_tisi_fragment(dag, "INJECTIONS_%s" % options.user_tag)
injpowerfrag = make_multipower_fragment(dag, [lscdatafindnode, binjfrag], [binjfrag, tisifrag], lscdatafindnode.get_output(), injsegs, "INJECTIONS_%s" % options.user_tag, injargs = {"burstinjection-file": binjfrag.get_output(), "calibration-cache": options.calibration_cache})
binjfindfrag = make_binjfind_fragment(dag, injpowerfrag, "INJECTIONS_%s" % options.user_tag)
make_publish_fragment(dag, "INJECTIONS", binjfindfrag, options.publish_dest)

make_gsiscp_fragment(dag, [darmpowerfrag, binjfindfrag], options.gsiscp_dest)

dag.write_sub_files()
dag.write_dag()

print options.trig_seg[0], options.trig_seg[1]
