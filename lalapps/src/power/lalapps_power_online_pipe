#!/usr/bin/python

#
# File info
#

"""
Pipeline generation script for the excess power online analysis.
"""

__author__ = "Kipp Cannon <kipp@gravity.phys.uwm.edu>"
__date__ = "$Date$"
__version__ = "$Revision$"


#
# Preamble
#

import ConfigParser
from optparse import OptionParser
import os
import sys
import tempfile

from glue import lal
from glue import pipeline
from glue import segments
from pylal import lalcache
import power


#
# Parse command line
#

parser = OptionParser(version="%prog CVS $Id$")
parser.add_option("-s", metavar = "GPSSECONDS", dest = "data_start", help = "set data segment start time")
parser.add_option("-e", metavar = "GPSSECONDS", dest = "data_end", help = "set data segment end time")
parser.add_option("-a", metavar = "GPSSECONDS", dest = "trig_start", help = "set analysis segment start time")
parser.add_option("-b", metavar = "GPSSECONDS", dest = "trig_end", help = "set analysis segment end time")
parser.add_option("-f", metavar = "FILENAME", dest = "dag_name", help = "set output .dag file name")
parser.add_option("-t", metavar = "PATH", dest = "aux_dir", help = "set auxiliary data directory")
parser.add_option("--condor-log-dir", metavar = "PATH", help = "set directory for Condor log")
parser.add_option("--config-file", metavar = "FILENAME", default = "online_power.ini", help = "set .ini config file name")
parser.add_option("--instrument", metavar = "INSTRUMENT", help = "set instrument name (default = value of instrument variable in [pipeline] section of .ini file)")
parser.add_option("--publish-dest", metavar = "PATH", help = "set directory for output triggers")
parser.add_option("--gsiscp-dest", metavar = "PATH", help = "set destination for gsiscp")
options, args = parser.parse_args()
del parser, args

# data segment
try:
	options.seg = segments.segment(lal.LIGOTimeGPS(options.data_start), lal.LIGOTimeGPS(options.data_end))
except:
	raise Exception, "failure parsing -s and/or -e; try --help"

# trigger segment
try:
	options.trig_seg = segments.segment(lal.LIGOTimeGPS(options.trig_start), lal.LIGOTimeGPS(options.trig_end))
except:
	raise Exception, "failure parsing -a and/or -b; try --help"
if options.trig_seg not in options.seg:
	raise Exception, "trigger segment not contained in data segment!"

# .dag name
try:
	options.dag_name = os.path.splitext(options.dag_name)[0]
except:
	raise Exception, "failure parsing -f; try --help"

# .ini file config parser
try:
	config_parser = ConfigParser.SafeConfigParser()
	config_parser.read(os.path.join(options.aux_dir, options.config_file))
except:
	raise Exception, "failure parsing -t and/or --config-file; try --help"

# Condor log file
try:
	handle, condor_log = tempfile.mkstemp(".log", "power_", options.condor_log_dir)
	del handle
except:
	raise Exception, "failure parsing --condor-log-dir; try --help"

# instrument
try:
	if options.instrument:
		config_parser.set('pipeline', 'instrument', options.instrument)
	else:
		options.instrument = config_parser.get('pipeline','instrument')
except:
	raise Exception, "must either provide --instrument, or instrument variable in [pipeline] section of .ini"

# publish location
try:
	if options.publish_dest:
		config_parser.set('publish', 'destination', options.publish_dest)
	else:
		options.publish_dest = config_parser.get('publish','destination')
except:
	raise Exception, "must either provide --publish-dest, or destination variable in [publish] section of .ini"

# gsiscp location
try:
	if options.gsiscp_dest:
		config_parser.set('gsiscp', 'destination', options.gsiscp_dest)
	else:
		options.publish_dest = config_parser.get('gsiscp', 'destination')
except:
	raise Exception, "must either provide --gsiscp-dest, or destination variable in [gsiscp] section of .ini"


#
# Some stuff used in DAG layout and construction
#

datafind_pad = 512
cache_dir = "cache/"
out_dir = "logs/"
psds_per_job = 2
observatory = options.instrument[0]
overlap = 0.25
lladdcachename = "triggers.cache"


#
# Define Condor job types (some are also found in pipeline.py and power.py)
#

class PublishJob(pipeline.CondorDAGJob, pipeline.AnalysisJob):
	def __init__(self, out_dir, config_parser):
		pipeline.CondorDAGJob.__init__(self, "scheduler", "/bin/cp")
		pipeline.AnalysisJob.__init__(self, config_parser)
		self.destination = config_parser.get("publish", "destination")
		self.set_stdout_file(os.path.join(out_dir, "publish-$(cluster)-$(process).out"))
		self.set_stderr_file(os.path.join(out_dir, "publish-$(cluster)-$(process).err"))
		self.set_sub_file("publish.sub")

	def write_sub_file(self):
		# this is a hack, but I can't be bothered...
		self.add_file_arg(self.destination)
		pipeline.CondorDAGJob.write_sub_file(self)


class GsiScpJob(pipeline.CondorDAGJob, pipeline.AnalysisJob):
	def __init__(self, out_dir, config_parser):
		pipeline.CondorDAGJob.__init__(self, "scheduler", "/ldcg/stow_pkgs/ldg-3.5/ldg/globus/bin/gsiscp")
		pipeline.AnalysisJob.__init__(self, config_parser)
		self.destination = config_parser.get("gsiscp", "destination")
		self.set_stdout_file(os.path.join(out_dir, "gsiscp-$(cluster)-$(process).out"))
		self.set_stderr_file(os.path.join(out_dir, "gsiscp-$(cluster)-$(process).err"))
		self.set_sub_file("gsiscp.sub")
		self.add_condor_cmd("getenv", "True")

	def write_sub_file(self):
		# this is a hack, but I can't be bothered...
		self.add_file_arg(self.destination)
		pipeline.CondorDAGJob.write_sub_file(self)


#
# Define DAG node types (some are also found in pipeline.py and power.py)
#

class PublishNode(pipeline.CondorDAGNode):
	def __init__(self, job):
		pipeline.CondorDAGNode.__init__(self, job)
		#self._CondorDAGNode__arguments.append(job.destination)


class GsiScpNode(pipeline.CondorDAGNode):
	pass


#
# Create Condor job types (define the .sub files)
#

# LSCdataFind
lscdatafindjob = pipeline.LSCDataFindJob(cache_dir, out_dir, config_parser)

# lalapps_power
powerjob = power.PowerJob(out_dir, config_parser)

# lalapps_lladd
lladdjob = power.LladdJob(cache_dir, out_dir, config_parser)

# publish
publishjob = PublishJob(out_dir, config_parser)

# gsiscp
gsiscpjob = GsiScpJob(out_dir, config_parser)


#
# Split the data segment.  We try to split the data segment into jobs of
# length joblength, but with a short one if needed.
#

psd_length = float(powerjob.get_opts()["psd-average-points"]) / float(powerjob.get_opts()["resample-rate"])
window_length = float(powerjob.get_opts()["window-length"]) / float(powerjob.get_opts()["resample-rate"])
window_shift = float(powerjob.get_opts()["window-shift"]) / float(powerjob.get_opts()["resample-rate"])
filter_corruption = float(powerjob.get_opts()["filter-corruption"]) / float(powerjob.get_opts()["resample-rate"])

psd_overlap = window_length - window_shift

joblength = psds_per_job * (psd_length - psd_overlap) + psd_overlap + 2 * filter_corruption
joboverlap = 2 * filter_corruption + psd_overlap

# can't use range() with non-integers
jobsegs = segments.segmentlist()
t = options.seg[0]
while t < options.seg[1] - joboverlap:
	jobsegs.append(segments.segment(t, t + joblength) & options.seg)
	t += joblength - joboverlap

print >>sys.stderr, "Segment split: " + str(jobsegs)


#
# Construct the DAG
#

dag = pipeline.CondorDAG(condor_log)
dag.set_dag_file(options.dag_name)

lscdatafindnode = pipeline.LSCDataFindNode(lscdatafindjob)
lscdatafindnode.set_name("LSCdataFind")
lscdatafindnode.set_start(options.seg[0] - datafind_pad)
lscdatafindnode.set_end(options.seg[1] + 1)
lscdatafindnode.set_observatory(observatory)
dag.add_node(lscdatafindnode)

lladdnode = power.LladdNode(lladdjob)
lladdnode.set_name("lalapps_lladd")
lladdnode.add_var_opt("input-cache", lladdcachename)
dag.add_node(lladdnode)

lladdcache = file(lladdcachename, "w")
for seg in jobsegs:
	powernode = power.PowerNode(powerjob)
	powernode.set_name("lalapps_power-" + str(seg[0]) + "-" + str(seg.duration()))
	powernode.set_cache(lscdatafindnode.get_output())
	powernode.set_ifo(options.instrument)
	powernode.set_start(seg[0])
	powernode.set_end(seg[1])
	powernode.add_parent(lscdatafindnode)
	lladdnode.add_parent(powernode)
	lladdnode.add_input_file(powernode.get_output())

	cache = lalcache.CacheEntry()
	cache.observatory = observatory
	cache.description = lladdnode.basename
	cache.segment = seg
	cache.url = "file://localhost" + os.path.join(os.getcwd(), powernode.get_output())
	print >>lladdcache, str(cache)

	dag.add_node(powernode)

publishnode = PublishNode(publishjob)
publishnode.set_name("publish")
publishnode.add_parent(lladdnode)
publishnode.add_file_arg(lladdnode.get_output())
dag.add_node(publishnode)

gsiscpnode = GsiScpNode(gsiscpjob)
gsiscpnode.set_name("gsiscp")
gsiscpnode.add_parent(lladdnode)
gsiscpnode.add_file_arg(lladdnode.get_output())
dag.add_node(gsiscpnode)

dag.write_sub_files()
dag.write_dag()

os.mkdir(cache_dir)
os.mkdir(out_dir)

print options.trig_seg[0] + overlap, options.trig_seg[1] - overlap
