#!/usr/bin/python
#
# $Id$
#
# Copyright (C) 2005  Kipp C. Cannon
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#

"""
Pipeline generation script for the excess power online analysis.
"""

import ConfigParser
import math
from optparse import OptionParser
import os
import stat
import sys
import tempfile

from glue import lal
from glue import pipeline
from glue import segments
import power

__author__ = "Kipp Cannon <kipp@gravity.phys.uwm.edu>"
__date__ = "$Date$"[11:-2]
__version__ = "$Revision$"[7:-2]


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#

parser = OptionParser(version="%prog CVS $Id$")
parser.add_option("-s", metavar = "GPSSECONDS", dest = "data_start", help = "set data segment start time")
parser.add_option("-e", metavar = "GPSSECONDS", dest = "data_end", help = "set data segment end time")
parser.add_option("-a", metavar = "GPSSECONDS", dest = "trig_start", help = "set analysis segment start time")
parser.add_option("-b", metavar = "GPSSECONDS", dest = "trig_end", help = "set analysis segment end time")
parser.add_option("-f", metavar = "FILENAME", dest = "dag_name", help = "set output .dag file name")
parser.add_option("-t", metavar = "PATH", dest = "aux_dir", help = "set auxiliary data directory")
parser.add_option("--condor-log-dir", metavar = "PATH", help = "set directory for Condor log")
parser.add_option("--config-file", metavar = "FILENAME", default = "online_power.ini", help = "set .ini config file name")
parser.add_option("--instrument", metavar = "INSTRUMENT", help = "set instrument name (default = value of instrument variable in [pipeline] section of .ini file)")
parser.add_option("--publish-dest", metavar = "PATH", help = "set directory for output triggers")
parser.add_option("--dmtmon-dest", metavar = "PATH", help = "set directory for DMT monitor output")
parser.add_option("--gsiscp-dest", metavar = "PATH", help = "set destination for gsiscp")
parser.add_option("--user-tag", metavar = "TAG", help = "set user tag on jobs that need it")
options, args = parser.parse_args()
del parser, args

# data segment
try:
	options.seg = segments.segment(lal.LIGOTimeGPS(options.data_start), lal.LIGOTimeGPS(options.data_end))
except:
	raise Exception, "failure parsing -s and/or -e; try --help"

# trigger segment
try:
	options.trig_seg = segments.segment(lal.LIGOTimeGPS(options.trig_start), lal.LIGOTimeGPS(options.trig_end))
except:
	raise Exception, "failure parsing -a and/or -b; try --help"
if options.trig_seg not in options.seg:
	raise Exception, "trigger segment not contained in data segment!"

# .dag name
try:
	options.dag_name = os.path.splitext(options.dag_name)[0]
except:
	raise Exception, "failure parsing -f; try --help"

# .ini file config parser
options.config_file = os.path.join(options.aux_dir, options.config_file)
try:
	config_parser = ConfigParser.SafeConfigParser()
	config_parser.read(options.config_file)
except:
	raise Exception, "failure parsing -t and/or --config-file; try --help"

# Condor log file
try:
	handle, condor_log = tempfile.mkstemp(".log", "power_", options.condor_log_dir)
	del handle
except:
	raise Exception, "failure parsing --condor-log-dir; try --help"

# instrument
try:
	if options.instrument:
		config_parser.set('pipeline', 'instrument', options.instrument)
	else:
		options.instrument = config_parser.get('pipeline', 'instrument')
except Exception, e:
	raise Exception, "must either provide --instrument, or instrument variable in [pipeline] section of .ini: %s" % str(e)

# publish location
try:
	if options.publish_dest:
		config_parser.set('publish', 'destination', options.publish_dest)
	else:
		options.publish_dest = config_parser.get('publish', 'destination')
except Exception, e:
	raise Exception, "must either provide --publish-dest, or destination variable in [publish] section of .ini: %s" % str(e)

# dmtmon location
try:
	if options.dmtmon_dest:
		config_parser.set('ligolw_burst2mon', 'destination', options.dmtmon_dest)
	else:
		options.dmtmon_dest = config_parser.get('ligolw_burst2mon', 'destination')
except Exception, e:
	raise Exception, "must either provide --dmtmon-dest, or destination variable in [ligolw_burst2mon] section of .ini: %s" % str(e)

# gsiscp location
try:
	if options.gsiscp_dest:
		config_parser.set('gsiscp', 'destination', options.gsiscp_dest)
	else:
		options.gsiscp_dest = config_parser.get('gsiscp', 'destination')
except Exception, e:
	raise Exception, "must either provide --gsiscp-dest, or destination variable in [gsiscp] section of .ini: %s" % str(e)

# user tag
try:
	if options.user_tag:
		config_parser.set('pipeline', 'user-tag', options.user_tag)
	else:
		options.user_tag = config_parser.get('pipeline', 'user-tag')
except Exception, e:
	raise Exception, "must either provide --user-tag, or user-tag variable in [pipeline] section of .ini: %s" % str(e)


#
# =============================================================================
#
#                              Static Parameters
#
# =============================================================================
#

cache_dir = "cache/"
out_dir = "logs/"
psds_per_job = 4
psds_per_injection = 4
overlap = 0.25
# try to do this many simultaneous injections beteen flow and fhigh
num_injections = 11


#
# =============================================================================
#
#                               Custom Job Types
#
# =============================================================================
#

class PublishJob(pipeline.CondorDAGJob):
	def __init__(self, out_dir):
		pipeline.CondorDAGJob.__init__(self, "scheduler", "/bin/cp")
		self.set_stdout_file(os.path.join(out_dir, "publish-$(cluster)-$(process).out"))
		self.set_stderr_file(os.path.join(out_dir, "publish-$(cluster)-$(process).err"))
		self.set_sub_file("publish.sub")

	def write_sub_file(self):
		# this is a hack, but I can't be bothered...
		self.add_file_arg("$(macrodestination)")
		pipeline.CondorDAGJob.write_sub_file(self)


class PublishNode(pipeline.CondorDAGNode):
	def set_output(self, destination):
		self.__output = destination
		self.add_macro("macrodestination", destination)

	def get_output(self):
		return self.__output


class GsiScpJob(pipeline.CondorDAGJob):
	def __init__(self, out_dir):
		pipeline.CondorDAGJob.__init__(self, "scheduler", "/ldcg/stow_pkgs/ldg-3.5/ldg/globus/bin/gsiscp")
		self.set_stdout_file(os.path.join(out_dir, "gsiscp-$(cluster)-$(process).out"))
		self.set_stderr_file(os.path.join(out_dir, "gsiscp-$(cluster)-$(process).err"))
		self.set_sub_file("gsiscp.sub")
		self.add_condor_cmd("getenv", "True")

	def write_sub_file(self):
		# this is a hack, but I can't be bothered...
		self.add_file_arg("$(macrodestination)")
		pipeline.CondorDAGJob.write_sub_file(self)


class GsiScpNode(pipeline.CondorDAGNode):
	def set_output(self, destination):
		self.__output = destination
		self.add_macro("macrodestination", destination)

	def get_output(self):
		return self.__output


class FixPowerJob(pipeline.CondorDAGJob):
	def __init__(self, out_dir):
		pipeline.CondorDAGJob.__init__(self, "scheduler", os.path.join(options.aux_dir, "fix_sngl_burst"))
		self.set_sub_file("fix_sngl_burst.sub")
		self.set_stdout_file("$(macrooutput)")
		self.set_stderr_file(os.path.join(out_dir, "fix_sngl_burst-$(cluster)-$(process).err"))
		self.add_condor_cmd("getenv", "True")
		self.add_condor_cmd("input", "$(macroinput)")
		self.add_arg(os.path.join(options.aux_dir, "fix_sngl_burst_add_event_id"))


class FixPowerNode(pipeline.CondorDAGNode):
	def set_input(self, filename):
		self.__input = filename
		self.add_macro("macroinput", filename)

	def set_output(self, filename):
		self.__output = filename
		self.add_macro("macrooutput", filename)

	def get_input(self):
		return self.__input

	def get_output(self):
		return self.__output


class Burst2MonJob(pipeline.CondorDAGJob):
	def __init__(self, out_dir, config_parser):
		pipeline.CondorDAGJob.__init__(self, "vanilla", config_parser.get("condor", "ligolw_burst2mon"))
		self.set_sub_file("ligolw_burst2mon.sub")
		self.set_stdout_file(os.path.join(out_dir, "ligolw_burst2mon-$(cluster)-$(process).out"))
		self.set_stderr_file(os.path.join(out_dir, "ligolw_burst2mon-$(cluster)-$(process).err"))
		self.add_condor_cmd("getenv", "True")
		self.add_ini_opts(config_parser, "ligolw_burst2mon")


class Burst2MonNode(pipeline.CondorDAGNode, pipeline.AnalysisNode):
	pass


class BuclusterJob(pipeline.CondorDAGJob):
	def __init__(self, out_dir, config_parser):
		pipeline.CondorDAGJob.__init__(self, "vanilla", config_parser.get("condor", "ligolw_bucluster"))
		self.set_sub_file("ligolw_bucluster.sub")
		self.set_stdout_file(os.path.join(out_dir, "ligolw_bucluster-$(cluster)-$(process).out"))
		self.set_stderr_file(os.path.join(out_dir, "ligolw_bucluster-$(cluster)-$(process).err"))
		self.add_condor_cmd("getenv", "True")
		self.add_ini_opts(config_parser, "ligolw_bucluster")


class BuclusterNode(pipeline.CondorDAGNode, pipeline.AnalysisNode):
	pass


class BinjfindJob(pipeline.CondorDAGJob):
	def __init__(self, out_dir, config_parser):
		pipeline.CondorDAGJob.__init__(self, "vanilla", config_parser.get("condor", "ligolw_binjfind"))
		self.set_sub_file("ligolw_binjfind.sub")
		self.set_stdout_file(os.path.join(out_dir, "ligolw_binjfind-$(cluster)-$(process).out"))
		self.set_stderr_file(os.path.join(out_dir, "ligolw_binjfind-$(cluster)-$(process).err"))
		self.add_condor_cmd("getenv", "True")
		self.add_ini_opts(config_parser, "ligolw_binjfind")


class BinjfindNode(pipeline.CondorDAGNode, pipeline.AnalysisNode):
	pass


#
# =============================================================================
#
#                              Define .sub Files
#
# =============================================================================
#

# LSCdataFind
lscdatafindjob = pipeline.LSCDataFindJob(cache_dir, out_dir, config_parser)

# lalapps_binj
binjjob = power.BurstInjJob(config_parser)

# lalapps_power
powerjob = power.PowerJob(out_dir, config_parser)
fixpowerjob = FixPowerJob(out_dir)

# ligolw_add
lladdjob = pipeline.LigolwAddJob(out_dir, config_parser)

# ligolw_binjfind
binjfindjob = BinjfindJob(out_dir, config_parser)

# ligolw_bucluster
buclusterjob = BuclusterJob(out_dir, config_parser)

# ligolw_burst2mon
llb2mjob = Burst2MonJob(out_dir, config_parser)

# publish
publishjob = PublishJob(out_dir)

# gsiscp
gsiscpjob = GsiScpJob(out_dir)


#
# =============================================================================
#
#                                 Segmentation
#
# =============================================================================
#

def split_segment(psds):
	"""
	Split the data segment into correctly-overlaping segments.  We try
	to have the numbers of PSDs in each segment be equal to psds, but
	with a short segment at the end if needed.
	"""
	psd_length = float(powerjob.get_opts()["psd-average-points"]) / float(powerjob.get_opts()["resample-rate"])
	window_length = float(powerjob.get_opts()["window-length"]) / float(powerjob.get_opts()["resample-rate"])
	window_shift = float(powerjob.get_opts()["window-shift"]) / float(powerjob.get_opts()["resample-rate"])
	filter_corruption = float(powerjob.get_opts()["filter-corruption"]) / float(powerjob.get_opts()["resample-rate"])

	psd_overlap = window_length - window_shift

	joblength = psds * (psd_length - psd_overlap) + psd_overlap + 2 * filter_corruption
	joboverlap = 2 * filter_corruption + psd_overlap

	# can't use range() with non-integers
	segs = segments.segmentlist()
	t = options.seg[0]
	while t < options.seg[1] - joboverlap:
		segs.append(segments.segment(t, t + joblength) & options.seg)
		t += joblength - joboverlap
	return segs

jobsegs = split_segment(psds_per_job)
injsegs = split_segment(psds_per_injection)

print >>sys.stderr, "Segment split: " + str(jobsegs)
print >>sys.stderr, "Injections split: " + str(injsegs)


#
# =============================================================================
#
#                           LSCdataFind DAG Fragment
#
# =============================================================================
#

def make_datafind_fragment(dag, observatory, seg):
	datafind_pad = 512

	node = pipeline.LSCDataFindNode(lscdatafindjob)
	node.set_name("LSCdataFind")
	node.set_start(options.seg[0] - datafind_pad)
	node.set_end(options.seg[1] + 1)
	node.set_observatory(observatory)
	dag.add_node(node)

	return node


#
# =============================================================================
#
#                          lalapps_power DAG Fragment
#
# =============================================================================
#

def make_power_fragment(dag, parents, framecache, seg, tag):
	suffix = "%s-%s-%s" % (tag, int(seg[0]), int(seg.duration()))

	powernode = power.PowerNode(powerjob)
	powernode.set_name("lalapps_power-%s" % suffix)
	map(powernode.add_parent, parents)
	powernode.set_cache(framecache)
	powernode.set_ifo(options.instrument)
	powernode.set_start(seg[0])
	powernode.set_end(seg[1])
	powernode.add_macro("macrotag", tag)
	dag.add_node(powernode)

	fixpowernode = FixPowerNode(fixpowerjob)
	fixpowernode.set_name("fix_sngl_burst-%s" % suffix)
	fixpowernode.add_parent(powernode)
	fixpowernode.set_input(powernode.get_output())
	fixpowernode.set_output(powernode.get_output() + ".fixed")
	dag.add_node(fixpowernode)

	return fixpowernode


#
# =============================================================================
#
#                           ligolw_add DAG Fragment
#
# =============================================================================
#

def make_lladd_fragment(dag, parents, seg, tag):
	cache_name = os.path.join(cache_dir, "lladd-%s.cache" % tag)
	cachefile = file(cache_name, "w")

	node = pipeline.LigolwAddNode(lladdjob)
	node.set_name("lladd-%s" % tag)
	for parent in parents:
		node.add_parent(parent)
		cache = lal.CacheEntry()
		cache.observatory = "ANY"
		cache.description = "EMPTY"
		cache.segment = seg
		cache.url = "file://localhost" + os.path.join(os.getcwd(), parent.get_output())
		print >>cachefile, str(cache)
	node.add_var_opt("input-cache", cache_name)
	dag.add_node(node)

	return node


#
# =============================================================================
#
#        DAG Fragment Combining Multiple lalapps_power With ligolw_add
#
# =============================================================================
#

def make_multipower_fragment(dag, powerparents, lladdparents, framecache, seglist, tag):
	# we need to run lladd even if there's only one job so that the output
	# file gets the right name (skipping lladd in the case of a single job
	# leaves file named .xml.fixed).  this can be changed when the
	# sngl_burst table produced by LAL is fixed, and we don't need the
	# fix_sngl_burst nodes in this DAG any more.
	node = make_lladd_fragment(dag, [make_power_fragment(dag, powerparents, framecache, seg, tag) for seg in seglist] + lladdparents, seglist.extent(), "POWER_%s" % tag)
	node.set_output("%s-POWER_%s-%s-%s.xml" % (options.instrument, tag, int(seglist.extent()[0]), int(seglist.extent().duration())))
	return node


#
# =============================================================================
#
#                             Publish DAG Fragment
#
# =============================================================================
#

def make_publish_fragment(dag, tag, parent, destination):
	node = PublishNode(publishjob)
	node.set_name("publish-%s" % tag)
	node.add_parent(parent)
	node.add_file_arg(parent.get_output())
	node.set_output(destination)
	dag.add_node(node)

	return node


#
# =============================================================================
#
#                       DMT Monitor Output DAG Fragment
#
# =============================================================================
#

def make_burst2mon_fragment(dag, parent, seg, tag, dest):
	cluster = BuclusterNode(buclusterjob)
	cluster.set_name("ligolw_bucluster-POWERMON")
	cluster.add_parent(parent)
	cluster.set_input(parent.get_output())
	cluster.set_output("%s-POWERMON_%s-%s-%s.xml" % (options.instrument, tag, int(seg[0]), int(seg.duration())))
	cluster.add_macro("macrotag", tag)
	dag.add_node(cluster)

	node = Burst2MonNode(llb2mjob)
	node.add_parent(cluster)
	node.set_name("ligolw_burst2mon")
	node.set_input(parent.get_output())
	node.set_output(parent.get_output())
	node.add_macro("macrotag", tag)
	dag.add_node(node)

	return make_publish_fragment(dag, "POWERMON", node, dest)


#
# =============================================================================
#
#                             GSISCP DAG Fragment
#
# =============================================================================
#

def make_gsiscp_fragment(dag, parents, destination):
	node = GsiScpNode(gsiscpjob)
	node.set_name("gsiscp")
	for parent in parents:
		node.add_parent(parent)
		node.add_file_arg(parent.get_output())
	node.set_output(destination)
	dag.add_node(node)

	return node


#
# =============================================================================
#
#                          lalapps_binj DAG Fragment
#
# =============================================================================
#

def make_binj_fragment(dag, tag, seg, offset, flow, fhigh, fratio):
	node = power.BurstInjNode(binjjob)
	node.set_start(str(seg[0] + float(binjjob.get_opts()["time-step"]) * offset))
	node.set_end(seg[1])
	node.set_name("lalapps_binj-%s-%s" % (node.get_start(), int(flow)))
	node.add_macro("macrotag", tag)
	node.add_macro("macroflow", flow)
	node.add_macro("macrofhigh", fhigh)
	node.add_macro("macrofratio", fratio)
	node.add_macro("macroseed", 0)	# FIXME
	dag.add_node(node)

	return node


#
# =============================================================================
#
#         DAG Fragment Combining Multiple lalapps_binj With ligolw_add
#
# =============================================================================
#

def make_multibinj_fragment(dag, tag, seg):
	flow = float(powerjob.get_opts()["low-freq-cutoff"])
	fhigh = flow + float(powerjob.get_opts()["bandwidth"])

	# smallest allowed neighbouring frequency ratio based on Q of
	# injections
	min_fratio = (1.0 + 1.0/float(binjjob.get_opts()["quality"])) ** 2.0

	# maximum number of simultaneous injections based on min separation
	injections = num_injections
	max_injections = int(math.log(fhigh/flow) / math.log(min_fratio)) + 1
	if injections > max_injections:
		injections = max_injections

	# finally, determine frequency ratio
	fratio = (fhigh / flow) ** (1.0/(injections - 1.0))

	binj1 = make_binj_fragment(dag, tag, seg, 0.0, flow, fhigh, fratio)
	binj2 = make_binj_fragment(dag, tag, seg, 0.5, flow * fratio**0.5, fhigh, fratio)

	node = make_lladd_fragment(dag, [binj1, binj2], seg, "INJECTIONS_%s" % tag)
	node.set_output("%s-INJECTIONS_%s-%s-%s.xml" % (options.instrument, tag, int(seg[0]), int(seg.duration())))

	return node


#
# =============================================================================
#
#                         ligolw_binjfind DAG Fragment
#
# =============================================================================
#

def make_binjfind_fragment(dag, tag, parent):
	cluster = BuclusterNode(buclusterjob)
	cluster.set_name("ligolw_bucluster-INJECTIONS")
	cluster.add_parent(parent)
	cluster.set_input(parent.get_output())
	cluster.set_output(parent.get_output())
	cluster.add_macro("macrotag", tag)
	dag.add_node(cluster)

	binjfind = BinjfindNode(binjfindjob)
	binjfind.set_name("ligolw_binjfind")
	binjfind.add_parent(cluster)
	binjfind.set_input(cluster.get_output())
	binjfind.set_output(cluster.get_output())
	binjfind.add_macro("macrotag", tag)
	dag.add_node(binjfind)

	return binjfind


#
# =============================================================================
#
#                               DAG Construction
#
# =============================================================================
#

os.mkdir(cache_dir)
os.mkdir(out_dir)

dag = pipeline.CondorDAG(condor_log)
dag.set_dag_file(options.dag_name)

lscdatafindnode = make_datafind_fragment(dag, options.instrument[0], options.seg)

darmpowerfrag = make_multipower_fragment(dag, [lscdatafindnode], [], lscdatafindnode.get_output(), jobsegs, options.user_tag)

make_burst2mon_fragment(dag, darmpowerfrag, jobsegs.extent(), options.user_tag, options.dmtmon_dest)

binjfrag = make_multibinj_fragment(dag, options.user_tag, injsegs.extent())
injpowerfrag = make_multipower_fragment(dag, [lscdatafindnode], [binjfrag], lscdatafindnode.get_output(), injsegs, "INJECTIONS_%s" % options.user_tag)	# FIXME: need to add --burstinjection-file and --calibration-cache to these power jobs
binjfindfrag = make_binjfind_fragment(dag, options.user_tag, injpowerfrag)

make_publish_fragment(dag, "POWER", darmpowerfrag, options.publish_dest)
make_gsiscp_fragment(dag, [darmpowerfrag, binjfindfrag], options.gsiscp_dest)

dag.write_sub_files()
dag.write_dag()

print options.trig_seg[0] + overlap, options.trig_seg[1] - overlap
