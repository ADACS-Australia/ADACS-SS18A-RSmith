#!/usr/bin/python
#
# $Id$
#
# Copyright (C) 2005  Kipp C. Cannon
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#

"""
Pipeline generation script for the excess power online analysis.
"""

import ConfigParser
import math
from optparse import OptionParser
import os
import stat
import sys
import tempfile
import time

from glue import lal
from glue import pipeline
from glue import segments
from glue import segmentsUtils
import power

__author__ = "Kipp Cannon <kipp@gravity.phys.uwm.edu>"
__date__ = "$Date$"[11:-2]
__version__ = "$Revision$"[7:-2]


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#

parser = OptionParser(version="%prog CVS $Id$")
parser.add_option("-s", metavar = "GPSSECONDS", dest = "data_start", help = "set data segment start time")
parser.add_option("-e", metavar = "GPSSECONDS", dest = "data_end", help = "set data segment end time")
parser.add_option("-a", metavar = "GPSSECONDS", dest = "trig_start", help = "set analysis segment start time")
parser.add_option("-b", metavar = "GPSSECONDS", dest = "trig_end", help = "set analysis segment end time")
parser.add_option("-f", metavar = "FILENAME", dest = "dag_name", help = "set output .dag file name")
parser.add_option("-t", metavar = "PATH", dest = "aux_dir", help = "set auxiliary data directory")
parser.add_option("--condor-log-dir", metavar = "PATH", help = "set directory for Condor log")
parser.add_option("--config-file", metavar = "FILENAME", default = "online_power.ini", help = "set .ini config file name")
parser.add_option("--instrument", metavar = "INSTRUMENT", help = "set instrument name (default = value of instrument variable in [pipeline] section of .ini file)")
parser.add_option("--publish-dest", metavar = "PATH", help = "set directory for output triggers")
parser.add_option("--dmtmon-dest", metavar = "PATH", help = "set directory for DMT monitor output")
parser.add_option("--gsiscp-dest", metavar = "PATH", help = "set destination for gsiscp")
parser.add_option("--user-tag", metavar = "TAG", help = "set user tag on jobs that need it")
parser.add_option("--calibration-cache", metavar = "FILENAME", help = "set name of calibration cache relative to auxiliary data directory")
options, args = parser.parse_args()
del parser, args

# data segment
try:
	options.seg = segments.segment(lal.LIGOTimeGPS(options.data_start), lal.LIGOTimeGPS(options.data_end))
except:
	raise Exception, "failure parsing -s and/or -e; try --help"

# trigger segment
try:
	options.trig_seg = segments.segment(lal.LIGOTimeGPS(options.trig_start), lal.LIGOTimeGPS(options.trig_end))
except:
	raise Exception, "failure parsing -a and/or -b; try --help"
if options.trig_seg not in options.seg:
	raise Exception, "trigger segment not contained in data segment!"

# .dag name
try:
	options.dag_name = os.path.splitext(options.dag_name)[0]
except:
	raise Exception, "failure parsing -f; try --help"

# .ini file config parser
options.config_file = os.path.join(options.aux_dir, options.config_file)
try:
	config_parser = ConfigParser.SafeConfigParser()
	config_parser.read(options.config_file)
except:
	raise Exception, "failure parsing -t and/or --config-file; try --help"

# Condor log file
try:
	handle, condor_log = tempfile.mkstemp(".log", "power_", options.condor_log_dir)
	del handle
except:
	raise Exception, "failure parsing --condor-log-dir; try --help"

# instrument
try:
	if options.instrument:
		config_parser.set('pipeline', 'instrument', options.instrument)
	else:
		options.instrument = config_parser.get('pipeline', 'instrument')
except Exception, e:
	raise Exception, "must either provide --instrument, or instrument variable in [pipeline] section of .ini: %s" % str(e)

# publish location
try:
	if options.publish_dest:
		config_parser.set('publish', 'destination', options.publish_dest)
	else:
		options.publish_dest = config_parser.get('publish', 'destination')
except Exception, e:
	raise Exception, "must either provide --publish-dest, or destination variable in [publish] section of .ini: %s" % str(e)

# gsiscp location
try:
	if options.gsiscp_dest:
		config_parser.set('gsiscp', 'destination', options.gsiscp_dest)
	else:
		options.gsiscp_dest = config_parser.get('gsiscp', 'destination')
except Exception, e:
	raise Exception, "must either provide --gsiscp-dest, or destination variable in [gsiscp] section of .ini: %s" % str(e)

# user tag
try:
	if options.user_tag:
		config_parser.set('pipeline', 'user-tag', options.user_tag)
	else:
		options.user_tag = config_parser.get('pipeline', 'user-tag')
except Exception, e:
	raise Exception, "must either provide --user-tag, or user-tag variable in [pipeline] section of .ini: %s" % str(e)

# calibration cache
try:
	if options.calibration_cache:
		config_parser.set('pipeline', 'calibration-cache', options.calibration_cache)
	else:
		options.calibration_cache = config_parser.get('pipeline', 'calibration-cache')
except Exception, e:
	raise Exception, "must either provide --calibration-cache, or calibration-cache variable in [pipeline] section of .ini: %s" % str(e)
options.calibration_cache = os.path.join(options.aux_dir, options.calibration_cache)


#
# =============================================================================
#
#                              Static Parameters
#
# =============================================================================
#

cache_dir = "cache/"
out_dir = "logs/"
if options.seg.duration() < 3600:
	psds_per_job = 4
	psds_per_injection = 4
else:
	psds_per_job = 16
	psds_per_injection = 16


#
# =============================================================================
#
#                               Custom Job Types
#
# =============================================================================
#

class Burst2MonJob(pipeline.CondorDAGJob):
	def __init__(self, out_dir, config_parser):
		pipeline.CondorDAGJob.__init__(self, "vanilla", config_parser.get("condor", "ligolw_burst2mon"))
		self.set_sub_file("ligolw_burst2mon.sub")
		self.set_stdout_file(os.path.join(out_dir, "ligolw_burst2mon-$(cluster)-$(process).out"))
		self.set_stderr_file(os.path.join(out_dir, "ligolw_burst2mon-$(cluster)-$(process).err"))
		self.add_condor_cmd("getenv", "True")
		self.add_ini_opts(config_parser, "ligolw_burst2mon")


class Burst2MonNode(pipeline.CondorDAGNode, pipeline.AnalysisNode):
	pass


class PublishJob(pipeline.CondorDAGJob):
	def __init__(self, out_dir):
		pipeline.CondorDAGJob.__init__(self, "vanilla", config_parser.get("condor", "publish"))
		self.set_stdout_file(os.path.join(out_dir, "publish-$(cluster)-$(process).out"))
		self.set_stderr_file(os.path.join(out_dir, "publish-$(cluster)-$(process).err"))
		self.set_sub_file("publish.sub")

	def write_sub_file(self):
		# this is a hack, but I can't be bothered...
		self.add_file_arg("$(macrodestination)")
		pipeline.CondorDAGJob.write_sub_file(self)


class PublishNode(pipeline.CondorDAGNode):
	def set_input(self, source):
		self.add_file_arg(source)

	def get_input(self):
		return self.get_input_files()

	def set_output(self, destination):
		self.__output = destination
		self.add_macro("macrodestination", destination)

	def get_output(self):
		return self.__output


class GsiScpJob(pipeline.CondorDAGJob):
	def __init__(self, out_dir):
		pipeline.CondorDAGJob.__init__(self, "scheduler", "/ldcg/stow_pkgs/ldg-3.5/ldg/globus/bin/gsiscp")
		self.set_stdout_file(os.path.join(out_dir, "gsiscp-$(cluster)-$(process).out"))
		self.set_stderr_file(os.path.join(out_dir, "gsiscp-$(cluster)-$(process).err"))
		self.set_sub_file("gsiscp.sub")
		self.add_condor_cmd("getenv", "True")

	def write_sub_file(self):
		# this is a hack, but I can't be bothered...
		self.add_file_arg("$(macrodestination)")
		pipeline.CondorDAGJob.write_sub_file(self)


class GsiScpNode(pipeline.CondorDAGNode):
	def set_output(self, destination):
		self.__output = destination
		self.add_macro("macrodestination", destination)

	def get_output(self):
		return self.__output


#
# =============================================================================
#
#                              Define .sub Files
#
# =============================================================================
#

power.init_job_types(cache_dir, out_dir, config_parser)

llb2mjob = Burst2MonJob(out_dir, config_parser)
publishjob = PublishJob(out_dir)
gsiscpjob = GsiScpJob(out_dir)


#
# =============================================================================
#
#                                 Segmentation
#
# =============================================================================
#

jobsegs = power.split_segment(power.powerjob, options.seg, psds_per_job)
injsegs = power.split_segment(power.powerjob, options.seg, psds_per_injection)

print >>sys.stderr, "Segment split: " + str(jobsegs)
print >>sys.stderr, "Injections split: " + str(injsegs)


#
# =============================================================================
#
#                             Publish DAG Fragment
#
# =============================================================================
#

def make_publish_fragment(dag, tag, parent, destination):
	start = segmentsUtils.fromfilenames([parent.get_output()])[0][0]
	destination = os.path.join(destination, "%d/%d" % (start/1000000, start/100000))
	try:
		os.makedirs(destination)
	except OSError, e:
		# errno 17 == "file exists"
		if e.errno != 17:
			raise e
	node = PublishNode(publishjob)
	node.set_name("publish-%s" % tag)
	node.add_parent(parent)
	node.set_input(parent.get_output())
	node.set_output(destination)
	dag.add_node(node)

	return node


#
# =============================================================================
#
#                       DMT Monitor Output DAG Fragment
#
# =============================================================================
#

def make_burst2mon_fragment(dag, parent, seg, tag):
	cluster = power.BuclusterNode(power.buclusterjob)
	cluster.set_name("ligolw_bucluster-POWERMON")
	cluster.add_parent(parent)
	cluster.set_input(parent.get_output())
	cluster.set_output("%s-POWERMON_%s-%s-%s.xml" % (options.instrument, tag, int(seg[0]), int(seg.duration())))
	cluster.add_macro("macrocomment", tag)
	dag.add_node(cluster)

	node = Burst2MonNode(llb2mjob)
	node.set_name("ligolw_burst2mon")
	node.add_parent(cluster)
	node.set_input(cluster.get_output())
	node.set_output(os.path.join(options.dmtmon_dest, cluster.get_output()))
	node.add_macro("macrocomment", tag)
	dag.add_node(node)

	return node


#
# =============================================================================
#
#                             GSISCP DAG Fragment
#
# =============================================================================
#

def make_gsiscp_fragment(dag, parents, destination):
	node = GsiScpNode(gsiscpjob)
	node.set_name("gsiscp")
	for parent in parents:
		node.add_parent(parent)
		node.add_file_arg(parent.get_output())
	node.set_output(destination)
	dag.add_node(node)

	return node


#
# =============================================================================
#
#                               DAG Construction
#
# =============================================================================
#

os.mkdir(cache_dir)
os.mkdir(out_dir)

dag = pipeline.CondorDAG(condor_log)
dag.set_dag_file(options.dag_name)

datafindnode = power.make_datafind_fragment(dag, options.instrument[0], options.seg)

darmpowerfrag = power.make_multipower_fragment(dag, cache_dir, [datafindnode], [], datafindnode.get_output(), jobsegs, options.instrument, options.user_tag)

make_publish_fragment(dag, "POWER", darmpowerfrag, options.publish_dest)
if options.dmtmon_dest:
	make_burst2mon_fragment(dag, darmpowerfrag, jobsegs.extent(), options.user_tag)

binjfrag = power.make_multibinj_fragment(dag, cache_dir, "INJECTIONS_%s" % options.user_tag, injsegs.extent())
tisifrag = power.make_tisi_fragment(dag, "INJECTIONS_%s" % options.user_tag)
injpowerfrag = power.make_multipower_fragment(dag, cache_dir, [datafindnode, binjfrag], [binjfrag, tisifrag], datafindnode.get_output(), injsegs, options.instrument, "INJECTIONS_%s" % options.user_tag, injargs = {"burstinjection-file": binjfrag.get_output(), "calibration-cache": options.calibration_cache})
binjfindfrag = power.make_binjfind_fragment(dag, injpowerfrag, "INJECTIONS_%s" % options.user_tag)
make_publish_fragment(dag, "INJECTIONS", binjfindfrag, options.publish_dest)

make_gsiscp_fragment(dag, [darmpowerfrag, binjfindfrag], options.gsiscp_dest)

dag.write_sub_files()
dag.write_dag()

print options.trig_seg[0], options.trig_seg[1]
