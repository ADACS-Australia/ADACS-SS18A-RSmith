#!/usr/bin/python
#
# $Id$
#
# Copyright (C) 2005  Kipp C. Cannon
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#

"""
Pipeline generation script for the excess power online analysis.
"""

import ConfigParser
from optparse import OptionParser
import os
import sys
import tempfile

from glue import pipeline
from glue import segments
from glue import segmentsUtils
from pylal.date import LIGOTimeGPS
from lalapps import power

__author__ = "Kipp Cannon <kipp@gravity.phys.uwm.edu>"
__date__ = "$Date$"[11:-2]
__version__ = "$Revision$"[7:-2]


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#

parser = OptionParser(version="%prog CVS $Id$")
parser.add_option("-s", metavar = "GPSSECONDS", dest = "data_start", help = "set data segment start time")
parser.add_option("-e", metavar = "GPSSECONDS", dest = "data_end", help = "set data segment end time")
parser.add_option("-a", metavar = "GPSSECONDS", dest = "trig_start", help = "set analysis segment start time")
parser.add_option("-b", metavar = "GPSSECONDS", dest = "trig_end", help = "set analysis segment end time")
parser.add_option("-f", metavar = "FILENAME", dest = "dag_name", help = "set output .dag file name")
parser.add_option("-t", metavar = "PATH", dest = "aux_dir", help = "set auxiliary data directory")
parser.add_option("--condor-log-dir", metavar = "PATH", help = "set directory for Condor log")
parser.add_option("--config-file", metavar = "FILENAME", default = "online_power.ini", help = "set .ini config file name")
parser.add_option("--instrument", metavar = "INSTRUMENT", help = "set instrument name (default = value of instrument variable in [pipeline] section of .ini file)")
parser.add_option("--publish-dest", metavar = "PATH", help = "set directory for output triggers")
parser.add_option("--dmtmon-dest", metavar = "PATH", help = "set directory for DMT monitor output")
parser.add_option("--gsiscp-dest", metavar = "PATH", help = "set destination for gsiscp")
parser.add_option("--user-tag", metavar = "TAG", help = "set user tag on jobs that need it")
options, args = parser.parse_args()
del parser, args

# data segment
try:
	options.seg = segments.segment(LIGOTimeGPS(options.data_start), LIGOTimeGPS(options.data_end))
except:
	raise Exception, "failure parsing -s and/or -e; try --help"

# trigger segment
try:
	options.trig_seg = segments.segment(LIGOTimeGPS(options.trig_start), LIGOTimeGPS(options.trig_end))
except:
	raise Exception, "failure parsing -a and/or -b; try --help"
if options.trig_seg not in options.seg:
	raise Exception, "trigger segment not contained in data segment!"

# .dag name
try:
	options.dag_name = os.path.splitext(options.dag_name)[0]
except:
	raise Exception, "failure parsing -f; try --help"

# .ini file config parser
options.config_file = os.path.join(options.aux_dir, options.config_file)
try:
	config_parser = ConfigParser.SafeConfigParser()
	config_parser.read(options.config_file)
except:
	raise Exception, "failure parsing -t and/or --config-file; try --help"

# Condor log file
try:
	handle, condor_log = tempfile.mkstemp(".log", "power_", options.condor_log_dir)
	del handle
except:
	raise Exception, "failure parsing --condor-log-dir; try --help"

# instrument
try:
	if options.instrument:
		config_parser.set('pipeline', 'instrument', options.instrument)
	else:
		options.instrument = config_parser.get('pipeline', 'instrument')
except Exception, e:
	raise Exception, "must either provide --instrument, or instrument variable in [pipeline] section of .ini: %s" % str(e)

# publish location
try:
	if options.publish_dest:
		config_parser.set('publish', 'destination', options.publish_dest)
	else:
		options.publish_dest = config_parser.get('publish', 'destination')
except Exception, e:
	raise Exception, "must either provide --publish-dest, or destination variable in [publish] section of .ini: %s" % str(e)

# gsiscp location
try:
	if options.gsiscp_dest:
		config_parser.set('gsiscp', 'destination', options.gsiscp_dest)
	else:
		options.gsiscp_dest = config_parser.get('gsiscp', 'destination')
except Exception, e:
	raise Exception, "must either provide --gsiscp-dest, or destination variable in [gsiscp] section of .ini: %s" % str(e)

# user tag
try:
	if options.user_tag:
		config_parser.set('pipeline', 'user_tag', options.user_tag)
	else:
		options.user_tag = config_parser.get('pipeline', 'user_tag')
except Exception, e:
	raise Exception, "must either provide --user-tag, or user-tag variable in [pipeline] section of .ini: %s" % str(e)


#
# =============================================================================
#
#                              Static Parameters
#
# =============================================================================
#

if options.seg.duration() < 3600:
	psds_per_job = 4
	psds_per_injection = 4
else:
	psds_per_job = 16
	psds_per_injection = 16


#
# =============================================================================
#
#                               Custom Job Types
#
# =============================================================================
#

class Burst2MonJob(pipeline.CondorDAGJob):
	def __init__(self, config_parser):
		pipeline.CondorDAGJob.__init__(self, "vanilla", power.get_executable(config_parser, "ligolw_burst2mon"))
		self.set_sub_file("ligolw_burst2mon.sub")
		self.set_stdout_file(os.path.join(power.get_out_dir(config_parser), "ligolw_burst2mon-$(cluster)-$(process).out"))
		self.set_stderr_file(os.path.join(power.get_out_dir(config_parser), "ligolw_burst2mon-$(cluster)-$(process).err"))
		self.add_condor_cmd("getenv", "True")
		self.add_ini_opts(config_parser, "ligolw_burst2mon")


class Burst2MonNode(pipeline.AnalysisNode):
	def __init__(self, job):
		# FIXME: this shouldn't be needed.
		pipeline.CondorDAGNode.__init__(self, job)
		pipeline.AnalysisNode.__init__(self)


class PublishJob(pipeline.CondorDAGJob):
	def __init__(self, config_parser):
		pipeline.CondorDAGJob.__init__(self, "vanilla", power.get_executable(config_parser, "publish"))
		self.set_stdout_file(os.path.join(power.get_out_dir(config_parser), "publish-$(cluster)-$(process).out"))
		self.set_stderr_file(os.path.join(power.get_out_dir(config_parser), "publish-$(cluster)-$(process).err"))
		self.set_sub_file("publish.sub")

	def write_sub_file(self):
		# this is a hack, but I can't be bothered...
		self.add_file_arg("$(macrodestination)")
		pipeline.CondorDAGJob.write_sub_file(self)


class PublishNode(pipeline.CondorDAGNode):
	def set_input(self, source):
		self.add_file_arg(source)

	def set_output(self, destination):
		self.add_macro("macrodestination", destination)


class GsiScpJob(pipeline.CondorDAGJob):
	def __init__(self, config_parser):
		pipeline.CondorDAGJob.__init__(self, "local", power.get_executable(config_parser, "gsiscp"))
		self.set_stdout_file(os.path.join(power.get_out_dir(config_parser), "gsiscp-$(cluster)-$(process).out"))
		self.set_stderr_file(os.path.join(power.get_out_dir(config_parser), "gsiscp-$(cluster)-$(process).err"))
		self.set_sub_file("gsiscp.sub")
		self.add_condor_cmd("getenv", "True")

	def write_sub_file(self):
		# this is a hack, but I can't be bothered...
		self.add_file_arg("$(macrodestination)")
		pipeline.CondorDAGJob.write_sub_file(self)


class GsiScpNode(pipeline.CondorDAGNode):
	def set_output(self, destination):
		self.add_macro("macrodestination", destination)


#
# =============================================================================
#
#                              Define .sub Files
#
# =============================================================================
#

power.init_job_types(config_parser)

llb2mjob = Burst2MonJob(config_parser)
publishjob = PublishJob(config_parser)
gsiscpjob = GsiScpJob(config_parser)


#
# =============================================================================
#
#                             Publish DAG Fragment
#
# =============================================================================
#

def make_publish_fragment(dag, tag, parent, destination):
	start = segmentsUtils.fromfilenames([parent.get_output_files()[0]])[0][0]
	destination = os.path.join(destination, "%d/%d" % (start/1000000, start/100000))
	try:
		os.makedirs(destination)
	except OSError, e:
		# errno 17 == "file exists"
		if e.errno != 17:
			raise e
	node = PublishNode(publishjob)
	node.set_name("publish-%s" % tag)
	node.add_parent(parent)
	node.set_input(parent.get_output_files()[0])
	node.set_output(destination)
	dag.add_node(node)

	return node


#
# =============================================================================
#
#                       DMT Monitor Output DAG Fragment
#
# =============================================================================
#

def make_burst2mon_fragment(dag, parent, instrument, seg, tag):
	cluster_output = "%s-POWERMON_%s-%s-%s.xml" % (instrument, tag, int(seg[0]), int(seg.duration()))
	cluster = power.make_bucluster_fragment(dag, [], instrument, seg, "POWERMON_%s" % tag)
	cluster.add_parent(parent)
	cluster.set_pre_script("/bin/cp %s %s" % (parent.get_output_files()[0], cluster_output))
	cluster.add_file_arg(cluster_output)

	node = Burst2MonNode(llb2mjob)
	node.set_name("ligolw_burst2mon")
	node.add_parent(cluster)
	node.set_input(cluster.get_output_files()[0])
	node.set_output(os.path.join(options.dmtmon_dest, cluster.get_output_files()[0]))
	node.add_macro("macrocomment", tag)
	dag.add_node(node)

	return node


#
# =============================================================================
#
#                             GSISCP DAG Fragment
#
# =============================================================================
#

def make_gsiscp_fragment(dag, parents, destination):
	node = GsiScpNode(gsiscpjob)
	node.set_name("gsiscp")
	for parent in parents:
		node.add_parent(parent)
		node.add_file_arg(parent.get_output_files()[0])
	node.set_output(destination)
	dag.add_node(node)

	return node


#
# =============================================================================
#
#                               DAG Construction
#
# =============================================================================
#

power.make_dag_directories(config_parser)

dag = pipeline.CondorDAG(condor_log)
dag.set_dag_file(options.dag_name)

datafindfrag = power.make_datafind_fragment(dag, options.instrument, options.seg)

darmpowerfrag = power.make_power_segment_fragment(dag, datafindfrag, options.seg, options.instrument, psds_per_job, options.user_tag)
if "cluster" in power.powerjob.get_opts():
	darmpowerfrag = power.make_bucluster_fragment(dag, [darmpowerfrag], options.instrument, options.seg, options.user_tag)

make_publish_fragment(dag, "POWER", darmpowerfrag, options.publish_dest)
if options.dmtmon_dest:
	make_burst2mon_fragment(dag, darmpowerfrag, options.instrument, options.seg, options.user_tag)

injpowerfrag = power.make_injection_segment_fragment(dag, datafindfrag, options.seg, options.instrument, psds_per_injection, options.user_tag)
if "cluster" in power.powerjob.get_opts():
	injpowerfrag = power.make_bucluster_fragment(dag, [injpowerfrag], options.instrument, options.seg, "INJECTIONS_%s" % options.user_tag)

binjfindfrag = power.make_binjfind_fragment(dag, [injpowerfrag], options.instrument, options.seg, "INJECTIONS_%s" % options.user_tag)
make_publish_fragment(dag, "INJECTIONS", binjfindfrag, options.publish_dest)

make_gsiscp_fragment(dag, [darmpowerfrag, binjfindfrag], options.gsiscp_dest)

dag.write_sub_files()
dag.write_dag()

print options.trig_seg[0], options.trig_seg[1]
