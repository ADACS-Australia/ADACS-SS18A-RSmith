#!/usr/bin/python

#
# File info
#

"""
Pipeline generation script for the excess power online analysis.
"""

__author__ = "Kipp Cannon <kipp@gravity.phys.uwm.edu>"
__date__ = "$Date$"
__version__ = "$Revision$"


#
# Preamble
#

import ConfigParser
from optparse import OptionParser
import os.path
import sys
import tempfile

from glue import lal
from glue import pipeline
from glue import segments
import power


#
# Parse command line
#

parser = OptionParser(version="%prog CVS $Id$")
parser.add_option("-s", metavar = "GPSSECONDS", dest = "data_start", help = "set data segment start time")
parser.add_option("-e", metavar = "GPSSECONDS", dest = "data_end", help = "set data segment end time")
parser.add_option("-a", metavar = "GPSSECONDS", dest = "trig_start", help = "set analysis segment start time")
parser.add_option("-b", metavar = "GPSSECONDS", dest = "trig_end", help = "set analysis segment end time")
parser.add_option("-f", metavar = "FILENAME", dest = "dag_name", help = "set output .dag file name")
parser.add_option("-t", metavar = "PATH", dest = "aux_dir", help = "set auxiliary data directory")
parser.add_option("--condor-log-dir", metavar = "PATH", help = "set directory for Condor log")
parser.add_option("--config-file", metavar = "FILENAME", default = "online_power.ini", help = "set .ini config file name")
parser.add_option("--instrument", metavar = "INSTRUMENT", help = "set instrument name (default = value of instrument variable in [pipeline] section of .ini file)")
options, args = parser.parse_args()
del parser, args

# data segment
try:
	options.seg = segments.segment(lal.LIGOTimeGPS(options.data_start), lal.LIGOTimeGPS(options.data_end))
except:
	raise Exception, "failure parsing -s and/or -e; try --help"

# trigger segment
try:
	options.trig_seg = segments.segment(lal.LIGOTimeGPS(options.trig_start), lal.LIGOTimeGPS(options.trig_end))
except:
	raise Exception, "failure parsing -a and/or -b; try --help"

# .dag name
try:
	options.dag_name = os.path.splitext(options.dag_name)[0]
except:
	raise Exception, "failure parsing -f; try --help"

# .ini file config parser
try:
	config_parser = ConfigParser.SafeConfigParser()
	config_parser.read(os.path.join(options.aux_dir, options.config_file))
except:
	raise Exception, "failure parsing -t and/or --config-file; try --help"

# Condor log file
try:
	handle, condor_log = tempfile.mkstemp(".log", "power_", options.condor_log_dir)
	del handle
except:
	raise Exception, "failure parsing --condor-log-dir; try --help"

# instrument
try:
	if options.instrument:
		config_parser.set('pipeline', 'instrument', options.instrument)
	else:
		options.instrument = config_parser.get('pipeline','instrument')
except:
	raise Exception, "must either provide --instrument, or instrument variable in [pipeline] section of .ini"


#
# Some stuff used in DAG layout and construction
#

datafind_pad = 512
cache_dir = "itchy/"
out_dir = "scratchy/"
psds_per_job = 2
observatory = options.instrument[0]


#
# Define Condor job types (some are also found in pipeline.py and power.py)
#

class PublishJob(pipeline.CondorDAGJob, pipeline.AnalysisJob):
	def __init__(self, out_dir, config_parser):
		pipeline.CondorDAGJob.__init__(self, "scheduler", "/bin/cp")
		pipeline.AnalysisJob.__init__(self, config_parser)


class GsiScpJob(pipeline.CondorDAGJob, pipeline.AnalysisJob):
	def __init__(self, out_dir, config_parser):
		pipeline.CondorDAGJob.__init__(self, "scheduler", "/ldcg/stow_pkgs/ldg-3.5/ldg/globus/bin/gsiscp")
		pipeline.AnalysisJob.__init__(self, config_parser)


#
# Define DAG node types (some are also found in pipeline.py and power.py)
#

class PublishNode(pipeline.CondorDAGNode):
	pass


class GsiScpNode(pipeline.CondorDAGNode):
	pass


#
# Create Condor job types (define the .sub files)
#

# LSCdataFind
lscdatafindjob = pipeline.LSCDataFindJob(cache_dir, out_dir, config_parser)

# lalapps_power
powerjob = power.PowerJob(out_dir, config_parser)

# lalapps_bread
lladdjob = power.LladdJob(cache_dir, out_dir, config_parser)

# publish
publishjob = PublishJob(out_dir, config_parser)

# gsiscp
gsiscpjob = GsiScpJob(out_dir, config_parser)


#
# Split the data segment.  We try to split the data segment into jobs of
# length joblength, but with a short one if needed.
#

psd_length = float(powerjob.get_opts()["psd-average-points"]) / float(powerjob.get_opts()["resample-rate"])
window_length = float(powerjob.get_opts()["window-length"]) / float(powerjob.get_opts()["resample-rate"])
window_shift = float(powerjob.get_opts()["window-shift"]) / float(powerjob.get_opts()["resample-rate"])
filter_corruption = float(powerjob.get_opts()["filter-corruption"]) / float(powerjob.get_opts()["resample-rate"])

psd_overlap = window_length - window_shift

joblength = psds_per_job * (psd_length - psd_overlap) + psd_overlap + 2 * filter_corruption
joboverlap = 2 * filter_corruption + psd_overlap

# can't use range()
jobsegs = segments.segmentlist()
t = options.seg[0]
while t < options.seg[1] - joboverlap:
	jobsegs.append(segments.segment(t, t + joblength) & options.seg)
	t += joblength - joboverlap

print >>sys.stderr, "Segment split: " + str(jobsegs)


#
# Construct the DAG
#

dag = pipeline.CondorDAG(condor_log)
dag.set_dag_file(options.dag_name)

lscdatafindnode = pipeline.LSCDataFindNode(lscdatafindjob)
lscdatafindnode.set_name("LSCdataFind")
lscdatafindnode.set_start(options.seg[0] - datafind_pad)
lscdatafindnode.set_end(options.seg[1] + 1)
lscdatafindnode.set_observatory(observatory)
dag.add_node(lscdatafindnode)

lladdnode = power.LladdNode(lladdjob)
lladdnode.set_name("lalapps_lladd")
dag.add_node(lladdnode)

for seg in jobsegs:
	powernode = power.PowerNode(powerjob)
	powernode.set_name("lalapps_power-" + str(seg[0]) + "-" + str(seg.duration()))
	powernode.set_cache(lscdatafindnode.get_output())
	powernode.set_ifo(options.instrument)
	powernode.set_start(seg[0])
	powernode.set_end(seg[1])
	powernode.add_parent(lscdatafindnode)
	lladdnode.add_parent(powernode)
	lladdnode.add_file_arg(powernode.get_output())
	dag.add_node(powernode)

dag.write_sub_files()
dag.write_dag()
