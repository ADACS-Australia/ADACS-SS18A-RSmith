#!/usr/bin/env @PYTHONPROG@
"""
power_pipe.in - standalone power pipeline driver script

$Id$

This script produces the condor submit and dag files to run
the standalone power code on LIGO data
"""

__author__ = 'Duncan Brown <duncan@gravity.phys.uwm.edu>'
__date__ = '$Date$'
__version__ = '$Revision$'[11:-2]

# import standard modules and append the lalapps prefix to the python path
import sys, os
import getopt, re, string
import tempfile
import ConfigParser
sys.path.append('@PYTHONLIBDIR@')

# import the modules we need to build the pipeline
from glue import pipeline
import power

def usage():
  msg = """\
Usage: lalapps_power_pipe [options]

  -h, --help               display this message
  -v, --version            print version information and exit
  -u, --user-tag TAG       tag the job with TAG (overrides value in ini file)

  -d, --datafind           run LSCdataFind to create frame cache files
  -r, --power              run lalapps_power on the first IFO
  -H, --ifo_h1             run on H1 
  -K, --ifo_h2             run on H2
  -L, --ifo_l              run on L1

  -c, --makeinjfiles       create the xml files with inj parameters
  -j, --injections         add simulated bursts
  -m, --mdcinjections      add mdc injections 

  -b, --coincidence        do coincidence analysis

  -p, --playground-only    only create chunks that overlap with playground
  -P, --priority PRIO      run jobs with condor priority PRIO

  -f, --config-file FILE   use configuration file FILE
  -l, --log-path PATH      directory to write condor log file
"""
  print >> sys.stderr, msg

# pasrse the command line options to figure out what we should do
shortop = "hvdrHKLcjmbp:u:P:f:l:"
longop = [
  "help",
  "version",
  "datafind",
  "power",
  "ifo_h1",
  "ifo_h2",
  "ifo_l1",
  "makeinjfiles",
  "injections",
  "mdcinjections",
  "coincidence",
  "playground-only",
  "user-tag=",
  "priority=",
  "config-file=",
  "log-path="
  ]

try:
  opts, args = getopt.getopt(sys.argv[1:], shortop, longop)
except getopt.GetoptError:
  usage()
  sys.exit(1)

config_file = None
do_datafind = None
do_power = None
do_H1 = None
do_H2 = None
do_L1 = None
do_makeinjfiles = None
do_coincidence = None
do_inj = None
do_mdcinj = None
usertag = None
playground_only = 0
condor_prio = None
config_file = None
log_path = None

for o, a in opts:
  if o in ("-v", "--version"):
    print "$Id$"
    sys.exit(0)
  elif o in ("-h", "--help"):
    usage()
    sys.exit(0)
  elif o in ("-d", "--datafind"):
    do_datafind = 1
  elif o in ("-r", "--power"):
    do_power = 1
  elif o in ("-H", "--ifo_H1"):
    do_H1 = 1
  elif o in ("-K", "--ifo_H2"):
    do_H2 = 1
  elif o in ("-L", "--ifo_L1"):
    do_L1 = 1
  elif o in ("-c", "--makeinjfiles"):
    do_makeinjfiles = 1
  elif o in ("-j", "--injections"):
    do_inj = 1
  elif o in ("-m", "--mdcinjections"):
    do_mdcinj = 1
  elif o in ("-b", "--coincidence"):
    do_coincidence = 1
  elif o in ("-u", "--user-tag"):
    usertag = a
  elif o in ("-p", "--playground-only"):
    playground_only = 1
  elif o in ("-P", "--priority"):
    condor_prio = a
  elif o in ("-f", "--config-file"):
    config_file = a
  elif o in ("-l", "--log-path"):
    log_path = a
  else:
    print >> sys.stderr, "Unknown option:", o
    usage()
    sys.exit(1)

if not config_file:
  print >> sys.stderr, "No configuration file specified."
  print >> sys.stderr, "Use --config-file FILE to specify location."
  sys.exit(1)

if not log_path:
  print >> sys.stderr, "No log file path specified."
  print >> sys.stderr, "Use --log-path PATH to specify a location."
  sys.exit(1)

# try and make a directory to store the cache files and job logs
try: os.mkdir('cache')
except: pass
try: os.mkdir('logs')
except: pass

#try and make a directory to store the mdccache files
if do_mdcinj: 
  try: os.mkdir('mdccache')
  except: pass

#try and make a directory to store the coincidence outputs
if do_coincidence: 
  try: os.mkdir('coincident')
  except: pass

# create the config parser object and read in the ini file
cp = ConfigParser.ConfigParser()
cp.read(config_file)

# if a usertag has been specified, override the config file
if usertag:
  cp.set('pipeline','user-tag',usertag)
else:
  try:
    usertag = string.strip(cp.get('pipeline','user-tag'))
  except:
    usertag = None

# create a log file that the Condor jobs will write to
basename = re.sub(r'\.ini',r'',config_file)
tempfile.tempdir = log_path
if usertag:
  tempfile.template = basename + '.' + usertag + '.dag.log.'
else:
  tempfile.template = basename + '.dag.log.'
logfile = tempfile.mktemp()
fh = open( logfile, "w" )
fh.close()

# create the DAG writing the log to the specified directory
dag = pipeline.CondorDAG(logfile)
if usertag:
  dag.set_dag_file(basename + '.' + usertag + '.dag')
else:
  dag.set_dag_file(basename + '.dag')

# create the Condor jobs that will be used in the DAG
df_job = pipeline.LSCDataFindJob('cache','logs',cp)
inj_job = power.InjJob(cp)
mdcinj_job = power.MdcDataFindJob(cp)
pow_job = power.PowerJob(cp)
burca_job = power.BurcaJob(cp)

# set better submit file names than the default
if usertag:
  subsuffix = '.' + usertag + '.sub'
else:
  subsuffix = '.sub'
df_job.set_sub_file( basename + '.datafind'+ subsuffix )
inj_job.set_sub_file( basename + '.inj' + subsuffix )
mdcinj_job.set_sub_file( basename + '.mdcinj' + subsuffix )
pow_job.set_sub_file( basename + '.power' + subsuffix )
burca_job.set_sub_file( basename + '.burca' + subsuffix )

# set the usertag in the jobs
if usertag:
  pow_job.add_opt('user-tag',usertag)
  

# set the condor job priority
if condor_prio:
  df_job.add_condor_cmd('priority',condor_prio)
  pow_job.add_condor_cmd('priority',condor_prio)
  burca_job.add_condor_cmd('priority',condor_prio)	

# PRB -- This is where I have to leave Duncan behind and understand
# what he's actually done ...... ;)
# get the pad and chunk lengths from the values in the ini file
pad = int(cp.get('segmentation', 'pad-data'))
minduration = int(cp.get('segmentation', 'min-duration'))

# read science segs that are >= to min duration from the input file
data = pipeline.ScienceData()
data.read(cp.get('input','segments'),minduration)


#get the order of ifos to filter
if do_H1:
  ifo1 = cp.get('pipeline','ifo1')
  ifo1_snr = cp.get('pipeline','thresholdifo1')
  ifo1_calibration = cp.get('calibration','H1') 
if do_H2:
  ifo2 = cp.get('pipeline','ifo2')
  ifo2_snr = cp.get('pipeline','thresholdifo2')
  ifo2_calibration_1 = cp.get('calibration','H2-1')
  ifo2_calibration_2 = cp.get('calibration','H2-2')
  ifo2_cal_epoch = cp.get('calibration','H2-cal-epoch-boundary')
if do_L1:
  ifo3 = cp.get('pipeline','ifo3')
  ifo3_snr = cp.get('pipeline','thresholdifo3')
  ifo3_calibration = cp.get('calibration','L1')

# create all the LSCdataFind jobs to run in sequence
prev_df1 = None
prev_df2 = None
first_df2 = None
prev_mdcdf1 = None
prev_mdcdf2 = None
first_mdcdf2 = None

# create a list to store the burst jobs
burst_nodes = []

for seg in data:
  ######### find all the data ##########
  if do_H1 or do_H2: 
    df1 = pipeline.LSCDataFindNode(df_job)
    df1.set_start(seg.start() - pad)
    df1.set_end(seg.end() + pad)
    df1.set_observatory(ifo1[0])
    if prev_df1: 
      df1.add_parent(prev_df1)

    if do_datafind:
      dag.add_node(df1)

    prev_df1 = df1
  
  if do_L1:
    df2 = pipeline.LSCDataFindNode(df_job)
    df2.set_start(seg.start() - pad)
    df2.set_end(seg.end() + pad)
    df2.set_observatory(ifo3[0])
    if prev_df2: 
      df2.add_parent(prev_df2)

    if do_datafind:
      dag.add_node(df2)

    prev_df2 = df2

  ####### find all the mdc data #########
  if do_H1 or do_H2:
    mdcdf1 = power.MdcDataFindNode(mdcinj_job)
    mdcdf1.set_start(seg.start() - pad)
    mdcdf1.set_end(seg.end() + pad)
    mdcdf1.set_observatory(ifo1[0])

    if do_datafind: 
      mdcdf1.add_parent(df1)
    if do_mdcinj:
      dag.add_node(mdcdf1)

  if do_L1:
    mdcdf2 = power.MdcDataFindNode(mdcinj_job)
    mdcdf2.set_start(seg.start() - pad)
    mdcdf2.set_end(seg.end() + pad)
    mdcdf2.set_observatory(ifo3[0])

    if do_datafind: 
      mdcdf2.add_parent(df2)
    if do_mdcinj:
      dag.add_node(mdcdf2)


  ####### set the injecton jobs #########
  inj1 = power.InjNode(inj_job)
  inj1.set_start(seg.start())
  inj1.set_end(seg.end())

 
  if do_datafind: 
    inj1.add_parent(df1)
  if do_makeinjfiles:
    dag.add_node(inj1)


  ####### set the power jobs ########
  if do_H1:
    pow1 = power.PowerNode(pow_job)
    pow1.set_start(seg.start())
    pow1.set_end(seg.end())
    pow1.set_ifo(ifo1)
    pow1.set_cache(df1.get_output())
    if do_mdcinj:
      pow1.set_mdccache(mdcdf1.get_output())
    if do_inj: 
      pow1.set_inj(inj1.get_output())
      pow1.add_var_opt('calibration-cache',ifo1_calibration)
    pow1.add_var_opt('threshold',ifo1_snr)

    if do_datafind: 
      pow1.add_parent(df1)
    if do_mdcinj: 
      pow1.add_parent(mdcdf1)
    if do_makeinjfiles:
      pow1.add_parent(inj1)	
    if do_power: 
      dag.add_node(pow1)

  if do_H2:
    pow2 = power.PowerNode(pow_job)
    pow2.set_start(seg.start())
    pow2.set_end(seg.end())
    pow2.set_ifo(ifo2)
    pow2.set_cache(df1.get_output())
    if do_mdcinj:
      pow2.set_mdccache(mdcdf1.get_output())
    if do_inj: 
      pow2.set_inj(inj1.get_output())
      if seg.start() >= ifo2_cal_epoch: 
        pow2.add_var_opt('calibration-cache',ifo2_calibration_2)
      else:
        pow2.add_var_opt('calibration-cache',ifo2_calibration_1)     
    pow2.add_var_opt('threshold',ifo2_snr)

    if do_datafind: 
      pow2.add_parent(df1)
    if do_mdcinj: 
      pow2.add_parent(mdcdf1)
    if do_makeinjfiles:
      pow2.add_parent(inj1)	
    if do_power: 
      dag.add_node(pow2)

  if do_L1:
    pow3 = power.PowerNode(pow_job)
    pow3.set_start(seg.start())
    pow3.set_end(seg.end())
    pow3.set_ifo(ifo3)
    pow3.set_cache(df2.get_output())
    if do_mdcinj:
      pow3.set_mdccache(mdcdf2.get_output())
    if do_inj: 
      pow3.set_inj(inj1.get_output())
      pow3.add_var_opt('calibration-cache',ifo3_calibration)
    pow3.add_var_opt('threshold',ifo3_snr)

    if do_datafind: 
      pow3.add_parent(df2)
    if do_mdcinj: 
      pow3.add_parent(mdcdf2)
    if do_makeinjfiles:
      pow3.add_parent(inj1)	
    if do_power: 
      dag.add_node(pow3)

  ########set the burca jobs ###########
  if do_coincidence:
    burca = power.BurcaNode(burca_job)
    if do_H1 and do_L1:	
      burca.set_ifoa(pow1.get_output())
      burca.set_ifob(pow3.get_output()) 
    if do_H2 and do_L1:	
      burca.set_ifoa(pow2.get_output())
      burca.set_ifob(pow3.get_output())
    if do_H1 and do_H2:	
      burca.set_ifoa(pow1.get_output())
      burca.set_ifob(pow2.get_output())
    burca.set_start(seg.start())
    burca.set_end(seg.end())
    burca.add_var_opt('outfile',burca.get_output())

    if do_datafind: 
      burca.add_parent(df1)
      if do_L1:
        burca.add_parent(df2)
    if do_mdcinj: 
      burca.add_parent(mdcdf1)
      if do_L1:
        burca.add_parent(mdcdf2)
    if do_makeinjfiles:
      burca.add_parent(inj1)	
    if do_power:
      if do_H1 and do_L1: 
        burca.add_parent(pow1)
        burca.add_parent(pow3)
      if do_H2 and do_L1: 
        burca.add_parent(pow2)
        burca.add_parent(pow3)
      if do_H1 and do_H2: 
        burca.add_parent(pow1)
        burca.add_parent(pow2)
    dag.add_node(burca)
          

# write out the DAG
dag.write_sub_files()
dag.write_dag()

# write a message telling the user that the DAG has been written
print "\nCreated a DAG file which can be submitted by executing"
print "\n   condor_submit_dag", dag.get_dag_file()
print """\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
If you are running LSCdataFind jobs, do not forget to initialize your grid 
proxy certificate on the condor submit machine by running the commands
 
  unset X509_USER_PROXY
  grid-proxy-init -hours 72

Enter your pass phrase when promted. The proxy will be valid for 72 hours. 
If you expect the LSCdataFind jobs to take longer to complete, increase the
time specified in the -hours option to grid-proxy-init. You can check that 
the grid proxy has been sucessfully created by executing the command:

  grid-cert-info -all -file /tmp/x509up_u`id -u`

This will also give the expiry time of the proxy. You should also make sure
that the environment variable LSC_DATAFIND_SERVER is set the hostname and
optional port of server to query. For example on the UWM medusa cluster this
you should use

  export LSC_DATAFIND_SERVER=dataserver.phys.uwm.edu

Contact the administrator of your cluster to find the hostname and port of the
LSCdataFind server.
"""

# write out a log file for this script
if usertag:
  log_fh = open(basename + '.pipeline.' + usertag + '.log', 'w')
else:
  log_fh = open(basename + '.pipeline.log', 'w')
  
log_fh.write( "$Id$" + "\n\n" )
log_fh.write( "Invoked with arguments:\n" )
for o, a in opts:
  log_fh.write( o + ' ' + a + '\n' )
log_fh.write( "\n" )
log_fh.write( "Parsed " + str(len(data)) + " science segments\n" )
total_data = 0
for seg in data:
  for chunk in seg:
    total_data += len(chunk)
print >> log_fh, "total data =", total_data

print >> log_fh, "\n===========================================\n"
print >> log_fh, data
for seg in data:
  print >> log_fh, seg
  for chunk in seg:
    print >> log_fh, chunk

sys.exit(0)

