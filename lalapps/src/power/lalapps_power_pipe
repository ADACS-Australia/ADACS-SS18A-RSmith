#!/usr/bin/python
#
# $Id$
#
# Copyright (C) 2006  Kipp C. Cannon
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.


#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


"""
Excess power offline pipeline construction script.
"""

__author__ = "Kipp Cannon <kipp@gravity.phys.uwm.edu>"
__date__ = "$Date$"[5:-2]
__version__ = "$Revision$"[11:-2]

import ConfigParser
import glob
import math
from optparse import OptionParser
import os
import sys
import tempfile

from glue import pipeline
from glue import segments
from glue import segmentsUtils
from glue.lal import CacheEntry
from pylal import ligolw_cafe
from pylal import llwapp
from pylal.date import LIGOTimeGPS
from lalapps import power


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


def parse_command_line():
	parser = OptionParser(version="%prog CVS $Id$")
	parser.add_option("--condor-log-dir", metavar = "path", default = ".", help = "set directory for Condor log (default = .)")
	parser.add_option("--config-file", metavar = "filename", default = "power.ini", help = "set .ini config file name (default = power.ini)")
	parser.add_option("--full-segments", action = "store_true", help = "analyze all data from segment lists, not just coincident times")
	parser.add_option("--minimum-gap", metavar = "seconds", default = "240", help = "merge jobs analyzing data from the same instrument if the gap between them is less than this many seconds (default = 240)")
	parser.add_option("--no-injections", action = "store_true", help = "do not do injections")
	parser.add_option("--time-slides", metavar = "filename", default = "time_slides.xml", help = "set file from which to obtain the time slide table")
	parser.add_option("-v", "--verbose", action = "store_true", help = "be verbose")
	options, filenames = parser.parse_args()

	options.time_slides = CacheEntry(None, None, None, "file://localhost" + os.path.abspath(options.time_slides))

	options.minimum_gap = float(options.minimum_gap)

	return options, (filenames or ["power.dag"])

try:
	options, filenames = parse_command_line()
except ValueError, e:
	print >>sys.stderr, "error: %s" % str(e)


#
# =============================================================================
#
#                                    Config
#
# =============================================================================
#


def parse_config_file(options):
	if options.verbose:
		print >>sys.stderr, "reading %s ..." % options.config_file
	config = ConfigParser.SafeConfigParser()
	config.read(options.config_file)

	options.tag = config.get("pipeline", "user_tag")
	options.enable_clustering = config.getboolean("pipeline", "enable_clustering")

	options.seglistdict = segments.segmentlistdict()
	for ifo in config.get("pipeline", "ifos").split():
		options.seglistdict[ifo] = segmentsUtils.fromsegwizard(file(config.get("pipeline", "seglist_%s" % ifo)), coltype = LIGOTimeGPS).coalesce()

	options.psds_per_power = config.getint("pipeline", "psds_per_power")
	options.psds_per_injection = config.getint("pipeline", "psds_per_injection")

	return config

config_parser = parse_config_file(options)


#
# =============================================================================
#
#                              Define .sub Files
#
# =============================================================================
#


power.init_job_types(config_parser)


#
# =============================================================================
#
#                            Determine Segment List
#
# =============================================================================
#


def remove_too_short_segments(seglistdict, powerjob):
	"""
	Remove segments from seglistdict that are too short to analyze.
	"""
	# CAUTION:  this function modifies seglistdict in place.
	for seglist in seglistdict.itervalues():
		for i in xrange(len(seglist) - 1, -1, -1):
			if not power.segment_ok(powerjob, seglist[i]):
				del seglist[i]


def compute_segment_lists(seglistdict, time_slides, minimum_gap, full_segments = True, verbose = False):
	# CAUTION:  this function modifies seglistdict in place.
	if verbose:
		print >>sys.stderr, "constructing segment list ..."

	if not full_segments:
		# cull too-short single-instrument segments from the input
		# segmentlist dictionary;  this can significantly increase
		# the speed of the llwapp.get_coincident_segmentlistdict()
		# function when the input segmentlists have had many data
		# quality holes poked out of them
		remove_too_short_segments(seglistdict, power.powerjob)

		# extract the segments that are coincident under the time
		# slides
		new = llwapp.get_coincident_segmentlistdict(seglistdict, time_slides)

		# adjust surviving segment lengths up to the next integer
		# number of PSDs
		for seglist in new.itervalues():
			for i in xrange(len(seglist)):
				psds = power.psds_from_job_length(power.powerjob, float(abs(seglist[i])))
				# FIXME: if psds is < 1, it means the
				# coincident segment is too short for even
				# a single job, but the original
				# single-instrument segments might be long
				# enough to run a job in each instrument.
				# Here, an attempt is made to search for
				# extra data in the time following the
				# coincident segment, but to maximize the
				# chances of analyzing the data we should
				# also try looking for data from the time
				# preceding the coincident segment.
				if psds < 1.0:
					psds = 1.0
				new_duration = power.job_length_from_psds(power.powerjob, int(math.ceil(psds)))
				seglist[i] = segments.segment(seglist[i][0], seglist[i][0] + new_duration)

		# try to fill gaps between jobs
		new.protract(minimum_gap / 2).contract(minimum_gap / 2)

		# and take intersection with original segments to not
		# exceed original bounds
		seglistdict &= new

	# remove segments that are too short
	remove_too_short_segments(seglistdict, power.powerjob)


#
# =============================================================================
#
#                          Single Instrument Fragment
#
# =============================================================================
#


def make_single_instrument_fragment(dag, options, instrument, seglist, binjnodes = None):
	powerfrags = []
	injpowerfrags = []
	for seg in seglist:
		if options.verbose:
			print >>sys.stderr, "generating %s fragment %s" % (instrument, str(seg))

		# LSCdataFind
		datafindfrag = power.make_datafind_fragment(dag, instrument, seg)

		# a segment without injections
		nodes = power.make_power_segment_fragment(dag, datafindfrag, instrument, seg, options.tag, options.psds_per_power, clustering = options.enable_clustering, verbose = options.verbose)

		# store fragment
		powerfrags.extend(nodes)

		# a segment with injections
		if binjnodes is not None:
			nodes = power.make_injection_segment_fragment(dag, datafindfrag, binjnodes, seg, instrument, options.psds_per_injection, options.tag, clustering = options.enable_clustering, verbose = options.verbose)

			# store fragment
			injpowerfrags.extend(nodes)

	return powerfrags, injpowerfrags


#
# =============================================================================
#
#                         Coincidence Post-Processing
#
# =============================================================================
#


def make_coinc_stage(dag, parents, tag, time_slides, time_slides_cache_entry, binjnodes = None, verbose = False):
	if verbose:
		print >>sys.stderr, "Generating coincidence nodes for tag = %s" % tag

	# arrange cache entries into groups using ligolw_cafe
	caches = []
	for node in parents:
		cache = node.get_output_cache()
		if len(cache) != 1:
			raise Exception, "each parent must provide exactly 1 file"
		caches.extend(cache)
	caches = [bin.objects for bin in ligolw_cafe.ligolw_cafe(caches, time_slides, verbose = verbose)[1]]

	# now that the cache entries have been grouped, figure out which
	# DAG node goes with which entry (assume each DAG fragment provided
	# a cache of output files with exactly 1 file);  also compute the
	# segment spanned by each cache while we're at it.
	segs = []
	fragments = []
	# so we can modify it
	parents = list(parents)
	for cache_num, cache in enumerate(caches):
		segs.append(segments.segmentlist([c.segment for c in cache]).extent())
		fragments.append([])
		for parent_num in xrange(len(parents) - 1, -1, -1):
			if parents[parent_num].get_output_cache()[0] in cache:
				fragments[cache_num].append(parents.pop(parent_num))
		if len(fragments[cache_num]) != len(cache):
			# couldn't find the node that provided a cache?
			raise Exception, "oops"
	if len(parents):
		# there were parents that didn't match any caches?
		print "Warning:  %d jobs produce output that will not be used by a coincidence job" % len(parents)

	# build the ligolw_add + ligolw_burca fragments;  if a binjnode has
	# been provided, add its output into the lladd node and add
	# binjfinding.
	for parents, cache, seg in zip(fragments, caches, segs):
		nodes = power.make_lladd_fragment(dag, parents, "ALL", seg, tag, preserves = [time_slides_cache_entry])
		nodes[0].set_output("ALL-COINC_%s-%s-%s.xml.gz" % (tag, int(seg[0]), int(abs(seg))))
		nodes[0].add_input_cache([time_slides_cache_entry])
		if binjnodes is not None:
			nodes[0].add_input_cache(binjnodes[0].get_output_cache())
			nodes[0].add_preserve_cache(binjnodes[0].get_output_cache())
			nodes = power.make_bucut_fragment(dag, nodes, "ALL", seg, tag)
			nodes = power.make_binjfind_fragment(dag, nodes, "ALL", seg, tag)
		nodes = power.make_burca_fragment(dag, nodes, "ALL", seg, tag)


#
# =============================================================================
#
#                               DAG Construction
#
# =============================================================================
#


time_slides = ligolw_cafe.get_time_slides(options.time_slides.path(), verbose = options.verbose, gz = options.time_slides.path()[-3:] == ".gz")

compute_segment_lists(options.seglistdict, time_slides, options.minimum_gap, full_segments = options.full_segments, verbose = options.verbose)

power.make_dag_directories(config_parser)

dag = pipeline.CondorDAG(tempfile.mkstemp(".log", "power_", options.condor_log_dir)[1])
dag.set_dag_file(os.path.splitext(filenames[0])[0])


#
# DAG "preamble":  injection and time slide lists.
#


if options.no_injections:
	binjnodes = None
else:
	binjnodes = power.make_multibinj_fragment(dag, options.seglistdict.extent_all(), "INJECTIONS_%s" % options.tag)


#
# Build single instrument fragments, one for each instrument for each
# segment.
#


sngl_inst_pwrfrags = []
sngl_inst_injfrags = []
for instrument, seglist in options.seglistdict.iteritems():
	a, b = make_single_instrument_fragment(dag, options, instrument, seglist, binjnodes)
	sngl_inst_pwrfrags.extend(a)
	sngl_inst_injfrags.extend(b)


#
# Coincidence
#


make_coinc_stage(dag, sngl_inst_pwrfrags, options.tag, time_slides, options.time_slides, verbose = options.verbose)
make_coinc_stage(dag, sngl_inst_injfrags, "INJECTIONS_%s" % options.tag, time_slides, options.time_slides, binjnode = binjnode, verbose = options.verbose)


#
# Output
#


if options.verbose:
	print >>sys.stderr, "writing dag ..."
dag.write_sub_files()
dag.write_dag()


#
# =============================================================================
#
#                               DAG Verification
#
# =============================================================================
#


def check_caches_against_segwizard(segwizard_filename, cache_filenames):
	"""
	Check that the segments in the segwizard file span the files in the
	given caches.  This ensures that no job has been created that
	analyzes data outside of the given input segment lists.
	"""
	segwizard_list = segmentsUtils.fromsegwizard(file(segwizard_filename), coltype = LIGOTimeGPS).coalesce()
	cache_list = segments.segmentlist()
	for cache_filename in cache_filenames:
		cache_list |= segmentsUtils.fromlalcache(file(cache_filename), coltype = LIGOTimeGPS).coalesce()
	extra = cache - segwizard
	if extra:
		raise ValueError, "caches span these times not spanned by segwizard: %s" % str(extra)
