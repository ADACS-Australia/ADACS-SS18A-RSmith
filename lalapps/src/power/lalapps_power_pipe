#!/usr/bin/python
#
# $Id$
#
# Copyright (C) 2006  Kipp C. Cannon
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.


#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


"""
Excess power offline pipeline construction script.
"""


import ConfigParser
import glob
import math
from optparse import OptionParser
import os
import sys
import tempfile


from glue import pipeline
from glue import segments
from glue import segmentsUtils
from glue.lal import CacheEntry
from pylal import ligolw_cafe
from pylal import llwapp
from pylal.date import LIGOTimeGPS
from lalapps import power


__author__ = "Kipp Cannon <kipp@gravity.phys.uwm.edu>"
__date__ = "$Date$"[5:-2]
__version__ = "$Revision$"[11:-2]


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


def parse_command_line():
	parser = OptionParser(
		version="%prog CVS $Id$"
	)
	parser.add_option("--condor-log-dir", metavar = "path", default = ".", help = "Set the directory for Condor log files (default = \".\").")
	parser.add_option("--config-file", metavar = "filename", default = "power.ini", help = "Set .ini configuration file name (default = \"power.ini\").")
	parser.add_option("--full-segments", action = "store_true", help = "Analyze all data from segment lists, not just coincident times.")
	parser.add_option("--minimum-gap", metavar = "seconds", default = "240", help = "Merge jobs analyzing data from the same instrument if the gap between them is less than this many seconds (default = 240).")
	parser.add_option("--variant", metavar = "[injections|noninjections|both]", default = "both", help = "Select the variant of the pipeline to construct.  \"injections\" produces a simulations-only version of the pipeline, \"noninjections\" produces a version with no simulation jobs, and \"both\" produces a full pipeline with both simulation and non-simulation jobs.")
	parser.add_option("--background-time-slides", metavar = "filename", default = [], action = "append", help = "Set file from which to obtain the time slide table for use in the background branch of the pipeline (default = \"background_time_slides.xml.gz\").  Provide this argument multiple times to provide multiple time slide files, each will result in a separate set of ligolw_burca jobs.")
	parser.add_option("--injection-time-slides", metavar = "filename", help = "Set file from which to obtain the time slide table for use in the injection branch of the pipeline (default = \"injection_time_slides.xml.gz\").")
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")
	options, filenames = parser.parse_args()

	options.minimum_gap = float(options.minimum_gap)

	if options.variant not in ("injections", "noninjections", "both"):
		raise ValueError, "unrecognized --variant %s" % options.variant
	options.do_injections = options.variant in ("injections", "both")
	options.do_noninjections = options.variant in ("noninjections", "both")

	if options.do_injections and not options.injection_time_slides:
		raise ValueError, "missing required --injection-time-slides argument"
	if options.do_noninjections and not options.background_time_slides:
		raise ValueError, "missing required --background-time-slides argument"

	return options, (filenames or ["power.dag"])


#
# =============================================================================
#
#                                    Config
#
# =============================================================================
#


def parse_config_file(options):
	if options.verbose:
		print >>sys.stderr, "reading %s ..." % options.config_file
	config = ConfigParser.SafeConfigParser()
	config.read(options.config_file)

	options.tag = config.get("pipeline", "user_tag")
	options.enable_clustering = config.getboolean("pipeline", "enable_clustering")

	seglistdict = segments.segmentlistdict()
	for ifo in config.get("pipeline", "ifos").split():
		seglistdict[ifo] = segmentsUtils.fromsegwizard(file(config.get("pipeline", "seglist_%s" % ifo)), coltype = LIGOTimeGPS).coalesce()

	options.psds_per_power = config.getint("pipeline", "psds_per_power")
	options.psds_per_injection = config.getint("pipeline", "psds_per_injection")

	return seglistdict, config


#
# =============================================================================
#
#                            Determine Segment List
#
# =============================================================================
#


def remove_too_short_segments(seglistdict, powerjob):
	"""
	Remove segments from seglistdict that are too short to analyze.
	"""
	# CAUTION:  this function modifies seglistdict in place.
	for seglist in seglistdict.itervalues():
		for i in xrange(len(seglist) - 1, -1, -1):
			if not power.segment_ok(powerjob, seglist[i]):
				del seglist[i]


def compute_segment_lists(seglistdict, time_slides, minimum_gap, full_segments = True, verbose = False):
	if verbose:
		print >>sys.stderr, "constructing segment list ..."

	seglistdict = seglistdict.copy()

	if not full_segments:
		# cull too-short single-instrument segments from the input
		# segmentlist dictionary;  this can significantly increase
		# the speed of the llwapp.get_coincident_segmentlistdict()
		# function when the input segmentlists have had many data
		# quality holes poked out of them
		remove_too_short_segments(seglistdict, power.powerjob)

		# extract the segments that are coincident under the time
		# slides
		new = llwapp.get_coincident_segmentlistdict(seglistdict, time_slides)

		# adjust surviving segment lengths up to the next integer
		# number of PSDs
		for seglist in new.itervalues():
			for i in xrange(len(seglist)):
				psds = power.psds_from_job_length(power.powerjob, float(abs(seglist[i])))
				# FIXME: if psds is < 1, it means the
				# coincident segment is too short for even
				# a single job, but the original
				# single-instrument segments might be long
				# enough to run a job in each instrument.
				# Here, an attempt is made to search for
				# extra data in the time following the
				# coincident segment, but to maximize the
				# chances of analyzing the data we should
				# also try looking for data from the time
				# preceding the coincident segment.
				if psds < 1.0:
					psds = 1.0
				new_duration = power.job_length_from_psds(power.powerjob, int(math.ceil(psds)))
				seglist[i] = segments.segment(seglist[i][0], seglist[i][0] + new_duration)

		# try to fill gaps between jobs
		new.protract(minimum_gap / 2).contract(minimum_gap / 2)

		# and take intersection with original segments to not
		# exceed original bounds
		seglistdict &= new

	# remove segments that are too short
	remove_too_short_segments(seglistdict, power.powerjob)

	# done
	return seglistdict


#
# =============================================================================
#
#                          Single Instrument Fragment
#
# =============================================================================
#


def make_datafind_fragment(dag, instrument, seglist, verbose = False):
	nodes = []
	for seg in seglist:
		if verbose:
			print >>sys.stderr, "making datafind job for %s spanning %s" % (instrument, seg)
		nodes += power.make_datafind_fragment(dag, instrument, seg)
	return nodes


def make_single_instrument_fragment(dag, datafinds, instrument, seglist, tag, options):
	nodes = []
	for seg in seglist:
		if options.verbose:
			print >>sys.stderr, "generating %s fragment %s" % (instrument, str(seg))

		# find the datafind job this power job is going to need,
		# taking the first if more than one is suitable
		dfnodes = [node for node in datafinds if (node.get_ifo() == instrument) and (seg in segments.segment(node.get_start(), node.get_end()))][:1]
		if not dfnodes:
			raise ValueError, "error, no datafind for power job at %s in %s" % (str(seg), instrument)

		# power job
		nodes += power.make_power_segment_fragment(dag, dfnodes, instrument, seg, tag, options.psds_per_power, verbose = options.verbose)

	return nodes


def make_single_instrument_injections_fragment(dag, datafinds, instrument, seglist, tag, binjnodes, options):
	nodes = []
	for seg in seglist:
		if options.verbose:
			print >>sys.stderr, "generating %s fragment %s" % (instrument, str(seg))

		# find the datafind job this power job is going to need
		dfnodes = [node for node in datafinds if (node.get_ifo() == instrument) and (seg in segments.segment(node.get_start(), node.get_end()))]
		if len(dfnodes) != 1:
			raise ValueError, "error, not exactly 1 datafind is suitable for power job at %s in %s" % (str(seg), instrument)

		# power job
		nodes += power.make_injection_segment_fragment(dag, dfnodes, binjnodes, instrument, seg, tag, options.psds_per_injection, verbose = options.verbose)

	return nodes


#
# =============================================================================
#
#                         Coincidence Post-Processing
#
# =============================================================================
#


def group_coinc_parents(parents, time_slides, verbose = False):
	if verbose:
		print >>sys.stderr, "Grouping jobs for coincidence analysis:"

	# group the output caches using ligolw_cafe according to how they
	# need to be combined to perform the coincidence analysis
	caches = []
	for parent in parents:
		caches += parent.get_output_cache()
	caches = [bin.objects for bin in ligolw_cafe.ligolw_cafe(caches, time_slides, verbose = verbose)[1]]

	# compute the segment spanned by each cache, store results in a
	# list in the same order as the cache list
	segs = [segments.segmentlist([c.segment for c in cache]).extent() for cache in caches]

	# for each cache, get a list of the nodes whose output files it
	# contains.  note that a parent node is allowed to provide more
	# than out output file, and thus can be listed in more than one
	# parent group
	if verbose:
		print >>sys.stderr, "Matching jobs to caches ..."
	# can't use [set()] * len(caches) for the normal reason
	parent_groups = [set() for c in caches]
	n_found = 0
	for n, parent in enumerate(parents):
		if verbose and not (n % 10):
			print >>sys.stderr, "\t%.1f%%\r" % (100.0 * n / len(parents)),
		found = False
		for output in parent.get_output_cache():
			# find the caches in which this output has been placed
			for i, cache in enumerate(caches):
				if output in cache:
					parent_groups[i].add(parent)
					found = True
		if found:
			n_found += 1
	if verbose:
		print >>sys.stderr, "\t100.0%"
	if verbose and len(parents) != n_found:
		# there were parents that didn't match any caches.  this
		# happens if ligolw_cafe decides their outputs aren't
		# needed
		print >>sys.stderr, "Notice:  %d jobs (of %d) produce output that will not be used by a coincidence job" % (len(parents) - n_found, len(parents))

	# done
	return zip(parent_groups, segs)


#
# =============================================================================
#
#                           Injection Identification
#
# =============================================================================
#


def make_binjfind_stage(dag, parents, tag, verbose = False):
	if verbose:
		print >>sys.stderr, "Adding injection identification nodes for tag = %s" % tag
	return power.make_binjfind_fragment(dag, power.make_bucut_fragment(dag, parents, tag), tag)


#
# =============================================================================
#
#                               DAG Construction
#
# =============================================================================
#


#
# Command line
#


options, filenames = parse_command_line()


#
# Parse .ini file.
#


seglistdict, config_parser = parse_config_file(options)


#
# Define .sub files
#


power.init_job_types(config_parser)


#
# Construct initial segment lists
#


if options.verbose:
	print >>sys.stderr, "Computing segments for which lalapps_power jobs are required ..."

if options.do_noninjections:
	background_time_slides = []
	background_seglistdict = segments.segmentlistdict()
	for background_time_slide_filename in options.background_time_slides:
		background_time_slides += [ligolw_cafe.get_time_slides(background_time_slide_filename, verbose = options.verbose, gz = background_time_slide_filename[-3:] == ".gz")]
		background_seglistdict |= compute_segment_lists(seglistdict, background_time_slides[-1], options.minimum_gap, full_segments = options.full_segments, verbose = options.verbose)
else:
	background_time_slides = [[]]
	background_seglistdict = segments.segmentlistdict()

# compute merged list of all background time slides
background_time_slides_all = reduce(lambda a, b: a + b, background_time_slides)


if options.do_injections:
	injection_time_slides = ligolw_cafe.get_time_slides(options.injection_time_slides, verbose = options.verbose, gz = options.injection_time_slides[-3:] == ".gz")
	injection_seglistdict = compute_segment_lists(seglistdict, injection_time_slides, options.minimum_gap, full_segments = options.full_segments, verbose = options.verbose)
else:
	injection_time_slides = []
	injection_seglistdict = segments.segmentlistdict()


#
# Start DAG
#


power.make_dag_directories(config_parser)
dag = pipeline.CondorDAG(tempfile.mkstemp(".log", "power_", options.condor_log_dir)[1])
dag.set_dag_file(os.path.splitext(filenames[0])[0])


#
# Injection list generation.
#


if options.verbose:
	print >>sys.stderr, "Building lalapps_binj jobs ..."
binjnodes = []
if options.do_injections:
	binjnodes += power.make_multibinj_fragment(dag, injection_seglistdict.extent_all(), "INJECTIONS_%s" % options.tag)


#
# Build datafind jobs, sort by instrument then by start time.  The
# segment list for each instrument is built from the union of the injection
# and background+foreground segment lists, with gaps smaller than the
# padding added to each datafind job filled in.  Filling in the gaps
# ensures that exactly 1 datafind job is suitable for each lalapps_power
# job.
#


if options.verbose:
	print >>sys.stderr, "building LSCdataFind jobs ..."
datafinds = []
for instrument, seglist in (injection_seglistdict | background_seglistdict).protract(power.datafind_pad / 2).contract(power.datafind_pad / 2).iteritems():
	datafinds += make_datafind_fragment(dag, instrument, seglist, verbose = options.verbose)
datafinds.sort(lambda a, b: cmp(a.get_ifo(), b.get_ifo()) or cmp(a.get_start(), b.get_start()))


#
# Build single instrument fragments, one for each instrument for each
# segment.
#


inj_pwr_nodes = []
for instrument, seglist in injection_seglistdict.iteritems():
	inj_pwr_nodes += make_single_instrument_injections_fragment(dag, datafinds, instrument, seglist, "INJECTIONS_%s" % options.tag, binjnodes, options)


pwr_nodes = []
for instrument, seglist in background_seglistdict.iteritems():
	pwr_nodes += make_single_instrument_fragment(dag, datafinds, instrument, seglist, options.tag, options)


#
# Coincidence
#


# injections

inj_coinc_nodes = []
for parents, seg in group_coinc_parents(inj_pwr_nodes, injection_time_slides, verbose = options.verbose):
	if options.enable_clustering:
		parents = power.make_lladded_bucluster_fragment(dag, parents, seg, "INJECTIONS_%s" % options.tag)
	inj_coinc_nodes += power.make_coinc_fragment(dag, parents, seg, "INJECTIONS_%s" % options.tag, options.injection_time_slides, binjnodes = binjnodes)


# non-injections

parent_groups_and_segs = group_coinc_parents(pwr_nodes, background_time_slides_all, verbose = options.verbose)
if options.enable_clustering:
	parent_groups_and_segs = [(power.make_lladded_bucluster_fragment(dag, parents, seg, options.tag), seg) for parents, seg in parent_groups_and_segs]
coinc_nodes = []
for parents, seg in parent_groups_and_segs:
	for n, time_slide_filename in enumerate(options.background_time_slides):
		coinc_nodes += power.make_coinc_fragment(dag, parents, seg, "%s_%d" % (options.tag, n), time_slide_filename, binjnodes = binjnodes)


#
# Injection identification
#


inj_coinc_nodes = make_binjfind_stage(dag, inj_coinc_nodes, "INJECTIONS_%s" % options.tag, verbose = options.verbose)


#
# Output
#


if options.verbose:
	print >>sys.stderr, "writing dag ..."
dag.write_sub_files()
dag.write_dag()


#
# =============================================================================
#
#                               DAG Verification
#
# =============================================================================
#


def check_caches_against_segwizard(segwizard_filename, cache_filenames):
	"""
	Check that the segments in the segwizard file span the files in the
	given caches.  This ensures that no job has been created that
	analyzes data outside of the given input segment lists.
	"""
	segwizard_list = segmentsUtils.fromsegwizard(file(segwizard_filename), coltype = LIGOTimeGPS).coalesce()
	cache_list = segments.segmentlist()
	for cache_filename in cache_filenames:
		cache_list |= segmentsUtils.fromlalcache(file(cache_filename), coltype = LIGOTimeGPS).coalesce()
	extra = cache - segwizard
	if extra:
		raise ValueError, "caches span these times not spanned by segwizard: %s" % str(extra)
