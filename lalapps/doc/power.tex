\DeclareGraphicsRule{.fig.pdf}{pdf}{.fig.pdf}{}

\chapter{Burst Search Programs}
\label{chapter:powertools}

 
\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% RUNNING A PIPELINE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Running the power code under Condor}
\label{subsection:running_power}
\idx[Running]{power}

This section is under construction!


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% subsection: power pipeline script
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Program \prog{lalapps\_power\_pipe}}
\label{program:lalapps-power-pipe}
\idx[Program]{\prog{lalapps\_power\_pipe}}

\begin{entry}

\item[Name]
\prog{lalapps\_power\_pipe} --- builds a one interferometer excess-power
search DAG   

\item[Synopsis]
\prog{lalapps\_power\_pipe}
[\option{--mdccache}~\parm{mdccache}] \newline \hspace*{0.5in}
[\option{--frame-cahce}~\parm{frcache}] \newline \hspace*{0.5in}
[\option{--help}] \newline \hspace*{0.5in}
[\option{--version}] \newline \hspace*{0.5in}
\option{--user-tag}~\parm{tag} \newline \hspace*{0.5in}
[\option{--datafind}] \newline \hspace*{0.5in}
\option{--power} \newline \hspace*{0.5in}
[\option{--injections}~\parm{file}] \newline \hspace*{0.5in}
[\option{--playground-only}] \newline \hspace*{0.5in}
[\option{--priority}~\parm{prio}] \newline \hspace*{0.5in}
\option{--config-file}~\parm{file} \newline \hspace*{0.5in}
\option{--log-path}~\parm{path}

\item[Description] 
\prog{lalapps\_power\_pipe} builds an excess power search DAG suitable for
running at the various LSC Data Grid sites.   The script requires a
configuration file.   An example file can be found in
\texttt{\$LALPREFIX/share/lalapps/power\_pipe.ini}.   Arguments to be
passed to the search code are supplied in this file and used the DAG
construction.  This is a standard python format configuration file.

\item[Options]\leavevmode
\begin{entry}
\item[\option{--user-tag} \parm{tag}]   The tag for the job.  This will
override the value set in the ini file

\item[\option{--datafind}] run LSCdataFind as part of the DAG to create the
cache files for each science segment

\item[\option{--power}] run \prog{lalapps\_power} on the data

\item[\option{--priority} \parm{prio}] run jobs with condor priority
\parm{prio}.

\item[\option{--config-file} \parm{file}] use configuration file
\parm{file}.

\item[\option{--log-path} \parm{path}] directory to write condor log file

\end{entry}


\item[Example]
To run the program,  type:
\begin{verbatim}
lalapps_power_pipe
\end{verbatim}

\item[Author]
Duncan Brown, Patrick Brady and Saikat Ray-Majumder
\end{entry}
\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% section: power code
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Program \prog{lalapps\_power}}
\label{program:lalapps-power}
\idx[Program]{\prog{lalapps\_power}}

\begin{entry}

\item[Name]
\prog{lalapps\_power} --- performs excess power analysis on real or
simulated data.

\item[Synopsis]
\prog{lalapps\_power} \newline \hspace*{0.5in}
\option{--bandwidth}~\parm{bandwidth} \newline \hspace*{0.5in}
[\option{--calibrated-data}~\parm{high pass frequency}] \newline \hspace*{0.5in}
[\option{--calibration-cache}~\parm{cache file}] \newline \hspace*{0.5in}
\option{--channel-name}~\parm{string} \newline \hspace*{0.5in}
[\option{--cluster}] \newline \hspace*{0.5in}
[\option{--debug-level}~\parm{level}] \newline \hspace*{0.5in}
\option{--default-alpha}~\parm{alpha} \newline \hspace*{0.5in}
[\option{--event-limit}~\parm{count}] \newline \hspace*{0.5in}
\option{--filter-corruption}~\parm{samples} \newline \hspace*{0.5in}
\option{--frame-cache}~\parm{cache file} \newline \hspace*{0.5in}
\option{--frame-dir}~\parm{directory} \newline \hspace*{0.5in}
\option{--frame-sample-rate}~\parm{Hz} \newline \hspace*{0.5in}
\option{--gps-end-time}~\parm{seconds} \newline \hspace*{0.5in}
\option{--gps-end-time-ns}~\parm{nanoseconds} \newline \hspace*{0.5in}
\option{--gps-start-time}~\parm{seconds} \newline \hspace*{0.5in}
\option{--gps-start-time-ns}~\parm{nanoseconds} \newline \hspace*{0.5in}
[\option{--help}] \newline \hspace*{0.5in}
[\option{--injection-file}~\parm{file name}] \newline \hspace*{0.5in}
\option{--low-freq-cutoff}~\parm{Hz} \newline \hspace*{0.5in}
[\option{--mdc-cache}~\parm{cache file}] \newline \hspace*{0.5in}
[\option{--mdc-channel}~\parm{channel name}] \newline \hspace*{0.5in}
\option{--min-freq-bin}~\parm{nfbin} \newline \hspace*{0.5in}
\option{--min-time-bin}~\parm{ntbin} \newline \hspace*{0.5in}
[\option{--noise-amplitude}~\parm{amplitude}] \newline \hspace*{0.5in}
\option{--nsigma}~\parm{sigma} \newline \hspace*{0.5in}
[\option{--printData}] \newline \hspace*{0.5in}
[\option{--printSpectrum}] \newline \hspace*{0.5in}
\option{--psd-average-method}~\parm{method} \newline \hspace*{0.5in}
\option{--psd-average-points}~\parm{samples} \newline \hspace*{0.5in}
[\option{--ram-limit}~\parm{MebiBytes}] \newline \hspace*{0.5in}
\option{--resample-filter}~\parm{filter type} \newline \hspace*{0.5in}
[\option{--seed}~\parm{seed}] \newline \hspace*{0.5in}
\option{--target-sample-rate}~\parm{Hz} \newline \hspace*{0.5in}
\option{--tile-overlap-factor}~\parm{factor} \newline \hspace*{0.5in}
\option{--threshold}~\parm{threshold} \newline \hspace*{0.5in}
[\option{--user-tag}~\parm{comment}] \newline \hspace*{0.5in}
[\option{--verbose}] \newline \hspace*{0.5in}
\option{--window}~\parm{window} \newline \hspace*{0.5in}
\option{--window-length}~\parm{samples} \newline \hspace*{0.5in}
\option{--window-shift}~\parm{samples}

\item[Description] 
\prog{lalapps\_power} performs an excess power analysis on real or
simulated data.  Consider searching for signals with the following
properties:
\begin{itemize}
\item Maximum signal time duration $T=2^a$ seconds where $a$ is a positive
or negative integer;  the sampling rate of the data stream is taken
assummed $\mbox{\texttt{srate}} = 2^b$ Hz.

\item The frequency band of the signal is between $f_{\mathrm{low}}$ Hz and
${f_{\mathrm{high}}}$ Hz.  Current versions of the code expect
${f_{\mathrm{high}}}-{f_{\mathrm{low}}}=2^d$ Hz where $d$ is an integer. 

\item Minimum time duration,

\item Minimum frequency bandwidth.
\end{itemize}

The input data for a search consists of LIGO/VIRGO \texttt{.gwf} frame
files.  These files can be collected together in a single directory, or in
locations described via the LAL frame cache file mechanism.  The code can
be used for Monte-Carlo simulations to determine search efficiency by
providing a list of injections to be made;  this injections list must be in
LIGO lightweight format and can be generated using the \prog{lalapps\_binj}
program described in Sec.~\ref{program:lalapps-binj}. 

The output data is written as \verb|sngl_burst| triggers in LIGO
lightweight XML files.  The files are named according to a standardized
naming convention
\begin{quote}
\{IFO\}-\{comment\}-POWER-\{GPS Start Time\}-\{duration\}.xml
\end{quote}
For example, if a search was run on the Hanford 4km interferometer and
generated triiggers starting at 731488397 and the triggers cover 33 seconds
after that time,  then the file name would be 
\begin{quote}
H1-test\_this\_again-POWER-731488397-33.xml
\end{quote}
where the comment was ``test\_this\_again''.  Note that the comment should
not include spaces and should use underscores instead.

\item[Options]\leavevmode
\begin{entry}
\item[\option{--bandwidth} \parm{bandwidth}]
Set the bandwidth in which the search is to be performed.  The integer
parameter \parm{bandwidth} is related to the physical bandwidth by the
formula
\[
\mbox{\parm{bandwidth}}
   = T \times (f_{\mathrm{high}} - f_{\mathrm{low}}).
\]

\item[\option{--calibrated-data} \parm{high pass frequency}]
When the \option{--calibrated-data} option is supplied, input time series
data is read as IEEE double-precision samples.  Calibrated data, for
example from the GEO detector, is stored in double-precision rather than
single-precision format.

This program uses IEEE single-precision internally, so double-precision
data must be quantized prior to processing.  It is typically necessary to
remove low-frequency noise from the signal prior to quantization in order
to reduce the loss of fidelity.  \parm{high pass frequency} sets the
cut-off frequency, in Hertz, of the high-pass filter applied to the time
series prior to quantization to single-precision.

\item[\option{--calibration-cache} \parm{cache file}]
Specify the location of calibration information.  \parm{cache file} gives
the path to a LAL-format frame cache file describing locations of
\texttt{.gwf} frame files that provide the calibration data ($\alpha$ and
$\beta$ coefficients) for the analysis.  Frame cache files are explained in
the ``framedata'' package in LAL.

\item[\option{--channel-name} \parm{string}]
Set the name of the data channel to analyze to \parm{string}.  This must
match the name of one of the data channels in the input frame files.  For
example, ``\verb|H2:LSC-AS_Q|''.

\item[\option{--cluster}]
Enable a clustering algorithm.  The result is the reduction of overlapping
triggers to a single trigger which covers a square time-frequency volume
which encompasses all overlapping trigger regions.   The signal-to-noise
and the confidence associated with a clustered trigger belong to the most
significant excess-power trigger in the cluster.  \emph{Note:}  The
algorithm is in a prototyping stage, and its behaviour is not robustly
tested.  Treat with care, and look at the code to insure you understand the
implication of enabling this option.

\item[\option{--debug-level} \parm{level}]
Sets the LAL debug level to \parm{level}.  The default value is
\texttt{LALMSGLVL2}.  A useful setting is 65 which turns off memory
padding, but keeps memory tracking and error messages.  If you want to turn
off memory tracking completely, then use 33.

\item[\option{--default-alpha} \parm{alpha}]
Set the default alpha value for tiles with sigma $<$ numSigmaMin to
\parm{alpha}.  See the LAL ``burstsearch'' package for details.

\item[\option{--event-limit} \parm{count}]
Limit the number of events reported in each time interval corresponding to
the PSD average length to \parm{count}.  The default value is 999.

\item[\option{--filter-corruption} \parm{samples}]
The input time series data is passed through a conditioning filter prior to
analysis.  Generally, the conditioning filter should be expected to corrupt
some amount of the beginning and end of the time series due to edge
effects.  This parameter tells the code how much data, in samples, should
be ignored from the start and end of the time series.  A reasonable value
is 0.5 seconds worth of data.

\item[\option{--frame-cache} \parm{cache file}]
Obtain the locations of input \texttt{.gwf} frame files from the LAL frame
cache file \parm{cache file}.  LAL frame cache files are explained in the
``framedata'' package in LAL and can be constructed by making calls to
\prog{LSCDataFind} on some systems.  If both \option{--frame-cache} and
\option{--frame-dir} are specified, \option{--frame-dir} will be used.  If
neither \option{--frame-cache} nor \option{--frame-dir} are provided, then
\option{--noise-amplitude} must be given in order to force the creation of
simulated noise.

\item[\option{--frame-dir} \parm{directory}]
Use the \texttt{.gwf} frame files found in \parm{directory} for input.  If
both \option{--frame-cache} and \option{--frame-dir} are specified,
\option{--frame-dir} will be used.  If neither \option{--frame-cache} nor
\option{--frame-dir} are provided, then \option{--noise-amplitude} must be
given in order to force the creation of simulated noise.

\item[\option{--frame-sample-rate} \parm{Hz}]
Set the sample rate of the data found in the input frame files to \parm{Hz}
samples per second.  \emph{Note:} This parameter must match the actual data
rate found in the files;  the code's behaviour is undefined if this
condition is not met.  The existance of this parameter is an unfortunate
side-effect of the code's input logic, and there is an on-going effort to
remove the need to specify this information on the command line.  A future
version of this program will be able to obtain this information directly
from the input files.

\item[\option{--gps-end-time} \parm{seconds}]
Set the integer part of the GPS time up to which input data should be read
to \parm{seconds}.

\item[\option{--gps-end-time-ns} \parm{nanoseconds}]
Set the fractional part of the GPS time up to which input data should be
read to \parm{nanoseconds}.

\item[\option{--gps-start-time} \parm{seconds}]
Set the integer part of the GPS time from which to start reading input data
to \parm{seconds}.

\item[\option{--gps-start-time-ns} \parm{nanoseconds}]
Set the fractional part of the GPS time from which to start reading input
data to \parm{seconds}.

\item[\option{--help}]
Display a usage message and exit.

\item[\option{--injection-file} \parm{file name}]
Use \parm{file name} as a LIGO lightweight XML file containing a list of
injections to be made.   The file should contain a \verb+sim_burst+ table
which is used to set information about the types of injections to be made.
This file may be constructed by hand, or one can use the
\verb+lalapps_binj+ program described in Section
\ref{program:lalapps-binj}.   

\item[\option{--low-freq-cutoff} \parm{Hz}]
Set the lowest frequency at which to search for gravitational waves to
\parm{Hz}.  This parameter is $f_{\mathrm{low}}$ from our description of
the desired signal parameters above.

\item[\option{--mdc-cache} \parm{cache file}]
Use \parm{cache file} as a LAL format frame cache file describing the
locations of MDC frames to be used for injections.

\item[\option{--mdc-channel} \parm{channel name}]
Use the data found in the channel \parm{channel name} in the MDC frames for
injections.

\item[\option{--min-freq-bin} \parm{nfbin}]
Set the smallest extent in frequency of TF tiles to search to the integer
\parm{nfbin}.  A reasonable value for this parameter is $2$.  The product
$\mbox{\textsc{minfbin}} \times \mbox{\textsc{mintbin}}$ is the minimum
time-frequency volume to be searched.  See LAL ``burstsearch'' package for
details.

\item[\option{--min-time-bin} \parm{ntbin}]
Set the smallest extent in time of TF tiles to search to the integer
\parm{ntbin}.  A reasonable value for this parameter is $2$.   The product
$\mbox{\textsc{minfbin}} \times \mbox{\textsc{mintbin}}$ is the minimum
time-frequency volume to be searched.  See LAL ``burstsearch'' package for
details.

\item[\option{--noise-amplitude} \parm{amplitude}]
If this parameter is provided, then Gaussian white noise will be mixed with
the input data.  If neither \option{--frame-cache} nor \option{--frame-dir}
is provided, then the noise will be the only data analyzed.

\item[\option{--nsigma} \parm{sigma}]
The threshold of the number of sigma to the floating point number
\parm{sigma}.  See the LAL ``burstsearch'' package for details.

\item[\option{--printData}]
Print the input time series at various stages of conditioning to data files
for diagnostic purposes.  Generally a version of the input time series is
dumped before and after each round of injections is made.

\item[\option{--printSpectrum}]
Print the average PSD measured over each PSD interval as well as the
frequency-domain representation of each analysis window to data files.
This is used for diagnostics.

\item[\option{--psd-average-method} \parm{method}]
Set the averaging method used in determining the average power spectral
density to \parm{method}.  This can be one of ``useMean'', ``useMedian'',
or ``useUnity''.

\item[\option{--psd-average-points} \parm{samples}]
Use \parm{samples} samples from the input time series to estimate the
average power spectral density of the detector's noise.  The average PSD is
used to whiten the data prior to applying the excess power statistic.  The
number of samples used for estimating the average PSD must be commensurate
with the analysis window length and analysis window spacing --- i.e.\ an
integer number of analysis windows must fit in the data used to estimate
the average PSD --- however this program will automatically round the
actual number of samples used down to the nearest integer for which this is
true.  This elliminates the need of the user to carefully determine a valid
number for this parameter, allowing him/her to instead select a number that
matches the observed length of time for which the instrument's noise is
stationary.

\item[\option{--ram-limit} \parm{MebiBytes}]
The start and stop GPS times may encompass a greater quantity of data than
can be analyzed at once due to RAM limitations.  This parameter can be used
to tell the code how much RAM, in MebiBytes, is available on the machine,
which it then uses to heursitically guess at a maximum time series length
that should be read.  The code then loops over the input data, processing
it in chunks of this size, until it has completed the analysis.  If this
parameter is not supplied, then the entire time series for the segment
identified by the GPS start and end times will be loaded into RAM.

\item[\option{--resample-filter} \parm{filter type}]
Set the type of filter used in the data rate down-conversion to
\parm{filter type}.  This can be ``butterworth'' or ``ldas''.

\item[\option{--seed} \parm{seed}]
When mixing noise into the input data, use \parm{seed} to seed the random
number generator.

\item[\option{--target-sample-rate} \parm{Hz}]
Down-convert the input data stream to a sample rate of \parm{Hz} samples
per second prior to analysis.  This can be used to reduce the number of CPU
cycles required to analyze a given quantity of input data.  \emph{Note:}
All other parameters are given with respect to \emph{this} sample rate.
For example, the number of samples used to estimate the average PSD as
given by \option{--psd-average-points} refers to the time series following
data rate down-conversion.

\item[\option{--tile-overlap-factor} \parm{factor}]
This parameter influences the amount of overlap between neighboring
time-frequency tiles.  It must be an integer.  A reasonable value for this
parameter is 3.  See the LAL ``burstsearch'' package for more information.

\item[\option{--threshold} \parm{threshold}]
Set the threshold in alpha above which identified events should be
discarded.  See the LAL ``burstsearch'' package for details.

\item[\option{--user-tag} \parm{comment}]
Set the user tag to the string \parm{comment}.  This string must not
contain spaces or dashes (``-'').  This string will appear in the name of
the file to which output information is written, and is recorded in the
various XML tables within the file.

\item[\option{--verbose}]
Enable the output of informational messages.

\item[\option{--window} \parm{window}]
Set the type of window to use when extracting an analysis window from the
time series to the LAL window corresponding to the integer parameter
\parm{window}.  See the LAL package ``window'' for a description of the
window that corresponds to each value of \parm{window}.

\item[\option{--window-length} \parm{samples}]
Set the number of samples to use for an analysis window to \parm{samples}.
Only the central half of the window will be analyzed, the first quarter and
last quarter of the window are used as padding to avoid corruption at
certain stages of the analysis.  For example, if you wish the code to
analyze the data in 1 second windows, you need to set this parameter to the
number of samples corresponding to 2 seconds of data.

\item[\option{--window-shift} \parm{samples}]
Set the number of samples each analysis window is shifted with respect to
the previous window to \parm{samples}.  This is typically set to one
quarter of the window length (i.e.\ half the length of data that is
actually analyzed).

\end{entry}


\item[Example]
To run the program, type:
\begin{verbatim}
lalapps_power \
--bandwidth 1024 \
--channel-name "H1:LSC-AS_Q" \
--cluster \
--default-alpha 0.5 \
--filter-corruption 8192 \
--frame-cache H-754008315-754008371.cache \
--frame-sample-rate 16384 \
--gps-end-time 754008363 \
--gps-end-time-ns 0 \
--gps-start-time 754008323 \
--gps-start-time-ns 0 \
--low-freq-cutoff 130.0 \
--min-freq-bin 2 \
--min-time-bin 2 \
--nsigma 2.0 \
--psd-average-method useMedian \
--psd-average-points 548864 \
--resample-filter butterworth \
--target-sample-rate 16384 \
--threshold 1.0e-9 \
--tile-overlap-factor 3 \
--user-tag testing \
--verbose \
--window 2 \
--window-length 32768 \
--window-shift 8192
\end{verbatim}
For this to succeed, the current directory must contain the file
\texttt{H-754008315-754008931.cache} describing the locations of the
\texttt{.gwf} frame files with channel \verb|H1:LSC-AS_Q| data at 16384
samples per second for the GPS times 754008323.0 s through 754008363.0 s.

\item[Authors]
Patrick Brady,  Kipp Cannon and Saikat Ray Majumder 
\end{entry}
\clearpage

\subsection{Algorithmic Implementation}

The Excess Power search method is motivated by the classical theory of
signal detection in Gaussian noise.  The method is the optimal search
strategy~\cite{Anderson:2000yy} having only knowledge of the time duration
and frequency band of the expected signal,  but having no other
information about the power distribution in advance of detection. 

Consider a sampled time-series $s_j$ consisting of $N$ data points
sampled once per $\Delta$ seconds.  In our case,  this data has been
high-pss filtered using a Butterworth filter as described in
Sec.~\ref{ss:butterworth} but this does not change the algorithm,  so
we ignore it here.  This data is then windowed (using a user specified
window) and transformed into the frequency domain
\begin{equation}
\tilde{s}_k =  \frac{\Delta}{\sigma_w} \sum_{j=0}^{N-1} w_js_j e^{-2 \pi i j k /N}
\end{equation}
where $w_j$ is the window function and $\sigma_w$ is  
\begin{equation}
{\sigma_w}^2 = \frac{1}{N}\sum_{j=0}^{N-1}{w_j}^2 \; .
\end{equation}
Finally,  the data is whitened
\begin{equation}
\hat{s}_k = 2\sqrt{\frac{\Delta f}{P_k}}\tilde{s}_k
\end{equation}
where $\Delta f = {1}/{N\Delta}$ is the sampling frequency interval and 
$P_k = 2 \Delta f\langle|\tilde{s}_k^2| \rangle$ is the 
power spectral estimate.  This spectrum is estimated using the median
power at each frequency for a number of overlapping segments.
The use of the median avoids bias in the spectrum caused by the
presence of a gravitational wave or other large non-astrophysical
transients present in any of the segments.   Notice that 
\begin{equation}
\langle | \hat{s}_k |^2 \rangle = 2 \; .
\end{equation}
If the original time series contains only Gaussian noise,  this
construction makes the real and imaginary parts of
each frequency component a unit variance Gaussian random variable. 

\subsubsection{The filter}

The excess-power method relies on having a time-frequency decomposition of
the the incoming data.   Consider construction of the the time-frequency
plane in a pass-band $f_0 < f < f_0+b$.  A naive construction would set all
Fourier components outside of this band to zero and inverse transform.  A
major problem with this approach is that the band-pass filter,  when viewed
in the time-domain, has infinite extent(as shown in
Figure~\ref{fig:timedomainfilter} in a wrap around fashion) and can lead to
data corruption of the band-passed data stream when contructed as described
above. 

\begin{figure}
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/timedomainfilter}
\caption{Filter in the time domain in a wrapped around fashion}
\label{fig:timedomainfilter}
\end{center}
\end{figure}
 
We can, however, start from this and construct a filter which
preferentially passes the appropriate frequencies \emph{and} has finite
duration in the time domain (Figure~\ref{fig:timedomainfilter_1} \&
Figure~\ref{fig:freqdomainfilter_1}).

\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{figures/timedomainfilter_1}
\caption{Filter in the time domain}
\label{fig:timedomainfilter_1}
\end{center}
\end{figure}
\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{figures/freqdomainfilter_1}
\caption{Filter in the frequency domain}
\label{fig:freqdomainfilter_1}
\end{center}
\end{figure}

Consider the rectangular band-pass in the frequency domain
\begin{eqnarray}
G(t) &=& \int_{-f_0 - b}^{-f_0} e^{2 \pi i f t} df  +
\int_{f_0}^{f_0+b}  e^{2 \pi i f t} df 
= 2 \int_{f_0}^{f_0+b} \cos ( 2 \pi f t ) df \\
&=& \frac{\sin[2\pi(f_0+b)t] - \sin[2 \pi f_0 t]}{\pi t} \; .
\end{eqnarray}
This is just the sum of a sine and cosine at
frequency $f_0$ modulated at frequency $b$.
Consequently,  the amplitude modulation has its first zero at
$t = 1/ (2 b)$ and we can use a truncated time-domain filter
\begin{equation}
\Theta(t - t_0; f_0 , b) = \left\{
\begin{array}{lrl}
\displaystyle{
\frac{\sin[2\pi(f_0+b)(t-t_0)] - \sin[2 \pi f_0 (t-t_0)]}{\pi (t-t_0)} 
} & \mbox{\ \ } & |t-t_0| < 1/(2b)  \\
0 & & \textrm{Otherwise}
\end{array} \right. 
\end{equation}
which covers a time frequency volume $V = b (1/b) = 1$ and has its
peak at time $t_0$.   Notice that
\begin{equation}
\lim_{t\rightarrow t_0} \Theta(t - t_0; f_0 , b) = 2 b\; .
\end{equation}

\subsubsection{The signal-to-noise}

The signal to noise for a filter determined by $f_0$ and $b$ can be
written as a function of the peak time $t_0$ as
\begin{equation}
\rho(t_0) = \frac{z(t_0)}{\sigma}
\end{equation}
where
\begin{equation}
z(t_0) = \int_{-\infty}^\infty \hat{s}(t) \Theta(t-t_0; f_0, b) 
dt
\end{equation} and $\sigma^2 = \langle |z(t_0)|^2 \rangle$.  The time
series $z(t_0)$ is most conveniently computed using Fourier
transforms,  therefore
\begin{equation}
z(t_0) = \int_{-\infty}^{\infty} \hat{s}(f) \tilde{\Theta}^\ast (f) 
e^{2 \pi i ft_0} df
\end{equation}

The Excess Power search then identifies triggers which may be
gravitational wave bursts using the following algorithm.  For each
segment $\hat{s}_k$,  construct multiple time-frequency planes
${\cal{P}}^a$ such that the frequency resolution of each plane is
$2^a \times 1 \textrm{ Hz}$ and the time resolution $2^{-a} \times 1
\textrm{ s}$ where $a$ can have a range of values depending on the 
minimum and maximum time-frequency resolutions one wants to tolerate 
in the search.  However this approach allows
multi-resolution analysis of the time-frequency structure of the input
time series $s_j$ while insuring that all pixels have unit
time-frequency volume (the product of time resolution by frequency
resolution).   The power in a given pixel of time-frequency plane
${\cal{P}}^b$ is given by
\begin{equation}
{\cal E}^b(t,f) = {\cal E}^b( 2^{-b} J, 2^b \Sigma ) = 4 | H^b_{J\Sigma} |^2
\end{equation}
where 
\begin{equation}
H^b_{J\Sigma} = \sum_{\gamma=0}^{N_T} e^{ 2 \pi i J \gamma / N_T }
\tilde{w}_{N_T \Sigma + \gamma}
\end{equation}
and $N_T = 2^b$.

The method owes its name to the fact that it searches for excess
signal power in all the rectangular regions, called tiles, of each
time-frequency plane.  The list of tiles is designed to explore all
possible start times, frequency bands and durations subject to the
following constraints.  Tiles have a minimum time duration either
equal to the minimum time resolution of the particular time-frequency
plane or equal to $2/64 \textrm{ s}$,  whichever is greater.  The
maximum frequency band of any tile is $64 \textrm{ Hz}$.  The tiles
have a maximum time duration of $0.5 \textrm{ s}$ and corresponding
minimum frequency band of $2 \textrm{ Hz}$;  tiles always have integer
numbers of pixels on each side and hence have time-frequency volume
greater than or equal to unity.  For each allowed tile duration and
bandwidth,  the associated time-frequency plane is tiled and each tile
searched for excess power
\begin{equation}
{\cal{E}} = \sum_{J=J_1}^{J2-1} \sum_{\Sigma=\Sigma_1}^{\Sigma_2-1}
{\cal E}^b( 2^{-b} J , 2^b \Sigma ) \;.
\end{equation}

In Gaussian noise,  the power in a given tile is $\chi^2$-distributed
with $2 V$ degrees of freedom where $V$ is the time-frequency volume
of the tile.    A burst trigger is identified if the probability of
obtaining the excess power in a tile from Gaussian noise is smaller
than some threshold $\alpha$.   For large bursts in the data stream,
many tiles can be identified as triggers with different sizes and
aspect ratios.   When the search over a particular segment is
complete,   triggers are clustered and the clustering is described in 
Section~\ref{section:clustering} .

\subsubsection{An alternate Time-Frequency Plane construction}
In the previous section we described an algorithmic implementation
where multiple time-frequency planes are constructed and then all 
possible tiles in a plane are searched for excess power.  Each plane 
in that implementation has a unique time-frequency resolution.  However
while searching the tiles it is very likely that we consider many 
duplicate tiles from different planes.
  
Here we describe an alternate implementation where we construct a single
time frequency plane and then search over all possible tiles of different
time durations and frequency bands.  One first has to decide the maximum 
frequency band and the maximum time duration that are to be tolerated in
the search.  The maximum frequency band determines the minimum duration
a tile can have requiring the time-frequency volume be $1$ and 
similarly the maximum time duration determines the minimum frequency band
of the tile.  The time frequency plane is then constructed with a 
time-frequency resolution finer by at least a factor of $2$ compared to
the minimum resolutions of the tiles.  As a concrete example,  
suppose one wants to search for a
 maximum time duration of  $0.5 sec$  and a maximum frequency band of 
$32 Hz$.  This implies the minimum duration of a tile in the search will
be  $1/32 sec (31.2 msec)$  and the minimum frequency band will be  $2 Hz$.
 The time-frequency plane is then constructed with a timing resolution
of $1/64 sec (15.6 msec.)$ and a frequency resolution of $1 Hz$.         

The way one approaches towards this is almost similar to the implementation
described in the previous section.  For each whitened segment
$\hat{s}_k$, one constructs multiple channels,  each of which is an
inverse Fourier tranform of $b Hz$ of data,  where $b Hz$
 is the frequency resolution of the plane.  The elements of the 
time-frequency plane are then selected from these channels depending
on the timing resolution of the plane.  If we consider the example
that we were discussing above,  under the assumption that the
bandwidth over which we want to search is $128 Hz$,  there will be
$128$ channels given the fact that the frequency resolution of our
time-frequency plane is $1 Hz$.  Each of these channels is thus a time
series constructed by the inverse Fourier transform of $1 Hz$ of 
whitened data.          

The elements of a channel whose lower frequency is,  say,  $k_1$ can 
be represented as:
\begin{equation}
z_{jk_1} = \sum_{k=k_1}^{k_1+b-1} ( \hat{s}_k \tilde{\Theta}^\ast e^{2 \pi ijk/N}
                                  + c.c )
\end{equation},  where $b = \frac{b Hz}{\delta f}$, $\delta f$ being the 
frequency resolution of the data and $j = 0,\cdots, N-1$.  Note,  the timing
resolution of $z_{jk_1}$ is equal to that of the sampled detector data $s_j$.
 The elements of the time-frequency plane which can be picked from this 
particular channel $z_{jk_1}$ will then be 
\begin{equation}
H_{l_1k_1} = H_{l_1n_f + k_1} = z_{l_1\frac{\Delta T}{\delta t} , k_1}
\end{equation}, where $n_f = 128$ , the number of frequency bins, 
$l_1$ can lie between $0, \cdots, n_t - 1$ , $n_t$ being the number of 
time bins in the plane,  $\Delta T$ is the timing resolution of the
plane and $\delta t$ is that of $z_{jk_1}$.  Thus $n_t$ elements can be
 picked from each one of the $n_f$ number of channels and hence there will be 
$n_t \times n_f$ number of elements representing the time-frequency plane.

A tile in this plane can be denoted as $\tau ={J_1, J_2, K_1, K_2}$,  where
$J_1$ and $J_2$ represent the indices corresponding to the start and 
stop times of the tile respectively while $K_1$ and $K_2$ represent the
minimum and maximum frequencies.  The degrees of freedom assosciated with
$\tau$ will then be $dof_{\tau} = 2 \times (J_2 - J_1) \times \Delta T \times (K_2 - K_1) \times \Delta F $ and the signal to noise ratio will be 
\begin{equation}
\rho_{\tau} = \frac{\sum_{n=0}^{m-1}(\sum_{k=K1}^{K2}H_{[J_1 + n\frac{J_2-J_1}{dof_{\tau}}],k})^2}{\sigma ^2}
\end{equation}
,  where $m\frac{J_2-J_1}{dof_\tau} = J_2$.



\subsection{Time Domain Segmentation}
The excess power analysis code does not process the input data as a
continuous time series;  rather the time series is split into a sequence of
discrete ``analysis windows'', which are each analyzed individually.  To
account for the possibility of a burst event stradling the boundary between
two analysis windows, successive windows are staggered in such a way that
they overlap one another in time.  In this way, a burst event occuring on
the boundary of one window will (typically) be centred in the next.

Because edge effects at various stages of the analysis can corrupt the
beginning and end of the analysis window, the actual quantity of data
extracted from the input time series to form a window is twice the amount
that is analyzed.  Only results from the central half of the window are
retained, with the first and last quarters of each window being discarded.
The arrangement is shown in the following diagram.
\begin{center}
\input{figures/power/windows.fig}
\end{center}
Here we see a discrete time series (represented by the bottom-most
horizontal line) that contains 57344 samples.  It has been divided into a
sequence of four analysis windows, each containing 32768 samples.  A fifth,
greyed-out, analysis window is shown to indicate where the next window in
the sequence would start.  In the analysis of each window, the first and
last 8192 samples (first and last quarter) are discarded as indicated by
the crossed-out sections in each window.  In this particular example, each
window is shifted 8192 samples (also equal to one quarter of the window
length) from the start of the previous window.  This choice of window
length and window shift causes the sections of each window that are
actually searched for events (the sections that are not crossed out) to
overlap their neighbours by half of their own width.  This is the typical
mode of operation for the search code.  Notice that the first and last
quarter window length of the complete time series (the cross-out sections
in the bottom line) are \emph{not} analyzed, as they are discarded from the
only analysis windows in which they appear.

The excess power code whitens the input time series using an estimate of
the instrument's noise power spectral density (PSD).  The estimated noise
PSD is computed by averaging the PSDs from a number of successive analysis
windows.  The noise PSD is not estimated by averaging over the entire time
series in order to allow the code to track the (possibly) changing
character of the instrument's noise.  For convenience, the user is
permitted to enter the number of samples that should be used to estimate
the PSD.  The number of samples entered should correspond to the time for
which the instrument's noise can be approximated as stationary for the
purpose of the excess power analysis.  Since, however, the actual
estimation procedure involves averaging over an integer number of analysis
windows, it is necessary for the number of samples selected to correspond
to the boundary of an analysis window.  For convenience,
\prog{lalapps\_power} will automatically round the value entered down to
the nearest analysis window boundary.

The LAL function \function{EPSearch()} performs the parts of the analysis
described above.  It is given a time series that it divides into analysis
windows, which it uses to estimate the noise PSD.  Using the estimated
noise PSD, it whitens each analysis window and then searches them for burst
events.  Only the analysis windows within the data used to estimate the
noise PSD are whitened using that estimate.  Once those windows have been
searched for burst events, \function{EPSearch()} returns to the calling
procedure which then extracts a new time series from the input data and the
process repeats.  The parameter provided via the command line option
\option{--psd-average-points} sets the length of the time series that is
passed to \function{EPSearch()}.

As successive time series are passed to \function{EPSearch()}, in order for
the first analysis window to correctly overlap the last window from the
previous time series --- i.e.\ to ensure the same overlap between analysis
windows in neighbouring time series as exists between neighbouring windows
within a series --- it is necessary for the latter time series to begin
$(\mbox{\parm{window length}} - \mbox{\parm{window shift}})$ samples before
the end of the former series.  The arrangement is shown in the following
figure.
\begin{center}
\input{figures/power/psds.fig}
\end{center}
Here we see two of the time series from the first diagram above, each of
which is to be passed to \function{EPSearch()} for analysis.  To see why
the overlap between these two time series must be chosen as it is, refer to
the first diagram above to see where the greyed-out fifth analysis window
was to be placed.  That is where the first analysis window in the second
time series here will be placed.

Prior to looping over the data one noise PSD estimation length at a time,
the data is passed through a conditioning filter.  To account for edge
effects in the filter, an amount of data set by the command line option
\option{--filter-corruption} is dropped from the analysis at both the
begining and end of the time series.  The arrangement is shown in the
following diagram.
\begin{center}
\input{figures/power/conditioning.fig}
\end{center}
The bottom-most line in this diagram represents the time series as read
into memory from disk.  In this example we have read 106496 samples into
memory, and after passing it through the conditioning filter 8192 samples
are dropped from the beginning and end of the series.  The remaining data
is then passed to \function{EPSearch()} 57344 samples at a time --- just as
was done in the earlier examples --- with appropriate overlaps.  In this
example, it happens that an integer number of overlaping noise PSD
intervals fits into the data that survives the conditioning.  In general
this will not be the case.  If the last noise PSD interval would extend
beyond the end of the time series, it is moved to an earlier time so that
its end is aligned with the end of the available data.

If more data needs to be analyzed than will fit in RAM at one time, we must
read it into memory and analyze it in pieces.  In doing this, we again want
the analysis windows in neighbouring read cycles to overlap one another in
the same manner that neighbouring analysis windows within a single noise
PSD interval overlap one another.  This will be assured if, in the diagram
above, the start of the next data to be read from disk is arranged so that
the first noise PSD interval to be analyzed within it starts at the correct
location relative to the last PSD interval analyzed from the previous read
cycle.  Consideration of the diagram above reveals that in order to meet
this condition, the next data to be read into memory should start $(2
\times \mbox{\parm{filter corruption}} + \mbox{\parm{window length}} -
\mbox{\parm{window shift}})$ samples prior to the end of the previous data
to have been read.

\subsection{Performance on Gaussian data}
\label{section:Gaussian}

\subsubsection{Multiple time-frequency planes}
In this section we will describe the performance of Excess Power algorithm  
on Gaussian data.  Instead of reading in real data from frame files we filled 
in the time series with Gaussian noise generated by the LAL 
function \textsc{LALNormalDeviate}.  This data is first high passed using
a Butterworth filter( Section~\ref{section:Butterworth} ) and an 
average spectrum is estimated.  Figure~\ref{fig:gaussianspectrum} shows the 
average spectrum which was calculated using $548864$ sample points
($\sim 33.5$ secs) of the high passed data.  This average spectrum is then 
used to whiten and normalise the data.   Figure(~\ref{fig:gaussianfreqseries}) 
shows such a whitened and normalised frequency series of two seconds of data 
(normalisation and whitening will be described in other sections).
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/averagespec_psd}
\caption{Average Spectrum}
\label{fig:gaussianspectrum}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/freqseries_psd}
\caption{Frequency series}
\label{fig:gaussianfreqseries}
\end{center}
\end{figure}
 
Our search method relies heavily on the fact that in Gaussian noise, 
 the power in a given tile is $\chi^2$-distributed with 
$2 V$ degrees of freedom where $V$ is the time-frequency volume of the 
tile(~\ref{chapter:powertools} ).  Hence to check if our measurements 
actually follow the right distribution we did the following:
\begin{itemize}
\item Selected a time-frequency volume $V = 4$ and a time-frequency plane
      $= 3 $.
\item Printed out the power in the first time-frequency tile from 
      plane $3$ which had volume $4$ for each iteration when we searched 
      $2$ second data segments.
\item Plotted the power distribution for these tiles and compared that to the plot
      of the theoretical $\chi^2$ distributed data with $8$ degrees of freedom. 
\end{itemize}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/compchisquare_psd}
\caption{Comparison of the measured and theoretical distributions }
\label{fig:compchisquare_psd}
\end{center}
\end{figure}
Figure~\ref{fig:compchisquare_psd} shows the result of our tests. 
The blue curve is the measured power distribution of the tiles and the green
curve is the theoretical $\chi^2$ distribution with $8$ degrees of freedom.
 It is evident from the figure that though the measured distribution 
matches largely with the theoretical distribution,  there is a slight mismatch
at the power range $ \sim 1 - 10$ units.  Our 
first suspicion was that we may have been underestimating the average spectrum. 
 So we increased the number of sample points in the average spectrum 
estimation and repeated the comparison for each one of those increments. 
Figure ~\ref{fig:compchisquare} shows the plots for different sample 
points superposed together.  In the plot,  $psd = 548864$,  
so curves corresponding to $2psd,  3psd$  imply that they are 
distributions when  $1097728,  1646592$  points were respectively used 
to estimate the spectrum and the curve corresponding to $f8$ is the 
theoretical distribution with $8$ degrees of freedom.  However from 
this figure  we see that the curves for the diffrent sample 
points are almost identical,  which implies that the mismatch between the 
measured and theoretical distributions does not depend on the average 
spectrum estimation.   
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/compchisquare}
\caption{Comparison of the measured and theoretical distributions}
\label{fig:compchisquare}
\end{center}
\end{figure}
    
Hence at this point,  it is not quite clear what causes the mismatch but 
we have to further investigate that.

\subsubsection{Single time-frequency plane}
We carried out similar tests to check if the power in tiles in the 
single time-frequency plane follow the chi-square distribution.
Figure ~\ref{fig:compchisquarenew}  shows the distributions for
degrees of freedom $2$ and $4$. 
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/compchisquarenew2_4}
\caption{Comparison of the measured and theoretical distributions for the
  new time-frequency plane}
\label{fig:compchisquarenew}
\end{center}
\end{figure}     


\clearpage

\subsection{Butterworth Filter used in Excess Power Search}
\label{section:Butterworth}

The LIGO data is dominated by noise at low frequencies. We 
therefore apply a Butterworth high pass filter to get rid off these 
low frequency noise components. Here we report on a set of tests 
that were performed to check the performance of the high pass filter.

Of primary interest to us is the impulse response and frequency response 
of the filter. Figure~\ref{fig:checkbuttertimeseries} shows the 
time series containing a $\delta$ function before and after the 
application of the high pass filter:
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/checkbuttertimeseries}
\includegraphics[width=0.9\textwidth]{figures/butter894comptimeseries}
\caption{Top panel: Time series before Butterworth and 
Bottom 2 panels: Time series after Butterworth} \label{fig:checkbuttertimeseries}
\end{center}
\end{figure}
The impulse response in Figure~\ref{fig:checkbuttertimeseries} shows  
time domain ringing produced by the filter. Maximum amplitude of 
the ringing is upto about $1.5$\% of the original time series amplitude.
However ringing in the side lobes die down to almost zero within about
20 milliseconds on both sides of the $\delta$ function. This ringing may
affect the timing resolution in real search, but we are yet
to quantify that effect.  

The Butterworth filter is created using the LAL routine,
\texttt{LALButterworthREAL4TimeSeries()} which uses the following 
parameters to specify the filter:
\begin{itemize}
  \item the low frequency cut off, f2. 
  \item The attenuation, a2. It gives a measure of power
    that is allowed to pass at f2.
  \item The filter order, nMax.
\end{itemize}

The frequency response of the filter is shown in Figure~\ref{fig:butterworthtest4} 
where the chosen parameters were $f2 = 120Hz$, $a2 = 0.1$ and $nMax = 4$.
\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/butterworthtest4}
\includegraphics[width=0.9\textwidth]{figures/testattenuation4}
\caption{Frequency response of the high pass filter} \label{fig:butterworthtest4} 
\end{figure}
The red dotted line shows the spectrum which was measured without
the high pass filter while the blue line shows the spectrum after the data was 
high passed. From Figure~\ref{fig:butterworthtest4} 
it is evident that most of the frequency content below 100 Hz is being
blocked as desired. However this choice of parameters leeds to the 
undesirable feature that the signal power is suppressed all the way upto
$210$ Hz.                                         . 

The Excess Power code is configured to search for bursts above some 
minimal frequency, flow. It is therefore desirable to have little
or no attenuation above that frequency. The choice of Butterworth filter 
parameters that appear to achieve that are: 
\begin{itemize}
\item cutoff frequency,$f2 = flow - 10 Hz$
\item attenuatoin, $a2 = 0.9$
\item order, $Nmax = 8$
\end{itemize}
The frequency response of such a filter with $f2 = 120 Hz$ is shown in
Figure~\ref{fig:testattenuation8_9} 
\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{figures/testattenuation8_9}
\includegraphics[width=0.9\textwidth]{figures/testattenuation8_9zoomed}
\caption{Comparison of the spectra: Filter order 8 and attenuation 0.9}
\label{fig:testattenuation8_9}
\end{figure}

\clearpage

\subsection{Clustering in Excess Power Search}
\label{section:clustering}

The excess power search is a multi-resolution analysis of the
time-frequency structure of the input time series.  A burst trigger is
identified if the probability of obtaining the excess power in a
time-frequency tile from Gaussian noise is smaller than some threshold
$\alpha$.   For large bursts in the data stream, many tiles can be
identified as triggers with different sizes and aspect ratios.   Hence when
the search over a particular segment is complete,   triggers have to be
clustered. 

The triggers are identified by a number of characteristic features like
start time,  peak time,  central frequency,  bandwidth,  excess power (the
excess compared to the power expected in Gaussian noise for the
corresponding time fequency volume)  and a confidence assosciated with this
measurement.  To do the clustering we first compare the peak times of the
triggers.  If the peak times are within an allowed range,  i.e. if
$|peaktime(trigger 1) - peaktime(trigger 2)| < dt$ where $dt$ is some
seconds,  we check whether the triggers  overlap in frequency.  If they
overlap,  we cluster them into one single trigger whose start time and
bandwidth are so chosen that they cover the time-frequency area which
includes the areas of the individual triggers.  The peak time of the
clustered trigger is equal to the peak time of that trigger which has more
excess power between the two.  Then we repeat the comparison but now
between the clustered trigger and another trigger.  If the peak times again
lie within the specified range and the frequencies overlap,  we modify,
otherwise we keep them as distinct triggers.  We repeat these steps until
there are no more distinct triggers which satisfy our criterion for
clustering.  The execess power of the cluster is the maximum of the
triggers involved in that particular cluster.

Top panel in Figure~\ref{fig:checkcluster}  shows a set of triggers before
clustering,  middle panel shows a set of clustered triggers when $dt$ was
chosen to be $10  nanosecs$ and the bottom panel shows the cluster when
$dt$ was $10  secs$. In the bottom panel there is only $1$ clustered
trigger and if one looks at the top panel this is expected because all the
peak  times of the triggers(before clustering)  lie within $10  secs$ of
each other.
\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{figures/checkclustering_dt10nsec10sec}
\caption{Clustering in Excess Power}
\label{fig:checkcluster}
\end{figure}

To understand this better let us look at the triggers themselves:

\vspace{0.1 in}

\begin{tabular}{||l|l|l|l|l|l|l|lr||} \hline
no. & start time(sec) & peak time(sec) & duration(sec) & central freq(Hz) & bandwidth(Hz) & snr \\ \hline
1 &729501161.0  & 729501161.25  & 0.5     & 170  & 8   & 66.121277 \\ \hline
2 &729501161.125  & 729501161.28125  & 0.3125  & 186  & 48  & 123.3495 \\ \hline
3 &729501161.0625  & 729501161.28125  & 0.4375  & 186  & 48  & 120.2974 \\ \hline
4 &729501161.125 &729501161.28125 &0.3125 &178 &32 &82.274979 \\ \hline
5 &729501161.25 &729501161.3125 &0.125 &186 &48 &72.011459  \\ \hline
6 &729501161.25 &729501161.34375 &0.1875 &186 &48 & 92.914597 \\ \hline
7 &729501161.3125 &729501161.375 &0.125 & 186 &48 & 74.42984 \\ \hline
\end{tabular}

\vspace{0.1 in} 

The table above contains the set of triggers before clustering while   
the table below contains the $5$ clustered events when $dt$ 
was $10 nanosec$.

\vspace{0.1 in}

\begin{tabular}{||l|l|l|l|l|l|l|lr||} \hline
no. & start time(sec) & peak time(sec) & duration(sec) & central freq(Hz) & bandwidth(Hz) & snr \\ \hline
1 &729501161.0 &729501161.25 &0.5 &170 &8 & 66.121277 \\ \hline
2 &729501161.0625 &729501161.28125 &0.4375 &186 &48 &123.3495 \\ \hline
3 &729501161.25 &729501161.34375 &0.1875 &186 &48 &92.914597 \\ \hline
4 &729501161.25 &729501161.3125 &0.125 &186 &48 &72.011459 \\ \hline
5 &729501161.3125 &729501161.375 &0.125 &186 &48 &74.42984 \\ \hline 
\end{tabular}

\vspace{0.1 in}

Thus from this set of clustered triggers it is quite clear what we have been 
discussing till now.

\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BURCA:  coincidence analysis code
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Program \prog{lalapps\_burca}}
\label{program:lalapps-burca}
\idx[Program]{\prog{lalapps\_burca}}

\begin{entry}
\item[Name]
\prog{lalapps\_burca} --- program does burst coincidence analysis.

\item[Synopsis]
\prog{lalapps\_burca} \newline \hspace*{0.5in}
[\option{--start-time}~\parm{GPS seconds}] \newline \hspace*{0.5in}
[\option{--stop-time}~\parm{GPS seconds}] \newline \hspace*{0.5in}
[\option{--dt}~\parm{deltat}] \newline \hspace*{0.5in}
[\option{--noplaygroud}] \newline \hspace*{0.5in}
[\option{--help}] \newline \hspace*{0.5in}
\option{--ifo-a}~\parm{trigfile.a} \newline \hspace*{0.5in}
\option{--ifo-b}~\parm{trigfile.b} \newline \hspace*{0.5in}
\option{--outfile}~\parm{outfile}

\item[Description] 
\prog{lalapps\_burca} performs coincidence on triggers from the burst
search code.  At present it works for only two interferometers.  It must be
called with at least one input file from each instrument.  The default
behavior outputs triggers during playground times to the file
\option{outfile};  to obtain all triggers,  use the \option{--noplayground}
flag.     

\item[Options]\leavevmode
\begin{entry}
\item[\option{--ifo-a} \parm{trigfile.a}]  LIGO lightweight XML file with
triggers from interferometer A.  This argument can be called multiple
times.  Triggers are sorted \emph{after} all files have been read in. 

\item[\option{--ifo-b} \parm{trigfile.b}]  LIGO lightweight XML file with
triggers from interferometer B.  This argument can be called multiple
times.  Triggers are sorted \emph{after} all files have been read in. 

\item[\option{--start-time} \parm{GPS seconds}]  Ignore triggers with start
times earlier than \parm{GPS seconds}.

\item[\option{--stop-time} \parm{GPS seconds}]  Ignore triggers with stop
times later than \parm{GPS seconds}.

\item[\option{--dt} \parm{deltat}]  Accept triggers as coincident if their
start times agree within \parm{deltat} msec.  If not supplied,  then
\parm{deltat} = 0.

\item[\option{--outfile} \parm{outfile}]  Name of the LIGO lightweight XML
file to be used for output.

\item[\option{--noplayground}]  Record all triggers.  The default behaviour
returns only those triggers which lie in the playground data set.  

\item[\option{--help}]  Print a help message.
\end{entry}

\item[Example]
\begin{verbatim}
lalapps_burca \
--ifo-a L-POWER-734357353-1024.xml \
--ifo-b H-POWER-734357353-1024.xml \
--dt 10 \
--outfile my.xml \
--start-time 734357353 \
--stop-time 734358353 \
--noplayground
\end{verbatim}
\item[Author] 
Patrick Brady and Saikat Ray-Majumder
\end{entry}
\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BINJ:  injection generation code
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Program \prog{lalapps\_binj}}
\label{program:lalapps-binj}
\idx[Program]{\prog{lalapps\_binj}}

\begin{entry}
\item[Name]
\prog{lalapps\_binj} --- produces burst injection data files.

\item[Synopsis]
\prog{lalapps\_binj} \newline \hspace*{0.5in}
[\option{--help}] \newline \hspace*{0.5in}
\option{--gps-start-time}~\parm{tstart} \newline \hspace*{0.5in}
\option{--gps-end-time}~\parm{tend} \newline \hspace*{0.5in}
[\option{--time-step}~\parm{tstep}] \newline \hspace*{0.5in}
[\option{--seed}~\parm{seed}] \newline \hspace*{0.5in}
[\option{--waveform}~\parm{wave}] \newline \hspace*{0.5in}
[\option{--coordinates}~\parm{coordinates}] \newline \hspace*{0.5in}
[\option{--freq}~\parm{freq}] \newline \hspace*{0.5in}
[\option{--flow}~\parm{flow}] \newline \hspace*{0.5in}
[\option{--fhigh}~\parm{fhigh}] \newline \hspace*{0.5in}
[\option{--deltaf}~\parm{deltaf}] \newline \hspace*{0.5in}
[\option{--quality}~\parm{quality}] \newline \hspace*{0.5in}
[\option{--tau}~\parm{tau}] \newline \hspace*{0.5in}
[\option{--hpeak}~\parm{hpeak}] \newline \hspace*{0.5in}
[\option{--log-hpeak-min}~\parm{log-hpeak-min}] \newline \hspace*{0.5in}
[\option{--log-hpeak-max}~\parm{log-hpeak-max}] \newline \hspace*{0.5in}
[\option{--usertag}~\parm{tag}]

\item[Description] 
\prog{lalapps\_binj} generates a number of burst  parameters suitable  for
using in a Monte Carlo injection to test the efficiency of a burst search.
The  various parameters (detailed  below)  are specified on the command
line or can be randomly chosen in a manner appropriate for an burst upper
limit search.

The longitude $\alpha$ of the source is uniformly distributed in the range
$[0,2\pi]$, the latitude $\delta$ is defined so that $\cos(\pi/2 - \delta)$
is uniformly distributed in the range $[-1,1]$,  and the polarization angle
$\psi$  is uniformly distributed in the range $[0,2\pi]$.

The output of this program  is  a  list  of  the  injected events,
starting at  the specified start time, ending at the specified end time,
and containing one set  of burst parameters every specified time step.  The
output is written to a file name in the standard burst pipeline format:
\begin{center}
\begin{verbatim}
        HL-INJECTIONS_USERTAG_SEED-GPSSTART-DURATION.xml
\end{verbatim}
\end{center}
where \verb$USERTAG$ is \parm{tag} as specfied on the command line,
\verb$SEED$ is the  value  of  the random number seed chosen and
\verb$GPSSTART$ and \verb$DURATION$ describes the GPS time interval that
the file covers. The file is in the standard LIGO lightweight XML format
containing a \texttt{sim\_burst} table that describes the injections.  This
table is described in the LAL \texttt{tools} package under
\texttt{LIGOMetadataTables.h} header.  

The output is also written to an ascii file named in the following way:
\begin{center}
\begin{verbatim}
        HLT-INJECTIONS_USERTAG_SEED-GPSSTART-DURATION.txt
\end{verbatim}
\end{center}
where \verb$USERTAG$ is \parm{tag} as specfied on the command line,
\verb$SEED$ is the  value  of  the random number seed chosen and
\verb$GPSSTART$ and \verb$DURATION$ describes the GPS time interval that
the file covers. The file is in the format agreed to for LIGO-TAMA
simulations.  

If a \option{--user-tag} is not specified on the command line, the
\texttt{\_USERTAG} part of the filename will be omitted.

\item[Options]\leavevmode
\begin{entry}
\item[\option{--help}] Print a help message.

\item[\option{--gps-start-time} \parm{tstart}]
Optional.  Start time of the injection data to be created. Defaults to the
start of S2, Feb 14 2003 16:00:00 UTC (GPS time 729273613)

\item[\option{--gps-end-time} \parm{tend}]
Optional. End time of the injection data to be created. Defaults to the end
of S2, Apr 14 2003 15:00:00 UTC (GPS time 734367613).

\item[\option{--time-step} \parm{tstep}]
Optional. Sets the time step interval between injections. The injections
will occour at \parm{tstep}$/\pi$ second intervals. Defaults to $2630/\pi$.

\item[\option{--seed} \parm{seed}]
Optional. Seed the random number generator with the integer \parm{seed}.
Defaults to $1$.

\item[\option{--coordinates} \parm{coordinates}] 
Optional.  The coordinate system to specify for the injections.  Use one
of:
\begin{itemize}
\item \texttt{GEOGRAPHIC}
\item \texttt{EQUATORIAL}
\item \texttt{ECLIPTIC}
\item \texttt{GALACTIC}
\item \texttt{ZENITH}
\end{itemize}
The default is \texttt{EQUATORIAL}.   Use \texttt{ZENITH} to describe an
injection from directly above the detector with linear polarization with
respect to the detector.  Note:  this is different than using
\texttt{HORIZON} coordinates since the polarization angle is usually
defined with respect to geo-centric coordinates.

\item[\option{--flow} \parm{flow}]
Optional.  The code can generate injections at multiple frequencies.  This
option sets the first frequency used in that case.  Default value is 150
Hz.

\item[\option{--fhigh} \parm{fhigh}]
Optional.  Only generate injections with frequencies below \parm{fhigh}.
Default value is 1000 Hz.

\item[\option{--deltaf} \parm{deltaf}]
Optional.  The linear spacing between frequencies used to make
injections.  Default value is 0 Hz.

\item[\option{--waveform} \parm{wave}]
Optional.  Default is \texttt{SineGaussian}.   The string \parm{wave} will
be written into the \texttt{waveform} column of the \texttt{sim\_burst}
table output. This is used by the burst code to determine which type of
waveforms it should inject into the data.  Types implemented in LAL inject
package are:
\begin{description}
\item[\texttt{SineGaussian}]  Inject a sine-Gaussian waveform defined by
\begin{eqnarray}
A_+(t) &=& h_0 \exp[ - (t-t_0)^2/ \tau^2 ] \sin[ 2 \pi f_0 (t-t_0)] \\
A_\times(t) &=& 0
\end{eqnarray}

\item[\texttt{CosGaussian}]  Inject a cos-Gaussian waveform defined by
\begin{eqnarray}
A_+(t) &=& h_0 \exp[ - (t-t_0)^2/ \tau^2 ] \cos[ 2 \pi f_0 (t-t_0)] \\
A_\times(t) &=& 0
\end{eqnarray}
\end{description}

\item[\option{--tau} \parm{tau}]
Optional.  The decay-time $\tau$ for sine-gaussian,  gaussian,  ringdown
and ring-up waveforms.

\item[\option{--quality} \parm{quality}]
Optional.  The quality factor for sine-gaussian,  gaussian,  ringdown and
ring-up waveforms.    This option overrides the decay-time \parm{tau} and
recalculates the duration for each waveform using the formula
$$ 
\tau = \frac{\textsc{quality} }{ \sqrt{2} \pi f_0 }
$$
where $f_0$ is the frequency of the injection.

\item[\option{--freq} \parm{freq}]
Optional.  The central frequency $f_0$ for sine-gaussian,  ringdown and
ring-up waveforms.

\item[\option{--hpeak} \parm{hpeak}]
Optional.  The peak dimensionless strain $h_0$ for sine-gaussian,
gaussian,  ringdown and ring-up waveforms.

\item[\option{--log-hpeak-min} \parm{log-hpeak-min}]
Optional.  When this option is invoked,  a range of values of $h_0$ such
that $\log_{10}(h_0)$ is uniformly distributed in the range
$[$\parm{log-hpeak-min}$, $\parm{log-hpeak-max}$]$.  If this option is
used, then \option{--log-hpeak-max} is required.

\item[\option{--log-hpeak-max} \parm{log-hpeak-max}]
Optional.  When this option is invoked,  a range of values of $h_0$ such
that $\log_{10}(h_0)$ is uniformly distributed in the range
$[$\parm{log-hpeak-min}$, $\parm{log-hpeak-max}$]$.  If this option is
used, then \option{--log-hpeak-min} is required.

\item[\option{--user-tag} \parm{string}]
Optional. Set the user tag for this job to be \parm{string}. May also be
specified on the command line as \option{-userTag} for LIGO database
compatibility.

\end{entry}

\item[Example]
\begin{verbatim}
lalapps_binj \
--time-step 1000 \
--flow 130 \
--fhigh 600 \
--deltaf 90 \
--quality 8.89 \
--hpeak 6.0e-20 \
--seed 45
\end{verbatim}

\item[Author] 
Jolien Creighton, Patrick Brady, Duncan Brown
\end{entry}
\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% bread:  trigger file manipulation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Program \prog{lalapps\_bread}}
\label{program:lalapps-bread}
\idx[Program]{\prog{lalapps\_bread}}

\begin{entry}
\item[Name]
\prog{lalapps\_bread} -- reads in triggers from multiple xml files into one
single xml file 

\item[Synopsis]
\prog{lalapps\_bread} \newline \hspace*{0.5in}
[\option{--help}] \newline \hspace*{0.5in}
\option{--input}~\parm{filename} \newline \hspace*{0.5in}
\option{--output}~\parm{filename} \newline \hspace*{0.5in}
[\option{--max-confidence}~\parm{maximum conf}] \newline \hspace*{0.5in}
[\option{--noplayground}] \newline \hspace*{0.5in}
[\option{--sort}] \newline \hspace*{0.5in}
[\option{--cluster}] \newline \hspace*{0.5in}
[\option{--min-duration}~\parm{min dur}] \newline \hspace*{0.5in}
[\option{--max-duration}~\parm{max dur}] \newline \hspace*{0.5in}
[\option{--min-centralfreq}~\parm{min centralfreq}] \newline \hspace*{0.5in}
[\option{--max-centralfreq}~\parm{max centralfreq}] \newline \hspace*{0.5in}
[\option{--max-bandwidth}~\parm{max bw}] \newline \hspace*{0.5in}
[\option{--min-bandwidth}~\parm{min bw}] \newline \hspace*{0.5in}
[\option{--min-amplitude}~\parm{min amp}] \newline \hspace*{0.5in}
[\option{--max-amplitude}~\parm{max amp}] \newline \hspace*{0.5in}
[\option{--min-snr}~\parm{min snr}] \newline \hspace*{0.5in}
[\option{--max-snr}~\parm{max snr}] \newline \hspace*{0.5in}
[\option{--min-start-time}~\parm{min start time}] \newline \hspace*{0.5in}
[\option{--max-start-time}~\parm{max start time}] \newline \hspace*{0.5in}
[\option{--verbose}]

\item[Description] 
\prog{lalapps\_bread} reads in triggers from multiple xml files, does any
requested cuts specified by the user and then writes the triggers in a
single xml file.  

\item[Options]\leavevmode
\begin{entry}
\item[\option{--help}] Print a help message.

\item[\option{--input} \parm{cache file}]
Obtain the locations of input \texttt{.xml} trigger files from the LAL
cache file \parm{cache file}.  LAL cache files are explained in the
``framedata'' package in LAL and can be constructed by making calls to
\prog{LSCDataFind} on some systems.  


\item[\option{--output} \parm{filename}]
\prog{lalapps\_bread} writes all the triggers out in one single xml file,
specified by filename.  The (old) option \option{--outfile} can also be
used.

\item[\option{--max-confidence} \parm{maximum conf}]
Optional. Outputs only the triggers with a confidence less than or equal to
the given value.

\item[\option{--noplayground}]
Optional. By default only triggers lying inside the playground are output.
If this option is specified then all the triggers will be output. 

\item[\option{--sort}]
Not properly implemented yet. Now the triggers are always sorted in time
before being written out.

\item[\option{--cluster}]
Apply the LAL SnglBurstTable clustering algorithm to the triggers before
applying any cuts.  This is the same algorithm used by
\prog{lalapps\_power}.  Running \prog{lalapps\_power} without clustering,
followed by a clustering pass with \prog{lalapps\_bread} is equivalent to
applying clustering in \prog{lalapps\_power}.

\item[\option{--min-duration} \parm{duration}]
Optional. Outputs only the triggers with a duration greater than or equal
to the value specified.

\item[\option{--max-duration} \parm{duration}]
Optional. Outputs only the triggers with a durations less than or equal to
the value specified.

\item[\option{--min-centralfreq} \parm{frequency}]
Optional. Outputs only the triggers with a central frequency greater than
or equal to the value specified.

\item[\option{--max-centralfreq} \parm{frequency}]
Optional. Outputs only the triggers with a central frequency less than or
equal to the value specified.

\item[\option{--min-bandwidth} \parm{bandwidth}]
Optional. Outputs only the triggers with a bandwidth greater than or equal
to the value specified.

\item[\option{--max-bandwidth} \parm{bandwidth}]
Optional. Outputs only the triggers with a bandwidth less than or equal to
the value specified.

\item[\option{--min-amplitude} \parm{amplitude}]
Optional. Outputs only the triggers with an amplitude greater than or equal
to the value specified.

\item[\option{--max-amplitude} \parm{amplitude}]
Optional. Outputs only the triggers with an amplitude less than or equal to
the value specified.

\item[\option{--min-snr} \parm{snr}]
Optional. Outputs only the triggers with a SNR greater than or equal to the
value specified.

\item[\option{--max-snr} \parm{snr}]
Optional. Outputs only the triggers with a SNR less than or equal to the
value specified.

\item[\option{--min-start-time} \parm{time}]
Optional. Outputs only the triggers with a start time later than or equal
to the time specified.

\item[\option{--max-start-time} \parm{time}]
Optional. Outputs only the triggers with a start time earlier than or equal
to the time specified.

\item[\option{--verbose}]
Optional. Prints out detailed message as the program runs including the
total no. of triggers.

\end{entry}

\item[Example]
\begin{verbatim}
lalapps_bread \
--input H1.txt \
--output H1.xml \
--max-confidence -50 \
--verbose
\end{verbatim}

\item[Author] 
Patrick Brady, Saikat Ray Majumder, Kipp Cannon
\end{entry}
