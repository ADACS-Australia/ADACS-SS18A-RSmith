\section{Program \texttt{lalapps\_inspiral\_hipe}}
\label{program:inspiral-hipe}
\idx[Program]{inspiral\_hipe.in}

\begin{entry}
\item[Name]
\verb$lalapps_inspiral_hipe$ --- python script to generate Condor DAGs to
run the inspiral hierarchical pipeline.

\item[Synopsis]
\begin{verbatim}
  -h, --help            show this help message and exit
  -v, --version         print version information and exit
  -u  USERTAG, --user-tag= USERTAG
                        tag the jobs with USERTAG (overrides value in ini
                        file)
  -g, --g1-data         analyze g1 data
  -a, --h1-data         analyze h1 data
  -b, --h2-data         analyze h2 data
  -l, --l1-data         analyze l1 data
  -S, --one-ifo         analyze single ifo data (not usable for GEO)
  -D, --two-ifo         analyze two interferometer data
  -T, --three-ifo       analyze three interferometer data
  -Q, --four-ifo        analyze four intereferometer data
  -A, --analyze-all     analyze all ifos and all data (over-rides above)
  -d, --datafind        run LSCdataFind to create frame cache files
  -t, --template-bank   run lalapps_tmpltbank to generate template banks
  -i, --inspiral        run lalapps_inspiral to generate triggers
  -c, --coincidence     run lalapps_thinca to test for coincidence
  -B, --trigbank        run lalapps_trigbank for banks of coinc triggers
  -V, --inspiral-veto   run lalapps_inspiral with vetos
  -C, --second-coinc    run lalapps_thinca on the inspiral veto triggers
  -j, --coherent-bank   run lalapps_coherentbank to make coherent bank
  -k, --coherent-inspiral
                        run lalapps_coherent_inspiral for coherent analysis
  -s, --sire            do sires to sweep up triggers
  -R, --read-cache      read cache file from ini-file (if LSCDataFind is
                        broken)
  -P  PRIO, --priority= PRIO
                        run jobs with condor priority PRIO
  -f  FILE, --config-file= FILE
                        use configuration file FILE
  -p  PATH, --log-path= PATH
                        directory to write condor log file
  -o, --output-segs     output the segment lists of analyzed data
  -x, --dax             create a dax instead of a dag
\end{verbatim}

\item[Description] \verb$lalapps_inspiral_hipe$ generates a Condor DAG to run
the hierarchical inspiral analysis pipeline.  It currently works for
the four LSC interferometers: G1, H1, H2, L1. 

The code reads in segment lists for the four instruments.  If one of the
segment files is not specified or is empty, it is assumed that there is no
data from that instrument.  From the segment files, the pipeline calculates
four lists of single ifo segments, for G1, H1, H2 and L1; six lists of double
ifo segments, for G1-H1, G1-H2, G1-L1, H1-H2, H1-L1 and H2-L1; four lists of
three ifo data, for G1-H1-H2, G1-H1-L1, G1-H2-L1 and H1-H2-L1, and one list of
four ifo segments for G1-H1-H2-L1.  The options \verb$--g1-data$,
\verb$--h1-data$, \verb$--h2-data$ and \verb$--l1-data$ allow you to choose
which of the interferometers' data to analyze.  Similarly, the
\verb$--one-ifo$, \verb$--two-ifo$, \verb$--three-ifo$ and \verb$--four-ifo$
flags determine whether to analyze times during which one, two, three or four
instruments respectively were operational.  Thus, by specifying
\verb$--h1-data$, \verb$--l1-data$ and \verb$--two-ifo$, the pipeline will
analyze only the H1-L1 double coincident times.  If the \verb$--analyze-all$
flag is set, the pipeline will analyze all data from all instruments.  If the
\verb$--output-segments$ option is chosen, the pipeline will output segment
lists for the non-empty data types.  The file names are
"h1\_l1\_segs\_analyzed.txt" etc, or if the analyis is restricted to
playground, they are "h1\_l1\_play\_segs\_analyzed.txt".

The pipeline uses a coincidence stage early on in order to cut down the number
of triggers for which we have to perform the computationally costly chi
squared and r-squared veto computations.  Thus, the pipeline performs a first
inspiral stage (without vetoes) which is followed immediately by a coincidence
stage.  The triggers which survive in coincidence are then passed back to the
inspiral code where the chi-squared and r-squared signal based vetoes are
computed.  The remaining triggers are then tested again for coincidence.  Any
triggers surviving this second coincidence stage are passed into the coherent
analysis.  Some of the steps described above require more than one code to be
run.  For example, before running the inspiral code for the first time, we
must first locate the data using datafind, then generate template-banks for
the analysis.

At present, the pipeline can perform the following steps of a hierarchical
inspiral search: \verb$--datafind$, \verb$--template-bank$, \verb$--inspiral$,
\verb$--coincidence$, \verb$--trigbank$, \verb$--inspiral-veto$,
\verb$--second-coinc$, \verb$--coherent-bank$ and \verb$--coherent-inspiral$.
Any or all of these options may be specified.  However, each step of the
pipeline relies on results files produced by the previous step (and in the
case of the \verb$inspiral$, \verb$inspiral-veto$ and \verb$coherent-inspiral$
steps, the output from \verb$datafind$ is also required).

The configuration file specifies the parameters needed to run the analysis jobs
contained in the pipeline.  It is specified with the \verb$--config-file$
option.  A typical .ini file is the following: 

\input{hipeinifile}  

The .ini file contains several sections.  The \verb$[condor]$ section contains
the names of the executables which will run the various stages of the
pipeline.  The \verb$[pipeline]$ section gives the CVS details of the
pipeline, the usertag (which can be overwritten on the command line with the
\verb$--user-tag$ option) and the \verb$playground-data-mask$ which must be
set to one of \verb$playground_only$, \verb$exclude_playground$ or
\verb$all_data$.  The \verb$input$ section contains the names of the segment
files for the four interferometers, the channel names, frame types, injection
file name and number of time slides.  If any of the segment files are left
blank, it is assumed that there are no segments for that instrument.
Similarly, a blank injection-file signifies that no injections are to be
performed, and a blank num-slides signifies no time slides.

The remaining sections set options for the various jobs to be run in the
pipeline.  The options in the \verb$[datafind]$, \verb$[tmpltbank]$,
\verb$[inspiral]$, \verb$[inca]$, \verb$[thinca]$, \verb$[trigtotmplt]$,
\verb$[cohbank]$ and \verb$[chia]$ sections are added to every instance of the
relevant executable.  Note that these options are set the same for all
interferometers.  The options in the \verb$[data]$ section are added to all
\verb$[inspiral]$ and \verb$[tmpltbank]$ jobs, while the \verb$[ligo-data]$
and \verb$[geo-data]$ commands are added to the LIGO/GEO jobs respectively.
The \verb$[calibration]$ information is used to determine the calibration data
for these jobs.  In the \verb$[ifo-thresholds]$ sections, the ifo specific
snr, chi squared and r-squared veto thresholds for the various intererometers
are set; while the \verb$[no-veto-inspiral]$ and \verb$veto-inspiral]$
sections contain arguments relevant for the first and second inspiral steps
respectively.  

The science segments are read in from the segment files for each instrument.
These science segments are split up into analysis chunks.  The analysis chunk
size is determined from the number of data segments and their length and
overlap specified in config file. Currently, we are using 256 second analysis
segments.  We use 15 segments in a chunk, and overlap them by 128 seconds to
give a chunk length of 2048 seconds.  The chunks are constructed for each of
the interferometers independently.  Any science segment shorter than the
length of a chunk is not analyzed.  Additionally, we cannot produce triggers
for the first and last overlap/2 seconds of a science segment, due to the
finite length of the inspiral templates.  Using this information, we construct
segment lists of analyzable data for each of the fifteen types of data we may
have (four single ifo, six two ifo, four three ifo and one four ifo).  If the
playground only option is specified, the segments are restricted to playground
times.  We decide which chunks should be analyzed by testing for overlap
between the chunk and the data we need to analyze.  Note that if the pipeline
is restricted to playground data, then only the playground times are analyzed
for triggers in the inspiral code.  This is done by setting the
\verb$trig-start-time$ and \verb$trig-end-time$ for the inspiral jobs
appropriately.

Once the DAG file has been created it should be submitted to the Condor pool
with the \verb$condor_submit_dag$ command.

\item[Options]\leavevmode
\begin{entry}
\item[\texttt{--help}] Display a brief usage summary.
\end{entry}
  
\begin{entry}
\item[\texttt{--version}] Display the version information and exit.
\end{entry}
  
\begin{entry}
\item[\texttt{--user-tag} \textsc{usertag}] Set the user-tag to \textsc{usertag}.  This overrides the user-tag which may have been set in the ini file.  The usertag will be added to the names of the output files of the pipeline.
\end{entry}
 
\begin{entry}
\item[\texttt{--g1-data}] Analyze the G1 data, the times of which are
determined from the \verb$g1-segments$ file specified in the ini file.  If not
set, then no data when G1 was operational will be analyzed.
\end{entry}

\begin{entry}
\item[\texttt{--h1-data}] Analyze the H1 data, the times of which are
determined from the \verb$h1-segments$ file specified in the ini file.  If not
set, then no data when H1 was operational will be analyzed.
\end{entry}

\begin{entry}
\item[\texttt{--h2-data}] Analyze the H2 data, the times of which are
determined from the \verb$h2-segments$ file specified in the ini file.  If not
set, then no data when H2 was operational will be analyzed.
\end{entry}

\begin{entry}
\item[\texttt{--l1-data}] Analyze the L1 data, the times of which are
determined from the \verb$l1-segments$ file specified in the ini file.  If not
set, then no data when H1 was operational will be analyzed.
\end{entry}

\begin{entry}
\item[\texttt{--one-ifo}] Analyze any times when one and only one instrument
was operational.  Note that this option works together with the IFO options
given above.  For example if \verb$--one-ifo$ and \verb$--h2-data$ were
specified, then only the single IFO H2 times would be analyzed.
\end{entry}

\begin{entry}
\item[\texttt{--two-ifo}] Analyze any times when two instruments were
operational.  Note that this option works together with the IFO options given
above.  For example if \verb$--two-ifo$, \verb$h1-data$ and \verb$--h2-data$
were specified, then the times when only H1 and H2 were operational would be
analyzed.  However, if only \verb$--two-ifo$ and \verb$h1-data$ were
specified, no data would be analyzed.
\end{entry}

\begin{entry}
\item[\texttt{--three-ifo}] Analyze any times when three instruments were
operational.  Note that this option works together with the IFO options given
above.  For example if \verb$--three-ifo$, \verb$h1-data$, \verb$--h2-data$
and \verb$--l1-data$ were specified, then the times when H1, H2 and L1
were operational, but G1 was not, would be analyzed. 
\end{entry}

\begin{entry}
\item[\texttt{--four-ifo}] Analyze any times when all four instruments were
operational.  Note that this option works together with the IFO options given
above.  For example if \verb$--four-ifo$, \verb$g1-data$, \verb$h1-data$,
\verb$--h2-data$ and \verb$--l1-data$ were specified, then the times when all
of G1,H1, H2 and L1 were operational would be analyzed.
\end{entry}

\begin{entry}
\item[\texttt{--analyze-all}] Analyze all ifos and all data.  This is
equivalent to setting all six of the options above.  Then, all the data is
analyzed.
\end{entry}

\begin{entry}
\item[\texttt{--datafind}] Run the datafind step of the pipeline.
\end{entry}

\begin{entry}
\item[\texttt{--template-bank}] Run the template-bank step of the pipeline.
Note that the template-bank jobs require the cache files created by datafind,
so \verb$--datafind$ must either be run in the pipeline or have been run
previously.
\end{entry}

\begin{entry}
\item[\texttt{--inspiral}] Run the inspiral step of the pipeline.  These jobs
will take the arguments from the \verb$[no-veto-inspiral]$ section in the ini
file.  Note that the inspiral jobs require the cache files created by datafind
and template banks, so both \verb$--datafind$ and \verb$template-bank$ must
either be run in the pipeline or have been run previously.
\end{entry}

\begin{entry} 
\item[\texttt{--coincidence}] Run the coincidence step of the pipeline.  Times
when two or more detectors were operational are analyzed by the thinca code.
This determines all coincidences between two or more operational detectors.
If \verb$num-slides$ is specified then time slides are also performed, and
output in a separate file named THINCA\_SLIDE.  Finally, for the times when
only one instrument was on, inca is run (in single ifo mode) to sweep up the
single ifo triggers.  These triggers are not used by later stages of the
pipeline.  Note that the thinca and inca jobs require the inspiral triggers
created by inspiral, so that \verb$--inspiral$ must either be run in the
pipeline or have been run previously.  For each single IFO segment, the
coincidence step simply creates a file containing all the triggers in the time
interval.  For two/three/four IFO segments, the coincidence step performs
coincidence and outputs the double, triple and quadruple coincidence triggers
in one file, and time slide coincident triggers in a second file.
\end{entry} 

\begin{entry}
\item[\texttt{--trigbank}] Run the triggered bank step of the pipeline.  This
step takes in the coincident, and thinca slide, triggers.  It outputs those
triggers which should be filtered by a specific instrument in the follow-up
inspiral stage.  This is done by keeping those triggers from the relevant ifo,
within the times of the chunk to be analyzed by the inspiral code.
\end{entry}

\begin{entry}
\item[\texttt{--inspiral-veto}] Run the second inspiral step of the pipeline.
These jobs will take the arguments from the \verb$[veto-inspiral]$ section in
the ini file, and are intended to do the computationally costly signal based
vetoes of coincident triggers.  These jobs are differentiated from the first
inspiral jobs by the inclusion of an ifo-tag, so an example job may be named
\verb$H1-INSPIRAL_H1L1-GPSTIME-DURATION.xml$.  Note that the inspiral jobs
require the cache files created by datafind and trigbanks, so both
\verb$--datafind$ and \verb$trigbank$ must either be run in the pipeline or
have been run previously.
\end{entry}

\begin{entry} 
\item[\texttt{--second-coinc}] Re-run the coincidence step of the pipeline.
This runs the thinca code on all times when two or more instruments were
operational.  As with the first coinc stage, this will perform both the zero
lag and time slide analysis if requested.  The output from these jobs also
have an ifo-tag added, so an example output file might be
\verb$H1H2L1-THINCA_H1H2L1-GPSTIME-DURATION.xml$ for the zero lag and
\verb$H1H2L1-THINCA_SLIDE_H1H2L1-GPSTIME-DURATION.xml$ for the time slides.
\end{entry} 

\begin{entry} 
\item[\texttt{--coherent-bank}] Run the coherent bank step of the pipeline.
This generates template banks ready for the coherent analysis.  These banks
are generated from the triggers output in the second coincidence stage of the
pipeline.
\end{entry} 


\begin{entry} 
\item[\texttt{--coherent-inspiral}] Run the coherent inspiral step of the
pipeline.  This runs the inspiral code, using the coherent-bank.  The inspiral
code outputs frames containing time series of snr data around the time of a
coincident event.  These frames are then read in by the coherent inspiral code
which calculates the coherent signal to noise ratio of the event.
\end{entry} 


\begin{entry}
\item[\texttt{--sire}] Run sire on the various stages of the pipeline.  This
will collect together the triggers from jobs performed in various stages of
the pipeline
\end{entry} 

\begin{entry}
\item[\texttt{--read-cache}] Specify a static cache file to use during the
analysis.  This should only be used if LSCdataFind is not available where the
pipeline is being run.
\end{entry} 

\begin{entry}
\item[\texttt{--priority} \textsc{PRIO}] Set the condor priority \textsc{PRIO}
of the condor jobs to be run in the pipeline.
\end{entry} 

\begin{entry}
\item[\texttt{--config-file} \texttt{config\_file}] Set the name of the
configuration file to be \textsc{config\_file}.  This is the which is used to
determine the parameters for the pipeline.  This is a required argument.
\end{entry} 
 
\begin{entry}
\item[\texttt{--log-path}] The directory in which to write the condor log
file.  This should generally be a local directory of the condor submit
machine.  This is a required argument.
\end{entry} 

\begin{entry}
\item[\texttt{--output-segs}] Output the segment lists of analyzed data.  Up
to seven files will be output, one for each of the types of interferometer
data (H1, H2, L1, H1\_H2, H1\_L1, H2\_L1, H1\_H2\_L1).  Any segment lists
which are non-empty will we written.
\end{entry} 

\begin{entry}
\item[\texttt{--dax}] Output a dax rather than a dag. 
\end{entry} 

\item[Author] 
Steve Fairhurst, Darren Woods
\end{entry}
