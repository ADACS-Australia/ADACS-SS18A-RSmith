% $Id$

\section{Running the LALApps Stochastic Pipeline Under Condor}

This section describes how to run the LALApps stochastic analysis
pipeline under Condor at the various LSC Data Grid Sites, such as CIT,
UWM, and the observatory clusters at LHO and LLO.

The pipeline is constructed as a directed acyclic graph (DAG) run under
Condor. A DAG represents a set of programs where the input, output, or
execution of one ore more programs is dependant upon one or more other
programs. The programs are represented by nodes (verticies) in the
graph, and the edges (arcs) identifies the dependancies. There are three
different nodes that make up the Stochastic analysis DAG - datafind,
stochastic and stopp. A short description of the nodes that make up the
stochastic DAG follows:

\subsubsection{Datafind Node}

The datafind node, as its name suggests, is for determining the location
of the data that is to be analysed. It uses \texttt{LSCdataFind} to
query the LDR database for the location of the data on the cluster. This
step in the pipeline does not need to be run for analysis, only when the
location of the data changes.

\subsubsection{Stochastic Node}

The stochastic node runs an instance of \prog{lalapps\_stochastic}, the
main stochastic analysis code. See
Section~\ref{program:lalapps-stochastic} for more information on
\prog{lalapps\_stochastic}.

\subsubsection{Stopp Node}

The stopp node runs an instance of \prog{lalapps\_stopp}, the STOchastic
Post Processing code. See Section~\ref{program:lalapps-stopp} for more
information on \prog{lalapps\_stopp}.

\subsection{Creating the DAG}

In order to create the DAG, a configuration file is required, an example
of which can be found in\\
\texttt{\$LALAPPS\_PREFIX\$/share/lalapps/stochastic\_S3\_H1L1.ini}.
This configuration specfies the parameters to use for each step of the
pipeline. See Section~\ref{program:lalapps-stochastic-pipe} for details
regarding the layout of the configuration file.

Any combination of nodes, see above, can be present in the DAG, assuming
that the dependancies for each node are satisfied. The datafine node has
no dependancies, the stochastic node requires cache files to be
available, so if cache have already been generated and the location of
the data on the cluster has not changed then there is no read for the
datafind node to be run, and finally the stopp node requires that the
xml output file from \prog{lalapps\_stochastic} are available.

The following example will create a DAG containing all nodes, datafind,
stochastic and stopp.

\begin{verbatim}
> lalapps_stochastic_pipe --datafind --stochastic --stopp \
    --config-file example_S3_H1L1.ini --log-path logs
\end{verbatim}

In addition to creating the DAG it will also create several other file
required for running the search under condor, descriptions of these
files can be found below,

\begin{description}
\item[\texttt{example\_S3\_H1L1.dag}]
This is main dag file that includes all the required information for
running the pipeline. This includes the dependancy information for each
job within the DAG along with all the dynamic command line arguments for
each job.
\item[\texttt{example\_S3\_H1L1.datafind.sub}]
This is the submit file used to submit datafind jobs to condor. It
includes all the static command line options along with variable command
line options, for each job, that are defined within the main
\texttt{.dag} file.
\item[\texttt{example\_S3\_H1L1.stochastic.sub}]
This is the submit file used to submit stochastic jobs to condor. It
includes all the static command line options along with variable command
line options, for each job, that are defined within the main
\texttt{.dag} file.
\item[\texttt{example\_S3\_H1L1.stopp.sub}]
This is the submit file used to submit stopp jobs to condor. It
includes all the static command line options along with variable command
line options, for each job, that are defined within the main
\texttt{.dag} file.
\item[\texttt{example\_S3\_H1L1.pipeline.log}]
This is the log file that details how the data, from the input segment
list, has been split up into different jobs.
\end{description}

\subsection{Running the DAG}

When the DAG has been created successfully, a message will be displayed
describing what needs to be done inorder to run the analysis pipeline at
a LSC DataGrid Site.

\prog{LSCdataFind} works by querying the LDRdataFind Server for the
location of the requested data, this requires the user running
LSCdataFind to have a valid grid proxy. Therefore if the DAG includes
any datafind nodes, a valid grid proxy must be available prior to the
job being submitted to the cluster. This is achieved by running the
following commands:

\begin{verbatim}
> unset X509_USER_PROXY
> grid-proxy-init
\end{verbatim}

This will remove any existing proxy and recreate another, requesting the
grid certificates pass phase. The default duration for the proxy to be
valid is 12 hours, if a longer duration is required it can be increased
by appending ``\option{-hours}~\parm{HOURS}'' to the call to
\prog{grid-proxy-init}.

\subsection{Example}

This section is under construction...
