#!/usr/bin/python

"""
Print things from LIGO LW XML files.  Inspired by lwtprint from LIGOTools.
"""

__author__ = "Kipp Cannon <kipp@gravity.phys.uwm.edu>"
__date__ = "$Date$"[7:-2]
__version__ = "$Revision$"[11:-2]


#
# Preamble.
#

from optparse import OptionParser
import sys
import urllib

from glue.lal import CacheEntry
from glue import segments
from glue.ligolw import ligolw
from glue.ligolw import metaio
from glue.ligolw import lsctables
from glue.ligolw import docutils


#
# Parse command line.
#

parser = OptionParser(version = "%prog CVS $Id$")
parser.add_option("-i", "--input-cache", metavar = "CACHEFILE", action = "append", help = "get input files from trigger cache CACHEFILE")
parser.add_option("-c", "--column", metavar = "COLUMN", action = "append", help = "print column named COLUMN")
parser.add_option("-d", "--delimiter", metavar = "DELIMITER", action = "store", help = "delimit output with DELIMITER")
parser.add_option("-r", "--rows", metavar = "ROWSPEC", action = "append", help = "print rows in the range(s) ROWSPEC")
parser.add_option("-t", "--table", metavar = "TABLE", action = "append", help = "read from table TABLE")
parser.add_option("-v", "--verbose", action = "store_true", help = "be verbose")
options, urls = parser.parse_args()
del parser

# add urls from cache files
if options.input_cache:
	for cache in options.input_cache:
		urls += [c.url for c in map(CacheEntry, file(cache))]

if len(urls) < 1:
	raise Exception, "no input files!"

# set the default delimiter
if not options.delimiter:
	options.delimiter = ","

# strip table names
if options.table:
	options.table = map(metaio.StripTableName, options.table)

# turn row requests into a segment list
if options.rows:
	rowlist = segments.segmentlist([])
	for parts in map(lambda x: str.split(x, ":"), options.rows):
		if len(parts) == 1:
			rowlist.append(segments.segment(int(parts[0]), int(parts[0])))
		elif len(parts) != 2:
			raise ValueError, "invalid rowspec \"" + ":".join(parts) + "\""
		else:
			try:
				parts[0] = int(parts[0])
			except ValueError:
				parts[0] = -segments.infinity()
			try:
				parts[1] = int(parts[1])
			except ValueError:
				parts[1] = segments.infinity()
			if parts[1] > parts[0]:
				parts[1] -= 1
			rowlist.append(segments.segment(parts[0], parts[1]))


#
# How to construct the content handler.
#

def ContentHandler(doc):
	if options.table:
		return docutils.PartialLIGOLWContentHandler(doc, lambda name, attrs: (name == ligolw.Table.tagName) and (metaio.StripTableName(attrs["Name"]) in options.table))
	else:
		return lsctables.LIGOLWContentHandler(doc)


#
# How to read a document
#

def load_document(url):
	if options.verbose:
		print >>sys.stderr, "Reading %s" % url
	doc = ligolw.Document()
	ligolw.make_parser(ContentHandler(doc)).parse(urllib.urlopen(url))
	return doc


#
# How to find the tables to dump
#

def get_tables(doc):
	if options.table:
		return doc.getElements(lambda e: (e.tagName == ligolw.Table.tagName) and (metaio.StripTableName(e.getAttribute("Name")) in options.table))
	else:
		return doc.getElements(lambda e: (e.tagName == ligolw.Table.tagName))


#
# How to choose rows
#

def is_requested_row(n):
	return (not options.rows) or (n in rowlist)


#
# How to print a row
#

def row_string(table, row):
	if options.column:
		return options.delimiter.join(map(lambda key: str(getattr(row, key)), options.column))
	else:
		return options.delimiter.join(map(lambda colname: str(getattr(row, colname)), table.columnnames))


#
# Loop over all input documents.
#

for url in urls:
	doc = load_document(url)
	for table in get_tables(doc):
		for n, row in enumerate(table):
			if is_requested_row(n):
				print row_string(table, row)
	doc.unlink()
