#!/usr/bin/python
#
# $Id$
#
# Copyright (C) 2006  Kipp C. Cannon
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

"""
Print things from LIGO LW XML files.  Inspired by lwtprint from LIGOTools.
"""

__author__ = "Kipp Cannon <kipp@gravity.phys.uwm.edu>"
__date__ = "$Date$"[7:-2]
__version__ = "$Revision$"[11:-2]


#
# Preamble.
#

from optparse import OptionParser
import sys
import urllib

from glue.lal import CacheEntry
from glue import segments
from glue.ligolw import ligolw
from glue.ligolw import metaio
from glue.ligolw import lsctables


#
# Parse command line.
#

parser = OptionParser(version = "%prog CVS $Id$")
parser.add_option("-i", "--input-cache", metavar = "CACHEFILE", action = "append", help = "get input files from trigger cache CACHEFILE")
parser.add_option("-c", "--column", metavar = "COLUMN", action = "append", help = "print column named COLUMN")
parser.add_option("-d", "--delimiter", metavar = "DELIMITER", action = "store", help = "delimit output with DELIMITER")
parser.add_option("-r", "--rows", metavar = "ROWSPEC", action = "append", help = "print rows in the range(s) ROWSPEC")
parser.add_option("-t", "--table", metavar = "TABLE", action = "append", help = "read from table TABLE")
parser.add_option("-v", "--verbose", action = "store_true", help = "be verbose")
options, urls = parser.parse_args()
del parser

# add urls from cache files
if options.input_cache:
	for cache in options.input_cache:
		urls += [c.url for c in map(CacheEntry, file(cache))]

if len(urls) < 1:
	raise Exception, "no input files!"

# set the default delimiter
if not options.delimiter:
	options.delimiter = ","

# strip table names
if options.table:
	options.table = map(metaio.StripTableName, options.table)

# turn row requests into a segment list
if options.rows:
	rowlist = segments.segmentlist([])
	for parts in map(lambda x: str.split(x, ":"), options.rows):
		if len(parts) == 1:
			rowlist.append(segments.segment(int(parts[0]), int(parts[0])))
		elif len(parts) != 2:
			raise ValueError, "invalid rowspec \"" + ":".join(parts) + "\""
		else:
			try:
				parts[0] = int(parts[0])
			except ValueError:
				parts[0] = -segments.infinity()
			try:
				parts[1] = int(parts[1])
			except ValueError:
				parts[1] = segments.infinity()
			if parts[1] > parts[0]:
				parts[1] -= 1
			rowlist.append(segments.segment(parts[0], parts[1]))


#
# How to construct the content handler.
#

def ContentHandler(doc):
	if options.table:
		return ligolw.PartialLIGOLWContentHandler(doc, lambda name, attrs: (name == ligolw.Table.tagName) and (metaio.StripTableName(attrs["Name"]) in options.table))
	else:
		return ligolw.LIGOLWContentHandler(doc)


#
# How to read a document
#

def load_document(url):
	if options.verbose:
		print >>sys.stderr, "Reading %s" % url
	doc = ligolw.Document()
	ligolw.make_parser(ContentHandler(doc)).parse(urllib.urlopen(url))
	return doc


#
# How to find the tables to dump
#

def get_tables(doc):
	if options.table:
		return doc.getElements(lambda e: (e.tagName == ligolw.Table.tagName) and (metaio.StripTableName(e.getAttribute("Name")) in options.table))
	else:
		return doc.getElements(lambda e: (e.tagName == ligolw.Table.tagName))


#
# How to choose rows
#

def is_requested_row(n):
	return (not options.rows) or (n in rowlist)


#
# How to print a row
#

def row_string(table, row):
	if options.column:
		return options.delimiter.join(map(lambda key: str(getattr(row, key)), options.column))
	else:
		return options.delimiter.join(map(lambda colname: str(getattr(row, colname)), table.columnnames))


#
# Loop over all input documents.
#

for url in urls:
	doc = load_document(url)
	for table in get_tables(doc):
		for n, row in enumerate(table):
			if is_requested_row(n):
				print row_string(table, row)
	doc.unlink()
