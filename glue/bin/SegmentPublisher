#!/ldcg/LDR/python/bin/python

"""
main driver script for publishing data
"""

import os
import re
import sys
import time
import getopt
import popen2
import gsiurlparse
from types import *

execfile('publisher.conf')

## Need to check vars here
## end var check

# import non-official python mods
try:
    from glue.LSCfileAdd import LSCfileAddException
    from glue.LSCfileAdd import Publisher
    from glue import gpstime
    from glue import statedb
    from glue import segments
        
except Exception,e:
    msg = "Unable to import one or more modules from lscsoft's glue. Error message was: %s" % (str(e),)
    print >>sys.stderr, msg
    sys.stdout.flush()
    sys.stderr.flush()
    sys.exit(2)

### Begin Function/Procedure/Subroutine/Method Definitions ###

def CreatePIDFile():
    """
    Creates file with name CONFIG['PID_FILE_PATH']
    containing the PID of this script's process.
    """
    try:
        pidfile = open(str(CONFIG['PID_FILE_PATH']),'w')
        mypid = os.getpid()
        pidfile.write(str(mypid))
        pidfile.close()
    except Exception,e:
        msg = "Could not create PID file, \"%s\". Error was: %s" % (str(CONFIG['PID_FILE_PATH']),str(e))
        raise LSCfileAddException,msg
## END CreatePIDFile()

def HeartBeat():
    """
    Updates mtime of PID file, a.k.a. "touch PID_FILE"
    """
    try:
        os.utime(str(CONFIG['PID_FILE_PATH']),None)
    except Exception,e:
        msg = "HeartBeat(): Cannot update mtime of PID file, \"%s\". Error was: %s" % (str(CONFIG['PID_FILE_PATH']),str(e))
        raise LSCfileAddException,msg
## END HeartBeat()

def DelPIDFile():
    """
    deletes PID file.
    
    Usually done on exit
    """
    try:
        pidfilename = str(CONFIG['PID_FILE_PATH'])
        os.remove(pidfilename)
    except Exception,e:
        msg = "Could not delete PID file, \"%s\". Error was: %s" % (str(CONFIG['PID_FILE_PATH']),str(e))
## END DelPIDFile

def Process_Command_Line():
        """
        Grabs parameters from command line
        Returns a urlType list and main_dir from which to search for files.
        """
        shortop = "hcd:u:f:g:p:"
        longop = ['help','continue','directory=','file=','url-type=','gsiftp-host=','gsiftp-port=']
        default_ftpport = 15000 # for gsiftp pfns

        try:
                opts, args = getopt.getopt(sys.argv[1:], shortop, longop)
        except getopt.GetoptError:
                print >>sys.stderr, "Error parsing command line"
                print >>sys.stderr, "Enter 'PublishRunFiles --help' for usage"
                sys.exit(1)

        if len(opts) < 1:
                print >>sys.stderr, "At least one option must be specified."
                print >>sys.stderr, "Enter 'PublishRunFiles --help' for usage"
                sys.exit(1)
        temptypes = tempruntag = tempfile = tempdir = tempftphost = tempftpport = restart = None
        filemethod = dirmethod = None
        for s,a in opts:
                o = s.lstrip("-")

                if o == "h" or o == "help":
                        Print_Usage()
                        sys.exit(0)
                elif o == "u" or o == "url-type":
                        if a:
                                temptypes = a
                        else:
                                msg = "%s option requires an arguement." % (s,)
                                print >> sys.stderr, msg
                                sys.exit(8)
                elif o == "f" or o == "file":
                        if a:
                                tempfile = a
                        else:
                                msg = "%s option requires an arguement." % (s,)
                                print >> sys.stderr, msg
                                sys.exit(8)
                elif o == "d" or o == "directory":
                        if a:
                                tempdir = a
                        else:
                                msg = "%s option requires an arguement." % (s,)
                                print >> sys.stderr, msg
                                sys.exit(8)
                elif o == "g" or o == "gsiftp-host":
                        if a:
                                tempftphost = a
                        else:
                                msg = "%s option requires an argument." % (s,)
                                print >>sys.stderr, msg
                                sys.exit(8)
                elif o == "p" or o == "gsiftp-port":
                        if a:
                                tempftpport = a
                        else:
                                msg = "%s option requires and arguement." % (s,)
                                print >>sys.stderr, msg
                                sys.exit(8)
                elif o == "c" or o == "continue":
                        restart = True
                else:
                        print "Option \"%s\" , not recognized. Please run \"PublishRunFiles --help\" for usage."
                        sys.exit(1)
        # If not restarting check options
        if restart:
                print >> sys.stderr, "This option not yet supported."
                sys.exit(11)
                #print "Script will attempt to continue from where it last left off."
                #recame = os.path.abspath("publish.rec")
                #try:
                #        recfile = open(recname,'r').readlines()
                #except:
                #        print >> sys.stderr, "Error: problem opening record file, %s. Exiting."
                #        sys.exit(11)
                #import pickle
                #contents = pickle.load(recfile)
                #del pickle
                #recfile.close()
         #if len(contents) !=  5:
                #        print >>sys.stderr, "Could not parse record file, %s. Exiting."
                #        sys.exit(12)
                #dirname = contents[0]
                #urlType,runtag,method,restart

        # required
        if not temptypes:
                print >> sys.stderr, "A url-type must be specified."
                sys.exit(7)


        if tempdir:
                dirname = os.path.abspath(tempdir)
                if not os.path.exists(dirname):
                        print >> sys.stderr, "Directory %s does not exist." % (dirname,)
                        sys.exit(9)
                if not os.path.isdir(dirname):
                        print >> sys.stderr, "%s is not a directory." % (dirname,)
                        sys.exit(9)
                else:
                        dirmethod = True
        else:
                dirmethod = False

        # process temptypes into urlType
        urlType = []
        typelist = temptypes.split(',')
        myidx = 0
        for mytype in typelist:
                if mytype == 'file':
                        mydict = {}
                        mydict['type'] = "file"
                        mydict['host'] = "localhost"

                elif mytype == 'gsiftp':
                        mydict = {}
                        mydict['type'] = "gsiftp"
                        if not tempftphost:
                                print >>sys.stderr, "A host must be specified with the gsiftp url option."
                                print >>sys.stderr, "Use -h to print usage."
                                sys.exit(11)
                        mydict['host'] = tempftphost
                        if tempftpport:
                                if tempftpport.isdigit():
                                        if tempftpport < 65536 and tempftpport > 0:
                                                mydict['port'] = tempftpport
                                        else:
                                                print >>sys.stderr, "gsiftp port %s, is not a valid tcp port (i.e. within (0,65536))." \
                                                                    % (tempftpport,)
                                else:
                                        print >>sys.stderr, "gsiftp port %s, is not a digit. " % (tempftpport,)
                        else:
                                mydict['port'] = default_ftpport

                else:
                        print >> sys.stderr, "urltype \"%s\" is not recognized." % (mytype,)
                        sys.exit(10)
                urlType.append(mydict)

        # print out config to stdout...
        print "Using the following parameters:"
        if restart:
                print "(paramters pulled from record file)"
        
        if len(urlType) > 1:
                print "url-types: %s" % (temptypes,)
                for myurltype in urlType:
                        if myurltype['type'] == "file":
                                print "\tfile host: %s" % (myurltype['host'],)
                        elif myurltype['type'] == "gsiftp":
                                print "\tgsiftp host: %s" % (myurltype['host'],)
        else:
                print "url-type: %s" % (temptypes,)
                if urlType[0]['type'] == "file":
                        print "\tfile host: %s" % (urlType[0]['host'],)
                elif urlType[0]['type'] == "gsiftp":
                        print "\tgsiftp host: %s" % (urlType[0]['host'],)


        return urlType
## END Process_Command_Line

def Print_Usage():
        """
        Prints usage instructions for this script.
        """
        msg = \
        """
Usage:
        PublishRunFiles [-c|--continue]|[MANDATORY OPTIONS]
        
Special options:
        -c, --continue    continues from last successfully published file.
                          requires a proper publishrunfiles.rec file in $PWD.

Mandatory options/Oxymorons (unless -c etc. is specified):
        -u, --url-type    commadelimited list of urltypes. e.g. "gsiftp,file"
        
Misc. options:
        -g,--gsiftp-name  hostname to put into gsiftp urls. Only necessary
                          if a gsiftp urltype ("-u gsiftp") is specified.
                          
        -p,--gsiftp-port  gsiftp server port (if option is not specified, 
                          default is 15000)
        
        """
        print msg
## END Print_Usage:



class SegData(object):
    """
    class for retrieving and processing
    file lists via segment a comparison method.
    (not the Python definition of method....)
    """
    
    def __init__(self):
        """
        Initializes ScienceData class.
        Sets default file limit.
        """
        # will hold list of segments thought to
        # be already published
        self.main_seglist = segments.segmentlist([])
        
        # Command to get the frame cache dump
        self.fcd_file = CONFIG['FRAME_CACHE_FILE']
        
        # init some vars to appropriate initial values
        self.first_file_list = []
        self.first_time_throug = True
        self.min_lookback = 0
        self.max_lookback = long(730 * 86400) # two years
        
        # max number of lfns to query RLS against
        # at a time.
        self.query_size = 500
        
        # initial length of main_seglist is 0
        self.old_len_main_seglist = 0
        
        # how many segments self.main_seglist must grow
        # before it's coalesced again
        self.limit_main_seglist = 1000
        
    ## END __init__
        
    def Set_Start_Time(self,start):
        """
        Sets the oldest possible GPS time
        of a file in the filelist returned by this class.
        """
        self.max_lookback = long(start)
    ## END Set_Start_Time()
    
    def Set_Wait_Time(self,wait_time):
        """
        Sets how far behind "now" to truncate gps
        time. The file list's newest GPS time will be
        older than now by this ammount.
        """
        self.min_lookback = long(wait_time)
    ## END Set_Wait_Time(self,wait_time)
    
    def MakeFirstSegList(self,x,dclist,main_path):
        """
        Makes first segment list
        """        
        self.GetSegList(x,dclist,main_path)
    ## END MakeFirstSegList(self,x,dclist,main_path)
    
    def GetSegList(self,x,dclist,main_path):
        """
        Puts current diskcache segments into x, and
        puts translation list into dclist.
        
        Reads from a frame cache dump command
        """
        # Make a hard link to use, so the original
        # can be removed by another process without
        # affecting this script.
        my_temp_file = "frame.cache.temp.%s" % (CONFIG['FRAME_LEVEL'],)
        
        try:
            os.link(self.fcd_file,my_temp_file)
        except OSError,e:
            print >>sys.stderr,"Cannot create link from frame.cache file, %s to temp file, %s: %s" % (self.fcd_file,my_temp_file,str(e))
            return
        
        fcd = open(my_temp_file)
        
        mylines = []
        mylines.extend(fcd.readlines())
        
        fcd.close()
        os.unlink(my_temp_file)
        
        # Do some initialization here, while waiting for the query to return
        dirpat = re.compile('^%s' % main_path)
        # example 
        # /archive/frames/A4/L0/LHO/H-R-7983,H,R,1,32 1114485254 3105 {798300000 798328448 798329088 798400000}
        diskcachepat = re.compile('([/\w\-\.]+),([A-Z]+),([\w]+),\d+,(\d+)\s\d+\s\d+\s{([\d\s]+)}')
        #                               0           1       2           3  
        
        # It's aparsin' time
        for line in mylines:
            if diskcachepat.search(line):
                templist = diskcachepat.search(line).groups()
                if dirpat.search(templist[0]):
                    tempstart = templist[4].split()[0::2]
                    tempstop = templist[4].split()[1::2]
                    
                    pythy_problem1 = []
                    pythy_problem2 = []
                    
                    i = 0
                    while i < len(tempstart):
                        
                        a = long(tempstart[i])
                        b = long(tempstop[i])
                        
                        # Python lacks explicit storage management
                        # as far as I know. 
                        pythy_problem1.append(a)
                        pythy_problem2.append(b)
                        
                        x += segments.segmentlist([segments.segment(a,b)])
                        i = i + 1
                    # end while
                    
                    duration = long(templist[3])
                    dclist.append((tuple(pythy_problem1),tuple(pythy_problem2),(templist[0],templist[1],templist[2],duration)))
                    del pythy_problem1
                    del pythy_problem2
                # fi
            # fi
        # done
    
        # Makes a union of all segments in x
        x.coalesce()
        
        ## Now mask this segment list
        
        # Create mask
        mask = segments.segmentlist([])
        gpsnow = long(gpstime.GpsSecondsFromPyUTC(time.time()))
        truncated_start = gpsnow - self.max_lookback
        lag = gpsnow - self.min_lookback
        
        lower_seg = segments.segment(0,truncated_start)
        upper_seg = segments.segment(lag,segments.infinity)
        mask.append(lower_seg)
        mask.append(upper_seg)
        
        # Apply mask
        x -= mask
        
    ## END GetSegList()
                        
    def genfiles_diskcache(self,a,b,dcline,gpslist,gps_to_path):
        """
        Returns a list of gps times, and a dictionary
        of lists which map gps time to a list of full paths
        which have files which start at the dictionary's key..
        
        This method assumes that the segment exists. Thus, at the moment,
        if a,b is a non-existant segment, or one that doesn't apply to dcline,
        this could return a bogus list.
        
        Also, "a" must be aligned to the segments in the line. In other words,
        the local boundary defined by the dcline. Otherwise a bogus filelist could
        be generated
        
        """
        filelist = []
        
        gpsstart = a
        fname_prefix = "%s-%s-" % (dcline[1],dcline[2])
        path_prefix = "%s/%s" % (dcline[0],fname_prefix)
        
        duration = dcline[3]
        extension = "gwf"
        
        count = len(self.urlType)
        while gpsstart < b :
            full_path = "%s%s-%s.%s" % (path_prefix,gpsstart,duration,extension)
            
            if not gps_to_path.has_key(gpsstart):
                gps_to_path[gpsstart] = []
                gpslist.append(gpsstart)
            gps_to_path[gpsstart].append(full_path)
            gpsstart += duration
        
    ## END genfile_diskcache
    
    
    def build_filelist_diskcache(self,x):
        """
        Returns list of fully qualified paths to
        files which exist in segment list, x, according
        to dclist.
        
        Where dclist is a parsed version of what comes
        from diskcacheAPI.
        """
        filelist = []
        gpslist = []
        gps_to_path = {}
      
        for seg in x:
            for T,P,line in self.dclist:
                i = 0
                duration = line[3]
                while i < len(T):
                    if seg[0] <= P[i]:
                        if  seg[1] > T[i]:
                            N = ( P[i] - seg[0] - 1 ) / duration # the -1 means match on half-open interval
                            Ta = P[i] - (N + 1)*duration
                            maxTa = max(Ta,T[i])

                            N = ( seg[1] - T[i] - 1 ) / duration # the -1 means match on half-open interval
                            Pb = T[i] + (N + 1)*duration
                            minPb = min(Pb,P[i])

                            self.genfiles_diskcache(maxTa,minPb,line,gpslist,gps_to_path)
                            
                            if minPb == seg[1]:
                                break
                    i = i + 1
        # list of files should be in GPS order
        gpslist.sort()
        filelist = []
     
        for gpstime in gpslist:
            for filename in gps_to_path[gpstime]:
               filelist.append(filename)
        del gps_to_path
        del gpslist
        return filelist
    ## END build_filelist_diskcache() 
    
    def cleanup_previous_state(self,main_path,files,urlType):
        """
        Gets initial diskcacheAPI query, converts file paths to URLs
        then 
        """
        filepat = re.compile(r'^[A-Za-z]+-\w+\-(\d+)\-(\d+)\.gwf')
        
        segstodo = segments.segmentlist([])
        
        # firstsegs is just a list of segments
        # it needs dclist to actually generate the
        # paths again.
        self.dcsegs = segments.segmentlist([])
        self.dclist = []
        
        # Make list of *all* segments that correspond to
        # main_path than can be obtained from diskCacheAPI
        self.MakeFirstSegList(self.dcsegs,self.dclist,main_path)
        # now get list of paths
        flist = self.build_filelist_diskcache(self.dcsegs)
        
        # make a list of LFNs out of this...
        # All this lambda stuff is for the case that
        # flist is *VERY* big
        i = 0
        j = self.query_size
        f = lambda x: x - len(flist) < 0 and x or len(flist)
        while i < len(flist):
            littlelist = flist[i:f(j)]
            lfns = []
            lfndict = {}
            pathdict = {}
            
            # Create necessary lfn list and lfndict
            for full_path in littlelist:
                name = os.path.basename(full_path)
                t = filepat.search(name)
                if t:
                    s = t.groups()
                    lfndict[name] = (long(s[0]), long(s[0]) + long(s[1]))
                    pathdict[name] = full_path
                    lfns.append(name)
            
            # Query RLS database
            try:
                mapdict = files.rls.lrc_get_pfn_bulk(lfns)
            except Exception,e:
                msg = "Caught exception while running files.rls.lrc_get_pfn_wc(\"*%s*\"): %s" % (main_path,str(e))
                print msg
                print "Retrying!"
                sys.stdout.flush()
                sys.stderr.flush()
                try:
                    mapdict = files.rls.lrc_get_pfn_bulk(lfns)
                except:
                    msg = "Caught another exception running files.rls.lrc_get_pfn_wc(\"*%s*\") again: %s" % (main_path,str(e))
                    print msg
                    print "Exiting!"
                    sys.stderr.flush()
                    sys.stdout.flush()
                sys.exit(255)
                
            for name in lfns:
                pfns = mapdict[name]
                # no entry exists at all?
                if not pfns:
                    segstodo.append(segments.segment(lfndict[name][0],lfndict[name][1]))
                else:
                    #by default assume file needs to be added,
                    # addcount reflects the number of pfns
                    # which have *already* been published!
                    addcount = 0
                    lfn, templist = create_lfn_and_pfns(pathdict[name],urlType)
                    if isinstance(pfns,StringType):
                        if pfns == templist:
                            addcount = addcount + 1
                    else:   # assume it's a list
                        for temp in templist:
                            if pfns.count(temp):
                                addcount = addcount + 1
                    # If even one PFN is not published, 
                    # try to publish whole thing anyway
                    # the RLS API handles insertion of identical tags well
                    # ... for now ...
                    if addcount < len(templist):
                        segstodo.append(segments.segment(lfndict[name][0],lfndict[name][1]))
                    else:
                        self.main_seglist.append(segments.segment(lfndict[name][0],lfndict[name][1]))
            # done
            
            # Coalescing is expensive, but if self.query_size is large,
            # this shouldn't be too much of an impact.
            segstodo.coalesce()
            self.main_seglist.coalesce()
            
            del lfns
            del lfndict
            del pathdict
            del mapdict
            i = f(j)
            j = i + self.query_size
        #
        # end while
        #del flist
        
        # now get list of files that need to be published
        # filelist will be in order if segstodo is in order
        filelist = self.build_filelist_diskcache(segstodo)
        
        return filelist
   ## END cleanup_previous_state() 
    def Get_New_File_List(self,main_path):
        """
        Gets list of segments (+ additional info) 
        in the frame cache, compares 
        
        """
        # coalesce self.main_seglist
        # whenever it increases by self.limit_main_seglist
        # this is done to keep the list reasonable in length
        # while keeping the expensive segmentlist.coalesce()
        # call at a reasonable rate
        if len(self.main_seglist) - self.old_len_main_seglist > self.limit_main_seglist:
            self.main_seglist.coalesce()
            
        self.dcsegs = segments.segmentlist([])
        self.dclist = []
        
        # Get segments that are currently on the filesystem
        self.GetSegList(self.dcsegs,self.dclist,main_path)
        
        # now the magic
        # find out which segments are not in self.main_seglist
        residue = self.dcsegs - self.main_seglist
        
        # Guarantee residue is in proper order, necessary????
        residue.coalesce()
        
        filelist = self.build_filelist_diskcache(residue)
        
        return filelist
        
## END SegData(object)



class FileListData(object):
    """
    class for retrieving and processing
    file lists, from a single file.
    
    It "tails" the file.
    
    """
    
    def __init__(self,filename):
        """
        Initializes FileListData class.
        
        """
        try:
            self.fd = open(filename,'r')
        except Exception,e:
            msg = "Could not open file, %s. Reason was: %s" % (main_path,)
            raise LSCfileAddException,e
            
    
    def Set_Wait_Time(self,wait_time):
        """
        Sets how far behind "now" to truncate gps
        time for FrameCacheQuery
        
        wait_time should be given in seconds
        """
        self.wait_time = wait_time
    ## END Set_Wait_Time(self,wait_time)
 
    def cleanup_previous_state(self,main_path,files,urlType):
        pass
            
    ## END  cleanup_previous_state(main_path,files,urlType)
    
      
    def Get_New_File_List(self,main_path):
        """
        Creates a python list of 
        
        """
        filelist = []
        
        filepat = re.compile(r'[A-Za-z]+\-\w+\-\d+\-\d+\.gwf$')
        
        for line in self.fd.readlines():
            line = line.strip()
            if filepat.search(line):
                filelist.append(line)
        
        return filelist
    ## END Get_New_File_List()          
## END FileListData()


def Gen_Science_Attribs(filelist,urlType,runTag,files):
        """
        Generates a datalist structure with most
        of the LIGO science data attributes filled.
        
        This also generates the PFNs according to
        what is in urlType.
        
        The following attributes are filled by this routine.
        runTag
        site
        frameType
        gpsStart
        gpsEnd
        duration
        size
        locked
        scienceMode
        injection
        md5
        """
        
        datalist = [] # to be filled with dictionaries
        if not len(filelist):
                return datalist
        
        ## Create subprocs for md5 sums,
        ## these will be backgrounded while everything
        ## else is calculated
        
      
        
        # Also remotely execute segment generation
        SVcmd = "%s " % (CONFIG['STATEVECTOR_COMMAND'],)
        
        # This is before file existance check, but
        # the md5sum utility can handle it. Results
        # will be checked against filelist anyway.
        count = 0
        tomd5 = ""
        toSV = ""
        
        count = 0
        overwrite_md = bool(CONFIG['OVERWRITE_METADATA'])
        publish_urls = bool(CONFIG['PUBLISH_URLS'])
        publish_segments = bool(CONFIG['PUBLISH_SEGMENTS'])
        publish_LDR_segments = bool(CONFIG['PUBLISH_LDR_SEGMENTS'])
        for fullpath in filelist:
                count += 1

                if publish_segments:
                        toSV += " %s" % (fullpath,)
                
        sys.stdout.flush()
        
        # Now start up md5sum and seggen processes, 
        #  if there are any files to run on.
        if count:
             
                
                if toSV:
                        SV_gen = popen2.Popen3(SVcmd)
                        SV_gen.tochild.writelines(toSV)
                        SV_gen.tochild.flush()
                        SV_gen.tochild.close()
        else:
                return datalist
        
        # Now create a datalist entry for each file though
        pub_coinc = bool(CONFIG['CALCULATE_COINCIDENCE'])
        pub_playg = bool(CONFIG['CALCULATE_PLAYGROUND'])
        for fullpath in filelist:
                data = {}
                # see if the physical file exists?
                fullpath = os.path.abspath(fullpath)
             
                # extract lfn and generate pfns
                lfn = os.path.basename(fullpath)
          
                # now try to set the attributes from the filename
                try:
                        attribs = set_attribs_from_filename(fullpath,lfn)
                except LSCfileAddException, e:
                        msg = "Could not generate attributes for file \"%s\".  \
                        Error was: %s" % (fullpath,str(e))
                        print >>sys.stderr,  "%s Skipping." % msg
                        continue
                data["name"] = lfn
                data["urls"] = ""
                data["pset"] = CONFIG['PSET']
                # add this even if metadata will never be published (due to existance)
                # it's so lightweight it doesn't matter
                data["metadata"] = attribs
                data["metadata"]["runTag"] = {}
                data["metadata"]["runTag"]["Value"] = str(runTag)
               
                
                datalist.append(data)
        # end for fullpath in filelist
        
        # Now wait for state vector seg gen process to finish
        SV_lines = []
        if toSV:
                try:
                        SV_lines.extend(SV_gen.fromchild.readlines())
                        SV_gen.wait()
                        SV_lines.extend(SV_gen.fromchild.readlines())
                except Exception:
                        msg = "Caught exception waiting for State Vector segment generation process to finish XOR reading output from process. Command was: %s" % (SVcmd,)
                
        # Now parse output and put into segresult and ifodict dictionaries
        segresult = {}
        ifodict = {}
        
        segresult, ifodict = Describe_Segments(SV_lines)
        if bool(toSV):
                #print "SEGRESULT: %s " % (segresult,)
                for path in toSV.split():
                        lfn = os.path.basename(path)
                        if not segresult.has_key(lfn):
                                msg = "Error. Did not find State Vector segment result for lfn \"%s\"" % (lfn,)
                                SV_gen.wait()
                               
                                raise LSCfileAddException,msg
                        elif not ifodict.has_key(lfn):
                                msg = "Error. Did not find ifo list for lfn \"%s\"" % (lfn,)
                                SV_gen.wait()
                               
                                raise LSCfileAddException,msg
        # end SV sanity check
        
        
        
                
        # now insert segment and md5 results into datalist
        # This loop effectively brings the results of the
        #  extrnal programs into the datalist
        for i in range(0,len(datalist)):
                lfn = datalist[i]["name"]
              
                # collect remaining LDR attributes, these are derived from the segment
                # generation programs
                datalist[i]["seginfo"] = {}
                # script currently doesn't handle these separately so they must be TRUE!!!!
                if bool(toSV):
                        # for publishing into segment database
                        datalist[i]["seginfo"] = segresult[lfn]
                       
               
        ## END for i in range(0,len(datalist))

        return datalist
## END Gen_Science_Attribs(...)



def set_attribs_from_filename(fullpath,lfn):
    """
    Sets the following metadata attributes from the given filename.
    (filename must be an lfn)
    site
    frameType
    gpsStart
    gpsEnd
    duration
    size
    """
    attribs = {}
    filepat = re.compile(r'^([A-Za-z]+)\-(\w+)\-(\d+)\-(\d+)\..+')
    try:
        parsedfilename = filepat.search(lfn).groups()
    except Exception, e:
        msg = "Invalid filename format \"%s\"" % (lfn,)
        raise LSCfileAddException

    # Must have 4 parts to name field
    if len(parsedfilename) is not 4:
        msg = "Invalid filename format \"%s\"" % (lfn,)
        raise LSCfileAddException

    # set the name fields in attribute dictionary
    attribs['site'] = {}
    attribs['frameType'] = {}
    attribs['gpsStart'] = {}
    attribs['duration'] = {}
    templist = list(parsedfilename[0])
    attribs['site']['Value'] = ",".join(templist)
    del templist
    attribs['frameType']['Value'] = parsedfilename[1]
    attribs['gpsStart']['Value'] = long(parsedfilename[2])
    attribs['duration']['Value'] = long(parsedfilename[3])
    
    # set end time
    starttime = long(attribs['gpsStart']['Value'])
    durtime = long(attribs['duration']['Value'])
    attribs['gpsEnd'] = {}
    attribs['gpsEnd']['Value'] = long(long(starttime) + long(durtime))
    
    
    return attribs
## END set_attribs_from_filename(fullpath,lfn)

def Read_Command_File():
    """
    Returns true if command file exists.
    
    This will be more sophisticated later.
    """
    if os.path.exists("sHuTdOwN"):
        os.remove("sHuTdOwN")
        return True
    else:
        return False
## END Read_Command_File(object)

def main_loop(main_path, urlType, runTag): 
    """
    Loops until told to stop.
    """ 
    
    try:
        CreatePIDFile()
    except LSCfileAddException,e:
        msg = "%s" % (str(e),)
        msg +="\nExiting!"
        print >>sys.stderr,msg
        sys.stdout.flush()
        sys.stderr.flush()
        sys.exit(14)
    
   
    ##### IF CLUSTER PUBLISING ######
    if CONFIG['SLASH_FRAMES_REALTIME']:
        ops = Cluster()
        ops.set_number_of_files(CONFIG['SLASH_FRAMES_FILECOUNT'])
    elif CONFIG['PUBLISH_GIVEN_FILE_LIST']:
        ops = FileListData(CONFIG['FILE_LIST'])
    else:
        # class for some science publishing operations
        #ops = ScienceData()
        #ops.Set_Start_Time(CONFIG['GPSSTART'])
        
        ops = SegData()
        ops.Set_Wait_Time(CONFIG['MINIMUM_LOOKBACK'])
        ops.Set_Start_Time(CONFIG['MAXIMUM_LOOKBACK'])
    
    # default sleep time
    sleep_time = CONFIG['SLEEP_TIME'] ;# seconds
    
    # Number of files to attempt to publish at
    # once
    file_loop_size = CONFIG['FILES_PER_LOOP']
   
    ops.urlType = urlType
  
    current_delay = 2
    max_delay = 128
    if CONFIG['PUBLISH_SEGMENTS']:
        print "Attempting to connect to segment database."
        try:
            sdb = statedb.StateSegmentDatabase(runTag,CONFIG['SEG_DBNAME'])
            print "Connected to segment database."
        except statedb.StateSegmentDatabaseException,e:
            msg = "main_loop(): Could not open connection to segment database. Error was: %s" % (str(e),)
            if current_delay <= max_delay:
                msg += "Retrying after sleeping for %d seconds." % (current_delay,)
                print >>sys.stderr, msg
                sys.stdout.flush()
                sys.stderr.flush()
                current_delay = current_delay << 1 # double current_delay
                time.sleep(current_delay)
            else:
                # Time out, raise an exception
                try:
                    sdb.close()
                except Exception,e:
                    pass
                raise LSCfileAddException,msg

    # Begin Main Publishing Loop Proper
    while True:

        # Get next list of files
        biglist = ops.Get_New_File_List(main_path)
        print "VEZ: %s len(biglist): %s" % (time.asctime(),len(biglist))
        
        # for postS3 and one-percent data
        #sub_segments = Get_Submitted_Segs(sub_seg_file)
        
        if not len(biglist):
            if Read_Command_File():
                print "Found shutdown file. Exiting."
                DelPIDFile()
                sys.stdout.flush()
                sys.stderr.flush()
                sys.exit(0)
            # fi
            print "%s: No new files found. Sleeping for %s seconds." % (time.asctime(),sleep_time,)
            sys.stdout.flush()
            sys.stderr.flush()
            time.sleep(sleep_time)
            
        else:
            # Only publish up to file_loop_size files at a time
            i = 0
            j = file_loop_size
            f = lambda x: x - len(biglist) < 0 and x or len(biglist)
            littlelist = biglist[i:f(j)]
            while len(littlelist):
                # Checks for existence of $PWD/sHuTdOwN
                if Read_Command_File():
                    print "Found shutdown file. Exiting."
                    DelPIDFile()
                    sys.stdout.flush()
                    sys.stderr.flush()
                    sys.exit(0)
                
                try:
                    HeartBeat()
                except LSCfileAddException,e:
                    msg = "Caught exception performing HeartBeat(). Continuing anyway. By the way, the error was: %s" % (str(e),)
                    print >>sys.stderr,msg
                    sys.stderr.flush()

                print sys.stdout.flush()
                try:
                    mydata = Gen_Science_Attribs(littlelist,urlType,runTag,files)
                except LSCfileAddException, e:
                    msg = "Caught exception while running, Gen_Lightweight_Science_Attribs(). Not publishing this list of files. Message was: %s" % (str(e),)
                    print >>sys.stderr, msg
                    sys.stderr.flush()
                    # get next chunk of biglist
                    i = f(j)
                    j = i + file_loop_size
                    littlelist = biglist[i:f(j)]
                    continue
                # The following branch actually does the
                # database transactions (a.k.a. (un)publication
                if not CONFIG['DRY_RUN']:
                    if CONFIG['SLASH_FRAMES_REALTIME']:
                        # These routines keep the number
                        # rls entries published by this script,
                        # less than cluster_ops.limit. Acts like a FIFO.
                        try:
                            ops.Add(mydata)
                            ops.Del(files)
                        except LSCfileAddException, e:
                            msg = "Caught exception while modifying cluster file state. Message was: %s" % (str(e),)
                            print >>sys.stderr, msg
                            sys.stderr.flush()
                            # get next chunk of biglist
                            i = f(j)
                            j = i + file_loop_size
                            littlelist = biglist[i:f(j)]
                            continue
                 
                    ### Publish into segment database
                    if CONFIG['PUBLISH_SEGMENTS']:
                        try:
                            Publish_Segments(mydata,runTag,sdb)
                        except LSCfileAddException, e:
                            msg = "Error during segment publishing: %s" % (str(e),)
                            print >>sys.stderr, msg
                            print >>sys.stderr, "EXITING!!!"
                            sys.stdout.flush()
                            sys.stderr.flush()
                            sys.exit(2)
                    else:
                        print "User did not request segment publishing (CONFIG['PUBLISH_SEGMENTS'] == False).\n"
                        print "Not publishing segments."
                ## END if not CONFIG['DRY_RUN']
                # Need to modify Publisher() class to return successes,
                # but until then, perform this,
                if CONFIG['SEGLIST_METHODS']:
                    # assume proper publication, and append to completed segment list
                    for data in mydata:
                        ops.main_seglist.append(segments.segment(data['metadata']['gpsStart']['Value'],data['metadata']['gpsEnd']['Value']))
                        #print "%s: %s %s" % (data["name"],data['metadata']['gpsStart']['Value'],data['metadata']['gpsEnd']['Value'])
                    ops.main_seglist.coalesce()
                    # done

                else:
                    print "SEGLIST_METHODS is %s" % (CONFIG['SEGLIST_METHODS'],)
                #fi
                
                # now print what was published
                for item in mydata:
                    print "Name: %s" % (item["name"],)
                    print "URLs: %s" % (item["urls"],)
                    print "PSET: %s" % (item["pset"],)
                    print "Metadata: %s" % (item["metadata"],)
                    print "Segments: %s" % (item["seginfo"],)
                    print ""
                print "%s" % (time.ctime(),)
                # force del of mydata
                if mydata:
                    del mydata
                #fi
                # flush stdout and stderr
                sys.stdout.flush()
                sys.stderr.flush()
                # now take next slice out of biglist
                i = f(j)
                j = i + file_loop_size
                littlelist = biglist[i:f(j)]
           ## END while len(littlelist)
    # END while True
## END main_loop(main_path, urlType, runtag)

def Publish_Segments(datalist,runTag,sdb):
    """
    Publishes segments
    """
    # for each file
    for item in datalist:
        name = item["name"]
        segments = item["seginfo"]
        fstart = item["metadata"]["gpsStart"]["Value"]
        fend = item["metadata"]["gpsEnd"]["Value"]
        lfn_exists = False
        try:
            sdb.register_lfn(name,fstart,fend)
        except statedb.StateSegmentDatabaseLFNExistsException,e:
            pass
        except statedb.StateSegmentDatabaseException,e:
            msg = "Publish_Segments(): Could not register lfn, %s. Error was: %s" % (str(e),)
            print >>sys.stderr,msg
            pass
        if not lfn_exists:
            print >> sys.stderr, "Publishing segments for %s attime %s" % (name,time.asctime())
            for seg in segments:
                try:
                        sdb.publish_state(seg[0],seg[1],seg[2],seg[3],seg[4],seg[5],seg[6],seg[7])
                except statedb.StateSegmentDatabaseSegmentExistsException,e:
                        msg = "Publish_Segments(): Segment already exists in database.  Segment tuple was: \"%s\" Error from statedb code was: %s" % (str(seg),str(e))
                        print >>sys.stderr, msg
                        # no need to raise an exception any higher up the food chain
                except statedb.StateSegmentDatabaseException,e:
                        msg = "Publish_Segments(): Could not publish state. Error was: %s" % (str(e),)
                        raise LSCfileAddException,msg
    # done
## END Publish_Segments()

def Describe_Segments(SV_lines):
    """
    Takes out put of State Vector Fetcher and Activity State Fetcher,
    and puts them into a dictionary, segresult, which is keyed on lfn.
    
    segresult[lfn] itself is a list of tuples,
    each tuple is formatted for the input of statedb.publish_state
    
    index           Description
    -----           -----------
    0               ifo (string)
    1               start (long)
    2               start_nano (long)
    3               end (long)
    4               end_nano (long)
    5               seg_version (long)
    6               state vector (long)
    7               segment number
    
    """
    
    sv_seg_version = CONFIG['SV_SEGMENT_VERSION']
    ligo_science_mode = CONFIG['SCIENCEMODE_BITS']
    injbits = CONFIG['INJECTION_BITS']
    
    # Both processes' output is identically formatted
    segpat = re.compile(r'^(\d+)\s(\d+)\s(\d+)\s(\d+)\s(\d+)\s(\d+)\s(\d+)\s([A-Z][0-9]+)\s(.+)')
    
    segresult = {}
    ifodict = {}
    
    # process state vector first
    for single_line in SV_lines:
        single_line = single_line.strip()
        if segpat.search(single_line):
            temp = segpat.search(single_line).groups()
            start = long(temp[0])
            start_nano = long(temp[1])
            end = long(temp[2])
            end_nano = long(temp[3])
            sv = long(temp[4])
            if sv == ligo_science_mode or sv == (ligo_science_mode & ~injbits): 
                segnum = long(temp[6])
            else:
                # Segment number only has meaning during science mode
                # or science mode + injection
                # so insert "NULL" for non-science mode segments.
                segnum = None
            ifo = str(temp[7])
            name = temp[8]
        else:
            # Output did not match regex, will be (partially) caught later when 
            # the results for a file name are not found in the segresult dictionary
            continue
        basename = os.path.basename(name)
    
        # for interferometer metadata later
        if not ifodict.has_key(basename):       
            ifodict[basename] = {}
        
        # make segresult[basename] list
        if segresult.has_key(basename):
            ifodict[basename][ifo] = 1
            segresult[basename].append(tuple([ifo,start,start_nano,end,end_nano,sv_seg_version,sv,segnum]))
        else:
            segresult[basename] = []
                
            ifodict[basename][ifo] = 1
            segresult[basename].append(tuple([ifo,start,start_nano,end,end_nano,sv_seg_version,sv,segnum]))
            
    return segresult,ifodict
## END Describe_Segments(SV_lines,ACT_lines)


if __name__ == '__main__':
    urlType = Process_Command_Line()
    main_path = CONFIG['MAIN_PATH']
    runTag = CONFIG['RUNTAG']
    main_loop(main_path, urlType, runTag)
    sys.exit(255) # never reached....
## END
